Beginning trial 1 of 1
Gathering sys log on lambda-quad
:::MLL 1573745847.656 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1573745847.657 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1573745847.658 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1573745847.659 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1573745847.660 submission_platform: {"value": "1xSystem Product Name", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1573745847.661 submission_entry: {"value": "{'hardware': 'System Product Name', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': ' ', 'os': 'Ubuntu 18.04.3 LTS / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.7-1.0.0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '1x Intel(R) Core(TM) i9-9820X CPU @ 3.30GHz', 'num_cores': '10', 'num_vcpus': '20', 'accelerator': 'GeForce RTX 2080 Ti', 'num_accelerators': '4', 'sys_mem_size': '125 GB', 'sys_storage_type': '<unknown bus> SSD', 'sys_storage_size': '2x 54.5M + 1x 1.8T + 2x 14.8M + 1x 156M + 2x 3.7M + 2x 140.7M + 1x 2.3M + 1x 34.6M + 1x 14.5M + 1x 956K + 1x 44.2M + 1x 4.2M + 2x 89.1M', 'cpu_accel_interconnect': 'UPI', 'network_card': '', 'num_network_cards': '0', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1573745847.662 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1573745847.662 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1573745848.713 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node lambda-quad
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=LambdaDual2080Ti -e 'MULTI_NODE= --master_port=4446' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191114073616144954374 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191114073616144954374 ./run_and_time.sh
Run vars: id 191114073616144954374 gpus 2 mparams  --master_port=4446
STARTING TIMING RUN AT 2019-11-14 03:37:29 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 1  --ncores_per_socket 10 --nproc_per_node 2  --master_port=4446'
+ echo 'running benchmark'
running benchmark
+ python -m bind_launch --nsockets_per_node 1 --ncores_per_socket 10 --nproc_per_node 2 --master_port=4446 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1573745851.031 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1573745851.050 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 3277654685
0: Worker 0 is using worker seed: 718922459
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1573745858.194 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1573745859.354 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1573745859.354 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1573745859.354 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1573745859.632 global_batch_size: {"value": 512, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1573745859.634 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1573745859.634 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1573745859.634 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1573745859.634 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1573745859.635 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1573745859.635 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1573745859.635 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1573745859.637 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1573745859.638 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 1916145710
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/7762]	Time 0.644 (0.644)	Data 4.05e-01 (4.05e-01)	Tok/s 15607 (15607)	Loss/tok 10.5885 (10.5885)	LR 2.000e-05
0: TRAIN [0][10/7762]	Time 0.161 (0.328)	Data 9.70e-05 (3.72e-02)	Tok/s 32677 (43359)	Loss/tok 9.7335 (10.2404)	LR 2.518e-05
0: TRAIN [0][20/7762]	Time 0.240 (0.315)	Data 9.66e-05 (1.95e-02)	Tok/s 42033 (45373)	Loss/tok 9.3016 (9.9229)	LR 3.170e-05
0: TRAIN [0][30/7762]	Time 0.241 (0.311)	Data 1.05e-04 (1.32e-02)	Tok/s 43356 (46021)	Loss/tok 9.0655 (9.7169)	LR 3.991e-05
0: TRAIN [0][40/7762]	Time 0.162 (0.312)	Data 1.02e-04 (1.00e-02)	Tok/s 31871 (46361)	Loss/tok 8.6623 (9.5539)	LR 5.024e-05
0: TRAIN [0][50/7762]	Time 0.330 (0.323)	Data 1.21e-04 (8.09e-03)	Tok/s 51054 (47225)	Loss/tok 8.6857 (9.3852)	LR 6.325e-05
0: TRAIN [0][60/7762]	Time 0.242 (0.313)	Data 1.16e-04 (6.78e-03)	Tok/s 42448 (46682)	Loss/tok 8.4274 (9.2735)	LR 7.962e-05
0: TRAIN [0][70/7762]	Time 0.242 (0.311)	Data 1.03e-04 (5.84e-03)	Tok/s 42234 (46538)	Loss/tok 8.1772 (9.1565)	LR 1.002e-04
0: TRAIN [0][80/7762]	Time 0.332 (0.315)	Data 1.03e-04 (5.13e-03)	Tok/s 50605 (46787)	Loss/tok 8.1804 (9.0330)	LR 1.262e-04
0: TRAIN [0][90/7762]	Time 0.527 (0.320)	Data 1.28e-04 (4.58e-03)	Tok/s 57369 (47185)	Loss/tok 8.2492 (8.9189)	LR 1.589e-04
0: TRAIN [0][100/7762]	Time 0.333 (0.317)	Data 1.07e-04 (4.14e-03)	Tok/s 49996 (47016)	Loss/tok 8.0306 (8.8401)	LR 2.000e-04
0: TRAIN [0][110/7762]	Time 0.420 (0.318)	Data 1.00e-04 (3.78e-03)	Tok/s 55381 (47171)	Loss/tok 8.0892 (8.7638)	LR 2.518e-04
0: TRAIN [0][120/7762]	Time 0.335 (0.315)	Data 1.03e-04 (3.47e-03)	Tok/s 50215 (47007)	Loss/tok 8.0069 (8.7045)	LR 3.170e-04
0: TRAIN [0][130/7762]	Time 0.247 (0.311)	Data 9.66e-05 (3.22e-03)	Tok/s 42235 (46661)	Loss/tok 7.7782 (8.6556)	LR 3.991e-04
0: TRAIN [0][140/7762]	Time 0.334 (0.310)	Data 1.03e-04 (3.00e-03)	Tok/s 50760 (46694)	Loss/tok 7.9587 (8.6055)	LR 5.024e-04
0: TRAIN [0][150/7762]	Time 0.248 (0.312)	Data 1.16e-04 (2.80e-03)	Tok/s 40973 (46812)	Loss/tok 7.6722 (8.5525)	LR 6.325e-04
0: TRAIN [0][160/7762]	Time 0.332 (0.310)	Data 1.24e-04 (2.64e-03)	Tok/s 50156 (46735)	Loss/tok 7.8309 (8.5068)	LR 7.962e-04
0: TRAIN [0][170/7762]	Time 0.332 (0.310)	Data 9.89e-05 (2.49e-03)	Tok/s 50714 (46731)	Loss/tok 7.6746 (8.4640)	LR 1.002e-03
0: TRAIN [0][180/7762]	Time 0.531 (0.309)	Data 1.03e-04 (2.36e-03)	Tok/s 55482 (46673)	Loss/tok 7.8133 (8.4180)	LR 1.262e-03
0: TRAIN [0][190/7762]	Time 0.338 (0.311)	Data 1.03e-04 (2.24e-03)	Tok/s 49679 (46835)	Loss/tok 7.3732 (8.3655)	LR 1.589e-03
0: TRAIN [0][200/7762]	Time 0.334 (0.311)	Data 1.06e-04 (2.13e-03)	Tok/s 49306 (46847)	Loss/tok 7.3140 (8.3162)	LR 2.000e-03
0: TRAIN [0][210/7762]	Time 0.249 (0.310)	Data 1.04e-04 (2.04e-03)	Tok/s 42533 (46797)	Loss/tok 7.0797 (8.2687)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][220/7762]	Time 0.165 (0.308)	Data 1.03e-04 (1.95e-03)	Tok/s 31112 (46681)	Loss/tok 6.3444 (8.2253)	LR 2.000e-03
0: TRAIN [0][230/7762]	Time 0.166 (0.307)	Data 1.05e-04 (1.87e-03)	Tok/s 31639 (46497)	Loss/tok 6.2845 (8.1818)	LR 2.000e-03
0: TRAIN [0][240/7762]	Time 0.248 (0.306)	Data 1.04e-04 (1.80e-03)	Tok/s 41868 (46466)	Loss/tok 6.6066 (8.1331)	LR 2.000e-03
0: TRAIN [0][250/7762]	Time 0.168 (0.306)	Data 1.26e-04 (1.73e-03)	Tok/s 31143 (46451)	Loss/tok 5.9965 (8.0814)	LR 2.000e-03
0: TRAIN [0][260/7762]	Time 0.424 (0.306)	Data 1.23e-04 (1.67e-03)	Tok/s 54709 (46460)	Loss/tok 6.9030 (8.0289)	LR 2.000e-03
0: TRAIN [0][270/7762]	Time 0.538 (0.307)	Data 9.97e-05 (1.61e-03)	Tok/s 54519 (46437)	Loss/tok 6.9039 (7.9778)	LR 2.000e-03
0: TRAIN [0][280/7762]	Time 0.251 (0.306)	Data 1.27e-04 (1.56e-03)	Tok/s 41043 (46317)	Loss/tok 6.3409 (7.9327)	LR 2.000e-03
0: TRAIN [0][290/7762]	Time 0.252 (0.306)	Data 1.04e-04 (1.51e-03)	Tok/s 40766 (46326)	Loss/tok 6.3096 (7.8808)	LR 2.000e-03
0: TRAIN [0][300/7762]	Time 0.251 (0.308)	Data 1.13e-04 (1.46e-03)	Tok/s 41558 (46395)	Loss/tok 6.0606 (7.8279)	LR 2.000e-03
0: TRAIN [0][310/7762]	Time 0.341 (0.311)	Data 1.05e-04 (1.42e-03)	Tok/s 48944 (46546)	Loss/tok 6.3615 (7.7657)	LR 2.000e-03
0: TRAIN [0][320/7762]	Time 0.539 (0.313)	Data 1.00e-04 (1.38e-03)	Tok/s 55156 (46616)	Loss/tok 6.4184 (7.7091)	LR 2.000e-03
0: TRAIN [0][330/7762]	Time 0.251 (0.313)	Data 1.01e-04 (1.34e-03)	Tok/s 41301 (46572)	Loss/tok 5.8942 (7.6649)	LR 2.000e-03
0: TRAIN [0][340/7762]	Time 0.435 (0.313)	Data 1.02e-04 (1.30e-03)	Tok/s 52995 (46564)	Loss/tok 6.2209 (7.6184)	LR 2.000e-03
0: TRAIN [0][350/7762]	Time 0.254 (0.314)	Data 1.03e-04 (1.27e-03)	Tok/s 40960 (46543)	Loss/tok 5.8555 (7.5737)	LR 2.000e-03
0: TRAIN [0][360/7762]	Time 0.166 (0.313)	Data 1.16e-04 (1.24e-03)	Tok/s 32296 (46450)	Loss/tok 4.9797 (7.5352)	LR 2.000e-03
0: TRAIN [0][370/7762]	Time 0.256 (0.313)	Data 1.19e-04 (1.20e-03)	Tok/s 40036 (46437)	Loss/tok 5.7306 (7.4940)	LR 2.000e-03
0: TRAIN [0][380/7762]	Time 0.171 (0.312)	Data 1.01e-04 (1.18e-03)	Tok/s 30808 (46248)	Loss/tok 4.7980 (7.4608)	LR 2.000e-03
0: TRAIN [0][390/7762]	Time 0.441 (0.312)	Data 1.03e-04 (1.15e-03)	Tok/s 53007 (46239)	Loss/tok 6.0601 (7.4197)	LR 2.000e-03
0: TRAIN [0][400/7762]	Time 0.169 (0.313)	Data 1.05e-04 (1.12e-03)	Tok/s 31374 (46184)	Loss/tok 4.6821 (7.3783)	LR 2.000e-03
0: TRAIN [0][410/7762]	Time 0.254 (0.313)	Data 1.04e-04 (1.10e-03)	Tok/s 41519 (46143)	Loss/tok 5.4490 (7.3396)	LR 2.000e-03
0: TRAIN [0][420/7762]	Time 0.254 (0.313)	Data 1.03e-04 (1.07e-03)	Tok/s 39922 (46092)	Loss/tok 5.3600 (7.3033)	LR 2.000e-03
0: TRAIN [0][430/7762]	Time 0.255 (0.313)	Data 1.03e-04 (1.05e-03)	Tok/s 40130 (46066)	Loss/tok 5.4580 (7.2648)	LR 2.000e-03
0: TRAIN [0][440/7762]	Time 0.348 (0.314)	Data 1.03e-04 (1.03e-03)	Tok/s 48788 (46054)	Loss/tok 5.5957 (7.2272)	LR 2.000e-03
0: TRAIN [0][450/7762]	Time 0.351 (0.313)	Data 1.09e-04 (1.01e-03)	Tok/s 47472 (45979)	Loss/tok 5.6126 (7.1940)	LR 2.000e-03
0: TRAIN [0][460/7762]	Time 0.433 (0.313)	Data 1.05e-04 (9.90e-04)	Tok/s 54677 (45909)	Loss/tok 5.8282 (7.1610)	LR 2.000e-03
0: TRAIN [0][470/7762]	Time 0.564 (0.314)	Data 1.13e-04 (9.71e-04)	Tok/s 52395 (45940)	Loss/tok 5.8458 (7.1202)	LR 2.000e-03
0: TRAIN [0][480/7762]	Time 0.350 (0.314)	Data 1.25e-04 (9.53e-04)	Tok/s 47559 (45861)	Loss/tok 5.4606 (7.0895)	LR 2.000e-03
0: TRAIN [0][490/7762]	Time 0.253 (0.313)	Data 1.06e-04 (9.36e-04)	Tok/s 40772 (45796)	Loss/tok 5.0496 (7.0575)	LR 2.000e-03
0: TRAIN [0][500/7762]	Time 0.355 (0.314)	Data 1.22e-04 (9.20e-04)	Tok/s 47136 (45771)	Loss/tok 5.2667 (7.0235)	LR 2.000e-03
0: TRAIN [0][510/7762]	Time 0.445 (0.314)	Data 1.07e-04 (9.04e-04)	Tok/s 52380 (45776)	Loss/tok 5.5975 (6.9881)	LR 2.000e-03
0: TRAIN [0][520/7762]	Time 0.174 (0.313)	Data 1.00e-04 (8.88e-04)	Tok/s 30302 (45667)	Loss/tok 4.1174 (6.9600)	LR 2.000e-03
0: TRAIN [0][530/7762]	Time 0.175 (0.314)	Data 1.05e-04 (8.74e-04)	Tok/s 29573 (45642)	Loss/tok 3.9315 (6.9269)	LR 2.000e-03
0: TRAIN [0][540/7762]	Time 0.250 (0.313)	Data 1.02e-04 (8.59e-04)	Tok/s 41311 (45561)	Loss/tok 4.6975 (6.8998)	LR 2.000e-03
0: TRAIN [0][550/7762]	Time 0.451 (0.313)	Data 1.01e-04 (8.46e-04)	Tok/s 52263 (45531)	Loss/tok 5.4461 (6.8683)	LR 2.000e-03
0: TRAIN [0][560/7762]	Time 0.453 (0.313)	Data 1.04e-04 (8.33e-04)	Tok/s 51817 (45534)	Loss/tok 5.2661 (6.8349)	LR 2.000e-03
0: TRAIN [0][570/7762]	Time 0.460 (0.315)	Data 1.06e-04 (8.20e-04)	Tok/s 50635 (45576)	Loss/tok 5.2132 (6.7987)	LR 2.000e-03
0: TRAIN [0][580/7762]	Time 0.449 (0.315)	Data 1.02e-04 (8.08e-04)	Tok/s 52387 (45555)	Loss/tok 5.2914 (6.7690)	LR 2.000e-03
0: TRAIN [0][590/7762]	Time 0.254 (0.314)	Data 1.04e-04 (7.96e-04)	Tok/s 40747 (45501)	Loss/tok 4.7793 (6.7417)	LR 2.000e-03
0: TRAIN [0][600/7762]	Time 0.176 (0.314)	Data 1.36e-04 (7.84e-04)	Tok/s 29426 (45432)	Loss/tok 3.9501 (6.7142)	LR 2.000e-03
0: TRAIN [0][610/7762]	Time 0.353 (0.315)	Data 1.00e-04 (7.73e-04)	Tok/s 47559 (45433)	Loss/tok 4.9185 (6.6823)	LR 2.000e-03
0: TRAIN [0][620/7762]	Time 0.446 (0.316)	Data 1.03e-04 (7.62e-04)	Tok/s 52822 (45452)	Loss/tok 5.0934 (6.6504)	LR 2.000e-03
0: TRAIN [0][630/7762]	Time 0.578 (0.317)	Data 1.20e-04 (7.52e-04)	Tok/s 51263 (45480)	Loss/tok 5.2622 (6.6151)	LR 2.000e-03
0: TRAIN [0][640/7762]	Time 0.366 (0.317)	Data 1.03e-04 (7.42e-04)	Tok/s 46015 (45465)	Loss/tok 4.8524 (6.5881)	LR 2.000e-03
0: TRAIN [0][650/7762]	Time 0.260 (0.317)	Data 1.01e-04 (7.32e-04)	Tok/s 39142 (45375)	Loss/tok 4.4886 (6.5655)	LR 2.000e-03
0: TRAIN [0][660/7762]	Time 0.258 (0.317)	Data 1.21e-04 (7.23e-04)	Tok/s 40297 (45390)	Loss/tok 4.3185 (6.5353)	LR 2.000e-03
0: TRAIN [0][670/7762]	Time 0.261 (0.317)	Data 1.02e-04 (7.13e-04)	Tok/s 39431 (45380)	Loss/tok 4.5539 (6.5085)	LR 2.000e-03
0: TRAIN [0][680/7762]	Time 0.362 (0.317)	Data 1.00e-04 (7.04e-04)	Tok/s 46168 (45313)	Loss/tok 4.7792 (6.4860)	LR 2.000e-03
0: TRAIN [0][690/7762]	Time 0.362 (0.317)	Data 1.08e-04 (6.96e-04)	Tok/s 46919 (45269)	Loss/tok 4.6817 (6.4617)	LR 2.000e-03
0: TRAIN [0][700/7762]	Time 0.354 (0.317)	Data 1.07e-04 (6.87e-04)	Tok/s 47617 (45253)	Loss/tok 4.7295 (6.4363)	LR 2.000e-03
0: TRAIN [0][710/7762]	Time 0.172 (0.316)	Data 1.09e-04 (6.79e-04)	Tok/s 29426 (45164)	Loss/tok 3.7199 (6.4165)	LR 2.000e-03
0: TRAIN [0][720/7762]	Time 0.344 (0.316)	Data 1.08e-04 (6.71e-04)	Tok/s 48783 (45120)	Loss/tok 4.7292 (6.3943)	LR 2.000e-03
0: TRAIN [0][730/7762]	Time 0.364 (0.316)	Data 1.21e-04 (6.64e-04)	Tok/s 46573 (45081)	Loss/tok 4.4684 (6.3718)	LR 2.000e-03
0: TRAIN [0][740/7762]	Time 0.259 (0.316)	Data 2.43e-04 (6.56e-04)	Tok/s 40006 (45063)	Loss/tok 4.3027 (6.3483)	LR 2.000e-03
0: TRAIN [0][750/7762]	Time 0.264 (0.316)	Data 1.04e-04 (6.49e-04)	Tok/s 39882 (45002)	Loss/tok 4.2929 (6.3275)	LR 2.000e-03
0: TRAIN [0][760/7762]	Time 0.255 (0.316)	Data 1.16e-04 (6.42e-04)	Tok/s 40879 (44976)	Loss/tok 4.2253 (6.3056)	LR 2.000e-03
0: TRAIN [0][770/7762]	Time 0.341 (0.316)	Data 9.89e-05 (6.35e-04)	Tok/s 48921 (44954)	Loss/tok 4.6637 (6.2835)	LR 2.000e-03
0: TRAIN [0][780/7762]	Time 0.266 (0.316)	Data 1.01e-04 (6.28e-04)	Tok/s 38465 (44911)	Loss/tok 4.2946 (6.2637)	LR 2.000e-03
0: TRAIN [0][790/7762]	Time 0.462 (0.316)	Data 1.05e-04 (6.21e-04)	Tok/s 50456 (44891)	Loss/tok 4.6889 (6.2419)	LR 2.000e-03
0: TRAIN [0][800/7762]	Time 0.255 (0.315)	Data 9.92e-05 (6.15e-04)	Tok/s 41104 (44815)	Loss/tok 4.2733 (6.2243)	LR 2.000e-03
0: TRAIN [0][810/7762]	Time 0.172 (0.315)	Data 1.04e-04 (6.09e-04)	Tok/s 30946 (44779)	Loss/tok 3.6322 (6.2044)	LR 2.000e-03
0: TRAIN [0][820/7762]	Time 0.352 (0.315)	Data 1.06e-04 (6.03e-04)	Tok/s 47690 (44740)	Loss/tok 4.5748 (6.1862)	LR 2.000e-03
0: TRAIN [0][830/7762]	Time 0.263 (0.315)	Data 9.92e-05 (5.97e-04)	Tok/s 38849 (44690)	Loss/tok 4.1740 (6.1685)	LR 2.000e-03
0: TRAIN [0][840/7762]	Time 0.343 (0.315)	Data 1.18e-04 (5.91e-04)	Tok/s 48423 (44683)	Loss/tok 4.6208 (6.1483)	LR 2.000e-03
0: TRAIN [0][850/7762]	Time 0.267 (0.315)	Data 1.02e-04 (5.85e-04)	Tok/s 37662 (44658)	Loss/tok 4.1410 (6.1292)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][860/7762]	Time 0.458 (0.315)	Data 2.35e-04 (5.80e-04)	Tok/s 51788 (44632)	Loss/tok 4.7775 (6.1106)	LR 2.000e-03
0: TRAIN [0][870/7762]	Time 0.265 (0.315)	Data 1.05e-04 (5.74e-04)	Tok/s 38934 (44627)	Loss/tok 4.3162 (6.0906)	LR 2.000e-03
0: TRAIN [0][880/7762]	Time 0.355 (0.316)	Data 1.01e-04 (5.69e-04)	Tok/s 46602 (44629)	Loss/tok 4.4639 (6.0696)	LR 2.000e-03
0: TRAIN [0][890/7762]	Time 0.262 (0.316)	Data 1.08e-04 (5.64e-04)	Tok/s 38503 (44580)	Loss/tok 4.1628 (6.0532)	LR 2.000e-03
0: TRAIN [0][900/7762]	Time 0.259 (0.316)	Data 1.04e-04 (5.59e-04)	Tok/s 39933 (44568)	Loss/tok 4.1272 (6.0343)	LR 2.000e-03
0: TRAIN [0][910/7762]	Time 0.267 (0.316)	Data 1.01e-04 (5.54e-04)	Tok/s 38087 (44539)	Loss/tok 3.9159 (6.0172)	LR 2.000e-03
0: TRAIN [0][920/7762]	Time 0.254 (0.316)	Data 1.03e-04 (5.49e-04)	Tok/s 40831 (44500)	Loss/tok 4.0708 (6.0009)	LR 2.000e-03
0: TRAIN [0][930/7762]	Time 0.574 (0.316)	Data 1.50e-04 (5.44e-04)	Tok/s 51973 (44451)	Loss/tok 4.8240 (5.9849)	LR 2.000e-03
0: TRAIN [0][940/7762]	Time 0.366 (0.316)	Data 9.56e-05 (5.40e-04)	Tok/s 46211 (44432)	Loss/tok 4.3345 (5.9680)	LR 2.000e-03
0: TRAIN [0][950/7762]	Time 0.461 (0.316)	Data 1.07e-04 (5.35e-04)	Tok/s 50675 (44411)	Loss/tok 4.6933 (5.9514)	LR 2.000e-03
0: TRAIN [0][960/7762]	Time 0.265 (0.316)	Data 1.19e-04 (5.31e-04)	Tok/s 38799 (44390)	Loss/tok 3.9814 (5.9351)	LR 2.000e-03
0: TRAIN [0][970/7762]	Time 0.176 (0.316)	Data 1.09e-04 (5.26e-04)	Tok/s 29814 (44388)	Loss/tok 3.3624 (5.9176)	LR 2.000e-03
0: TRAIN [0][980/7762]	Time 0.265 (0.317)	Data 1.04e-04 (5.22e-04)	Tok/s 38744 (44381)	Loss/tok 4.0259 (5.9017)	LR 2.000e-03
0: TRAIN [0][990/7762]	Time 0.268 (0.317)	Data 1.07e-04 (5.18e-04)	Tok/s 38515 (44388)	Loss/tok 4.0853 (5.8846)	LR 2.000e-03
0: TRAIN [0][1000/7762]	Time 0.593 (0.317)	Data 1.02e-04 (5.14e-04)	Tok/s 50002 (44376)	Loss/tok 4.8342 (5.8684)	LR 2.000e-03
0: TRAIN [0][1010/7762]	Time 0.367 (0.317)	Data 1.04e-04 (5.10e-04)	Tok/s 45798 (44359)	Loss/tok 4.3003 (5.8531)	LR 2.000e-03
0: TRAIN [0][1020/7762]	Time 0.453 (0.318)	Data 1.06e-04 (5.06e-04)	Tok/s 52148 (44352)	Loss/tok 4.5981 (5.8377)	LR 2.000e-03
0: TRAIN [0][1030/7762]	Time 0.263 (0.318)	Data 1.05e-04 (5.02e-04)	Tok/s 39611 (44332)	Loss/tok 3.9288 (5.8230)	LR 2.000e-03
0: TRAIN [0][1040/7762]	Time 0.353 (0.318)	Data 1.05e-04 (4.98e-04)	Tok/s 48069 (44346)	Loss/tok 4.3701 (5.8065)	LR 2.000e-03
0: TRAIN [0][1050/7762]	Time 0.366 (0.318)	Data 1.08e-04 (4.94e-04)	Tok/s 45923 (44333)	Loss/tok 4.2543 (5.7920)	LR 2.000e-03
0: TRAIN [0][1060/7762]	Time 0.266 (0.318)	Data 1.09e-04 (4.91e-04)	Tok/s 38056 (44320)	Loss/tok 3.9477 (5.7774)	LR 2.000e-03
0: TRAIN [0][1070/7762]	Time 0.459 (0.319)	Data 1.09e-04 (4.87e-04)	Tok/s 51394 (44315)	Loss/tok 4.4756 (5.7631)	LR 2.000e-03
0: TRAIN [0][1080/7762]	Time 0.365 (0.319)	Data 1.08e-04 (4.84e-04)	Tok/s 45993 (44315)	Loss/tok 4.2280 (5.7483)	LR 2.000e-03
0: TRAIN [0][1090/7762]	Time 0.265 (0.319)	Data 1.08e-04 (4.80e-04)	Tok/s 39050 (44268)	Loss/tok 3.8946 (5.7368)	LR 2.000e-03
0: TRAIN [0][1100/7762]	Time 0.463 (0.319)	Data 1.10e-04 (4.77e-04)	Tok/s 50570 (44291)	Loss/tok 4.4086 (5.7206)	LR 2.000e-03
0: TRAIN [0][1110/7762]	Time 0.254 (0.320)	Data 1.21e-04 (4.73e-04)	Tok/s 40893 (44302)	Loss/tok 4.0115 (5.7057)	LR 2.000e-03
0: TRAIN [0][1120/7762]	Time 0.357 (0.319)	Data 9.97e-05 (4.70e-04)	Tok/s 47250 (44267)	Loss/tok 4.2701 (5.6941)	LR 2.000e-03
0: TRAIN [0][1130/7762]	Time 0.365 (0.319)	Data 1.02e-04 (4.67e-04)	Tok/s 46398 (44267)	Loss/tok 4.2449 (5.6810)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1140/7762]	Time 0.591 (0.319)	Data 1.03e-04 (4.64e-04)	Tok/s 50880 (44242)	Loss/tok 4.6298 (5.6687)	LR 2.000e-03
0: TRAIN [0][1150/7762]	Time 0.262 (0.319)	Data 1.07e-04 (4.61e-04)	Tok/s 38688 (44215)	Loss/tok 3.9420 (5.6578)	LR 2.000e-03
0: TRAIN [0][1160/7762]	Time 0.346 (0.319)	Data 1.59e-04 (4.58e-04)	Tok/s 48633 (44179)	Loss/tok 4.2241 (5.6466)	LR 2.000e-03
0: TRAIN [0][1170/7762]	Time 0.444 (0.319)	Data 1.06e-04 (4.55e-04)	Tok/s 53333 (44182)	Loss/tok 4.3973 (5.6335)	LR 2.000e-03
0: TRAIN [0][1180/7762]	Time 0.179 (0.319)	Data 1.21e-04 (4.52e-04)	Tok/s 29877 (44180)	Loss/tok 3.3238 (5.6207)	LR 2.000e-03
0: TRAIN [0][1190/7762]	Time 0.262 (0.319)	Data 1.16e-04 (4.49e-04)	Tok/s 39580 (44163)	Loss/tok 3.8187 (5.6089)	LR 2.000e-03
0: TRAIN [0][1200/7762]	Time 0.367 (0.319)	Data 1.17e-04 (4.46e-04)	Tok/s 45995 (44147)	Loss/tok 4.0096 (5.5972)	LR 2.000e-03
0: TRAIN [0][1210/7762]	Time 0.341 (0.320)	Data 9.70e-05 (4.43e-04)	Tok/s 48586 (44183)	Loss/tok 4.2594 (5.5828)	LR 2.000e-03
0: TRAIN [0][1220/7762]	Time 0.457 (0.320)	Data 1.05e-04 (4.40e-04)	Tok/s 51194 (44163)	Loss/tok 4.4234 (5.5717)	LR 2.000e-03
0: TRAIN [0][1230/7762]	Time 0.260 (0.320)	Data 1.04e-04 (4.38e-04)	Tok/s 39676 (44163)	Loss/tok 3.9063 (5.5596)	LR 2.000e-03
0: TRAIN [0][1240/7762]	Time 0.263 (0.320)	Data 1.04e-04 (4.35e-04)	Tok/s 39317 (44157)	Loss/tok 3.9819 (5.5485)	LR 2.000e-03
0: TRAIN [0][1250/7762]	Time 0.459 (0.320)	Data 1.07e-04 (4.32e-04)	Tok/s 50746 (44155)	Loss/tok 4.3349 (5.5372)	LR 2.000e-03
0: TRAIN [0][1260/7762]	Time 0.438 (0.321)	Data 1.01e-04 (4.30e-04)	Tok/s 53896 (44162)	Loss/tok 4.2826 (5.5252)	LR 2.000e-03
0: TRAIN [0][1270/7762]	Time 0.264 (0.321)	Data 1.05e-04 (4.27e-04)	Tok/s 38304 (44165)	Loss/tok 3.8622 (5.5140)	LR 2.000e-03
0: TRAIN [0][1280/7762]	Time 0.344 (0.320)	Data 1.00e-04 (4.25e-04)	Tok/s 48619 (44134)	Loss/tok 3.9812 (5.5044)	LR 2.000e-03
0: TRAIN [0][1290/7762]	Time 0.264 (0.320)	Data 1.02e-04 (4.22e-04)	Tok/s 37945 (44113)	Loss/tok 3.9744 (5.4944)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1300/7762]	Time 0.349 (0.320)	Data 1.02e-04 (4.20e-04)	Tok/s 48070 (44105)	Loss/tok 4.1625 (5.4842)	LR 2.000e-03
0: TRAIN [0][1310/7762]	Time 0.266 (0.320)	Data 1.41e-04 (4.18e-04)	Tok/s 39341 (44104)	Loss/tok 3.8656 (5.4735)	LR 2.000e-03
0: TRAIN [0][1320/7762]	Time 0.271 (0.320)	Data 1.04e-04 (4.15e-04)	Tok/s 38286 (44069)	Loss/tok 3.8033 (5.4645)	LR 2.000e-03
0: TRAIN [0][1330/7762]	Time 0.260 (0.320)	Data 1.03e-04 (4.13e-04)	Tok/s 38887 (44072)	Loss/tok 3.8549 (5.4540)	LR 2.000e-03
0: TRAIN [0][1340/7762]	Time 0.253 (0.320)	Data 1.21e-04 (4.11e-04)	Tok/s 40819 (44064)	Loss/tok 3.9103 (5.4445)	LR 2.000e-03
0: TRAIN [0][1350/7762]	Time 0.360 (0.321)	Data 9.82e-05 (4.08e-04)	Tok/s 47016 (44078)	Loss/tok 4.2837 (5.4334)	LR 2.000e-03
0: TRAIN [0][1360/7762]	Time 0.267 (0.321)	Data 2.32e-04 (4.06e-04)	Tok/s 38708 (44068)	Loss/tok 3.7894 (5.4239)	LR 2.000e-03
0: TRAIN [0][1370/7762]	Time 0.366 (0.321)	Data 1.06e-04 (4.04e-04)	Tok/s 46298 (44076)	Loss/tok 4.1301 (5.4132)	LR 2.000e-03
0: TRAIN [0][1380/7762]	Time 0.173 (0.321)	Data 1.03e-04 (4.02e-04)	Tok/s 30310 (44062)	Loss/tok 3.2385 (5.4038)	LR 2.000e-03
0: TRAIN [0][1390/7762]	Time 0.265 (0.321)	Data 1.06e-04 (4.00e-04)	Tok/s 38802 (44068)	Loss/tok 3.8757 (5.3933)	LR 2.000e-03
0: TRAIN [0][1400/7762]	Time 0.257 (0.321)	Data 1.03e-04 (3.98e-04)	Tok/s 40480 (44055)	Loss/tok 3.7947 (5.3842)	LR 2.000e-03
0: TRAIN [0][1410/7762]	Time 0.444 (0.321)	Data 1.01e-04 (3.95e-04)	Tok/s 52910 (44029)	Loss/tok 4.2842 (5.3761)	LR 2.000e-03
0: TRAIN [0][1420/7762]	Time 0.341 (0.321)	Data 1.07e-04 (3.93e-04)	Tok/s 49557 (44028)	Loss/tok 4.1553 (5.3669)	LR 2.000e-03
0: TRAIN [0][1430/7762]	Time 0.363 (0.321)	Data 1.01e-04 (3.91e-04)	Tok/s 46583 (44022)	Loss/tok 4.1945 (5.3580)	LR 2.000e-03
0: TRAIN [0][1440/7762]	Time 0.348 (0.321)	Data 1.03e-04 (3.89e-04)	Tok/s 48192 (44015)	Loss/tok 4.1479 (5.3490)	LR 2.000e-03
0: TRAIN [0][1450/7762]	Time 0.457 (0.321)	Data 1.03e-04 (3.87e-04)	Tok/s 50405 (44029)	Loss/tok 4.3502 (5.3393)	LR 2.000e-03
0: TRAIN [0][1460/7762]	Time 0.355 (0.321)	Data 1.06e-04 (3.85e-04)	Tok/s 47084 (44016)	Loss/tok 4.0786 (5.3308)	LR 2.000e-03
0: TRAIN [0][1470/7762]	Time 0.356 (0.322)	Data 1.08e-04 (3.84e-04)	Tok/s 46535 (44014)	Loss/tok 4.0075 (5.3219)	LR 2.000e-03
0: TRAIN [0][1480/7762]	Time 0.349 (0.321)	Data 1.04e-04 (3.82e-04)	Tok/s 48291 (43998)	Loss/tok 4.0647 (5.3138)	LR 2.000e-03
0: TRAIN [0][1490/7762]	Time 0.461 (0.321)	Data 1.05e-04 (3.80e-04)	Tok/s 50205 (43992)	Loss/tok 4.3343 (5.3055)	LR 2.000e-03
0: TRAIN [0][1500/7762]	Time 0.261 (0.321)	Data 1.02e-04 (3.78e-04)	Tok/s 39625 (44000)	Loss/tok 3.9543 (5.2965)	LR 2.000e-03
0: TRAIN [0][1510/7762]	Time 0.253 (0.321)	Data 1.10e-04 (3.76e-04)	Tok/s 40807 (43994)	Loss/tok 3.8465 (5.2884)	LR 2.000e-03
0: TRAIN [0][1520/7762]	Time 0.367 (0.322)	Data 1.03e-04 (3.75e-04)	Tok/s 45778 (43995)	Loss/tok 3.9595 (5.2797)	LR 2.000e-03
0: TRAIN [0][1530/7762]	Time 0.268 (0.321)	Data 1.07e-04 (3.73e-04)	Tok/s 39148 (43973)	Loss/tok 3.7091 (5.2723)	LR 2.000e-03
0: TRAIN [0][1540/7762]	Time 0.356 (0.321)	Data 1.02e-04 (3.71e-04)	Tok/s 47392 (43967)	Loss/tok 4.1393 (5.2647)	LR 2.000e-03
0: TRAIN [0][1550/7762]	Time 0.570 (0.322)	Data 1.10e-04 (3.69e-04)	Tok/s 51518 (43968)	Loss/tok 4.6212 (5.2569)	LR 2.000e-03
0: TRAIN [0][1560/7762]	Time 0.588 (0.322)	Data 1.05e-04 (3.68e-04)	Tok/s 49982 (43969)	Loss/tok 4.5588 (5.2490)	LR 2.000e-03
0: TRAIN [0][1570/7762]	Time 0.446 (0.322)	Data 1.08e-04 (3.66e-04)	Tok/s 51992 (43964)	Loss/tok 4.2388 (5.2412)	LR 2.000e-03
0: TRAIN [0][1580/7762]	Time 0.443 (0.322)	Data 1.03e-04 (3.64e-04)	Tok/s 52106 (43967)	Loss/tok 4.2500 (5.2332)	LR 2.000e-03
0: TRAIN [0][1590/7762]	Time 0.264 (0.322)	Data 1.04e-04 (3.63e-04)	Tok/s 39658 (43973)	Loss/tok 3.6036 (5.2252)	LR 2.000e-03
0: TRAIN [0][1600/7762]	Time 0.264 (0.322)	Data 1.01e-04 (3.61e-04)	Tok/s 38963 (43941)	Loss/tok 3.8840 (5.2186)	LR 2.000e-03
0: TRAIN [0][1610/7762]	Time 0.264 (0.321)	Data 1.03e-04 (3.60e-04)	Tok/s 38777 (43921)	Loss/tok 3.6602 (5.2117)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1620/7762]	Time 0.265 (0.322)	Data 1.02e-04 (3.58e-04)	Tok/s 38810 (43915)	Loss/tok 3.8171 (5.2042)	LR 2.000e-03
0: TRAIN [0][1630/7762]	Time 0.267 (0.322)	Data 1.04e-04 (3.56e-04)	Tok/s 38443 (43911)	Loss/tok 3.7028 (5.1967)	LR 2.000e-03
0: TRAIN [0][1640/7762]	Time 0.362 (0.322)	Data 1.06e-04 (3.55e-04)	Tok/s 46097 (43912)	Loss/tok 4.0058 (5.1891)	LR 2.000e-03
0: TRAIN [0][1650/7762]	Time 0.263 (0.322)	Data 1.05e-04 (3.53e-04)	Tok/s 39592 (43908)	Loss/tok 3.6585 (5.1817)	LR 2.000e-03
0: TRAIN [0][1660/7762]	Time 0.364 (0.322)	Data 1.07e-04 (3.52e-04)	Tok/s 46093 (43910)	Loss/tok 4.0599 (5.1744)	LR 2.000e-03
0: TRAIN [0][1670/7762]	Time 0.177 (0.322)	Data 1.02e-04 (3.50e-04)	Tok/s 28999 (43894)	Loss/tok 3.1227 (5.1676)	LR 2.000e-03
0: TRAIN [0][1680/7762]	Time 0.360 (0.322)	Data 1.05e-04 (3.49e-04)	Tok/s 46924 (43890)	Loss/tok 3.9877 (5.1607)	LR 2.000e-03
0: TRAIN [0][1690/7762]	Time 0.265 (0.322)	Data 1.26e-04 (3.48e-04)	Tok/s 38313 (43884)	Loss/tok 3.6825 (5.1539)	LR 2.000e-03
0: TRAIN [0][1700/7762]	Time 0.252 (0.322)	Data 1.02e-04 (3.46e-04)	Tok/s 40064 (43885)	Loss/tok 3.5351 (5.1468)	LR 2.000e-03
0: TRAIN [0][1710/7762]	Time 0.364 (0.322)	Data 1.15e-04 (3.45e-04)	Tok/s 45914 (43859)	Loss/tok 4.0704 (5.1408)	LR 2.000e-03
0: TRAIN [0][1720/7762]	Time 0.260 (0.322)	Data 1.04e-04 (3.43e-04)	Tok/s 39074 (43867)	Loss/tok 3.7720 (5.1337)	LR 2.000e-03
0: TRAIN [0][1730/7762]	Time 0.264 (0.322)	Data 1.02e-04 (3.42e-04)	Tok/s 39231 (43862)	Loss/tok 3.6697 (5.1272)	LR 2.000e-03
0: TRAIN [0][1740/7762]	Time 0.360 (0.322)	Data 1.01e-04 (3.41e-04)	Tok/s 46724 (43858)	Loss/tok 4.0806 (5.1208)	LR 2.000e-03
0: TRAIN [0][1750/7762]	Time 0.254 (0.322)	Data 1.04e-04 (3.39e-04)	Tok/s 41040 (43848)	Loss/tok 3.6195 (5.1144)	LR 2.000e-03
0: TRAIN [0][1760/7762]	Time 0.454 (0.321)	Data 1.02e-04 (3.38e-04)	Tok/s 51273 (43821)	Loss/tok 4.1719 (5.1088)	LR 2.000e-03
0: TRAIN [0][1770/7762]	Time 0.364 (0.322)	Data 1.09e-04 (3.37e-04)	Tok/s 45842 (43827)	Loss/tok 4.0997 (5.1018)	LR 2.000e-03
0: TRAIN [0][1780/7762]	Time 0.258 (0.321)	Data 1.02e-04 (3.35e-04)	Tok/s 40124 (43813)	Loss/tok 3.7700 (5.0958)	LR 2.000e-03
0: TRAIN [0][1790/7762]	Time 0.462 (0.321)	Data 1.07e-04 (3.34e-04)	Tok/s 50282 (43817)	Loss/tok 4.2967 (5.0890)	LR 2.000e-03
0: TRAIN [0][1800/7762]	Time 0.343 (0.321)	Data 1.06e-04 (3.33e-04)	Tok/s 48703 (43817)	Loss/tok 3.9920 (5.0827)	LR 2.000e-03
0: TRAIN [0][1810/7762]	Time 0.582 (0.322)	Data 1.18e-04 (3.32e-04)	Tok/s 50758 (43821)	Loss/tok 4.3767 (5.0759)	LR 2.000e-03
0: TRAIN [0][1820/7762]	Time 0.174 (0.321)	Data 1.03e-04 (3.30e-04)	Tok/s 30099 (43797)	Loss/tok 3.0856 (5.0705)	LR 2.000e-03
0: TRAIN [0][1830/7762]	Time 0.264 (0.321)	Data 1.04e-04 (3.29e-04)	Tok/s 38339 (43797)	Loss/tok 3.5745 (5.0642)	LR 2.000e-03
0: TRAIN [0][1840/7762]	Time 0.362 (0.321)	Data 1.06e-04 (3.28e-04)	Tok/s 46205 (43794)	Loss/tok 3.9272 (5.0581)	LR 2.000e-03
0: TRAIN [0][1850/7762]	Time 0.580 (0.321)	Data 9.94e-05 (3.27e-04)	Tok/s 51642 (43784)	Loss/tok 4.2224 (5.0521)	LR 2.000e-03
0: TRAIN [0][1860/7762]	Time 0.260 (0.321)	Data 1.03e-04 (3.26e-04)	Tok/s 39935 (43779)	Loss/tok 3.7343 (5.0461)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [0][1870/7762]	Time 0.336 (0.321)	Data 1.05e-04 (3.24e-04)	Tok/s 50303 (43779)	Loss/tok 3.9868 (5.0402)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1880/7762]	Time 0.265 (0.321)	Data 9.97e-05 (3.23e-04)	Tok/s 39025 (43770)	Loss/tok 3.6508 (5.0346)	LR 2.000e-03
0: TRAIN [0][1890/7762]	Time 0.263 (0.321)	Data 1.01e-04 (3.22e-04)	Tok/s 39921 (43776)	Loss/tok 3.7177 (5.0287)	LR 2.000e-03
0: TRAIN [0][1900/7762]	Time 0.358 (0.321)	Data 1.21e-04 (3.21e-04)	Tok/s 47825 (43780)	Loss/tok 3.8848 (5.0226)	LR 2.000e-03
0: TRAIN [0][1910/7762]	Time 0.362 (0.321)	Data 1.03e-04 (3.20e-04)	Tok/s 46088 (43770)	Loss/tok 3.9615 (5.0172)	LR 2.000e-03
0: TRAIN [0][1920/7762]	Time 0.263 (0.321)	Data 1.08e-04 (3.19e-04)	Tok/s 40026 (43760)	Loss/tok 3.6234 (5.0119)	LR 2.000e-03
0: TRAIN [0][1930/7762]	Time 0.352 (0.321)	Data 9.82e-05 (3.18e-04)	Tok/s 48155 (43758)	Loss/tok 3.9330 (5.0061)	LR 2.000e-03
0: TRAIN [0][1940/7762]	Time 0.361 (0.321)	Data 1.05e-04 (3.17e-04)	Tok/s 46408 (43751)	Loss/tok 3.8372 (5.0005)	LR 2.000e-03
0: TRAIN [0][1950/7762]	Time 0.345 (0.321)	Data 9.97e-05 (3.15e-04)	Tok/s 48546 (43753)	Loss/tok 3.9913 (4.9947)	LR 2.000e-03
0: TRAIN [0][1960/7762]	Time 0.362 (0.321)	Data 1.08e-04 (3.14e-04)	Tok/s 46118 (43748)	Loss/tok 4.0181 (4.9894)	LR 2.000e-03
0: TRAIN [0][1970/7762]	Time 0.453 (0.321)	Data 1.06e-04 (3.13e-04)	Tok/s 52113 (43738)	Loss/tok 4.0725 (4.9840)	LR 2.000e-03
0: TRAIN [0][1980/7762]	Time 0.178 (0.321)	Data 1.06e-04 (3.12e-04)	Tok/s 30329 (43731)	Loss/tok 3.0262 (4.9788)	LR 2.000e-03
0: TRAIN [0][1990/7762]	Time 0.350 (0.321)	Data 1.05e-04 (3.11e-04)	Tok/s 47185 (43728)	Loss/tok 3.9233 (4.9733)	LR 2.000e-03
0: TRAIN [0][2000/7762]	Time 0.175 (0.321)	Data 1.01e-04 (3.10e-04)	Tok/s 30138 (43711)	Loss/tok 3.0343 (4.9683)	LR 2.000e-03
0: TRAIN [0][2010/7762]	Time 0.355 (0.321)	Data 1.02e-04 (3.09e-04)	Tok/s 47815 (43699)	Loss/tok 3.9239 (4.9632)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][2020/7762]	Time 0.268 (0.321)	Data 1.04e-04 (3.08e-04)	Tok/s 37700 (43705)	Loss/tok 3.6276 (4.9578)	LR 2.000e-03
0: TRAIN [0][2030/7762]	Time 0.586 (0.321)	Data 1.06e-04 (3.07e-04)	Tok/s 50345 (43702)	Loss/tok 4.4942 (4.9528)	LR 2.000e-03
0: TRAIN [0][2040/7762]	Time 0.365 (0.321)	Data 9.73e-05 (3.06e-04)	Tok/s 45877 (43696)	Loss/tok 3.9515 (4.9477)	LR 2.000e-03
0: TRAIN [0][2050/7762]	Time 0.458 (0.321)	Data 1.05e-04 (3.05e-04)	Tok/s 51571 (43688)	Loss/tok 4.1843 (4.9427)	LR 2.000e-03
0: TRAIN [0][2060/7762]	Time 0.356 (0.321)	Data 9.73e-05 (3.04e-04)	Tok/s 47294 (43686)	Loss/tok 3.9370 (4.9374)	LR 2.000e-03
0: TRAIN [0][2070/7762]	Time 0.349 (0.321)	Data 1.03e-04 (3.03e-04)	Tok/s 47992 (43684)	Loss/tok 3.8451 (4.9322)	LR 2.000e-03
0: TRAIN [0][2080/7762]	Time 0.264 (0.321)	Data 1.03e-04 (3.02e-04)	Tok/s 39196 (43679)	Loss/tok 3.5687 (4.9273)	LR 2.000e-03
0: TRAIN [0][2090/7762]	Time 0.261 (0.321)	Data 1.01e-04 (3.01e-04)	Tok/s 39480 (43676)	Loss/tok 3.6542 (4.9223)	LR 2.000e-03
0: TRAIN [0][2100/7762]	Time 0.361 (0.321)	Data 1.03e-04 (3.00e-04)	Tok/s 46444 (43687)	Loss/tok 3.9315 (4.9168)	LR 2.000e-03
0: TRAIN [0][2110/7762]	Time 0.264 (0.322)	Data 1.08e-04 (3.00e-04)	Tok/s 39044 (43697)	Loss/tok 3.5855 (4.9112)	LR 2.000e-03
0: TRAIN [0][2120/7762]	Time 0.264 (0.321)	Data 1.05e-04 (2.99e-04)	Tok/s 38801 (43682)	Loss/tok 3.6337 (4.9066)	LR 2.000e-03
0: TRAIN [0][2130/7762]	Time 0.264 (0.322)	Data 1.00e-04 (2.98e-04)	Tok/s 39058 (43689)	Loss/tok 3.6146 (4.9013)	LR 2.000e-03
0: TRAIN [0][2140/7762]	Time 0.366 (0.322)	Data 1.04e-04 (2.97e-04)	Tok/s 46009 (43688)	Loss/tok 3.8808 (4.8966)	LR 2.000e-03
0: TRAIN [0][2150/7762]	Time 0.457 (0.321)	Data 1.04e-04 (2.96e-04)	Tok/s 50841 (43674)	Loss/tok 4.1912 (4.8924)	LR 2.000e-03
0: TRAIN [0][2160/7762]	Time 0.582 (0.321)	Data 1.06e-04 (2.95e-04)	Tok/s 51272 (43658)	Loss/tok 4.2063 (4.8881)	LR 2.000e-03
0: TRAIN [0][2170/7762]	Time 0.258 (0.321)	Data 1.15e-04 (2.94e-04)	Tok/s 39446 (43649)	Loss/tok 3.6993 (4.8837)	LR 2.000e-03
0: TRAIN [0][2180/7762]	Time 0.259 (0.321)	Data 1.05e-04 (2.93e-04)	Tok/s 40342 (43642)	Loss/tok 3.5917 (4.8793)	LR 2.000e-03
0: TRAIN [0][2190/7762]	Time 0.259 (0.321)	Data 1.05e-04 (2.93e-04)	Tok/s 39382 (43640)	Loss/tok 3.5385 (4.8746)	LR 2.000e-03
0: TRAIN [0][2200/7762]	Time 0.257 (0.321)	Data 1.05e-04 (2.92e-04)	Tok/s 40383 (43622)	Loss/tok 3.5953 (4.8705)	LR 2.000e-03
0: TRAIN [0][2210/7762]	Time 0.363 (0.321)	Data 1.07e-04 (2.91e-04)	Tok/s 46526 (43620)	Loss/tok 3.8128 (4.8658)	LR 2.000e-03
0: TRAIN [0][2220/7762]	Time 0.362 (0.321)	Data 1.04e-04 (2.90e-04)	Tok/s 46986 (43620)	Loss/tok 3.8907 (4.8612)	LR 2.000e-03
0: TRAIN [0][2230/7762]	Time 0.350 (0.321)	Data 1.06e-04 (2.89e-04)	Tok/s 47687 (43614)	Loss/tok 3.8299 (4.8568)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][2240/7762]	Time 0.257 (0.321)	Data 1.00e-04 (2.88e-04)	Tok/s 40430 (43617)	Loss/tok 3.6122 (4.8520)	LR 2.000e-03
0: TRAIN [0][2250/7762]	Time 0.262 (0.321)	Data 9.78e-05 (2.88e-04)	Tok/s 39187 (43600)	Loss/tok 3.5818 (4.8481)	LR 2.000e-03
0: TRAIN [0][2260/7762]	Time 0.177 (0.321)	Data 1.10e-04 (2.87e-04)	Tok/s 30012 (43595)	Loss/tok 3.0194 (4.8438)	LR 2.000e-03
0: TRAIN [0][2270/7762]	Time 0.583 (0.321)	Data 1.04e-04 (2.86e-04)	Tok/s 51325 (43587)	Loss/tok 4.3974 (4.8398)	LR 2.000e-03
0: TRAIN [0][2280/7762]	Time 0.348 (0.321)	Data 1.02e-04 (2.85e-04)	Tok/s 48793 (43586)	Loss/tok 4.0028 (4.8355)	LR 2.000e-03
0: TRAIN [0][2290/7762]	Time 0.258 (0.321)	Data 1.04e-04 (2.84e-04)	Tok/s 40222 (43587)	Loss/tok 3.5918 (4.8309)	LR 2.000e-03
0: TRAIN [0][2300/7762]	Time 0.259 (0.321)	Data 1.07e-04 (2.84e-04)	Tok/s 39649 (43593)	Loss/tok 3.5987 (4.8263)	LR 2.000e-03
0: TRAIN [0][2310/7762]	Time 0.361 (0.321)	Data 1.06e-04 (2.83e-04)	Tok/s 46420 (43593)	Loss/tok 3.8965 (4.8219)	LR 2.000e-03
0: TRAIN [0][2320/7762]	Time 0.460 (0.321)	Data 1.03e-04 (2.82e-04)	Tok/s 51256 (43596)	Loss/tok 4.0042 (4.8174)	LR 2.000e-03
0: TRAIN [0][2330/7762]	Time 0.360 (0.321)	Data 1.12e-04 (2.81e-04)	Tok/s 46636 (43592)	Loss/tok 3.9278 (4.8132)	LR 2.000e-03
0: TRAIN [0][2340/7762]	Time 0.450 (0.321)	Data 1.03e-04 (2.81e-04)	Tok/s 51589 (43588)	Loss/tok 3.9458 (4.8090)	LR 2.000e-03
0: TRAIN [0][2350/7762]	Time 0.263 (0.321)	Data 1.27e-04 (2.80e-04)	Tok/s 38584 (43595)	Loss/tok 3.5808 (4.8045)	LR 2.000e-03
0: TRAIN [0][2360/7762]	Time 0.362 (0.321)	Data 1.16e-04 (2.79e-04)	Tok/s 46088 (43582)	Loss/tok 4.0535 (4.8006)	LR 2.000e-03
0: TRAIN [0][2370/7762]	Time 0.260 (0.321)	Data 1.05e-04 (2.78e-04)	Tok/s 39840 (43577)	Loss/tok 3.4653 (4.7965)	LR 2.000e-03
0: TRAIN [0][2380/7762]	Time 0.264 (0.321)	Data 9.85e-05 (2.78e-04)	Tok/s 39094 (43563)	Loss/tok 3.5653 (4.7927)	LR 2.000e-03
0: TRAIN [0][2390/7762]	Time 0.264 (0.321)	Data 1.02e-04 (2.77e-04)	Tok/s 39601 (43565)	Loss/tok 3.6069 (4.7887)	LR 2.000e-03
0: TRAIN [0][2400/7762]	Time 0.179 (0.321)	Data 1.01e-04 (2.76e-04)	Tok/s 28507 (43554)	Loss/tok 2.8877 (4.7847)	LR 2.000e-03
0: TRAIN [0][2410/7762]	Time 0.358 (0.321)	Data 1.03e-04 (2.76e-04)	Tok/s 47625 (43545)	Loss/tok 3.7843 (4.7809)	LR 2.000e-03
0: TRAIN [0][2420/7762]	Time 0.357 (0.321)	Data 1.06e-04 (2.75e-04)	Tok/s 47143 (43551)	Loss/tok 3.9470 (4.7767)	LR 2.000e-03
0: TRAIN [0][2430/7762]	Time 0.264 (0.321)	Data 1.02e-04 (2.74e-04)	Tok/s 39091 (43553)	Loss/tok 3.5988 (4.7726)	LR 2.000e-03
0: TRAIN [0][2440/7762]	Time 0.585 (0.321)	Data 1.02e-04 (2.73e-04)	Tok/s 50934 (43556)	Loss/tok 4.1578 (4.7685)	LR 2.000e-03
0: TRAIN [0][2450/7762]	Time 0.263 (0.321)	Data 9.89e-05 (2.73e-04)	Tok/s 39406 (43541)	Loss/tok 3.6434 (4.7649)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][2460/7762]	Time 0.462 (0.321)	Data 1.22e-04 (2.72e-04)	Tok/s 50418 (43546)	Loss/tok 3.9480 (4.7609)	LR 2.000e-03
0: TRAIN [0][2470/7762]	Time 0.259 (0.321)	Data 9.99e-05 (2.71e-04)	Tok/s 41401 (43544)	Loss/tok 3.5001 (4.7571)	LR 2.000e-03
0: TRAIN [0][2480/7762]	Time 0.458 (0.321)	Data 1.02e-04 (2.71e-04)	Tok/s 51102 (43547)	Loss/tok 4.0678 (4.7532)	LR 2.000e-03
0: TRAIN [0][2490/7762]	Time 0.359 (0.321)	Data 1.02e-04 (2.70e-04)	Tok/s 46511 (43534)	Loss/tok 3.8163 (4.7497)	LR 2.000e-03
0: TRAIN [0][2500/7762]	Time 0.170 (0.321)	Data 1.07e-04 (2.69e-04)	Tok/s 32035 (43533)	Loss/tok 3.0902 (4.7461)	LR 2.000e-03
0: TRAIN [0][2510/7762]	Time 0.176 (0.321)	Data 1.07e-04 (2.69e-04)	Tok/s 30274 (43529)	Loss/tok 2.9855 (4.7425)	LR 2.000e-03
0: TRAIN [0][2520/7762]	Time 0.176 (0.321)	Data 1.04e-04 (2.68e-04)	Tok/s 29769 (43526)	Loss/tok 3.1683 (4.7388)	LR 2.000e-03
0: TRAIN [0][2530/7762]	Time 0.354 (0.321)	Data 1.05e-04 (2.67e-04)	Tok/s 47753 (43527)	Loss/tok 3.8273 (4.7350)	LR 2.000e-03
0: TRAIN [0][2540/7762]	Time 0.257 (0.321)	Data 1.04e-04 (2.67e-04)	Tok/s 40144 (43524)	Loss/tok 3.4653 (4.7312)	LR 2.000e-03
0: TRAIN [0][2550/7762]	Time 0.263 (0.321)	Data 1.04e-04 (2.66e-04)	Tok/s 39664 (43517)	Loss/tok 3.5091 (4.7277)	LR 2.000e-03
0: TRAIN [0][2560/7762]	Time 0.266 (0.321)	Data 1.03e-04 (2.66e-04)	Tok/s 39117 (43513)	Loss/tok 3.4566 (4.7241)	LR 2.000e-03
0: TRAIN [0][2570/7762]	Time 0.346 (0.321)	Data 1.03e-04 (2.65e-04)	Tok/s 49368 (43512)	Loss/tok 3.8198 (4.7206)	LR 2.000e-03
0: TRAIN [0][2580/7762]	Time 0.261 (0.321)	Data 1.04e-04 (2.64e-04)	Tok/s 39431 (43513)	Loss/tok 3.5074 (4.7171)	LR 2.000e-03
0: TRAIN [0][2590/7762]	Time 0.456 (0.321)	Data 1.10e-04 (2.64e-04)	Tok/s 50803 (43515)	Loss/tok 4.0110 (4.7133)	LR 2.000e-03
0: TRAIN [0][2600/7762]	Time 0.174 (0.321)	Data 1.02e-04 (2.63e-04)	Tok/s 30927 (43508)	Loss/tok 3.0589 (4.7098)	LR 2.000e-03
0: TRAIN [0][2610/7762]	Time 0.260 (0.321)	Data 1.01e-04 (2.63e-04)	Tok/s 39210 (43498)	Loss/tok 3.5415 (4.7065)	LR 2.000e-03
0: TRAIN [0][2620/7762]	Time 0.349 (0.321)	Data 1.08e-04 (2.62e-04)	Tok/s 48520 (43488)	Loss/tok 3.8242 (4.7033)	LR 2.000e-03
0: TRAIN [0][2630/7762]	Time 0.355 (0.321)	Data 9.82e-05 (2.61e-04)	Tok/s 48011 (43499)	Loss/tok 3.6046 (4.6994)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][2640/7762]	Time 0.454 (0.321)	Data 1.05e-04 (2.61e-04)	Tok/s 51119 (43511)	Loss/tok 4.0553 (4.6956)	LR 2.000e-03
0: TRAIN [0][2650/7762]	Time 0.464 (0.321)	Data 1.17e-04 (2.60e-04)	Tok/s 50201 (43511)	Loss/tok 3.9727 (4.6921)	LR 2.000e-03
0: TRAIN [0][2660/7762]	Time 0.263 (0.321)	Data 1.06e-04 (2.60e-04)	Tok/s 39668 (43498)	Loss/tok 3.6027 (4.6891)	LR 2.000e-03
0: TRAIN [0][2670/7762]	Time 0.175 (0.321)	Data 1.00e-04 (2.59e-04)	Tok/s 30074 (43494)	Loss/tok 2.9862 (4.6856)	LR 2.000e-03
0: TRAIN [0][2680/7762]	Time 0.259 (0.321)	Data 1.04e-04 (2.58e-04)	Tok/s 40119 (43489)	Loss/tok 3.4943 (4.6825)	LR 2.000e-03
0: TRAIN [0][2690/7762]	Time 0.266 (0.321)	Data 1.04e-04 (2.58e-04)	Tok/s 39525 (43491)	Loss/tok 3.5310 (4.6791)	LR 2.000e-03
0: TRAIN [0][2700/7762]	Time 0.445 (0.321)	Data 1.03e-04 (2.57e-04)	Tok/s 52703 (43490)	Loss/tok 4.0262 (4.6758)	LR 2.000e-03
0: TRAIN [0][2710/7762]	Time 0.175 (0.321)	Data 1.10e-04 (2.57e-04)	Tok/s 29644 (43490)	Loss/tok 3.0689 (4.6723)	LR 2.000e-03
0: TRAIN [0][2720/7762]	Time 0.175 (0.321)	Data 1.09e-04 (2.56e-04)	Tok/s 30484 (43476)	Loss/tok 2.9869 (4.6695)	LR 2.000e-03
0: TRAIN [0][2730/7762]	Time 0.263 (0.321)	Data 1.29e-04 (2.56e-04)	Tok/s 38998 (43485)	Loss/tok 3.4660 (4.6659)	LR 2.000e-03
0: TRAIN [0][2740/7762]	Time 0.262 (0.321)	Data 9.92e-05 (2.55e-04)	Tok/s 39251 (43468)	Loss/tok 3.5831 (4.6629)	LR 2.000e-03
0: TRAIN [0][2750/7762]	Time 0.452 (0.321)	Data 2.16e-04 (2.55e-04)	Tok/s 51958 (43463)	Loss/tok 3.9927 (4.6597)	LR 2.000e-03
0: TRAIN [0][2760/7762]	Time 0.260 (0.321)	Data 1.09e-04 (2.54e-04)	Tok/s 39750 (43454)	Loss/tok 3.6894 (4.6567)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][2770/7762]	Time 0.253 (0.321)	Data 1.02e-04 (2.54e-04)	Tok/s 40184 (43466)	Loss/tok 3.5646 (4.6533)	LR 2.000e-03
0: TRAIN [0][2780/7762]	Time 0.263 (0.321)	Data 1.03e-04 (2.53e-04)	Tok/s 38941 (43458)	Loss/tok 3.6112 (4.6503)	LR 2.000e-03
0: TRAIN [0][2790/7762]	Time 0.252 (0.321)	Data 1.01e-04 (2.52e-04)	Tok/s 41211 (43448)	Loss/tok 3.4564 (4.6475)	LR 2.000e-03
0: TRAIN [0][2800/7762]	Time 0.354 (0.321)	Data 1.03e-04 (2.52e-04)	Tok/s 47629 (43448)	Loss/tok 3.7835 (4.6442)	LR 2.000e-03
0: TRAIN [0][2810/7762]	Time 0.269 (0.321)	Data 1.06e-04 (2.51e-04)	Tok/s 37864 (43452)	Loss/tok 3.5201 (4.6409)	LR 2.000e-03
0: TRAIN [0][2820/7762]	Time 0.260 (0.321)	Data 1.03e-04 (2.51e-04)	Tok/s 39686 (43450)	Loss/tok 3.4302 (4.6377)	LR 2.000e-03
0: TRAIN [0][2830/7762]	Time 0.259 (0.321)	Data 1.01e-04 (2.50e-04)	Tok/s 39539 (43445)	Loss/tok 3.5297 (4.6347)	LR 2.000e-03
0: TRAIN [0][2840/7762]	Time 0.264 (0.321)	Data 1.04e-04 (2.50e-04)	Tok/s 38302 (43450)	Loss/tok 3.3773 (4.6314)	LR 2.000e-03
0: TRAIN [0][2850/7762]	Time 0.361 (0.321)	Data 1.04e-04 (2.49e-04)	Tok/s 46258 (43450)	Loss/tok 3.8296 (4.6282)	LR 2.000e-03
0: TRAIN [0][2860/7762]	Time 0.264 (0.321)	Data 1.04e-04 (2.49e-04)	Tok/s 39580 (43429)	Loss/tok 3.5678 (4.6256)	LR 2.000e-03
0: TRAIN [0][2870/7762]	Time 0.260 (0.321)	Data 1.19e-04 (2.48e-04)	Tok/s 39732 (43417)	Loss/tok 3.5680 (4.6228)	LR 2.000e-03
0: TRAIN [0][2880/7762]	Time 0.262 (0.321)	Data 3.44e-04 (2.48e-04)	Tok/s 38980 (43411)	Loss/tok 3.5855 (4.6200)	LR 2.000e-03
0: TRAIN [0][2890/7762]	Time 0.350 (0.321)	Data 1.66e-04 (2.48e-04)	Tok/s 47983 (43399)	Loss/tok 3.7211 (4.6174)	LR 2.000e-03
0: TRAIN [0][2900/7762]	Time 0.354 (0.320)	Data 9.97e-05 (2.47e-04)	Tok/s 47634 (43398)	Loss/tok 3.7355 (4.6144)	LR 2.000e-03
0: TRAIN [0][2910/7762]	Time 0.452 (0.320)	Data 1.07e-04 (2.47e-04)	Tok/s 51537 (43385)	Loss/tok 3.9940 (4.6118)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][2920/7762]	Time 0.262 (0.320)	Data 1.01e-04 (2.46e-04)	Tok/s 39038 (43382)	Loss/tok 3.4885 (4.6089)	LR 2.000e-03
0: TRAIN [0][2930/7762]	Time 0.173 (0.320)	Data 1.15e-04 (2.46e-04)	Tok/s 30908 (43376)	Loss/tok 2.9519 (4.6061)	LR 2.000e-03
0: TRAIN [0][2940/7762]	Time 0.264 (0.320)	Data 1.08e-04 (2.45e-04)	Tok/s 38849 (43380)	Loss/tok 3.3940 (4.6030)	LR 2.000e-03
0: TRAIN [0][2950/7762]	Time 0.175 (0.320)	Data 1.06e-04 (2.45e-04)	Tok/s 29472 (43371)	Loss/tok 3.0266 (4.6002)	LR 2.000e-03
0: TRAIN [0][2960/7762]	Time 0.176 (0.320)	Data 1.06e-04 (2.44e-04)	Tok/s 29699 (43361)	Loss/tok 2.9573 (4.5975)	LR 2.000e-03
0: TRAIN [0][2970/7762]	Time 0.447 (0.320)	Data 1.03e-04 (2.44e-04)	Tok/s 52491 (43350)	Loss/tok 4.0648 (4.5948)	LR 2.000e-03
0: TRAIN [0][2980/7762]	Time 0.261 (0.320)	Data 1.43e-04 (2.43e-04)	Tok/s 39692 (43344)	Loss/tok 3.4951 (4.5920)	LR 2.000e-03
0: TRAIN [0][2990/7762]	Time 0.573 (0.320)	Data 1.27e-04 (2.43e-04)	Tok/s 51662 (43350)	Loss/tok 4.1865 (4.5891)	LR 2.000e-03
0: TRAIN [0][3000/7762]	Time 0.361 (0.320)	Data 1.04e-04 (2.42e-04)	Tok/s 46855 (43362)	Loss/tok 3.7864 (4.5859)	LR 2.000e-03
0: TRAIN [0][3010/7762]	Time 0.358 (0.320)	Data 1.05e-04 (2.42e-04)	Tok/s 46572 (43374)	Loss/tok 3.7676 (4.5828)	LR 2.000e-03
0: TRAIN [0][3020/7762]	Time 0.261 (0.320)	Data 1.01e-04 (2.42e-04)	Tok/s 40106 (43365)	Loss/tok 3.4832 (4.5800)	LR 2.000e-03
0: TRAIN [0][3030/7762]	Time 0.583 (0.320)	Data 1.04e-04 (2.41e-04)	Tok/s 50769 (43367)	Loss/tok 4.2081 (4.5772)	LR 2.000e-03
0: TRAIN [0][3040/7762]	Time 0.171 (0.320)	Data 1.19e-04 (2.41e-04)	Tok/s 31078 (43366)	Loss/tok 3.0171 (4.5746)	LR 2.000e-03
0: TRAIN [0][3050/7762]	Time 0.178 (0.321)	Data 1.04e-04 (2.40e-04)	Tok/s 29400 (43370)	Loss/tok 3.0681 (4.5718)	LR 2.000e-03
0: TRAIN [0][3060/7762]	Time 0.261 (0.321)	Data 1.07e-04 (2.40e-04)	Tok/s 40321 (43379)	Loss/tok 3.5567 (4.5688)	LR 2.000e-03
0: TRAIN [0][3070/7762]	Time 0.368 (0.321)	Data 1.03e-04 (2.39e-04)	Tok/s 45682 (43384)	Loss/tok 3.6759 (4.5659)	LR 2.000e-03
0: TRAIN [0][3080/7762]	Time 0.262 (0.321)	Data 1.03e-04 (2.39e-04)	Tok/s 39781 (43380)	Loss/tok 3.4870 (4.5633)	LR 2.000e-03
0: TRAIN [0][3090/7762]	Time 0.261 (0.321)	Data 1.01e-04 (2.39e-04)	Tok/s 39018 (43370)	Loss/tok 3.5707 (4.5608)	LR 2.000e-03
0: TRAIN [0][3100/7762]	Time 0.261 (0.321)	Data 1.04e-04 (2.38e-04)	Tok/s 39103 (43368)	Loss/tok 3.4366 (4.5582)	LR 2.000e-03
0: TRAIN [0][3110/7762]	Time 0.266 (0.321)	Data 1.03e-04 (2.38e-04)	Tok/s 38911 (43363)	Loss/tok 3.3826 (4.5556)	LR 2.000e-03
0: TRAIN [0][3120/7762]	Time 0.353 (0.321)	Data 1.03e-04 (2.37e-04)	Tok/s 47854 (43365)	Loss/tok 3.7148 (4.5529)	LR 2.000e-03
0: TRAIN [0][3130/7762]	Time 0.265 (0.321)	Data 1.10e-04 (2.37e-04)	Tok/s 38915 (43368)	Loss/tok 3.5354 (4.5503)	LR 2.000e-03
0: TRAIN [0][3140/7762]	Time 0.587 (0.321)	Data 1.04e-04 (2.36e-04)	Tok/s 51065 (43374)	Loss/tok 4.0650 (4.5474)	LR 2.000e-03
0: TRAIN [0][3150/7762]	Time 0.259 (0.321)	Data 1.04e-04 (2.36e-04)	Tok/s 39884 (43382)	Loss/tok 3.5534 (4.5447)	LR 2.000e-03
0: TRAIN [0][3160/7762]	Time 0.354 (0.321)	Data 1.06e-04 (2.36e-04)	Tok/s 48218 (43379)	Loss/tok 3.7305 (4.5422)	LR 2.000e-03
0: TRAIN [0][3170/7762]	Time 0.363 (0.321)	Data 1.22e-04 (2.35e-04)	Tok/s 46101 (43382)	Loss/tok 3.5820 (4.5396)	LR 2.000e-03
0: TRAIN [0][3180/7762]	Time 0.178 (0.321)	Data 1.04e-04 (2.35e-04)	Tok/s 29585 (43381)	Loss/tok 2.8820 (4.5369)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [0][3190/7762]	Time 0.361 (0.321)	Data 1.21e-04 (2.34e-04)	Tok/s 46418 (43381)	Loss/tok 3.6613 (4.5342)	LR 2.000e-03
0: TRAIN [0][3200/7762]	Time 0.579 (0.321)	Data 1.02e-04 (2.34e-04)	Tok/s 52194 (43378)	Loss/tok 4.0583 (4.5317)	LR 2.000e-03
0: TRAIN [0][3210/7762]	Time 0.268 (0.321)	Data 1.03e-04 (2.34e-04)	Tok/s 38295 (43374)	Loss/tok 3.3824 (4.5294)	LR 2.000e-03
0: TRAIN [0][3220/7762]	Time 0.450 (0.321)	Data 1.00e-04 (2.33e-04)	Tok/s 51749 (43368)	Loss/tok 3.9916 (4.5269)	LR 2.000e-03
0: TRAIN [0][3230/7762]	Time 0.356 (0.321)	Data 1.04e-04 (2.33e-04)	Tok/s 47669 (43377)	Loss/tok 3.7711 (4.5242)	LR 2.000e-03
0: TRAIN [0][3240/7762]	Time 0.261 (0.321)	Data 1.03e-04 (2.33e-04)	Tok/s 39713 (43368)	Loss/tok 3.5678 (4.5218)	LR 2.000e-03
0: TRAIN [0][3250/7762]	Time 0.257 (0.321)	Data 1.03e-04 (2.32e-04)	Tok/s 40115 (43369)	Loss/tok 3.4685 (4.5193)	LR 2.000e-03
0: TRAIN [0][3260/7762]	Time 0.456 (0.321)	Data 1.08e-04 (2.32e-04)	Tok/s 51055 (43370)	Loss/tok 3.9735 (4.5169)	LR 2.000e-03
0: TRAIN [0][3270/7762]	Time 0.343 (0.321)	Data 1.05e-04 (2.31e-04)	Tok/s 48956 (43376)	Loss/tok 3.7921 (4.5145)	LR 2.000e-03
0: TRAIN [0][3280/7762]	Time 0.360 (0.321)	Data 1.05e-04 (2.31e-04)	Tok/s 46575 (43370)	Loss/tok 3.7657 (4.5121)	LR 2.000e-03
0: TRAIN [0][3290/7762]	Time 0.458 (0.321)	Data 1.01e-04 (2.31e-04)	Tok/s 50581 (43370)	Loss/tok 3.8404 (4.5095)	LR 2.000e-03
0: TRAIN [0][3300/7762]	Time 0.451 (0.321)	Data 1.09e-04 (2.30e-04)	Tok/s 52042 (43372)	Loss/tok 3.8054 (4.5071)	LR 2.000e-03
0: TRAIN [0][3310/7762]	Time 0.440 (0.321)	Data 1.04e-04 (2.30e-04)	Tok/s 52329 (43380)	Loss/tok 3.9312 (4.5045)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [0][3320/7762]	Time 0.347 (0.321)	Data 1.10e-04 (2.29e-04)	Tok/s 48644 (43371)	Loss/tok 3.7714 (4.5021)	LR 2.000e-03
0: TRAIN [0][3330/7762]	Time 0.266 (0.321)	Data 1.08e-04 (2.29e-04)	Tok/s 39009 (43366)	Loss/tok 3.3910 (4.4997)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3340/7762]	Time 0.565 (0.322)	Data 1.06e-04 (2.29e-04)	Tok/s 52606 (43385)	Loss/tok 3.9936 (4.4970)	LR 2.000e-03
0: TRAIN [0][3350/7762]	Time 0.452 (0.322)	Data 1.10e-04 (2.28e-04)	Tok/s 52448 (43397)	Loss/tok 3.8585 (4.4945)	LR 2.000e-03
0: TRAIN [0][3360/7762]	Time 0.264 (0.322)	Data 1.00e-04 (2.28e-04)	Tok/s 38263 (43389)	Loss/tok 3.5708 (4.4922)	LR 2.000e-03
0: TRAIN [0][3370/7762]	Time 0.268 (0.322)	Data 1.18e-04 (2.28e-04)	Tok/s 38644 (43386)	Loss/tok 3.4929 (4.4898)	LR 2.000e-03
0: TRAIN [0][3380/7762]	Time 0.254 (0.322)	Data 1.03e-04 (2.27e-04)	Tok/s 40603 (43386)	Loss/tok 3.5686 (4.4876)	LR 2.000e-03
0: TRAIN [0][3390/7762]	Time 0.353 (0.322)	Data 9.92e-05 (2.27e-04)	Tok/s 47614 (43378)	Loss/tok 3.7584 (4.4853)	LR 2.000e-03
0: TRAIN [0][3400/7762]	Time 0.268 (0.322)	Data 9.85e-05 (2.27e-04)	Tok/s 38971 (43380)	Loss/tok 3.5051 (4.4829)	LR 2.000e-03
0: TRAIN [0][3410/7762]	Time 0.260 (0.322)	Data 1.04e-04 (2.26e-04)	Tok/s 39697 (43377)	Loss/tok 3.3989 (4.4806)	LR 2.000e-03
0: TRAIN [0][3420/7762]	Time 0.367 (0.322)	Data 1.04e-04 (2.26e-04)	Tok/s 46014 (43376)	Loss/tok 3.7269 (4.4782)	LR 2.000e-03
0: TRAIN [0][3430/7762]	Time 0.361 (0.322)	Data 1.01e-04 (2.25e-04)	Tok/s 46421 (43372)	Loss/tok 3.7996 (4.4760)	LR 2.000e-03
0: TRAIN [0][3440/7762]	Time 0.342 (0.322)	Data 1.01e-04 (2.25e-04)	Tok/s 49284 (43382)	Loss/tok 3.7582 (4.4736)	LR 2.000e-03
0: TRAIN [0][3450/7762]	Time 0.456 (0.322)	Data 1.05e-04 (2.25e-04)	Tok/s 51233 (43393)	Loss/tok 3.7176 (4.4711)	LR 2.000e-03
0: TRAIN [0][3460/7762]	Time 0.357 (0.322)	Data 1.07e-04 (2.24e-04)	Tok/s 47699 (43396)	Loss/tok 3.7104 (4.4687)	LR 2.000e-03
0: TRAIN [0][3470/7762]	Time 0.353 (0.322)	Data 1.02e-04 (2.24e-04)	Tok/s 47819 (43396)	Loss/tok 3.6383 (4.4665)	LR 2.000e-03
0: TRAIN [0][3480/7762]	Time 0.435 (0.322)	Data 1.02e-04 (2.24e-04)	Tok/s 54403 (43400)	Loss/tok 3.7942 (4.4640)	LR 2.000e-03
0: TRAIN [0][3490/7762]	Time 0.267 (0.322)	Data 1.05e-04 (2.23e-04)	Tok/s 38583 (43403)	Loss/tok 3.4427 (4.4618)	LR 2.000e-03
0: TRAIN [0][3500/7762]	Time 0.258 (0.322)	Data 1.07e-04 (2.23e-04)	Tok/s 39954 (43397)	Loss/tok 3.4675 (4.4597)	LR 2.000e-03
0: TRAIN [0][3510/7762]	Time 0.267 (0.322)	Data 1.07e-04 (2.23e-04)	Tok/s 39370 (43394)	Loss/tok 3.3165 (4.4576)	LR 2.000e-03
0: TRAIN [0][3520/7762]	Time 0.263 (0.322)	Data 1.15e-04 (2.22e-04)	Tok/s 38662 (43400)	Loss/tok 3.4020 (4.4552)	LR 2.000e-03
0: TRAIN [0][3530/7762]	Time 0.445 (0.322)	Data 1.01e-04 (2.22e-04)	Tok/s 52751 (43395)	Loss/tok 3.8177 (4.4530)	LR 2.000e-03
0: TRAIN [0][3540/7762]	Time 0.175 (0.322)	Data 1.07e-04 (2.22e-04)	Tok/s 30531 (43395)	Loss/tok 3.0102 (4.4507)	LR 2.000e-03
0: TRAIN [0][3550/7762]	Time 0.258 (0.322)	Data 1.05e-04 (2.21e-04)	Tok/s 39870 (43394)	Loss/tok 3.4376 (4.4486)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3560/7762]	Time 0.579 (0.322)	Data 1.07e-04 (2.21e-04)	Tok/s 51937 (43399)	Loss/tok 3.9537 (4.4462)	LR 2.000e-03
0: TRAIN [0][3570/7762]	Time 0.351 (0.322)	Data 1.05e-04 (2.21e-04)	Tok/s 48346 (43404)	Loss/tok 3.7245 (4.4438)	LR 2.000e-03
0: TRAIN [0][3580/7762]	Time 0.358 (0.322)	Data 1.04e-04 (2.21e-04)	Tok/s 46824 (43401)	Loss/tok 3.5190 (4.4417)	LR 2.000e-03
0: TRAIN [0][3590/7762]	Time 0.263 (0.323)	Data 1.05e-04 (2.20e-04)	Tok/s 39965 (43404)	Loss/tok 3.4158 (4.4395)	LR 2.000e-03
0: TRAIN [0][3600/7762]	Time 0.354 (0.322)	Data 1.00e-04 (2.20e-04)	Tok/s 47476 (43398)	Loss/tok 3.6480 (4.4375)	LR 2.000e-03
0: TRAIN [0][3610/7762]	Time 0.264 (0.322)	Data 1.18e-04 (2.20e-04)	Tok/s 39447 (43391)	Loss/tok 3.5827 (4.4355)	LR 2.000e-03
0: TRAIN [0][3620/7762]	Time 0.365 (0.322)	Data 1.05e-04 (2.19e-04)	Tok/s 45954 (43393)	Loss/tok 3.6265 (4.4333)	LR 2.000e-03
0: TRAIN [0][3630/7762]	Time 0.177 (0.322)	Data 1.08e-04 (2.19e-04)	Tok/s 29896 (43393)	Loss/tok 3.0290 (4.4312)	LR 2.000e-03
0: TRAIN [0][3640/7762]	Time 0.258 (0.322)	Data 1.04e-04 (2.19e-04)	Tok/s 40605 (43399)	Loss/tok 3.4001 (4.4289)	LR 2.000e-03
0: TRAIN [0][3650/7762]	Time 0.347 (0.322)	Data 1.03e-04 (2.18e-04)	Tok/s 47881 (43400)	Loss/tok 3.5994 (4.4268)	LR 2.000e-03
0: TRAIN [0][3660/7762]	Time 0.461 (0.323)	Data 1.03e-04 (2.18e-04)	Tok/s 50498 (43411)	Loss/tok 3.9290 (4.4244)	LR 2.000e-03
0: TRAIN [0][3670/7762]	Time 0.174 (0.323)	Data 1.05e-04 (2.18e-04)	Tok/s 30057 (43401)	Loss/tok 3.0218 (4.4226)	LR 2.000e-03
0: TRAIN [0][3680/7762]	Time 0.363 (0.323)	Data 1.02e-04 (2.17e-04)	Tok/s 46832 (43409)	Loss/tok 3.6116 (4.4204)	LR 2.000e-03
0: TRAIN [0][3690/7762]	Time 0.355 (0.323)	Data 1.06e-04 (2.17e-04)	Tok/s 47894 (43409)	Loss/tok 3.6659 (4.4183)	LR 2.000e-03
0: TRAIN [0][3700/7762]	Time 0.262 (0.322)	Data 1.03e-04 (2.17e-04)	Tok/s 38942 (43402)	Loss/tok 3.5477 (4.4164)	LR 2.000e-03
0: TRAIN [0][3710/7762]	Time 0.364 (0.323)	Data 1.03e-04 (2.17e-04)	Tok/s 46531 (43405)	Loss/tok 3.6513 (4.4142)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3720/7762]	Time 0.361 (0.323)	Data 1.05e-04 (2.16e-04)	Tok/s 46038 (43408)	Loss/tok 3.5877 (4.4122)	LR 2.000e-03
0: TRAIN [0][3730/7762]	Time 0.359 (0.322)	Data 1.06e-04 (2.16e-04)	Tok/s 46597 (43398)	Loss/tok 3.5975 (4.4102)	LR 2.000e-03
0: TRAIN [0][3740/7762]	Time 0.178 (0.323)	Data 1.08e-04 (2.16e-04)	Tok/s 29995 (43397)	Loss/tok 2.9112 (4.4083)	LR 2.000e-03
0: TRAIN [0][3750/7762]	Time 0.352 (0.323)	Data 1.09e-04 (2.15e-04)	Tok/s 46846 (43407)	Loss/tok 3.7453 (4.4060)	LR 2.000e-03
0: TRAIN [0][3760/7762]	Time 0.175 (0.323)	Data 1.20e-04 (2.15e-04)	Tok/s 29573 (43403)	Loss/tok 2.8620 (4.4041)	LR 2.000e-03
0: TRAIN [0][3770/7762]	Time 0.355 (0.323)	Data 1.07e-04 (2.15e-04)	Tok/s 47421 (43396)	Loss/tok 3.6874 (4.4022)	LR 2.000e-03
0: TRAIN [0][3780/7762]	Time 0.583 (0.323)	Data 1.02e-04 (2.14e-04)	Tok/s 50541 (43394)	Loss/tok 4.0610 (4.4003)	LR 2.000e-03
0: TRAIN [0][3790/7762]	Time 0.364 (0.323)	Data 1.03e-04 (2.14e-04)	Tok/s 46312 (43398)	Loss/tok 3.6551 (4.3983)	LR 2.000e-03
0: TRAIN [0][3800/7762]	Time 0.176 (0.323)	Data 1.06e-04 (2.14e-04)	Tok/s 29310 (43390)	Loss/tok 2.9285 (4.3965)	LR 2.000e-03
0: TRAIN [0][3810/7762]	Time 0.456 (0.323)	Data 1.04e-04 (2.14e-04)	Tok/s 50850 (43385)	Loss/tok 3.9586 (4.3947)	LR 2.000e-03
0: TRAIN [0][3820/7762]	Time 0.254 (0.323)	Data 9.75e-05 (2.13e-04)	Tok/s 39980 (43388)	Loss/tok 3.4477 (4.3927)	LR 2.000e-03
0: TRAIN [0][3830/7762]	Time 0.259 (0.323)	Data 1.06e-04 (2.13e-04)	Tok/s 40027 (43390)	Loss/tok 3.3719 (4.3907)	LR 2.000e-03
0: TRAIN [0][3840/7762]	Time 0.262 (0.323)	Data 1.08e-04 (2.13e-04)	Tok/s 39077 (43391)	Loss/tok 3.3847 (4.3887)	LR 2.000e-03
0: TRAIN [0][3850/7762]	Time 0.459 (0.323)	Data 1.09e-04 (2.13e-04)	Tok/s 51008 (43390)	Loss/tok 3.8737 (4.3869)	LR 2.000e-03
0: TRAIN [0][3860/7762]	Time 0.464 (0.323)	Data 1.08e-04 (2.12e-04)	Tok/s 51000 (43399)	Loss/tok 3.8026 (4.3847)	LR 2.000e-03
0: TRAIN [0][3870/7762]	Time 0.364 (0.323)	Data 1.17e-04 (2.12e-04)	Tok/s 45852 (43406)	Loss/tok 3.6697 (4.3827)	LR 2.000e-03
0: TRAIN [0][3880/7762]	Time 0.357 (0.323)	Data 1.03e-04 (2.12e-04)	Tok/s 47270 (43401)	Loss/tok 3.6552 (4.3809)	LR 2.000e-03
0: TRAIN [0][3890/7762]	Time 0.253 (0.323)	Data 1.04e-04 (2.11e-04)	Tok/s 39905 (43392)	Loss/tok 3.3714 (4.3791)	LR 2.000e-03
0: TRAIN [0][3900/7762]	Time 0.258 (0.323)	Data 1.05e-04 (2.11e-04)	Tok/s 40144 (43385)	Loss/tok 3.3675 (4.3775)	LR 2.000e-03
0: TRAIN [0][3910/7762]	Time 0.260 (0.323)	Data 1.05e-04 (2.11e-04)	Tok/s 40089 (43390)	Loss/tok 3.3800 (4.3756)	LR 2.000e-03
0: TRAIN [0][3920/7762]	Time 0.264 (0.323)	Data 1.01e-04 (2.11e-04)	Tok/s 38948 (43380)	Loss/tok 3.5021 (4.3739)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3930/7762]	Time 0.583 (0.323)	Data 1.02e-04 (2.10e-04)	Tok/s 51420 (43378)	Loss/tok 3.9706 (4.3721)	LR 2.000e-03
0: TRAIN [0][3940/7762]	Time 0.254 (0.323)	Data 1.05e-04 (2.10e-04)	Tok/s 40737 (43373)	Loss/tok 3.5174 (4.3704)	LR 2.000e-03
0: TRAIN [0][3950/7762]	Time 0.446 (0.323)	Data 1.07e-04 (2.10e-04)	Tok/s 52426 (43378)	Loss/tok 3.7764 (4.3685)	LR 2.000e-03
0: TRAIN [0][3960/7762]	Time 0.254 (0.323)	Data 1.05e-04 (2.10e-04)	Tok/s 40337 (43374)	Loss/tok 3.3293 (4.3667)	LR 2.000e-03
0: TRAIN [0][3970/7762]	Time 0.264 (0.323)	Data 1.00e-04 (2.09e-04)	Tok/s 38598 (43366)	Loss/tok 3.5377 (4.3649)	LR 2.000e-03
0: TRAIN [0][3980/7762]	Time 0.259 (0.323)	Data 1.04e-04 (2.09e-04)	Tok/s 40167 (43368)	Loss/tok 3.4072 (4.3631)	LR 2.000e-03
0: TRAIN [0][3990/7762]	Time 0.359 (0.323)	Data 1.03e-04 (2.09e-04)	Tok/s 46865 (43362)	Loss/tok 3.6510 (4.3613)	LR 2.000e-03
0: TRAIN [0][4000/7762]	Time 0.363 (0.323)	Data 1.05e-04 (2.09e-04)	Tok/s 46280 (43370)	Loss/tok 3.6268 (4.3593)	LR 2.000e-03
0: TRAIN [0][4010/7762]	Time 0.265 (0.323)	Data 1.01e-04 (2.08e-04)	Tok/s 40260 (43370)	Loss/tok 3.3324 (4.3575)	LR 2.000e-03
0: TRAIN [0][4020/7762]	Time 0.460 (0.323)	Data 1.05e-04 (2.08e-04)	Tok/s 50548 (43372)	Loss/tok 3.8260 (4.3558)	LR 2.000e-03
0: TRAIN [0][4030/7762]	Time 0.180 (0.323)	Data 1.02e-04 (2.08e-04)	Tok/s 29614 (43373)	Loss/tok 2.8382 (4.3539)	LR 2.000e-03
0: TRAIN [0][4040/7762]	Time 0.179 (0.323)	Data 1.23e-04 (2.08e-04)	Tok/s 29551 (43372)	Loss/tok 2.8216 (4.3523)	LR 2.000e-03
0: TRAIN [0][4050/7762]	Time 0.261 (0.323)	Data 9.82e-05 (2.07e-04)	Tok/s 39816 (43372)	Loss/tok 3.4284 (4.3505)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][4060/7762]	Time 0.262 (0.323)	Data 1.06e-04 (2.07e-04)	Tok/s 39618 (43377)	Loss/tok 3.5305 (4.3488)	LR 2.000e-03
0: TRAIN [0][4070/7762]	Time 0.451 (0.323)	Data 1.03e-04 (2.07e-04)	Tok/s 51057 (43380)	Loss/tok 3.9349 (4.3470)	LR 2.000e-03
0: TRAIN [0][4080/7762]	Time 0.459 (0.323)	Data 1.04e-04 (2.07e-04)	Tok/s 51760 (43379)	Loss/tok 3.8006 (4.3452)	LR 2.000e-03
0: TRAIN [0][4090/7762]	Time 0.259 (0.323)	Data 1.05e-04 (2.06e-04)	Tok/s 39846 (43373)	Loss/tok 3.4277 (4.3435)	LR 2.000e-03
0: TRAIN [0][4100/7762]	Time 0.262 (0.323)	Data 1.06e-04 (2.06e-04)	Tok/s 39468 (43374)	Loss/tok 3.3023 (4.3418)	LR 2.000e-03
0: TRAIN [0][4110/7762]	Time 0.449 (0.323)	Data 1.04e-04 (2.06e-04)	Tok/s 52090 (43375)	Loss/tok 3.7691 (4.3401)	LR 2.000e-03
0: TRAIN [0][4120/7762]	Time 0.262 (0.323)	Data 1.02e-04 (2.06e-04)	Tok/s 40076 (43368)	Loss/tok 3.4289 (4.3384)	LR 2.000e-03
0: TRAIN [0][4130/7762]	Time 0.352 (0.323)	Data 1.01e-04 (2.05e-04)	Tok/s 47335 (43365)	Loss/tok 3.6802 (4.3368)	LR 2.000e-03
0: TRAIN [0][4140/7762]	Time 0.259 (0.323)	Data 1.05e-04 (2.05e-04)	Tok/s 39862 (43366)	Loss/tok 3.4841 (4.3351)	LR 2.000e-03
0: TRAIN [0][4150/7762]	Time 0.340 (0.323)	Data 1.03e-04 (2.05e-04)	Tok/s 48743 (43359)	Loss/tok 3.6161 (4.3335)	LR 2.000e-03
0: TRAIN [0][4160/7762]	Time 0.175 (0.323)	Data 1.05e-04 (2.05e-04)	Tok/s 30669 (43366)	Loss/tok 3.0025 (4.3318)	LR 2.000e-03
0: TRAIN [0][4170/7762]	Time 0.341 (0.323)	Data 1.08e-04 (2.04e-04)	Tok/s 49404 (43368)	Loss/tok 3.6163 (4.3302)	LR 2.000e-03
0: TRAIN [0][4180/7762]	Time 0.172 (0.323)	Data 1.22e-04 (2.04e-04)	Tok/s 29977 (43360)	Loss/tok 2.8185 (4.3285)	LR 2.000e-03
0: TRAIN [0][4190/7762]	Time 0.577 (0.323)	Data 1.03e-04 (2.04e-04)	Tok/s 51648 (43358)	Loss/tok 3.9939 (4.3270)	LR 2.000e-03
0: TRAIN [0][4200/7762]	Time 0.575 (0.323)	Data 1.05e-04 (2.04e-04)	Tok/s 52297 (43363)	Loss/tok 3.9777 (4.3252)	LR 2.000e-03
0: TRAIN [0][4210/7762]	Time 0.262 (0.323)	Data 1.03e-04 (2.03e-04)	Tok/s 39440 (43363)	Loss/tok 3.3101 (4.3235)	LR 2.000e-03
0: TRAIN [0][4220/7762]	Time 0.267 (0.323)	Data 1.05e-04 (2.03e-04)	Tok/s 39009 (43360)	Loss/tok 3.3775 (4.3219)	LR 2.000e-03
0: TRAIN [0][4230/7762]	Time 0.266 (0.323)	Data 1.03e-04 (2.03e-04)	Tok/s 38531 (43356)	Loss/tok 3.4759 (4.3204)	LR 2.000e-03
0: TRAIN [0][4240/7762]	Time 0.460 (0.323)	Data 9.94e-05 (2.03e-04)	Tok/s 51301 (43355)	Loss/tok 3.7574 (4.3187)	LR 2.000e-03
0: TRAIN [0][4250/7762]	Time 0.364 (0.323)	Data 1.02e-04 (2.03e-04)	Tok/s 46180 (43353)	Loss/tok 3.7163 (4.3170)	LR 2.000e-03
0: TRAIN [0][4260/7762]	Time 0.268 (0.322)	Data 1.03e-04 (2.02e-04)	Tok/s 38949 (43340)	Loss/tok 3.5478 (4.3156)	LR 2.000e-03
0: TRAIN [0][4270/7762]	Time 0.257 (0.322)	Data 1.05e-04 (2.02e-04)	Tok/s 40088 (43334)	Loss/tok 3.3983 (4.3140)	LR 2.000e-03
0: TRAIN [0][4280/7762]	Time 0.262 (0.322)	Data 1.02e-04 (2.02e-04)	Tok/s 39507 (43333)	Loss/tok 3.3457 (4.3124)	LR 2.000e-03
0: TRAIN [0][4290/7762]	Time 0.582 (0.322)	Data 1.01e-04 (2.02e-04)	Tok/s 51615 (43336)	Loss/tok 3.9143 (4.3106)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][4300/7762]	Time 0.451 (0.322)	Data 1.08e-04 (2.01e-04)	Tok/s 51918 (43333)	Loss/tok 3.8404 (4.3091)	LR 2.000e-03
0: TRAIN [0][4310/7762]	Time 0.262 (0.322)	Data 1.62e-04 (2.01e-04)	Tok/s 39610 (43336)	Loss/tok 3.3479 (4.3075)	LR 2.000e-03
0: TRAIN [0][4320/7762]	Time 0.354 (0.322)	Data 1.04e-04 (2.01e-04)	Tok/s 47480 (43339)	Loss/tok 3.5703 (4.3059)	LR 2.000e-03
0: TRAIN [0][4330/7762]	Time 0.265 (0.322)	Data 1.01e-04 (2.01e-04)	Tok/s 38910 (43333)	Loss/tok 3.2438 (4.3044)	LR 2.000e-03
0: TRAIN [0][4340/7762]	Time 0.356 (0.322)	Data 1.03e-04 (2.00e-04)	Tok/s 47289 (43331)	Loss/tok 3.6993 (4.3027)	LR 2.000e-03
0: TRAIN [0][4350/7762]	Time 0.179 (0.322)	Data 1.04e-04 (2.00e-04)	Tok/s 29319 (43327)	Loss/tok 2.8158 (4.3012)	LR 2.000e-03
0: TRAIN [0][4360/7762]	Time 0.352 (0.322)	Data 1.22e-04 (2.00e-04)	Tok/s 48583 (43331)	Loss/tok 3.6084 (4.2996)	LR 2.000e-03
0: TRAIN [0][4370/7762]	Time 0.260 (0.322)	Data 1.02e-04 (2.00e-04)	Tok/s 40262 (43327)	Loss/tok 3.4410 (4.2980)	LR 2.000e-03
0: TRAIN [0][4380/7762]	Time 0.264 (0.322)	Data 1.29e-04 (2.00e-04)	Tok/s 38670 (43324)	Loss/tok 3.3347 (4.2964)	LR 2.000e-03
0: TRAIN [0][4390/7762]	Time 0.458 (0.322)	Data 1.12e-04 (1.99e-04)	Tok/s 50382 (43323)	Loss/tok 3.7428 (4.2949)	LR 2.000e-03
0: TRAIN [0][4400/7762]	Time 0.453 (0.322)	Data 1.05e-04 (1.99e-04)	Tok/s 51263 (43322)	Loss/tok 3.8292 (4.2933)	LR 2.000e-03
0: TRAIN [0][4410/7762]	Time 0.358 (0.322)	Data 1.06e-04 (1.99e-04)	Tok/s 46692 (43319)	Loss/tok 3.6878 (4.2918)	LR 2.000e-03
0: TRAIN [0][4420/7762]	Time 0.264 (0.322)	Data 1.04e-04 (1.99e-04)	Tok/s 39460 (43323)	Loss/tok 3.4609 (4.2901)	LR 2.000e-03
0: TRAIN [0][4430/7762]	Time 0.362 (0.322)	Data 1.08e-04 (1.99e-04)	Tok/s 46392 (43321)	Loss/tok 3.6590 (4.2887)	LR 2.000e-03
0: TRAIN [0][4440/7762]	Time 0.177 (0.322)	Data 1.02e-04 (1.98e-04)	Tok/s 30274 (43310)	Loss/tok 2.8250 (4.2873)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][4450/7762]	Time 0.583 (0.322)	Data 1.05e-04 (1.98e-04)	Tok/s 50454 (43318)	Loss/tok 4.1138 (4.2857)	LR 2.000e-03
0: TRAIN [0][4460/7762]	Time 0.260 (0.322)	Data 9.99e-05 (1.98e-04)	Tok/s 39489 (43314)	Loss/tok 3.3703 (4.2842)	LR 2.000e-03
0: TRAIN [0][4470/7762]	Time 0.175 (0.322)	Data 1.07e-04 (1.98e-04)	Tok/s 29962 (43313)	Loss/tok 2.8025 (4.2826)	LR 2.000e-03
0: TRAIN [0][4480/7762]	Time 0.352 (0.322)	Data 1.08e-04 (1.98e-04)	Tok/s 47794 (43314)	Loss/tok 3.5571 (4.2811)	LR 2.000e-03
0: TRAIN [0][4490/7762]	Time 0.584 (0.322)	Data 1.07e-04 (1.97e-04)	Tok/s 51682 (43320)	Loss/tok 3.9299 (4.2795)	LR 2.000e-03
0: TRAIN [0][4500/7762]	Time 0.459 (0.322)	Data 1.28e-04 (1.97e-04)	Tok/s 50598 (43315)	Loss/tok 3.9182 (4.2782)	LR 2.000e-03
0: TRAIN [0][4510/7762]	Time 0.259 (0.322)	Data 1.18e-04 (1.97e-04)	Tok/s 39420 (43313)	Loss/tok 3.4627 (4.2769)	LR 2.000e-03
0: TRAIN [0][4520/7762]	Time 0.253 (0.322)	Data 1.06e-04 (1.97e-04)	Tok/s 39956 (43311)	Loss/tok 3.4214 (4.2755)	LR 2.000e-03
0: TRAIN [0][4530/7762]	Time 0.174 (0.322)	Data 1.05e-04 (1.97e-04)	Tok/s 30362 (43315)	Loss/tok 3.0690 (4.2742)	LR 2.000e-03
0: TRAIN [0][4540/7762]	Time 0.363 (0.322)	Data 1.03e-04 (1.96e-04)	Tok/s 46802 (43316)	Loss/tok 3.7440 (4.2727)	LR 2.000e-03
0: TRAIN [0][4550/7762]	Time 0.256 (0.322)	Data 1.06e-04 (1.96e-04)	Tok/s 40056 (43319)	Loss/tok 3.2625 (4.2712)	LR 2.000e-03
0: TRAIN [0][4560/7762]	Time 0.261 (0.322)	Data 1.04e-04 (1.96e-04)	Tok/s 39589 (43312)	Loss/tok 3.3567 (4.2698)	LR 2.000e-03
0: TRAIN [0][4570/7762]	Time 0.260 (0.322)	Data 1.04e-04 (1.96e-04)	Tok/s 39118 (43311)	Loss/tok 3.2858 (4.2683)	LR 2.000e-03
0: TRAIN [0][4580/7762]	Time 0.587 (0.322)	Data 1.05e-04 (1.96e-04)	Tok/s 50513 (43306)	Loss/tok 3.9606 (4.2669)	LR 2.000e-03
0: TRAIN [0][4590/7762]	Time 0.355 (0.323)	Data 1.06e-04 (1.95e-04)	Tok/s 47215 (43316)	Loss/tok 3.4759 (4.2652)	LR 2.000e-03
0: TRAIN [0][4600/7762]	Time 0.348 (0.323)	Data 1.07e-04 (1.95e-04)	Tok/s 49093 (43322)	Loss/tok 3.5442 (4.2636)	LR 2.000e-03
0: TRAIN [0][4610/7762]	Time 0.366 (0.323)	Data 1.05e-04 (1.95e-04)	Tok/s 46145 (43320)	Loss/tok 3.7017 (4.2622)	LR 2.000e-03
0: TRAIN [0][4620/7762]	Time 0.260 (0.323)	Data 1.08e-04 (1.95e-04)	Tok/s 39773 (43314)	Loss/tok 3.3380 (4.2609)	LR 2.000e-03
0: TRAIN [0][4630/7762]	Time 0.264 (0.323)	Data 1.06e-04 (1.95e-04)	Tok/s 39951 (43315)	Loss/tok 3.3397 (4.2595)	LR 2.000e-03
0: TRAIN [0][4640/7762]	Time 0.268 (0.323)	Data 1.07e-04 (1.94e-04)	Tok/s 38835 (43314)	Loss/tok 3.1767 (4.2580)	LR 2.000e-03
0: TRAIN [0][4650/7762]	Time 0.462 (0.323)	Data 1.07e-04 (1.94e-04)	Tok/s 50300 (43319)	Loss/tok 3.7829 (4.2565)	LR 2.000e-03
0: TRAIN [0][4660/7762]	Time 0.265 (0.323)	Data 1.03e-04 (1.94e-04)	Tok/s 39059 (43317)	Loss/tok 3.3704 (4.2552)	LR 2.000e-03
0: TRAIN [0][4670/7762]	Time 0.264 (0.323)	Data 1.03e-04 (1.94e-04)	Tok/s 39010 (43316)	Loss/tok 3.3674 (4.2537)	LR 2.000e-03
0: TRAIN [0][4680/7762]	Time 0.266 (0.323)	Data 1.03e-04 (1.94e-04)	Tok/s 37952 (43312)	Loss/tok 3.3468 (4.2524)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][4690/7762]	Time 0.361 (0.323)	Data 1.05e-04 (1.93e-04)	Tok/s 45677 (43314)	Loss/tok 3.6281 (4.2510)	LR 2.000e-03
0: TRAIN [0][4700/7762]	Time 0.175 (0.323)	Data 1.00e-04 (1.93e-04)	Tok/s 29972 (43307)	Loss/tok 2.8464 (4.2497)	LR 2.000e-03
0: TRAIN [0][4710/7762]	Time 0.357 (0.323)	Data 1.06e-04 (1.93e-04)	Tok/s 47063 (43310)	Loss/tok 3.5199 (4.2482)	LR 2.000e-03
0: TRAIN [0][4720/7762]	Time 0.177 (0.323)	Data 1.19e-04 (1.93e-04)	Tok/s 29812 (43307)	Loss/tok 2.9511 (4.2468)	LR 2.000e-03
0: TRAIN [0][4730/7762]	Time 0.261 (0.323)	Data 1.06e-04 (1.93e-04)	Tok/s 39275 (43306)	Loss/tok 3.4610 (4.2455)	LR 2.000e-03
0: TRAIN [0][4740/7762]	Time 0.261 (0.323)	Data 1.04e-04 (1.93e-04)	Tok/s 39635 (43308)	Loss/tok 3.3679 (4.2442)	LR 2.000e-03
0: TRAIN [0][4750/7762]	Time 0.260 (0.323)	Data 1.13e-04 (1.92e-04)	Tok/s 38659 (43307)	Loss/tok 3.3586 (4.2429)	LR 2.000e-03
0: TRAIN [0][4760/7762]	Time 0.264 (0.323)	Data 1.23e-04 (1.92e-04)	Tok/s 39112 (43304)	Loss/tok 3.4044 (4.2415)	LR 2.000e-03
0: TRAIN [0][4770/7762]	Time 0.173 (0.322)	Data 1.06e-04 (1.92e-04)	Tok/s 30714 (43299)	Loss/tok 2.8593 (4.2402)	LR 2.000e-03
0: TRAIN [0][4780/7762]	Time 0.266 (0.322)	Data 1.03e-04 (1.92e-04)	Tok/s 38675 (43301)	Loss/tok 3.3052 (4.2388)	LR 2.000e-03
0: TRAIN [0][4790/7762]	Time 0.261 (0.323)	Data 1.08e-04 (1.92e-04)	Tok/s 39694 (43299)	Loss/tok 3.3446 (4.2375)	LR 2.000e-03
0: TRAIN [0][4800/7762]	Time 0.253 (0.322)	Data 1.04e-04 (1.91e-04)	Tok/s 41142 (43294)	Loss/tok 3.4760 (4.2364)	LR 2.000e-03
0: TRAIN [0][4810/7762]	Time 0.357 (0.322)	Data 1.06e-04 (1.91e-04)	Tok/s 47275 (43293)	Loss/tok 3.6446 (4.2351)	LR 2.000e-03
0: TRAIN [0][4820/7762]	Time 0.343 (0.322)	Data 9.97e-05 (1.91e-04)	Tok/s 48974 (43297)	Loss/tok 3.6501 (4.2338)	LR 2.000e-03
0: TRAIN [0][4830/7762]	Time 0.263 (0.322)	Data 1.01e-04 (1.91e-04)	Tok/s 38985 (43289)	Loss/tok 3.4016 (4.2325)	LR 2.000e-03
0: TRAIN [0][4840/7762]	Time 0.263 (0.322)	Data 1.05e-04 (1.91e-04)	Tok/s 38939 (43293)	Loss/tok 3.4086 (4.2311)	LR 2.000e-03
0: TRAIN [0][4850/7762]	Time 0.364 (0.323)	Data 1.04e-04 (1.91e-04)	Tok/s 46240 (43301)	Loss/tok 3.5761 (4.2298)	LR 2.000e-03
0: TRAIN [0][4860/7762]	Time 0.264 (0.323)	Data 1.09e-04 (1.90e-04)	Tok/s 40136 (43300)	Loss/tok 3.3923 (4.2284)	LR 2.000e-03
0: TRAIN [0][4870/7762]	Time 0.264 (0.323)	Data 1.09e-04 (1.90e-04)	Tok/s 39183 (43298)	Loss/tok 3.4002 (4.2271)	LR 2.000e-03
0: TRAIN [0][4880/7762]	Time 0.259 (0.323)	Data 1.02e-04 (1.90e-04)	Tok/s 40243 (43298)	Loss/tok 3.3760 (4.2258)	LR 2.000e-03
0: TRAIN [0][4890/7762]	Time 0.172 (0.323)	Data 1.05e-04 (1.90e-04)	Tok/s 31256 (43299)	Loss/tok 2.7910 (4.2246)	LR 2.000e-03
0: TRAIN [0][4900/7762]	Time 0.461 (0.323)	Data 1.09e-04 (1.90e-04)	Tok/s 50706 (43296)	Loss/tok 3.8400 (4.2235)	LR 2.000e-03
0: TRAIN [0][4910/7762]	Time 0.364 (0.323)	Data 1.01e-04 (1.90e-04)	Tok/s 46267 (43296)	Loss/tok 3.5665 (4.2221)	LR 2.000e-03
0: TRAIN [0][4920/7762]	Time 0.356 (0.323)	Data 1.02e-04 (1.89e-04)	Tok/s 46873 (43292)	Loss/tok 3.5552 (4.2209)	LR 2.000e-03
0: TRAIN [0][4930/7762]	Time 0.343 (0.323)	Data 1.03e-04 (1.89e-04)	Tok/s 48888 (43289)	Loss/tok 3.5349 (4.2196)	LR 2.000e-03
0: TRAIN [0][4940/7762]	Time 0.357 (0.323)	Data 1.05e-04 (1.89e-04)	Tok/s 47006 (43290)	Loss/tok 3.5719 (4.2182)	LR 2.000e-03
0: TRAIN [0][4950/7762]	Time 0.262 (0.323)	Data 1.03e-04 (1.89e-04)	Tok/s 40086 (43287)	Loss/tok 3.4477 (4.2170)	LR 2.000e-03
0: TRAIN [0][4960/7762]	Time 0.174 (0.322)	Data 1.16e-04 (1.89e-04)	Tok/s 30863 (43284)	Loss/tok 2.8350 (4.2157)	LR 2.000e-03
0: TRAIN [0][4970/7762]	Time 0.462 (0.322)	Data 1.07e-04 (1.89e-04)	Tok/s 50239 (43284)	Loss/tok 3.7345 (4.2143)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [0][4980/7762]	Time 0.461 (0.323)	Data 1.22e-04 (1.88e-04)	Tok/s 50618 (43289)	Loss/tok 3.8458 (4.2130)	LR 2.000e-03
0: TRAIN [0][4990/7762]	Time 0.461 (0.323)	Data 1.01e-04 (1.88e-04)	Tok/s 50216 (43292)	Loss/tok 3.7642 (4.2117)	LR 2.000e-03
0: TRAIN [0][5000/7762]	Time 0.461 (0.323)	Data 1.19e-04 (1.88e-04)	Tok/s 50540 (43296)	Loss/tok 3.7095 (4.2104)	LR 2.000e-03
0: TRAIN [0][5010/7762]	Time 0.263 (0.323)	Data 9.85e-05 (1.88e-04)	Tok/s 39349 (43288)	Loss/tok 3.3688 (4.2093)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][5020/7762]	Time 0.174 (0.323)	Data 1.09e-04 (1.88e-04)	Tok/s 30549 (43289)	Loss/tok 2.8574 (4.2081)	LR 2.000e-03
0: TRAIN [0][5030/7762]	Time 0.255 (0.323)	Data 1.06e-04 (1.88e-04)	Tok/s 40473 (43292)	Loss/tok 3.3359 (4.2068)	LR 2.000e-03
0: TRAIN [0][5040/7762]	Time 0.363 (0.323)	Data 1.02e-04 (1.87e-04)	Tok/s 47122 (43291)	Loss/tok 3.5944 (4.2055)	LR 2.000e-03
0: TRAIN [0][5050/7762]	Time 0.260 (0.323)	Data 1.05e-04 (1.87e-04)	Tok/s 39855 (43291)	Loss/tok 3.3453 (4.2043)	LR 2.000e-03
0: TRAIN [0][5060/7762]	Time 0.445 (0.323)	Data 1.02e-04 (1.87e-04)	Tok/s 52011 (43292)	Loss/tok 3.8801 (4.2031)	LR 2.000e-03
0: TRAIN [0][5070/7762]	Time 0.269 (0.323)	Data 1.03e-04 (1.87e-04)	Tok/s 37684 (43291)	Loss/tok 3.4928 (4.2019)	LR 2.000e-03
0: TRAIN [0][5080/7762]	Time 0.450 (0.323)	Data 1.19e-04 (1.87e-04)	Tok/s 51793 (43296)	Loss/tok 3.8371 (4.2007)	LR 2.000e-03
0: TRAIN [0][5090/7762]	Time 0.258 (0.323)	Data 1.03e-04 (1.87e-04)	Tok/s 39734 (43294)	Loss/tok 3.4946 (4.1995)	LR 2.000e-03
0: TRAIN [0][5100/7762]	Time 0.346 (0.323)	Data 1.04e-04 (1.86e-04)	Tok/s 48287 (43299)	Loss/tok 3.4800 (4.1982)	LR 2.000e-03
0: TRAIN [0][5110/7762]	Time 0.574 (0.323)	Data 1.07e-04 (1.86e-04)	Tok/s 52485 (43304)	Loss/tok 3.7460 (4.1970)	LR 2.000e-03
0: TRAIN [0][5120/7762]	Time 0.466 (0.323)	Data 1.01e-04 (1.86e-04)	Tok/s 50191 (43304)	Loss/tok 3.6077 (4.1957)	LR 2.000e-03
0: TRAIN [0][5130/7762]	Time 0.584 (0.323)	Data 1.24e-04 (1.86e-04)	Tok/s 50916 (43304)	Loss/tok 3.9608 (4.1946)	LR 2.000e-03
0: TRAIN [0][5140/7762]	Time 0.459 (0.323)	Data 1.07e-04 (1.86e-04)	Tok/s 51183 (43302)	Loss/tok 3.8383 (4.1934)	LR 2.000e-03
0: TRAIN [0][5150/7762]	Time 0.265 (0.323)	Data 1.02e-04 (1.86e-04)	Tok/s 39275 (43294)	Loss/tok 3.2875 (4.1923)	LR 2.000e-03
0: TRAIN [0][5160/7762]	Time 0.357 (0.323)	Data 1.06e-04 (1.85e-04)	Tok/s 46933 (43291)	Loss/tok 3.5964 (4.1911)	LR 2.000e-03
0: TRAIN [0][5170/7762]	Time 0.446 (0.323)	Data 1.06e-04 (1.85e-04)	Tok/s 52416 (43289)	Loss/tok 3.7597 (4.1899)	LR 2.000e-03
0: TRAIN [0][5180/7762]	Time 0.363 (0.323)	Data 1.07e-04 (1.85e-04)	Tok/s 45628 (43293)	Loss/tok 3.7061 (4.1888)	LR 2.000e-03
0: TRAIN [0][5190/7762]	Time 0.174 (0.323)	Data 1.04e-04 (1.85e-04)	Tok/s 30756 (43292)	Loss/tok 2.8902 (4.1876)	LR 2.000e-03
0: TRAIN [0][5200/7762]	Time 0.365 (0.323)	Data 9.80e-05 (1.85e-04)	Tok/s 46376 (43294)	Loss/tok 3.7033 (4.1864)	LR 2.000e-03
0: TRAIN [0][5210/7762]	Time 0.355 (0.323)	Data 1.03e-04 (1.85e-04)	Tok/s 47088 (43292)	Loss/tok 3.6050 (4.1852)	LR 2.000e-03
0: TRAIN [0][5220/7762]	Time 0.362 (0.323)	Data 1.04e-04 (1.85e-04)	Tok/s 46825 (43296)	Loss/tok 3.5549 (4.1839)	LR 2.000e-03
0: TRAIN [0][5230/7762]	Time 0.173 (0.323)	Data 1.08e-04 (1.84e-04)	Tok/s 30413 (43298)	Loss/tok 2.7094 (4.1827)	LR 2.000e-03
0: TRAIN [0][5240/7762]	Time 0.353 (0.323)	Data 1.28e-04 (1.84e-04)	Tok/s 48199 (43295)	Loss/tok 3.4512 (4.1815)	LR 2.000e-03
0: TRAIN [0][5250/7762]	Time 0.457 (0.323)	Data 1.06e-04 (1.84e-04)	Tok/s 51042 (43296)	Loss/tok 3.6963 (4.1802)	LR 2.000e-03
0: TRAIN [0][5260/7762]	Time 0.266 (0.323)	Data 1.02e-04 (1.84e-04)	Tok/s 39217 (43294)	Loss/tok 3.4220 (4.1791)	LR 2.000e-03
0: TRAIN [0][5270/7762]	Time 0.459 (0.323)	Data 1.06e-04 (1.84e-04)	Tok/s 51383 (43299)	Loss/tok 3.8242 (4.1780)	LR 2.000e-03
0: TRAIN [0][5280/7762]	Time 0.354 (0.323)	Data 1.03e-04 (1.84e-04)	Tok/s 47289 (43292)	Loss/tok 3.5781 (4.1769)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [0][5290/7762]	Time 0.177 (0.323)	Data 1.04e-04 (1.84e-04)	Tok/s 30025 (43291)	Loss/tok 2.7989 (4.1757)	LR 2.000e-03
0: TRAIN [0][5300/7762]	Time 0.362 (0.323)	Data 1.02e-04 (1.83e-04)	Tok/s 45696 (43287)	Loss/tok 3.5960 (4.1747)	LR 2.000e-03
0: TRAIN [0][5310/7762]	Time 0.466 (0.323)	Data 1.06e-04 (1.83e-04)	Tok/s 49704 (43291)	Loss/tok 3.7144 (4.1735)	LR 2.000e-03
0: TRAIN [0][5320/7762]	Time 0.362 (0.323)	Data 1.06e-04 (1.83e-04)	Tok/s 46270 (43294)	Loss/tok 3.5740 (4.1723)	LR 2.000e-03
0: TRAIN [0][5330/7762]	Time 0.365 (0.323)	Data 1.23e-04 (1.83e-04)	Tok/s 45668 (43292)	Loss/tok 3.6542 (4.1712)	LR 2.000e-03
0: TRAIN [0][5340/7762]	Time 0.259 (0.323)	Data 1.00e-04 (1.83e-04)	Tok/s 39992 (43288)	Loss/tok 3.3179 (4.1701)	LR 2.000e-03
0: TRAIN [0][5350/7762]	Time 0.454 (0.323)	Data 1.08e-04 (1.83e-04)	Tok/s 50834 (43288)	Loss/tok 3.9142 (4.1690)	LR 2.000e-03
0: TRAIN [0][5360/7762]	Time 0.172 (0.323)	Data 1.03e-04 (1.83e-04)	Tok/s 30675 (43287)	Loss/tok 2.8025 (4.1679)	LR 2.000e-03
0: TRAIN [0][5370/7762]	Time 0.259 (0.323)	Data 1.04e-04 (1.82e-04)	Tok/s 40823 (43283)	Loss/tok 3.2155 (4.1668)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][5380/7762]	Time 0.176 (0.323)	Data 1.05e-04 (1.82e-04)	Tok/s 30521 (43284)	Loss/tok 2.9683 (4.1657)	LR 2.000e-03
0: TRAIN [0][5390/7762]	Time 0.177 (0.323)	Data 1.26e-04 (1.82e-04)	Tok/s 29702 (43286)	Loss/tok 2.9552 (4.1646)	LR 2.000e-03
0: TRAIN [0][5400/7762]	Time 0.357 (0.323)	Data 1.09e-04 (1.82e-04)	Tok/s 47641 (43284)	Loss/tok 3.6069 (4.1634)	LR 2.000e-03
0: TRAIN [0][5410/7762]	Time 0.257 (0.323)	Data 1.02e-04 (1.82e-04)	Tok/s 39776 (43277)	Loss/tok 3.4747 (4.1624)	LR 2.000e-03
0: TRAIN [0][5420/7762]	Time 0.352 (0.323)	Data 1.06e-04 (1.82e-04)	Tok/s 46922 (43279)	Loss/tok 3.6211 (4.1613)	LR 2.000e-03
0: TRAIN [0][5430/7762]	Time 0.263 (0.323)	Data 1.05e-04 (1.82e-04)	Tok/s 40159 (43280)	Loss/tok 3.3494 (4.1601)	LR 2.000e-03
0: TRAIN [0][5440/7762]	Time 0.175 (0.323)	Data 1.07e-04 (1.81e-04)	Tok/s 30051 (43285)	Loss/tok 2.7708 (4.1589)	LR 2.000e-03
0: TRAIN [0][5450/7762]	Time 0.179 (0.323)	Data 9.94e-05 (1.81e-04)	Tok/s 30208 (43290)	Loss/tok 2.7909 (4.1579)	LR 2.000e-03
0: TRAIN [0][5460/7762]	Time 0.456 (0.323)	Data 1.05e-04 (1.81e-04)	Tok/s 50556 (43290)	Loss/tok 3.8177 (4.1567)	LR 2.000e-03
0: TRAIN [0][5470/7762]	Time 0.264 (0.323)	Data 9.94e-05 (1.81e-04)	Tok/s 38404 (43285)	Loss/tok 3.3259 (4.1557)	LR 2.000e-03
0: TRAIN [0][5480/7762]	Time 0.460 (0.323)	Data 1.07e-04 (1.81e-04)	Tok/s 50317 (43282)	Loss/tok 3.8585 (4.1546)	LR 2.000e-03
0: TRAIN [0][5490/7762]	Time 0.267 (0.323)	Data 1.07e-04 (1.81e-04)	Tok/s 37559 (43286)	Loss/tok 3.2946 (4.1535)	LR 2.000e-03
0: TRAIN [0][5500/7762]	Time 0.566 (0.323)	Data 1.03e-04 (1.81e-04)	Tok/s 51989 (43287)	Loss/tok 3.9679 (4.1524)	LR 2.000e-03
0: TRAIN [0][5510/7762]	Time 0.349 (0.323)	Data 1.11e-04 (1.80e-04)	Tok/s 48037 (43288)	Loss/tok 3.5520 (4.1514)	LR 2.000e-03
0: TRAIN [0][5520/7762]	Time 0.461 (0.323)	Data 1.14e-04 (1.80e-04)	Tok/s 50984 (43292)	Loss/tok 3.7670 (4.1502)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][5530/7762]	Time 0.273 (0.323)	Data 1.04e-04 (1.80e-04)	Tok/s 37305 (43285)	Loss/tok 3.1282 (4.1492)	LR 2.000e-03
0: TRAIN [0][5540/7762]	Time 0.360 (0.323)	Data 1.03e-04 (1.80e-04)	Tok/s 46070 (43278)	Loss/tok 3.6262 (4.1483)	LR 2.000e-03
0: TRAIN [0][5550/7762]	Time 0.461 (0.323)	Data 1.04e-04 (1.80e-04)	Tok/s 49928 (43283)	Loss/tok 3.7581 (4.1472)	LR 2.000e-03
0: TRAIN [0][5560/7762]	Time 0.344 (0.323)	Data 1.18e-04 (1.80e-04)	Tok/s 49244 (43281)	Loss/tok 3.4962 (4.1460)	LR 2.000e-03
0: TRAIN [0][5570/7762]	Time 0.363 (0.323)	Data 1.06e-04 (1.80e-04)	Tok/s 46022 (43278)	Loss/tok 3.5480 (4.1449)	LR 2.000e-03
0: TRAIN [0][5580/7762]	Time 0.586 (0.323)	Data 1.09e-04 (1.80e-04)	Tok/s 51010 (43282)	Loss/tok 3.8834 (4.1438)	LR 2.000e-03
0: TRAIN [0][5590/7762]	Time 0.367 (0.323)	Data 1.05e-04 (1.79e-04)	Tok/s 46114 (43287)	Loss/tok 3.4444 (4.1426)	LR 2.000e-03
0: TRAIN [0][5600/7762]	Time 0.363 (0.323)	Data 1.08e-04 (1.79e-04)	Tok/s 45822 (43288)	Loss/tok 3.5510 (4.1415)	LR 2.000e-03
0: TRAIN [0][5610/7762]	Time 0.552 (0.323)	Data 1.04e-04 (1.79e-04)	Tok/s 54007 (43286)	Loss/tok 3.9362 (4.1405)	LR 2.000e-03
0: TRAIN [0][5620/7762]	Time 0.366 (0.323)	Data 1.23e-04 (1.79e-04)	Tok/s 46358 (43287)	Loss/tok 3.4263 (4.1394)	LR 2.000e-03
0: TRAIN [0][5630/7762]	Time 0.583 (0.323)	Data 1.05e-04 (1.79e-04)	Tok/s 50706 (43287)	Loss/tok 3.9189 (4.1383)	LR 2.000e-03
0: TRAIN [0][5640/7762]	Time 0.363 (0.323)	Data 1.01e-04 (1.79e-04)	Tok/s 46325 (43289)	Loss/tok 3.5073 (4.1372)	LR 2.000e-03
0: TRAIN [0][5650/7762]	Time 0.453 (0.323)	Data 1.02e-04 (1.79e-04)	Tok/s 51406 (43292)	Loss/tok 3.7062 (4.1361)	LR 2.000e-03
0: TRAIN [0][5660/7762]	Time 0.260 (0.323)	Data 1.07e-04 (1.79e-04)	Tok/s 38960 (43292)	Loss/tok 3.2527 (4.1351)	LR 2.000e-03
0: TRAIN [0][5670/7762]	Time 0.260 (0.323)	Data 1.21e-04 (1.78e-04)	Tok/s 40073 (43293)	Loss/tok 3.2838 (4.1340)	LR 2.000e-03
0: TRAIN [0][5680/7762]	Time 0.356 (0.324)	Data 1.01e-04 (1.78e-04)	Tok/s 47121 (43293)	Loss/tok 3.4141 (4.1329)	LR 2.000e-03
0: TRAIN [0][5690/7762]	Time 0.357 (0.323)	Data 1.00e-04 (1.78e-04)	Tok/s 47874 (43287)	Loss/tok 3.7151 (4.1320)	LR 2.000e-03
0: TRAIN [0][5700/7762]	Time 0.442 (0.324)	Data 1.02e-04 (1.78e-04)	Tok/s 53343 (43291)	Loss/tok 3.6313 (4.1309)	LR 2.000e-03
0: TRAIN [0][5710/7762]	Time 0.175 (0.323)	Data 1.05e-04 (1.78e-04)	Tok/s 29797 (43287)	Loss/tok 2.8294 (4.1299)	LR 2.000e-03
0: TRAIN [0][5720/7762]	Time 0.266 (0.323)	Data 1.03e-04 (1.78e-04)	Tok/s 38964 (43286)	Loss/tok 3.3077 (4.1288)	LR 2.000e-03
0: TRAIN [0][5730/7762]	Time 0.589 (0.323)	Data 1.04e-04 (1.78e-04)	Tok/s 51037 (43285)	Loss/tok 3.8168 (4.1278)	LR 2.000e-03
0: TRAIN [0][5740/7762]	Time 0.177 (0.323)	Data 9.87e-05 (1.77e-04)	Tok/s 29527 (43281)	Loss/tok 2.7799 (4.1267)	LR 2.000e-03
0: TRAIN [0][5750/7762]	Time 0.266 (0.323)	Data 1.05e-04 (1.77e-04)	Tok/s 39370 (43284)	Loss/tok 3.2929 (4.1256)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][5760/7762]	Time 0.262 (0.323)	Data 1.07e-04 (1.77e-04)	Tok/s 38818 (43281)	Loss/tok 3.2249 (4.1246)	LR 2.000e-03
0: TRAIN [0][5770/7762]	Time 0.358 (0.323)	Data 1.05e-04 (1.77e-04)	Tok/s 46861 (43282)	Loss/tok 3.5998 (4.1237)	LR 2.000e-03
0: TRAIN [0][5780/7762]	Time 0.263 (0.323)	Data 1.04e-04 (1.77e-04)	Tok/s 38349 (43280)	Loss/tok 3.2641 (4.1227)	LR 2.000e-03
0: TRAIN [0][5790/7762]	Time 0.266 (0.323)	Data 1.05e-04 (1.77e-04)	Tok/s 39058 (43281)	Loss/tok 3.2024 (4.1217)	LR 2.000e-03
0: TRAIN [0][5800/7762]	Time 0.365 (0.323)	Data 9.82e-05 (1.77e-04)	Tok/s 46048 (43281)	Loss/tok 3.5266 (4.1207)	LR 2.000e-03
0: TRAIN [0][5810/7762]	Time 0.363 (0.323)	Data 9.99e-05 (1.77e-04)	Tok/s 45942 (43284)	Loss/tok 3.5731 (4.1197)	LR 2.000e-03
0: TRAIN [0][5820/7762]	Time 0.177 (0.323)	Data 1.02e-04 (1.76e-04)	Tok/s 30093 (43278)	Loss/tok 2.8952 (4.1187)	LR 2.000e-03
0: TRAIN [0][5830/7762]	Time 0.353 (0.323)	Data 1.03e-04 (1.76e-04)	Tok/s 47587 (43275)	Loss/tok 3.5572 (4.1178)	LR 2.000e-03
0: TRAIN [0][5840/7762]	Time 0.349 (0.323)	Data 1.02e-04 (1.76e-04)	Tok/s 47546 (43281)	Loss/tok 3.5504 (4.1166)	LR 2.000e-03
0: TRAIN [0][5850/7762]	Time 0.263 (0.323)	Data 1.15e-04 (1.76e-04)	Tok/s 38942 (43283)	Loss/tok 3.3700 (4.1156)	LR 2.000e-03
0: TRAIN [0][5860/7762]	Time 0.368 (0.324)	Data 1.33e-04 (1.76e-04)	Tok/s 45631 (43287)	Loss/tok 3.5900 (4.1146)	LR 2.000e-03
0: TRAIN [0][5870/7762]	Time 0.586 (0.324)	Data 1.05e-04 (1.76e-04)	Tok/s 51041 (43285)	Loss/tok 3.8800 (4.1137)	LR 2.000e-03
0: TRAIN [0][5880/7762]	Time 0.260 (0.324)	Data 1.03e-04 (1.76e-04)	Tok/s 39817 (43282)	Loss/tok 3.3396 (4.1127)	LR 2.000e-03
0: TRAIN [0][5890/7762]	Time 0.363 (0.324)	Data 1.06e-04 (1.76e-04)	Tok/s 45957 (43282)	Loss/tok 3.4979 (4.1118)	LR 2.000e-03
0: TRAIN [0][5900/7762]	Time 0.360 (0.324)	Data 1.06e-04 (1.76e-04)	Tok/s 47169 (43282)	Loss/tok 3.5778 (4.1108)	LR 2.000e-03
0: TRAIN [0][5910/7762]	Time 0.258 (0.324)	Data 1.05e-04 (1.75e-04)	Tok/s 39863 (43282)	Loss/tok 3.2511 (4.1099)	LR 2.000e-03
0: TRAIN [0][5920/7762]	Time 0.454 (0.324)	Data 1.03e-04 (1.75e-04)	Tok/s 51369 (43285)	Loss/tok 3.8366 (4.1089)	LR 2.000e-03
0: TRAIN [0][5930/7762]	Time 0.460 (0.324)	Data 1.02e-04 (1.75e-04)	Tok/s 50400 (43286)	Loss/tok 3.7447 (4.1079)	LR 2.000e-03
0: TRAIN [0][5940/7762]	Time 0.561 (0.324)	Data 1.05e-04 (1.75e-04)	Tok/s 53971 (43288)	Loss/tok 3.7663 (4.1070)	LR 2.000e-03
0: TRAIN [0][5950/7762]	Time 0.364 (0.324)	Data 1.05e-04 (1.75e-04)	Tok/s 46666 (43286)	Loss/tok 3.6283 (4.1061)	LR 2.000e-03
0: TRAIN [0][5960/7762]	Time 0.364 (0.324)	Data 1.01e-04 (1.75e-04)	Tok/s 46415 (43289)	Loss/tok 3.5943 (4.1051)	LR 2.000e-03
0: TRAIN [0][5970/7762]	Time 0.177 (0.324)	Data 1.03e-04 (1.75e-04)	Tok/s 29383 (43288)	Loss/tok 2.7660 (4.1042)	LR 2.000e-03
0: TRAIN [0][5980/7762]	Time 0.462 (0.324)	Data 1.03e-04 (1.75e-04)	Tok/s 50921 (43292)	Loss/tok 3.6860 (4.1032)	LR 2.000e-03
0: TRAIN [0][5990/7762]	Time 0.364 (0.324)	Data 1.05e-04 (1.74e-04)	Tok/s 46752 (43295)	Loss/tok 3.5793 (4.1022)	LR 2.000e-03
0: TRAIN [0][6000/7762]	Time 0.269 (0.324)	Data 1.01e-04 (1.74e-04)	Tok/s 37246 (43300)	Loss/tok 3.3700 (4.1013)	LR 2.000e-03
0: TRAIN [0][6010/7762]	Time 0.177 (0.324)	Data 1.11e-04 (1.74e-04)	Tok/s 30350 (43299)	Loss/tok 2.8482 (4.1004)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [0][6020/7762]	Time 0.264 (0.324)	Data 1.02e-04 (1.74e-04)	Tok/s 38859 (43297)	Loss/tok 3.2772 (4.0994)	LR 2.000e-03
0: TRAIN [0][6030/7762]	Time 0.455 (0.324)	Data 1.07e-04 (1.74e-04)	Tok/s 51395 (43300)	Loss/tok 3.6581 (4.0984)	LR 2.000e-03
0: TRAIN [0][6040/7762]	Time 0.264 (0.324)	Data 1.05e-04 (1.74e-04)	Tok/s 39525 (43298)	Loss/tok 3.2304 (4.0975)	LR 2.000e-03
0: TRAIN [0][6050/7762]	Time 0.360 (0.324)	Data 1.12e-04 (1.74e-04)	Tok/s 46566 (43296)	Loss/tok 3.6150 (4.0966)	LR 2.000e-03
0: TRAIN [0][6060/7762]	Time 0.265 (0.324)	Data 1.04e-04 (1.74e-04)	Tok/s 38648 (43296)	Loss/tok 3.3308 (4.0957)	LR 2.000e-03
0: TRAIN [0][6070/7762]	Time 0.264 (0.324)	Data 1.02e-04 (1.74e-04)	Tok/s 39224 (43297)	Loss/tok 3.4098 (4.0947)	LR 2.000e-03
0: TRAIN [0][6080/7762]	Time 0.256 (0.324)	Data 1.05e-04 (1.73e-04)	Tok/s 40208 (43291)	Loss/tok 3.3451 (4.0938)	LR 2.000e-03
0: TRAIN [0][6090/7762]	Time 0.354 (0.324)	Data 9.73e-05 (1.73e-04)	Tok/s 47244 (43287)	Loss/tok 3.5181 (4.0930)	LR 2.000e-03
0: TRAIN [0][6100/7762]	Time 0.461 (0.324)	Data 1.04e-04 (1.73e-04)	Tok/s 50750 (43285)	Loss/tok 3.6984 (4.0921)	LR 2.000e-03
0: TRAIN [0][6110/7762]	Time 0.265 (0.324)	Data 1.01e-04 (1.73e-04)	Tok/s 38327 (43287)	Loss/tok 3.3977 (4.0912)	LR 2.000e-03
0: TRAIN [0][6120/7762]	Time 0.353 (0.324)	Data 1.06e-04 (1.73e-04)	Tok/s 48147 (43288)	Loss/tok 3.4978 (4.0902)	LR 2.000e-03
0: TRAIN [0][6130/7762]	Time 0.264 (0.324)	Data 1.09e-04 (1.73e-04)	Tok/s 39683 (43286)	Loss/tok 3.2056 (4.0892)	LR 2.000e-03
0: TRAIN [0][6140/7762]	Time 0.454 (0.324)	Data 1.04e-04 (1.73e-04)	Tok/s 50894 (43285)	Loss/tok 3.7463 (4.0884)	LR 2.000e-03
0: TRAIN [0][6150/7762]	Time 0.175 (0.324)	Data 1.03e-04 (1.73e-04)	Tok/s 30388 (43283)	Loss/tok 2.7526 (4.0874)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][6160/7762]	Time 0.266 (0.324)	Data 1.03e-04 (1.73e-04)	Tok/s 38232 (43286)	Loss/tok 3.3327 (4.0867)	LR 2.000e-03
0: TRAIN [0][6170/7762]	Time 0.355 (0.324)	Data 1.11e-04 (1.72e-04)	Tok/s 47327 (43289)	Loss/tok 3.4794 (4.0857)	LR 2.000e-03
0: TRAIN [0][6180/7762]	Time 0.362 (0.324)	Data 1.06e-04 (1.72e-04)	Tok/s 46421 (43292)	Loss/tok 3.5242 (4.0848)	LR 2.000e-03
0: TRAIN [0][6190/7762]	Time 0.259 (0.324)	Data 1.03e-04 (1.72e-04)	Tok/s 40710 (43290)	Loss/tok 3.1689 (4.0839)	LR 2.000e-03
0: TRAIN [0][6200/7762]	Time 0.268 (0.324)	Data 1.02e-04 (1.72e-04)	Tok/s 38336 (43289)	Loss/tok 3.2164 (4.0830)	LR 2.000e-03
0: TRAIN [0][6210/7762]	Time 0.364 (0.324)	Data 1.06e-04 (1.72e-04)	Tok/s 45893 (43290)	Loss/tok 3.5651 (4.0821)	LR 2.000e-03
0: TRAIN [0][6220/7762]	Time 0.263 (0.324)	Data 1.06e-04 (1.72e-04)	Tok/s 39844 (43290)	Loss/tok 3.2874 (4.0811)	LR 2.000e-03
0: TRAIN [0][6230/7762]	Time 0.346 (0.324)	Data 1.04e-04 (1.72e-04)	Tok/s 48803 (43287)	Loss/tok 3.4761 (4.0802)	LR 2.000e-03
0: TRAIN [0][6240/7762]	Time 0.257 (0.324)	Data 1.05e-04 (1.72e-04)	Tok/s 39601 (43281)	Loss/tok 3.3516 (4.0794)	LR 2.000e-03
0: TRAIN [0][6250/7762]	Time 0.268 (0.324)	Data 1.05e-04 (1.72e-04)	Tok/s 38915 (43284)	Loss/tok 3.3706 (4.0785)	LR 2.000e-03
0: TRAIN [0][6260/7762]	Time 0.464 (0.324)	Data 1.03e-04 (1.71e-04)	Tok/s 49422 (43286)	Loss/tok 3.6351 (4.0776)	LR 2.000e-03
0: TRAIN [0][6270/7762]	Time 0.256 (0.324)	Data 9.78e-05 (1.71e-04)	Tok/s 41272 (43285)	Loss/tok 3.3678 (4.0768)	LR 2.000e-03
0: TRAIN [0][6280/7762]	Time 0.251 (0.324)	Data 9.82e-05 (1.71e-04)	Tok/s 41081 (43282)	Loss/tok 3.3120 (4.0760)	LR 2.000e-03
0: TRAIN [0][6290/7762]	Time 0.258 (0.324)	Data 1.03e-04 (1.71e-04)	Tok/s 40766 (43280)	Loss/tok 3.2305 (4.0751)	LR 2.000e-03
0: TRAIN [0][6300/7762]	Time 0.258 (0.324)	Data 1.01e-04 (1.71e-04)	Tok/s 40052 (43284)	Loss/tok 3.2430 (4.0743)	LR 2.000e-03
0: TRAIN [0][6310/7762]	Time 0.369 (0.324)	Data 1.03e-04 (1.71e-04)	Tok/s 46372 (43287)	Loss/tok 3.3861 (4.0733)	LR 2.000e-03
0: TRAIN [0][6320/7762]	Time 0.361 (0.324)	Data 1.03e-04 (1.71e-04)	Tok/s 47215 (43287)	Loss/tok 3.5613 (4.0723)	LR 2.000e-03
0: TRAIN [0][6330/7762]	Time 0.367 (0.324)	Data 1.19e-04 (1.71e-04)	Tok/s 45885 (43288)	Loss/tok 3.5696 (4.0714)	LR 2.000e-03
0: TRAIN [0][6340/7762]	Time 0.173 (0.324)	Data 1.08e-04 (1.71e-04)	Tok/s 30621 (43284)	Loss/tok 2.8067 (4.0705)	LR 2.000e-03
0: TRAIN [0][6350/7762]	Time 0.364 (0.324)	Data 9.97e-05 (1.71e-04)	Tok/s 46424 (43279)	Loss/tok 3.5205 (4.0696)	LR 2.000e-03
0: TRAIN [0][6360/7762]	Time 0.259 (0.324)	Data 1.19e-04 (1.70e-04)	Tok/s 40018 (43282)	Loss/tok 3.2784 (4.0687)	LR 2.000e-03
0: TRAIN [0][6370/7762]	Time 0.354 (0.324)	Data 1.05e-04 (1.70e-04)	Tok/s 46878 (43287)	Loss/tok 3.6067 (4.0679)	LR 2.000e-03
0: TRAIN [0][6380/7762]	Time 0.175 (0.324)	Data 1.22e-04 (1.70e-04)	Tok/s 30189 (43284)	Loss/tok 2.8052 (4.0671)	LR 2.000e-03
0: TRAIN [0][6390/7762]	Time 0.252 (0.324)	Data 1.02e-04 (1.70e-04)	Tok/s 40591 (43282)	Loss/tok 3.3531 (4.0662)	LR 2.000e-03
0: TRAIN [0][6400/7762]	Time 0.366 (0.324)	Data 1.08e-04 (1.70e-04)	Tok/s 46090 (43286)	Loss/tok 3.5627 (4.0653)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][6410/7762]	Time 0.175 (0.324)	Data 1.21e-04 (1.70e-04)	Tok/s 30533 (43285)	Loss/tok 2.7783 (4.0645)	LR 2.000e-03
0: TRAIN [0][6420/7762]	Time 0.460 (0.324)	Data 1.05e-04 (1.70e-04)	Tok/s 50594 (43286)	Loss/tok 3.6317 (4.0636)	LR 2.000e-03
0: TRAIN [0][6430/7762]	Time 0.359 (0.324)	Data 1.05e-04 (1.70e-04)	Tok/s 46448 (43286)	Loss/tok 3.5282 (4.0627)	LR 2.000e-03
0: TRAIN [0][6440/7762]	Time 0.583 (0.324)	Data 1.03e-04 (1.70e-04)	Tok/s 51248 (43290)	Loss/tok 3.9746 (4.0618)	LR 2.000e-03
0: TRAIN [0][6450/7762]	Time 0.363 (0.324)	Data 1.02e-04 (1.70e-04)	Tok/s 46442 (43294)	Loss/tok 3.4829 (4.0610)	LR 2.000e-03
0: TRAIN [0][6460/7762]	Time 0.577 (0.324)	Data 1.06e-04 (1.70e-04)	Tok/s 51086 (43296)	Loss/tok 3.8186 (4.0601)	LR 2.000e-03
0: TRAIN [0][6470/7762]	Time 0.177 (0.324)	Data 1.01e-04 (1.69e-04)	Tok/s 30033 (43295)	Loss/tok 2.7730 (4.0594)	LR 2.000e-03
0: TRAIN [0][6480/7762]	Time 0.177 (0.324)	Data 1.05e-04 (1.69e-04)	Tok/s 29722 (43298)	Loss/tok 2.8555 (4.0586)	LR 2.000e-03
0: TRAIN [0][6490/7762]	Time 0.589 (0.324)	Data 1.02e-04 (1.69e-04)	Tok/s 49733 (43298)	Loss/tok 4.0258 (4.0578)	LR 1.000e-03
0: TRAIN [0][6500/7762]	Time 0.573 (0.324)	Data 1.29e-04 (1.69e-04)	Tok/s 52268 (43301)	Loss/tok 3.8415 (4.0570)	LR 1.000e-03
0: TRAIN [0][6510/7762]	Time 0.264 (0.324)	Data 1.04e-04 (1.69e-04)	Tok/s 39242 (43302)	Loss/tok 3.2984 (4.0562)	LR 1.000e-03
0: TRAIN [0][6520/7762]	Time 0.367 (0.324)	Data 1.02e-04 (1.69e-04)	Tok/s 45852 (43303)	Loss/tok 3.3989 (4.0553)	LR 1.000e-03
0: TRAIN [0][6530/7762]	Time 0.585 (0.324)	Data 1.03e-04 (1.69e-04)	Tok/s 51210 (43298)	Loss/tok 3.9706 (4.0545)	LR 1.000e-03
0: TRAIN [0][6540/7762]	Time 0.172 (0.324)	Data 1.01e-04 (1.69e-04)	Tok/s 31188 (43293)	Loss/tok 2.8063 (4.0537)	LR 1.000e-03
0: TRAIN [0][6550/7762]	Time 0.444 (0.324)	Data 1.04e-04 (1.69e-04)	Tok/s 53102 (43292)	Loss/tok 3.7833 (4.0529)	LR 1.000e-03
0: TRAIN [0][6560/7762]	Time 0.259 (0.324)	Data 1.05e-04 (1.69e-04)	Tok/s 39292 (43284)	Loss/tok 3.3056 (4.0521)	LR 1.000e-03
0: TRAIN [0][6570/7762]	Time 0.357 (0.324)	Data 1.04e-04 (1.68e-04)	Tok/s 46998 (43287)	Loss/tok 3.5550 (4.0512)	LR 1.000e-03
0: TRAIN [0][6580/7762]	Time 0.587 (0.324)	Data 1.02e-04 (1.68e-04)	Tok/s 50670 (43282)	Loss/tok 3.8541 (4.0504)	LR 1.000e-03
0: TRAIN [0][6590/7762]	Time 0.461 (0.324)	Data 1.06e-04 (1.68e-04)	Tok/s 50937 (43286)	Loss/tok 3.6025 (4.0495)	LR 1.000e-03
0: TRAIN [0][6600/7762]	Time 0.255 (0.324)	Data 9.42e-05 (1.68e-04)	Tok/s 41247 (43282)	Loss/tok 3.2056 (4.0487)	LR 1.000e-03
0: TRAIN [0][6610/7762]	Time 0.169 (0.324)	Data 1.03e-04 (1.68e-04)	Tok/s 31372 (43279)	Loss/tok 2.8343 (4.0478)	LR 1.000e-03
0: TRAIN [0][6620/7762]	Time 0.585 (0.324)	Data 1.07e-04 (1.68e-04)	Tok/s 50845 (43281)	Loss/tok 3.8031 (4.0469)	LR 1.000e-03
0: TRAIN [0][6630/7762]	Time 0.463 (0.324)	Data 1.04e-04 (1.68e-04)	Tok/s 50560 (43279)	Loss/tok 3.6280 (4.0461)	LR 1.000e-03
0: TRAIN [0][6640/7762]	Time 0.364 (0.324)	Data 1.04e-04 (1.68e-04)	Tok/s 46830 (43283)	Loss/tok 3.4188 (4.0451)	LR 1.000e-03
0: TRAIN [0][6650/7762]	Time 0.586 (0.324)	Data 1.05e-04 (1.68e-04)	Tok/s 51315 (43286)	Loss/tok 3.6576 (4.0442)	LR 1.000e-03
0: TRAIN [0][6660/7762]	Time 0.575 (0.324)	Data 1.04e-04 (1.68e-04)	Tok/s 51220 (43281)	Loss/tok 3.9024 (4.0434)	LR 1.000e-03
0: TRAIN [0][6670/7762]	Time 0.264 (0.324)	Data 1.02e-04 (1.68e-04)	Tok/s 38684 (43282)	Loss/tok 3.2974 (4.0425)	LR 1.000e-03
0: TRAIN [0][6680/7762]	Time 0.462 (0.324)	Data 1.16e-04 (1.68e-04)	Tok/s 50550 (43283)	Loss/tok 3.6681 (4.0416)	LR 1.000e-03
0: TRAIN [0][6690/7762]	Time 0.365 (0.324)	Data 1.01e-04 (1.67e-04)	Tok/s 46371 (43280)	Loss/tok 3.3769 (4.0407)	LR 1.000e-03
0: TRAIN [0][6700/7762]	Time 0.176 (0.324)	Data 1.06e-04 (1.67e-04)	Tok/s 29962 (43279)	Loss/tok 2.8176 (4.0399)	LR 1.000e-03
0: TRAIN [0][6710/7762]	Time 0.354 (0.324)	Data 1.06e-04 (1.67e-04)	Tok/s 47505 (43279)	Loss/tok 3.4168 (4.0391)	LR 1.000e-03
0: TRAIN [0][6720/7762]	Time 0.464 (0.324)	Data 1.14e-04 (1.67e-04)	Tok/s 50895 (43277)	Loss/tok 3.6405 (4.0382)	LR 1.000e-03
0: TRAIN [0][6730/7762]	Time 0.359 (0.324)	Data 1.09e-04 (1.67e-04)	Tok/s 46475 (43280)	Loss/tok 3.5553 (4.0373)	LR 1.000e-03
0: TRAIN [0][6740/7762]	Time 0.257 (0.324)	Data 1.04e-04 (1.67e-04)	Tok/s 40791 (43277)	Loss/tok 3.2113 (4.0365)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [0][6750/7762]	Time 0.247 (0.324)	Data 1.08e-04 (1.67e-04)	Tok/s 42333 (43281)	Loss/tok 3.3162 (4.0356)	LR 1.000e-03
0: TRAIN [0][6760/7762]	Time 0.260 (0.324)	Data 1.16e-04 (1.67e-04)	Tok/s 38839 (43282)	Loss/tok 3.2769 (4.0346)	LR 1.000e-03
0: TRAIN [0][6770/7762]	Time 0.262 (0.324)	Data 1.03e-04 (1.67e-04)	Tok/s 40731 (43283)	Loss/tok 3.2902 (4.0337)	LR 1.000e-03
0: TRAIN [0][6780/7762]	Time 0.459 (0.324)	Data 1.09e-04 (1.67e-04)	Tok/s 50697 (43287)	Loss/tok 3.5836 (4.0328)	LR 1.000e-03
0: TRAIN [0][6790/7762]	Time 0.460 (0.324)	Data 1.08e-04 (1.67e-04)	Tok/s 50029 (43290)	Loss/tok 3.7824 (4.0321)	LR 1.000e-03
0: TRAIN [0][6800/7762]	Time 0.358 (0.324)	Data 1.04e-04 (1.66e-04)	Tok/s 47041 (43290)	Loss/tok 3.4364 (4.0312)	LR 1.000e-03
0: TRAIN [0][6810/7762]	Time 0.361 (0.324)	Data 1.03e-04 (1.66e-04)	Tok/s 46466 (43289)	Loss/tok 3.5056 (4.0303)	LR 1.000e-03
0: TRAIN [0][6820/7762]	Time 0.263 (0.324)	Data 1.02e-04 (1.66e-04)	Tok/s 38677 (43287)	Loss/tok 3.2527 (4.0295)	LR 1.000e-03
0: TRAIN [0][6830/7762]	Time 0.255 (0.324)	Data 1.54e-04 (1.66e-04)	Tok/s 40911 (43289)	Loss/tok 3.2355 (4.0286)	LR 1.000e-03
0: TRAIN [0][6840/7762]	Time 0.263 (0.324)	Data 1.01e-04 (1.66e-04)	Tok/s 40138 (43285)	Loss/tok 3.2376 (4.0278)	LR 1.000e-03
0: TRAIN [0][6850/7762]	Time 0.342 (0.324)	Data 1.07e-04 (1.66e-04)	Tok/s 49001 (43287)	Loss/tok 3.5411 (4.0270)	LR 1.000e-03
0: TRAIN [0][6860/7762]	Time 0.363 (0.324)	Data 1.04e-04 (1.66e-04)	Tok/s 45977 (43293)	Loss/tok 3.3790 (4.0261)	LR 1.000e-03
0: TRAIN [0][6870/7762]	Time 0.365 (0.324)	Data 1.05e-04 (1.66e-04)	Tok/s 46061 (43294)	Loss/tok 3.4529 (4.0253)	LR 1.000e-03
0: TRAIN [0][6880/7762]	Time 0.450 (0.324)	Data 1.07e-04 (1.66e-04)	Tok/s 51565 (43295)	Loss/tok 3.6445 (4.0244)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [0][6890/7762]	Time 0.357 (0.324)	Data 1.04e-04 (1.66e-04)	Tok/s 46549 (43297)	Loss/tok 3.4140 (4.0236)	LR 1.000e-03
0: TRAIN [0][6900/7762]	Time 0.576 (0.325)	Data 1.03e-04 (1.66e-04)	Tok/s 52497 (43303)	Loss/tok 3.8214 (4.0227)	LR 1.000e-03
0: TRAIN [0][6910/7762]	Time 0.260 (0.325)	Data 1.08e-04 (1.66e-04)	Tok/s 40026 (43303)	Loss/tok 3.2238 (4.0218)	LR 1.000e-03
0: TRAIN [0][6920/7762]	Time 0.363 (0.325)	Data 1.03e-04 (1.65e-04)	Tok/s 45595 (43303)	Loss/tok 3.5413 (4.0210)	LR 1.000e-03
0: TRAIN [0][6930/7762]	Time 0.360 (0.325)	Data 2.42e-04 (1.65e-04)	Tok/s 46919 (43301)	Loss/tok 3.3552 (4.0202)	LR 1.000e-03
0: TRAIN [0][6940/7762]	Time 0.251 (0.325)	Data 1.04e-04 (1.65e-04)	Tok/s 41622 (43301)	Loss/tok 3.0788 (4.0194)	LR 1.000e-03
0: TRAIN [0][6950/7762]	Time 0.364 (0.325)	Data 1.04e-04 (1.65e-04)	Tok/s 46524 (43301)	Loss/tok 3.3769 (4.0185)	LR 1.000e-03
0: TRAIN [0][6960/7762]	Time 0.265 (0.325)	Data 1.06e-04 (1.65e-04)	Tok/s 39118 (43298)	Loss/tok 3.3457 (4.0177)	LR 1.000e-03
0: TRAIN [0][6970/7762]	Time 0.364 (0.324)	Data 1.02e-04 (1.65e-04)	Tok/s 46615 (43296)	Loss/tok 3.3951 (4.0168)	LR 1.000e-03
0: TRAIN [0][6980/7762]	Time 0.361 (0.325)	Data 1.00e-04 (1.65e-04)	Tok/s 46658 (43297)	Loss/tok 3.3435 (4.0160)	LR 1.000e-03
0: TRAIN [0][6990/7762]	Time 0.345 (0.325)	Data 1.05e-04 (1.65e-04)	Tok/s 48018 (43298)	Loss/tok 3.5109 (4.0152)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][7000/7762]	Time 0.589 (0.325)	Data 1.05e-04 (1.65e-04)	Tok/s 50272 (43299)	Loss/tok 3.8019 (4.0144)	LR 1.000e-03
0: TRAIN [0][7010/7762]	Time 0.364 (0.325)	Data 1.07e-04 (1.65e-04)	Tok/s 45754 (43298)	Loss/tok 3.4398 (4.0135)	LR 1.000e-03
0: TRAIN [0][7020/7762]	Time 0.362 (0.325)	Data 1.08e-04 (1.65e-04)	Tok/s 46169 (43297)	Loss/tok 3.3977 (4.0127)	LR 1.000e-03
0: TRAIN [0][7030/7762]	Time 0.348 (0.325)	Data 1.08e-04 (1.65e-04)	Tok/s 47805 (43295)	Loss/tok 3.4951 (4.0119)	LR 1.000e-03
0: TRAIN [0][7040/7762]	Time 0.345 (0.325)	Data 1.15e-04 (1.64e-04)	Tok/s 47549 (43297)	Loss/tok 3.4411 (4.0111)	LR 1.000e-03
0: TRAIN [0][7050/7762]	Time 0.264 (0.325)	Data 1.04e-04 (1.64e-04)	Tok/s 38353 (43296)	Loss/tok 3.2211 (4.0103)	LR 1.000e-03
0: TRAIN [0][7060/7762]	Time 0.259 (0.325)	Data 1.04e-04 (1.64e-04)	Tok/s 40261 (43294)	Loss/tok 3.1346 (4.0094)	LR 1.000e-03
0: TRAIN [0][7070/7762]	Time 0.350 (0.324)	Data 1.01e-04 (1.64e-04)	Tok/s 47746 (43295)	Loss/tok 3.5223 (4.0086)	LR 1.000e-03
0: TRAIN [0][7080/7762]	Time 0.259 (0.324)	Data 2.31e-04 (1.64e-04)	Tok/s 40992 (43290)	Loss/tok 3.2703 (4.0078)	LR 1.000e-03
0: TRAIN [0][7090/7762]	Time 0.352 (0.324)	Data 1.06e-04 (1.64e-04)	Tok/s 47486 (43291)	Loss/tok 3.4610 (4.0070)	LR 1.000e-03
0: TRAIN [0][7100/7762]	Time 0.176 (0.324)	Data 1.25e-04 (1.64e-04)	Tok/s 29500 (43287)	Loss/tok 2.8127 (4.0062)	LR 1.000e-03
0: TRAIN [0][7110/7762]	Time 0.364 (0.324)	Data 1.21e-04 (1.64e-04)	Tok/s 46087 (43292)	Loss/tok 3.3950 (4.0054)	LR 1.000e-03
0: TRAIN [0][7120/7762]	Time 0.354 (0.325)	Data 1.08e-04 (1.64e-04)	Tok/s 47642 (43299)	Loss/tok 3.4615 (4.0045)	LR 1.000e-03
0: TRAIN [0][7130/7762]	Time 0.572 (0.325)	Data 1.02e-04 (1.64e-04)	Tok/s 52800 (43301)	Loss/tok 3.7489 (4.0038)	LR 1.000e-03
0: TRAIN [0][7140/7762]	Time 0.261 (0.325)	Data 1.06e-04 (1.64e-04)	Tok/s 39015 (43298)	Loss/tok 3.1645 (4.0030)	LR 1.000e-03
0: TRAIN [0][7150/7762]	Time 0.365 (0.325)	Data 1.64e-04 (1.64e-04)	Tok/s 45730 (43300)	Loss/tok 3.5013 (4.0021)	LR 1.000e-03
0: TRAIN [0][7160/7762]	Time 0.457 (0.325)	Data 1.07e-04 (1.63e-04)	Tok/s 50719 (43299)	Loss/tok 3.5187 (4.0013)	LR 1.000e-03
0: TRAIN [0][7170/7762]	Time 0.353 (0.325)	Data 1.05e-04 (1.63e-04)	Tok/s 48002 (43303)	Loss/tok 3.5786 (4.0005)	LR 1.000e-03
0: TRAIN [0][7180/7762]	Time 0.266 (0.325)	Data 1.05e-04 (1.63e-04)	Tok/s 38420 (43302)	Loss/tok 3.2983 (3.9997)	LR 1.000e-03
0: TRAIN [0][7190/7762]	Time 0.352 (0.325)	Data 1.09e-04 (1.63e-04)	Tok/s 47518 (43303)	Loss/tok 3.4533 (3.9989)	LR 1.000e-03
0: TRAIN [0][7200/7762]	Time 0.266 (0.325)	Data 1.22e-04 (1.63e-04)	Tok/s 39771 (43299)	Loss/tok 3.2057 (3.9982)	LR 1.000e-03
0: TRAIN [0][7210/7762]	Time 0.339 (0.325)	Data 1.06e-04 (1.63e-04)	Tok/s 49099 (43302)	Loss/tok 3.3918 (3.9974)	LR 1.000e-03
0: TRAIN [0][7220/7762]	Time 0.258 (0.325)	Data 1.01e-04 (1.63e-04)	Tok/s 40595 (43297)	Loss/tok 3.2110 (3.9967)	LR 1.000e-03
0: TRAIN [0][7230/7762]	Time 0.446 (0.325)	Data 1.07e-04 (1.63e-04)	Tok/s 52526 (43299)	Loss/tok 3.5840 (3.9960)	LR 1.000e-03
0: TRAIN [0][7240/7762]	Time 0.357 (0.325)	Data 1.05e-04 (1.63e-04)	Tok/s 46421 (43300)	Loss/tok 3.4756 (3.9952)	LR 1.000e-03
0: TRAIN [0][7250/7762]	Time 0.172 (0.325)	Data 1.01e-04 (1.63e-04)	Tok/s 30420 (43301)	Loss/tok 2.7320 (3.9944)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [0][7260/7762]	Time 0.466 (0.325)	Data 1.03e-04 (1.63e-04)	Tok/s 50074 (43297)	Loss/tok 3.5369 (3.9936)	LR 1.000e-03
0: TRAIN [0][7270/7762]	Time 0.450 (0.325)	Data 1.07e-04 (1.63e-04)	Tok/s 51765 (43296)	Loss/tok 3.6287 (3.9929)	LR 1.000e-03
0: TRAIN [0][7280/7762]	Time 0.260 (0.325)	Data 1.08e-04 (1.63e-04)	Tok/s 39364 (43295)	Loss/tok 3.2572 (3.9921)	LR 1.000e-03
0: TRAIN [0][7290/7762]	Time 0.262 (0.325)	Data 1.05e-04 (1.62e-04)	Tok/s 39859 (43294)	Loss/tok 3.2190 (3.9914)	LR 1.000e-03
0: TRAIN [0][7300/7762]	Time 0.260 (0.325)	Data 1.81e-04 (1.62e-04)	Tok/s 39650 (43293)	Loss/tok 3.1944 (3.9906)	LR 5.000e-04
0: TRAIN [0][7310/7762]	Time 0.169 (0.325)	Data 1.16e-04 (1.62e-04)	Tok/s 30797 (43291)	Loss/tok 2.8321 (3.9899)	LR 5.000e-04
0: TRAIN [0][7320/7762]	Time 0.364 (0.325)	Data 1.03e-04 (1.62e-04)	Tok/s 46869 (43290)	Loss/tok 3.4198 (3.9891)	LR 5.000e-04
0: TRAIN [0][7330/7762]	Time 0.257 (0.325)	Data 1.08e-04 (1.62e-04)	Tok/s 39552 (43288)	Loss/tok 3.2432 (3.9884)	LR 5.000e-04
0: TRAIN [0][7340/7762]	Time 0.355 (0.325)	Data 1.17e-04 (1.62e-04)	Tok/s 47914 (43291)	Loss/tok 3.5360 (3.9876)	LR 5.000e-04
0: TRAIN [0][7350/7762]	Time 0.265 (0.325)	Data 1.05e-04 (1.62e-04)	Tok/s 39069 (43290)	Loss/tok 3.0979 (3.9868)	LR 5.000e-04
0: TRAIN [0][7360/7762]	Time 0.346 (0.325)	Data 1.02e-04 (1.62e-04)	Tok/s 48209 (43287)	Loss/tok 3.4333 (3.9861)	LR 5.000e-04
0: TRAIN [0][7370/7762]	Time 0.361 (0.325)	Data 1.02e-04 (1.62e-04)	Tok/s 46422 (43291)	Loss/tok 3.3331 (3.9853)	LR 5.000e-04
0: TRAIN [0][7380/7762]	Time 0.355 (0.325)	Data 1.02e-04 (1.62e-04)	Tok/s 47603 (43291)	Loss/tok 3.4490 (3.9845)	LR 5.000e-04
0: TRAIN [0][7390/7762]	Time 0.177 (0.325)	Data 1.01e-04 (1.62e-04)	Tok/s 30189 (43290)	Loss/tok 2.7504 (3.9837)	LR 5.000e-04
0: TRAIN [0][7400/7762]	Time 0.265 (0.325)	Data 1.08e-04 (1.62e-04)	Tok/s 39440 (43287)	Loss/tok 3.2677 (3.9830)	LR 5.000e-04
0: TRAIN [0][7410/7762]	Time 0.266 (0.325)	Data 1.05e-04 (1.62e-04)	Tok/s 39157 (43288)	Loss/tok 3.1906 (3.9822)	LR 5.000e-04
0: TRAIN [0][7420/7762]	Time 0.260 (0.325)	Data 1.08e-04 (1.61e-04)	Tok/s 39212 (43287)	Loss/tok 3.1947 (3.9814)	LR 5.000e-04
0: TRAIN [0][7430/7762]	Time 0.363 (0.325)	Data 1.03e-04 (1.61e-04)	Tok/s 46914 (43287)	Loss/tok 3.3715 (3.9806)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [0][7440/7762]	Time 0.260 (0.325)	Data 1.04e-04 (1.61e-04)	Tok/s 40641 (43293)	Loss/tok 3.2872 (3.9798)	LR 5.000e-04
0: TRAIN [0][7450/7762]	Time 0.444 (0.325)	Data 1.06e-04 (1.61e-04)	Tok/s 51969 (43295)	Loss/tok 3.6170 (3.9790)	LR 5.000e-04
0: TRAIN [0][7460/7762]	Time 0.269 (0.325)	Data 1.65e-04 (1.61e-04)	Tok/s 39167 (43294)	Loss/tok 3.2514 (3.9782)	LR 5.000e-04
0: TRAIN [0][7470/7762]	Time 0.345 (0.325)	Data 1.04e-04 (1.61e-04)	Tok/s 48883 (43295)	Loss/tok 3.3996 (3.9775)	LR 5.000e-04
0: TRAIN [0][7480/7762]	Time 0.366 (0.325)	Data 1.00e-04 (1.61e-04)	Tok/s 46284 (43293)	Loss/tok 3.4034 (3.9767)	LR 5.000e-04
0: TRAIN [0][7490/7762]	Time 0.365 (0.325)	Data 1.02e-04 (1.61e-04)	Tok/s 45461 (43294)	Loss/tok 3.3836 (3.9759)	LR 5.000e-04
0: TRAIN [0][7500/7762]	Time 0.266 (0.325)	Data 1.07e-04 (1.61e-04)	Tok/s 38991 (43291)	Loss/tok 3.2543 (3.9752)	LR 5.000e-04
0: TRAIN [0][7510/7762]	Time 0.366 (0.325)	Data 1.03e-04 (1.61e-04)	Tok/s 45797 (43295)	Loss/tok 3.3862 (3.9744)	LR 5.000e-04
0: TRAIN [0][7520/7762]	Time 0.260 (0.325)	Data 1.03e-04 (1.61e-04)	Tok/s 39271 (43294)	Loss/tok 3.1764 (3.9737)	LR 5.000e-04
0: TRAIN [0][7530/7762]	Time 0.461 (0.325)	Data 1.05e-04 (1.61e-04)	Tok/s 50288 (43291)	Loss/tok 3.6329 (3.9730)	LR 5.000e-04
0: TRAIN [0][7540/7762]	Time 0.263 (0.325)	Data 1.03e-04 (1.61e-04)	Tok/s 38611 (43292)	Loss/tok 3.1680 (3.9722)	LR 5.000e-04
0: TRAIN [0][7550/7762]	Time 0.586 (0.325)	Data 1.03e-04 (1.61e-04)	Tok/s 50365 (43292)	Loss/tok 3.8843 (3.9715)	LR 5.000e-04
0: TRAIN [0][7560/7762]	Time 0.261 (0.325)	Data 1.04e-04 (1.60e-04)	Tok/s 39223 (43293)	Loss/tok 3.0930 (3.9707)	LR 5.000e-04
0: TRAIN [0][7570/7762]	Time 0.260 (0.325)	Data 1.07e-04 (1.60e-04)	Tok/s 39538 (43292)	Loss/tok 3.1264 (3.9699)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [0][7580/7762]	Time 0.580 (0.325)	Data 9.78e-05 (1.60e-04)	Tok/s 51039 (43292)	Loss/tok 3.8934 (3.9692)	LR 5.000e-04
0: TRAIN [0][7590/7762]	Time 0.365 (0.325)	Data 1.16e-04 (1.60e-04)	Tok/s 46031 (43293)	Loss/tok 3.3967 (3.9684)	LR 5.000e-04
0: TRAIN [0][7600/7762]	Time 0.266 (0.325)	Data 1.04e-04 (1.60e-04)	Tok/s 38614 (43298)	Loss/tok 3.1226 (3.9677)	LR 5.000e-04
0: TRAIN [0][7610/7762]	Time 0.262 (0.325)	Data 1.03e-04 (1.60e-04)	Tok/s 39191 (43296)	Loss/tok 3.0729 (3.9670)	LR 5.000e-04
0: TRAIN [0][7620/7762]	Time 0.259 (0.325)	Data 1.05e-04 (1.60e-04)	Tok/s 39920 (43294)	Loss/tok 3.2013 (3.9663)	LR 5.000e-04
0: TRAIN [0][7630/7762]	Time 0.253 (0.325)	Data 1.14e-04 (1.60e-04)	Tok/s 42050 (43293)	Loss/tok 3.1858 (3.9655)	LR 5.000e-04
0: TRAIN [0][7640/7762]	Time 0.173 (0.325)	Data 1.02e-04 (1.60e-04)	Tok/s 29821 (43292)	Loss/tok 2.6971 (3.9649)	LR 5.000e-04
0: TRAIN [0][7650/7762]	Time 0.266 (0.325)	Data 1.10e-04 (1.60e-04)	Tok/s 38342 (43292)	Loss/tok 3.2665 (3.9641)	LR 5.000e-04
0: TRAIN [0][7660/7762]	Time 0.178 (0.325)	Data 1.05e-04 (1.60e-04)	Tok/s 29375 (43293)	Loss/tok 2.6044 (3.9634)	LR 5.000e-04
0: TRAIN [0][7670/7762]	Time 0.261 (0.325)	Data 1.01e-04 (1.60e-04)	Tok/s 39827 (43289)	Loss/tok 3.1176 (3.9627)	LR 5.000e-04
0: TRAIN [0][7680/7762]	Time 0.260 (0.325)	Data 1.03e-04 (1.60e-04)	Tok/s 39266 (43290)	Loss/tok 3.0937 (3.9620)	LR 5.000e-04
0: TRAIN [0][7690/7762]	Time 0.353 (0.325)	Data 1.03e-04 (1.60e-04)	Tok/s 48226 (43286)	Loss/tok 3.3034 (3.9613)	LR 5.000e-04
0: TRAIN [0][7700/7762]	Time 0.453 (0.325)	Data 1.05e-04 (1.59e-04)	Tok/s 51344 (43284)	Loss/tok 3.5648 (3.9605)	LR 5.000e-04
0: TRAIN [0][7710/7762]	Time 0.264 (0.325)	Data 1.06e-04 (1.59e-04)	Tok/s 39170 (43285)	Loss/tok 3.1992 (3.9598)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [0][7720/7762]	Time 0.345 (0.325)	Data 1.02e-04 (1.59e-04)	Tok/s 49077 (43284)	Loss/tok 3.3676 (3.9591)	LR 5.000e-04
0: TRAIN [0][7730/7762]	Time 0.263 (0.325)	Data 1.02e-04 (1.59e-04)	Tok/s 39376 (43279)	Loss/tok 3.2193 (3.9584)	LR 5.000e-04
0: TRAIN [0][7740/7762]	Time 0.355 (0.325)	Data 1.03e-04 (1.59e-04)	Tok/s 47376 (43280)	Loss/tok 3.3753 (3.9576)	LR 5.000e-04
0: TRAIN [0][7750/7762]	Time 0.263 (0.325)	Data 1.05e-04 (1.59e-04)	Tok/s 40116 (43277)	Loss/tok 3.1466 (3.9569)	LR 5.000e-04
0: TRAIN [0][7760/7762]	Time 0.263 (0.325)	Data 1.48e-02 (1.61e-04)	Tok/s 39219 (43276)	Loss/tok 3.1166 (3.9561)	LR 5.000e-04
:::MLL 1573748380.835 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1573748380.836 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/12]	Time 0.892 (0.892)	Decoder iters 149.0 (149.0)	Tok/s 18305 (18305)
0: TEST [0][10/12]	Time 0.120 (0.319)	Decoder iters 26.0 (64.9)	Tok/s 30901 (27654)
0: Running moses detokenizer
0: BLEU(score=21.710730820762077, counts=[35565, 16983, 9287, 5289], totals=[65046, 62043, 59041, 56043], precisions=[54.67669034221935, 27.372951017842464, 15.72974712487932, 9.43739628499545], bp=1.0, sys_len=65046, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1573748386.736 eval_accuracy: {"value": 21.71, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1573748386.737 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 3.9565	Test BLEU: 21.71
0: Performance: Epoch: 0	Training: 86546 Tok/s
0: Finished epoch 0
:::MLL 1573748386.737 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1573748386.738 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1573748386.738 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 1665495489
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][0/7762]	Time 0.659 (0.659)	Data 3.02e-01 (3.02e-01)	Tok/s 25645 (25645)	Loss/tok 3.3865 (3.3865)	LR 5.000e-04
0: TRAIN [1][10/7762]	Time 0.175 (0.311)	Data 1.04e-04 (2.75e-02)	Tok/s 30214 (39584)	Loss/tok 2.7485 (3.2233)	LR 5.000e-04
0: TRAIN [1][20/7762]	Time 0.343 (0.312)	Data 1.02e-04 (1.45e-02)	Tok/s 48611 (41876)	Loss/tok 3.3950 (3.2510)	LR 5.000e-04
0: TRAIN [1][30/7762]	Time 0.570 (0.321)	Data 1.03e-04 (9.84e-03)	Tok/s 52410 (42495)	Loss/tok 3.8053 (3.3119)	LR 5.000e-04
0: TRAIN [1][40/7762]	Time 0.260 (0.318)	Data 1.02e-04 (7.46e-03)	Tok/s 39361 (42560)	Loss/tok 3.2173 (3.3102)	LR 5.000e-04
0: TRAIN [1][50/7762]	Time 0.358 (0.324)	Data 1.06e-04 (6.02e-03)	Tok/s 46222 (43140)	Loss/tok 3.2730 (3.3254)	LR 5.000e-04
0: TRAIN [1][60/7762]	Time 0.347 (0.322)	Data 9.87e-05 (5.05e-03)	Tok/s 47136 (43275)	Loss/tok 3.2979 (3.3228)	LR 5.000e-04
0: TRAIN [1][70/7762]	Time 0.362 (0.328)	Data 1.01e-04 (4.35e-03)	Tok/s 46796 (43758)	Loss/tok 3.2313 (3.3239)	LR 5.000e-04
0: TRAIN [1][80/7762]	Time 0.176 (0.325)	Data 2.31e-04 (3.83e-03)	Tok/s 29779 (43487)	Loss/tok 2.7106 (3.3221)	LR 5.000e-04
0: TRAIN [1][90/7762]	Time 0.345 (0.324)	Data 1.04e-04 (3.42e-03)	Tok/s 49100 (43528)	Loss/tok 3.2688 (3.3146)	LR 5.000e-04
0: TRAIN [1][100/7762]	Time 0.363 (0.320)	Data 1.07e-04 (3.09e-03)	Tok/s 45397 (43210)	Loss/tok 3.4114 (3.3041)	LR 5.000e-04
0: TRAIN [1][110/7762]	Time 0.461 (0.323)	Data 1.07e-04 (2.83e-03)	Tok/s 50927 (43463)	Loss/tok 3.5389 (3.3070)	LR 5.000e-04
0: TRAIN [1][120/7762]	Time 0.174 (0.322)	Data 1.01e-04 (2.60e-03)	Tok/s 29788 (43406)	Loss/tok 2.5561 (3.3041)	LR 5.000e-04
0: TRAIN [1][130/7762]	Time 0.259 (0.320)	Data 9.61e-05 (2.41e-03)	Tok/s 40152 (43207)	Loss/tok 3.1463 (3.2984)	LR 5.000e-04
0: TRAIN [1][140/7762]	Time 0.439 (0.320)	Data 1.02e-04 (2.25e-03)	Tok/s 53990 (43257)	Loss/tok 3.4793 (3.2975)	LR 5.000e-04
0: TRAIN [1][150/7762]	Time 0.251 (0.318)	Data 9.56e-05 (2.10e-03)	Tok/s 40086 (43129)	Loss/tok 3.0349 (3.2929)	LR 5.000e-04
0: TRAIN [1][160/7762]	Time 0.176 (0.318)	Data 1.02e-04 (1.98e-03)	Tok/s 30092 (43031)	Loss/tok 2.6882 (3.2947)	LR 5.000e-04
0: TRAIN [1][170/7762]	Time 0.175 (0.318)	Data 1.02e-04 (1.87e-03)	Tok/s 29919 (43010)	Loss/tok 2.6650 (3.2960)	LR 5.000e-04
0: TRAIN [1][180/7762]	Time 0.253 (0.315)	Data 9.70e-05 (1.77e-03)	Tok/s 41407 (42783)	Loss/tok 3.0161 (3.2897)	LR 5.000e-04
0: TRAIN [1][190/7762]	Time 0.361 (0.314)	Data 1.05e-04 (1.69e-03)	Tok/s 46587 (42676)	Loss/tok 3.1883 (3.2890)	LR 5.000e-04
0: TRAIN [1][200/7762]	Time 0.178 (0.315)	Data 1.02e-04 (1.61e-03)	Tok/s 29722 (42723)	Loss/tok 2.6467 (3.2909)	LR 5.000e-04
0: TRAIN [1][210/7762]	Time 0.461 (0.315)	Data 1.05e-04 (1.54e-03)	Tok/s 51199 (42631)	Loss/tok 3.6083 (3.2916)	LR 5.000e-04
0: TRAIN [1][220/7762]	Time 0.349 (0.316)	Data 1.05e-04 (1.47e-03)	Tok/s 48439 (42680)	Loss/tok 3.3157 (3.2948)	LR 5.000e-04
0: TRAIN [1][230/7762]	Time 0.262 (0.317)	Data 1.04e-04 (1.41e-03)	Tok/s 39933 (42744)	Loss/tok 3.1311 (3.2938)	LR 5.000e-04
0: TRAIN [1][240/7762]	Time 0.264 (0.317)	Data 1.17e-04 (1.36e-03)	Tok/s 38346 (42785)	Loss/tok 3.0116 (3.2941)	LR 5.000e-04
0: TRAIN [1][250/7762]	Time 0.356 (0.319)	Data 1.06e-04 (1.31e-03)	Tok/s 47692 (42841)	Loss/tok 3.2232 (3.2987)	LR 5.000e-04
0: TRAIN [1][260/7762]	Time 0.260 (0.318)	Data 1.01e-04 (1.26e-03)	Tok/s 39330 (42779)	Loss/tok 2.9595 (3.2948)	LR 5.000e-04
0: TRAIN [1][270/7762]	Time 0.445 (0.318)	Data 1.04e-04 (1.22e-03)	Tok/s 52869 (42821)	Loss/tok 3.5086 (3.2969)	LR 5.000e-04
0: TRAIN [1][280/7762]	Time 0.457 (0.318)	Data 1.00e-04 (1.18e-03)	Tok/s 51111 (42756)	Loss/tok 3.5988 (3.2963)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][290/7762]	Time 0.258 (0.316)	Data 9.92e-05 (1.14e-03)	Tok/s 40374 (42592)	Loss/tok 3.0287 (3.2935)	LR 5.000e-04
0: TRAIN [1][300/7762]	Time 0.266 (0.315)	Data 1.03e-04 (1.11e-03)	Tok/s 38419 (42507)	Loss/tok 3.0413 (3.2915)	LR 5.000e-04
0: TRAIN [1][310/7762]	Time 0.268 (0.314)	Data 9.87e-05 (1.08e-03)	Tok/s 39687 (42511)	Loss/tok 3.1340 (3.2892)	LR 5.000e-04
0: TRAIN [1][320/7762]	Time 0.266 (0.315)	Data 1.05e-04 (1.05e-03)	Tok/s 37898 (42502)	Loss/tok 3.0601 (3.2930)	LR 5.000e-04
0: TRAIN [1][330/7762]	Time 0.259 (0.314)	Data 1.04e-04 (1.02e-03)	Tok/s 41248 (42464)	Loss/tok 3.0762 (3.2912)	LR 5.000e-04
0: TRAIN [1][340/7762]	Time 0.254 (0.316)	Data 1.23e-04 (9.90e-04)	Tok/s 41338 (42534)	Loss/tok 3.1371 (3.2948)	LR 5.000e-04
0: TRAIN [1][350/7762]	Time 0.358 (0.316)	Data 1.05e-04 (9.65e-04)	Tok/s 47589 (42582)	Loss/tok 3.3734 (3.2953)	LR 5.000e-04
0: TRAIN [1][360/7762]	Time 0.258 (0.318)	Data 1.11e-04 (9.41e-04)	Tok/s 39297 (42649)	Loss/tok 3.2191 (3.2988)	LR 2.500e-04
0: TRAIN [1][370/7762]	Time 0.445 (0.319)	Data 1.06e-04 (9.19e-04)	Tok/s 52590 (42767)	Loss/tok 3.6130 (3.3040)	LR 2.500e-04
0: TRAIN [1][380/7762]	Time 0.363 (0.319)	Data 9.85e-05 (8.98e-04)	Tok/s 46216 (42777)	Loss/tok 3.2981 (3.3022)	LR 2.500e-04
0: TRAIN [1][390/7762]	Time 0.176 (0.319)	Data 1.01e-04 (8.77e-04)	Tok/s 29676 (42761)	Loss/tok 2.6493 (3.3017)	LR 2.500e-04
0: TRAIN [1][400/7762]	Time 0.359 (0.321)	Data 1.05e-04 (8.58e-04)	Tok/s 46793 (42828)	Loss/tok 3.5050 (3.3057)	LR 2.500e-04
0: TRAIN [1][410/7762]	Time 0.365 (0.322)	Data 1.04e-04 (8.40e-04)	Tok/s 46145 (42914)	Loss/tok 3.3065 (3.3093)	LR 2.500e-04
0: TRAIN [1][420/7762]	Time 0.364 (0.324)	Data 1.05e-04 (8.23e-04)	Tok/s 46550 (42981)	Loss/tok 3.3104 (3.3148)	LR 2.500e-04
0: TRAIN [1][430/7762]	Time 0.257 (0.323)	Data 1.21e-04 (8.06e-04)	Tok/s 40850 (42940)	Loss/tok 3.1271 (3.3117)	LR 2.500e-04
0: TRAIN [1][440/7762]	Time 0.258 (0.322)	Data 1.03e-04 (7.90e-04)	Tok/s 40423 (42888)	Loss/tok 3.1597 (3.3095)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][450/7762]	Time 0.255 (0.323)	Data 1.03e-04 (7.75e-04)	Tok/s 39735 (42848)	Loss/tok 3.1675 (3.3102)	LR 2.500e-04
0: TRAIN [1][460/7762]	Time 0.268 (0.322)	Data 1.02e-04 (7.60e-04)	Tok/s 37905 (42828)	Loss/tok 3.0901 (3.3081)	LR 2.500e-04
0: TRAIN [1][470/7762]	Time 0.263 (0.321)	Data 1.03e-04 (7.46e-04)	Tok/s 39958 (42775)	Loss/tok 3.1449 (3.3059)	LR 2.500e-04
0: TRAIN [1][480/7762]	Time 0.265 (0.322)	Data 1.18e-04 (7.33e-04)	Tok/s 39645 (42790)	Loss/tok 3.0283 (3.3061)	LR 2.500e-04
0: TRAIN [1][490/7762]	Time 0.359 (0.322)	Data 1.06e-04 (7.20e-04)	Tok/s 45278 (42855)	Loss/tok 3.3403 (3.3065)	LR 2.500e-04
0: TRAIN [1][500/7762]	Time 0.356 (0.322)	Data 1.03e-04 (7.08e-04)	Tok/s 46830 (42893)	Loss/tok 3.3928 (3.3057)	LR 2.500e-04
0: TRAIN [1][510/7762]	Time 0.363 (0.322)	Data 1.01e-04 (6.96e-04)	Tok/s 45785 (42887)	Loss/tok 3.2262 (3.3042)	LR 2.500e-04
0: TRAIN [1][520/7762]	Time 0.266 (0.322)	Data 1.18e-04 (6.85e-04)	Tok/s 38988 (42885)	Loss/tok 3.0519 (3.3054)	LR 2.500e-04
0: TRAIN [1][530/7762]	Time 0.460 (0.324)	Data 1.03e-04 (6.74e-04)	Tok/s 50618 (42900)	Loss/tok 3.5443 (3.3085)	LR 2.500e-04
0: TRAIN [1][540/7762]	Time 0.345 (0.324)	Data 1.04e-04 (6.63e-04)	Tok/s 48381 (42898)	Loss/tok 3.3177 (3.3102)	LR 2.500e-04
0: TRAIN [1][550/7762]	Time 0.355 (0.325)	Data 1.03e-04 (6.53e-04)	Tok/s 47040 (42966)	Loss/tok 3.2467 (3.3106)	LR 2.500e-04
0: TRAIN [1][560/7762]	Time 0.266 (0.325)	Data 1.00e-04 (6.44e-04)	Tok/s 38570 (42964)	Loss/tok 3.1313 (3.3098)	LR 2.500e-04
0: TRAIN [1][570/7762]	Time 0.254 (0.324)	Data 9.85e-05 (6.34e-04)	Tok/s 41132 (42947)	Loss/tok 3.2164 (3.3086)	LR 2.500e-04
0: TRAIN [1][580/7762]	Time 0.268 (0.324)	Data 1.06e-04 (6.25e-04)	Tok/s 38281 (42926)	Loss/tok 3.0305 (3.3073)	LR 2.500e-04
0: TRAIN [1][590/7762]	Time 0.364 (0.324)	Data 1.04e-04 (6.16e-04)	Tok/s 46681 (42886)	Loss/tok 3.3473 (3.3061)	LR 2.500e-04
0: TRAIN [1][600/7762]	Time 0.258 (0.324)	Data 1.07e-04 (6.08e-04)	Tok/s 39317 (42918)	Loss/tok 3.0152 (3.3059)	LR 2.500e-04
0: TRAIN [1][610/7762]	Time 0.461 (0.324)	Data 1.04e-04 (6.00e-04)	Tok/s 50791 (42948)	Loss/tok 3.4982 (3.3058)	LR 2.500e-04
0: TRAIN [1][620/7762]	Time 0.459 (0.324)	Data 1.09e-04 (5.92e-04)	Tok/s 50841 (42944)	Loss/tok 3.4759 (3.3060)	LR 2.500e-04
0: TRAIN [1][630/7762]	Time 0.462 (0.325)	Data 1.06e-04 (5.84e-04)	Tok/s 50828 (43001)	Loss/tok 3.5144 (3.3079)	LR 2.500e-04
0: TRAIN [1][640/7762]	Time 0.256 (0.324)	Data 1.03e-04 (5.77e-04)	Tok/s 39946 (42994)	Loss/tok 3.1017 (3.3063)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][650/7762]	Time 0.366 (0.325)	Data 1.04e-04 (5.70e-04)	Tok/s 45649 (43008)	Loss/tok 3.2839 (3.3075)	LR 2.500e-04
0: TRAIN [1][660/7762]	Time 0.364 (0.325)	Data 1.03e-04 (5.63e-04)	Tok/s 46436 (43016)	Loss/tok 3.3318 (3.3082)	LR 2.500e-04
0: TRAIN [1][670/7762]	Time 0.263 (0.325)	Data 1.08e-04 (5.56e-04)	Tok/s 39643 (42974)	Loss/tok 3.0654 (3.3072)	LR 2.500e-04
0: TRAIN [1][680/7762]	Time 0.341 (0.325)	Data 1.07e-04 (5.49e-04)	Tok/s 48476 (42980)	Loss/tok 3.3490 (3.3066)	LR 2.500e-04
0: TRAIN [1][690/7762]	Time 0.362 (0.326)	Data 1.04e-04 (5.43e-04)	Tok/s 45910 (43045)	Loss/tok 3.2367 (3.3086)	LR 2.500e-04
0: TRAIN [1][700/7762]	Time 0.176 (0.326)	Data 9.92e-05 (5.36e-04)	Tok/s 29481 (43031)	Loss/tok 2.6781 (3.3077)	LR 2.500e-04
0: TRAIN [1][710/7762]	Time 0.461 (0.325)	Data 1.08e-04 (5.31e-04)	Tok/s 50445 (43033)	Loss/tok 3.4854 (3.3078)	LR 2.500e-04
0: TRAIN [1][720/7762]	Time 0.255 (0.326)	Data 1.05e-04 (5.25e-04)	Tok/s 40434 (43056)	Loss/tok 3.0443 (3.3077)	LR 2.500e-04
0: TRAIN [1][730/7762]	Time 0.352 (0.326)	Data 1.08e-04 (5.19e-04)	Tok/s 47500 (43092)	Loss/tok 3.2856 (3.3094)	LR 2.500e-04
0: TRAIN [1][740/7762]	Time 0.262 (0.326)	Data 1.06e-04 (5.14e-04)	Tok/s 39406 (43081)	Loss/tok 3.1248 (3.3086)	LR 2.500e-04
0: TRAIN [1][750/7762]	Time 0.268 (0.326)	Data 1.03e-04 (5.08e-04)	Tok/s 38637 (43079)	Loss/tok 3.0263 (3.3086)	LR 2.500e-04
0: TRAIN [1][760/7762]	Time 0.265 (0.327)	Data 9.99e-05 (5.03e-04)	Tok/s 38732 (43090)	Loss/tok 3.0981 (3.3103)	LR 2.500e-04
0: TRAIN [1][770/7762]	Time 0.178 (0.327)	Data 1.03e-04 (4.98e-04)	Tok/s 29664 (43086)	Loss/tok 2.6750 (3.3094)	LR 2.500e-04
0: TRAIN [1][780/7762]	Time 0.265 (0.327)	Data 1.05e-04 (4.93e-04)	Tok/s 38782 (43081)	Loss/tok 3.1062 (3.3101)	LR 2.500e-04
0: TRAIN [1][790/7762]	Time 0.264 (0.326)	Data 9.78e-05 (4.88e-04)	Tok/s 39380 (43067)	Loss/tok 3.0932 (3.3095)	LR 2.500e-04
0: TRAIN [1][800/7762]	Time 0.357 (0.326)	Data 1.00e-04 (4.83e-04)	Tok/s 47328 (43045)	Loss/tok 3.3545 (3.3097)	LR 2.500e-04
0: TRAIN [1][810/7762]	Time 0.362 (0.327)	Data 1.05e-04 (4.78e-04)	Tok/s 46431 (43091)	Loss/tok 3.3416 (3.3107)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][820/7762]	Time 0.178 (0.327)	Data 1.04e-04 (4.74e-04)	Tok/s 30157 (43065)	Loss/tok 2.6827 (3.3103)	LR 2.500e-04
0: TRAIN [1][830/7762]	Time 0.564 (0.326)	Data 9.85e-05 (4.69e-04)	Tok/s 52310 (43042)	Loss/tok 3.6691 (3.3096)	LR 2.500e-04
0: TRAIN [1][840/7762]	Time 0.451 (0.326)	Data 1.02e-04 (4.65e-04)	Tok/s 51583 (43049)	Loss/tok 3.3679 (3.3093)	LR 2.500e-04
0: TRAIN [1][850/7762]	Time 0.350 (0.327)	Data 9.97e-05 (4.61e-04)	Tok/s 47489 (43055)	Loss/tok 3.2927 (3.3092)	LR 2.500e-04
0: TRAIN [1][860/7762]	Time 0.365 (0.326)	Data 1.00e-04 (4.56e-04)	Tok/s 45635 (43010)	Loss/tok 3.4690 (3.3080)	LR 2.500e-04
0: TRAIN [1][870/7762]	Time 0.260 (0.326)	Data 1.03e-04 (4.52e-04)	Tok/s 39800 (43046)	Loss/tok 3.0664 (3.3091)	LR 2.500e-04
0: TRAIN [1][880/7762]	Time 0.586 (0.327)	Data 1.03e-04 (4.48e-04)	Tok/s 51151 (43048)	Loss/tok 3.5893 (3.3095)	LR 2.500e-04
0: TRAIN [1][890/7762]	Time 0.349 (0.327)	Data 1.03e-04 (4.45e-04)	Tok/s 47886 (43061)	Loss/tok 3.3525 (3.3098)	LR 2.500e-04
0: TRAIN [1][900/7762]	Time 0.264 (0.326)	Data 1.06e-04 (4.41e-04)	Tok/s 39678 (43036)	Loss/tok 3.2171 (3.3087)	LR 2.500e-04
0: TRAIN [1][910/7762]	Time 0.570 (0.327)	Data 1.03e-04 (4.37e-04)	Tok/s 52227 (43057)	Loss/tok 3.5893 (3.3104)	LR 2.500e-04
0: TRAIN [1][920/7762]	Time 0.363 (0.327)	Data 9.92e-05 (4.34e-04)	Tok/s 45310 (43048)	Loss/tok 3.4492 (3.3095)	LR 2.500e-04
0: TRAIN [1][930/7762]	Time 0.591 (0.328)	Data 1.03e-04 (4.30e-04)	Tok/s 50110 (43091)	Loss/tok 3.6923 (3.3117)	LR 2.500e-04
0: TRAIN [1][940/7762]	Time 0.367 (0.328)	Data 1.03e-04 (4.27e-04)	Tok/s 45414 (43106)	Loss/tok 3.3445 (3.3126)	LR 2.500e-04
0: TRAIN [1][950/7762]	Time 0.362 (0.328)	Data 1.04e-04 (4.23e-04)	Tok/s 46540 (43124)	Loss/tok 3.3802 (3.3136)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][960/7762]	Time 0.273 (0.329)	Data 1.08e-04 (4.20e-04)	Tok/s 37773 (43141)	Loss/tok 3.0129 (3.3143)	LR 2.500e-04
0: TRAIN [1][970/7762]	Time 0.347 (0.328)	Data 9.97e-05 (4.17e-04)	Tok/s 48486 (43140)	Loss/tok 3.3475 (3.3134)	LR 2.500e-04
0: TRAIN [1][980/7762]	Time 0.455 (0.329)	Data 1.03e-04 (4.13e-04)	Tok/s 51599 (43156)	Loss/tok 3.4519 (3.3144)	LR 2.500e-04
0: TRAIN [1][990/7762]	Time 0.452 (0.328)	Data 1.07e-04 (4.10e-04)	Tok/s 51989 (43139)	Loss/tok 3.4688 (3.3132)	LR 2.500e-04
0: TRAIN [1][1000/7762]	Time 0.261 (0.328)	Data 9.99e-05 (4.07e-04)	Tok/s 40404 (43119)	Loss/tok 3.0539 (3.3122)	LR 2.500e-04
0: TRAIN [1][1010/7762]	Time 0.363 (0.328)	Data 1.01e-04 (4.04e-04)	Tok/s 45606 (43119)	Loss/tok 3.3038 (3.3130)	LR 2.500e-04
0: TRAIN [1][1020/7762]	Time 0.364 (0.328)	Data 1.06e-04 (4.01e-04)	Tok/s 45913 (43112)	Loss/tok 3.2751 (3.3125)	LR 2.500e-04
0: TRAIN [1][1030/7762]	Time 0.259 (0.328)	Data 1.07e-04 (3.98e-04)	Tok/s 39457 (43123)	Loss/tok 3.0368 (3.3129)	LR 2.500e-04
0: TRAIN [1][1040/7762]	Time 0.357 (0.328)	Data 1.07e-04 (3.96e-04)	Tok/s 46573 (43115)	Loss/tok 3.2492 (3.3119)	LR 2.500e-04
0: TRAIN [1][1050/7762]	Time 0.363 (0.328)	Data 1.02e-04 (3.93e-04)	Tok/s 47175 (43120)	Loss/tok 3.3419 (3.3116)	LR 2.500e-04
0: TRAIN [1][1060/7762]	Time 0.251 (0.328)	Data 1.03e-04 (3.90e-04)	Tok/s 40573 (43148)	Loss/tok 3.2016 (3.3122)	LR 2.500e-04
0: TRAIN [1][1070/7762]	Time 0.451 (0.328)	Data 1.03e-04 (3.88e-04)	Tok/s 51488 (43183)	Loss/tok 3.4085 (3.3127)	LR 2.500e-04
0: TRAIN [1][1080/7762]	Time 0.262 (0.328)	Data 1.07e-04 (3.85e-04)	Tok/s 39691 (43180)	Loss/tok 3.1611 (3.3117)	LR 2.500e-04
0: TRAIN [1][1090/7762]	Time 0.355 (0.328)	Data 1.06e-04 (3.82e-04)	Tok/s 46928 (43202)	Loss/tok 3.2492 (3.3115)	LR 2.500e-04
0: TRAIN [1][1100/7762]	Time 0.358 (0.328)	Data 1.03e-04 (3.80e-04)	Tok/s 46764 (43189)	Loss/tok 3.3080 (3.3107)	LR 2.500e-04
0: TRAIN [1][1110/7762]	Time 0.261 (0.328)	Data 1.07e-04 (3.78e-04)	Tok/s 38986 (43189)	Loss/tok 3.2164 (3.3108)	LR 2.500e-04
0: TRAIN [1][1120/7762]	Time 0.265 (0.328)	Data 1.09e-04 (3.75e-04)	Tok/s 39096 (43185)	Loss/tok 3.0882 (3.3101)	LR 2.500e-04
0: TRAIN [1][1130/7762]	Time 0.177 (0.328)	Data 1.03e-04 (3.73e-04)	Tok/s 30045 (43178)	Loss/tok 2.6709 (3.3096)	LR 2.500e-04
0: TRAIN [1][1140/7762]	Time 0.359 (0.327)	Data 9.44e-05 (3.71e-04)	Tok/s 46862 (43170)	Loss/tok 3.3110 (3.3089)	LR 2.500e-04
0: TRAIN [1][1150/7762]	Time 0.359 (0.327)	Data 1.04e-04 (3.68e-04)	Tok/s 47098 (43171)	Loss/tok 3.2010 (3.3078)	LR 2.500e-04
0: TRAIN [1][1160/7762]	Time 0.175 (0.327)	Data 1.04e-04 (3.66e-04)	Tok/s 30016 (43171)	Loss/tok 2.6470 (3.3079)	LR 2.500e-04
0: TRAIN [1][1170/7762]	Time 0.170 (0.327)	Data 1.07e-04 (3.64e-04)	Tok/s 30818 (43182)	Loss/tok 2.6733 (3.3083)	LR 1.250e-04
0: TRAIN [1][1180/7762]	Time 0.262 (0.328)	Data 1.04e-04 (3.61e-04)	Tok/s 39435 (43205)	Loss/tok 3.1642 (3.3084)	LR 1.250e-04
0: TRAIN [1][1190/7762]	Time 0.344 (0.327)	Data 1.03e-04 (3.59e-04)	Tok/s 48709 (43188)	Loss/tok 3.2030 (3.3075)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1200/7762]	Time 0.267 (0.328)	Data 1.05e-04 (3.57e-04)	Tok/s 38477 (43194)	Loss/tok 3.1164 (3.3078)	LR 1.250e-04
0: TRAIN [1][1210/7762]	Time 0.254 (0.327)	Data 1.01e-04 (3.55e-04)	Tok/s 40545 (43178)	Loss/tok 3.0226 (3.3073)	LR 1.250e-04
0: TRAIN [1][1220/7762]	Time 0.264 (0.327)	Data 1.01e-04 (3.53e-04)	Tok/s 39070 (43160)	Loss/tok 3.0434 (3.3062)	LR 1.250e-04
0: TRAIN [1][1230/7762]	Time 0.261 (0.327)	Data 1.03e-04 (3.51e-04)	Tok/s 39187 (43147)	Loss/tok 3.0600 (3.3057)	LR 1.250e-04
0: TRAIN [1][1240/7762]	Time 0.364 (0.327)	Data 9.70e-05 (3.49e-04)	Tok/s 46313 (43159)	Loss/tok 3.1628 (3.3052)	LR 1.250e-04
0: TRAIN [1][1250/7762]	Time 0.176 (0.326)	Data 1.03e-04 (3.47e-04)	Tok/s 30340 (43137)	Loss/tok 2.6889 (3.3041)	LR 1.250e-04
0: TRAIN [1][1260/7762]	Time 0.265 (0.326)	Data 1.14e-04 (3.45e-04)	Tok/s 38824 (43127)	Loss/tok 3.1604 (3.3040)	LR 1.250e-04
0: TRAIN [1][1270/7762]	Time 0.261 (0.326)	Data 1.00e-04 (3.43e-04)	Tok/s 39882 (43123)	Loss/tok 3.1139 (3.3038)	LR 1.250e-04
0: TRAIN [1][1280/7762]	Time 0.262 (0.326)	Data 1.10e-04 (3.42e-04)	Tok/s 38696 (43127)	Loss/tok 3.0969 (3.3038)	LR 1.250e-04
0: TRAIN [1][1290/7762]	Time 0.359 (0.326)	Data 1.03e-04 (3.40e-04)	Tok/s 46718 (43135)	Loss/tok 3.2711 (3.3036)	LR 1.250e-04
0: TRAIN [1][1300/7762]	Time 0.264 (0.326)	Data 1.02e-04 (3.38e-04)	Tok/s 39150 (43133)	Loss/tok 3.0225 (3.3035)	LR 1.250e-04
0: TRAIN [1][1310/7762]	Time 0.355 (0.326)	Data 1.00e-04 (3.36e-04)	Tok/s 47418 (43137)	Loss/tok 3.2575 (3.3032)	LR 1.250e-04
0: TRAIN [1][1320/7762]	Time 0.256 (0.326)	Data 1.02e-04 (3.34e-04)	Tok/s 40941 (43146)	Loss/tok 3.0796 (3.3036)	LR 1.250e-04
0: TRAIN [1][1330/7762]	Time 0.456 (0.326)	Data 1.00e-04 (3.33e-04)	Tok/s 51396 (43133)	Loss/tok 3.4765 (3.3033)	LR 1.250e-04
0: TRAIN [1][1340/7762]	Time 0.265 (0.326)	Data 1.04e-04 (3.31e-04)	Tok/s 38203 (43128)	Loss/tok 3.1320 (3.3030)	LR 1.250e-04
0: TRAIN [1][1350/7762]	Time 0.176 (0.326)	Data 1.04e-04 (3.30e-04)	Tok/s 31083 (43121)	Loss/tok 2.6638 (3.3024)	LR 1.250e-04
0: TRAIN [1][1360/7762]	Time 0.363 (0.326)	Data 2.37e-04 (3.28e-04)	Tok/s 45858 (43122)	Loss/tok 3.2052 (3.3016)	LR 1.250e-04
0: TRAIN [1][1370/7762]	Time 0.456 (0.326)	Data 1.05e-04 (3.26e-04)	Tok/s 51434 (43151)	Loss/tok 3.4258 (3.3024)	LR 1.250e-04
0: TRAIN [1][1380/7762]	Time 0.174 (0.326)	Data 1.05e-04 (3.25e-04)	Tok/s 30226 (43141)	Loss/tok 2.6902 (3.3021)	LR 1.250e-04
0: TRAIN [1][1390/7762]	Time 0.362 (0.326)	Data 1.06e-04 (3.23e-04)	Tok/s 46643 (43168)	Loss/tok 3.2847 (3.3023)	LR 1.250e-04
0: TRAIN [1][1400/7762]	Time 0.269 (0.326)	Data 1.01e-04 (3.22e-04)	Tok/s 39390 (43154)	Loss/tok 3.0849 (3.3021)	LR 1.250e-04
0: TRAIN [1][1410/7762]	Time 0.260 (0.326)	Data 1.06e-04 (3.20e-04)	Tok/s 39570 (43159)	Loss/tok 3.0868 (3.3021)	LR 1.250e-04
0: TRAIN [1][1420/7762]	Time 0.261 (0.326)	Data 1.04e-04 (3.19e-04)	Tok/s 39807 (43153)	Loss/tok 3.1345 (3.3014)	LR 1.250e-04
0: TRAIN [1][1430/7762]	Time 0.264 (0.326)	Data 1.06e-04 (3.17e-04)	Tok/s 39586 (43169)	Loss/tok 3.0491 (3.3016)	LR 1.250e-04
0: TRAIN [1][1440/7762]	Time 0.260 (0.326)	Data 1.08e-04 (3.16e-04)	Tok/s 39158 (43162)	Loss/tok 3.1354 (3.3007)	LR 1.250e-04
0: TRAIN [1][1450/7762]	Time 0.175 (0.326)	Data 1.19e-04 (3.14e-04)	Tok/s 30075 (43145)	Loss/tok 2.7065 (3.3000)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][1460/7762]	Time 0.353 (0.326)	Data 1.05e-04 (3.13e-04)	Tok/s 47506 (43145)	Loss/tok 3.3538 (3.2999)	LR 1.250e-04
0: TRAIN [1][1470/7762]	Time 0.172 (0.326)	Data 1.03e-04 (3.11e-04)	Tok/s 30402 (43135)	Loss/tok 2.6374 (3.2993)	LR 1.250e-04
0: TRAIN [1][1480/7762]	Time 0.461 (0.326)	Data 1.01e-04 (3.10e-04)	Tok/s 50663 (43137)	Loss/tok 3.4495 (3.2988)	LR 1.250e-04
0: TRAIN [1][1490/7762]	Time 0.262 (0.326)	Data 1.10e-04 (3.09e-04)	Tok/s 38978 (43146)	Loss/tok 3.1368 (3.2991)	LR 1.250e-04
0: TRAIN [1][1500/7762]	Time 0.362 (0.326)	Data 1.05e-04 (3.07e-04)	Tok/s 46597 (43154)	Loss/tok 3.3820 (3.2992)	LR 1.250e-04
0: TRAIN [1][1510/7762]	Time 0.362 (0.326)	Data 1.04e-04 (3.06e-04)	Tok/s 46236 (43158)	Loss/tok 3.2864 (3.2992)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1520/7762]	Time 0.587 (0.326)	Data 1.06e-04 (3.05e-04)	Tok/s 51289 (43155)	Loss/tok 3.6207 (3.2991)	LR 1.250e-04
0: TRAIN [1][1530/7762]	Time 0.346 (0.326)	Data 1.05e-04 (3.03e-04)	Tok/s 47854 (43156)	Loss/tok 3.4317 (3.2989)	LR 1.250e-04
0: TRAIN [1][1540/7762]	Time 0.451 (0.326)	Data 1.05e-04 (3.02e-04)	Tok/s 51769 (43167)	Loss/tok 3.4049 (3.2989)	LR 1.250e-04
0: TRAIN [1][1550/7762]	Time 0.363 (0.326)	Data 1.03e-04 (3.01e-04)	Tok/s 45788 (43159)	Loss/tok 3.4431 (3.2997)	LR 1.250e-04
0: TRAIN [1][1560/7762]	Time 0.260 (0.327)	Data 1.04e-04 (3.00e-04)	Tok/s 40621 (43182)	Loss/tok 3.1060 (3.2999)	LR 1.250e-04
0: TRAIN [1][1570/7762]	Time 0.366 (0.327)	Data 1.04e-04 (2.98e-04)	Tok/s 45678 (43192)	Loss/tok 3.2671 (3.3003)	LR 1.250e-04
0: TRAIN [1][1580/7762]	Time 0.266 (0.327)	Data 1.04e-04 (2.97e-04)	Tok/s 39087 (43195)	Loss/tok 3.0775 (3.3003)	LR 1.250e-04
0: TRAIN [1][1590/7762]	Time 0.459 (0.327)	Data 1.01e-04 (2.96e-04)	Tok/s 51130 (43186)	Loss/tok 3.4511 (3.3000)	LR 1.250e-04
0: TRAIN [1][1600/7762]	Time 0.354 (0.327)	Data 1.03e-04 (2.95e-04)	Tok/s 47314 (43207)	Loss/tok 3.2952 (3.3003)	LR 1.250e-04
0: TRAIN [1][1610/7762]	Time 0.269 (0.326)	Data 1.04e-04 (2.94e-04)	Tok/s 38588 (43185)	Loss/tok 3.0362 (3.2995)	LR 1.250e-04
0: TRAIN [1][1620/7762]	Time 0.361 (0.326)	Data 1.03e-04 (2.93e-04)	Tok/s 46376 (43181)	Loss/tok 3.3079 (3.2990)	LR 1.250e-04
0: TRAIN [1][1630/7762]	Time 0.463 (0.326)	Data 1.07e-04 (2.91e-04)	Tok/s 49882 (43171)	Loss/tok 3.4828 (3.2983)	LR 1.250e-04
0: TRAIN [1][1640/7762]	Time 0.354 (0.326)	Data 1.01e-04 (2.90e-04)	Tok/s 48087 (43167)	Loss/tok 3.1727 (3.2980)	LR 1.250e-04
0: TRAIN [1][1650/7762]	Time 0.256 (0.326)	Data 9.80e-05 (2.89e-04)	Tok/s 39685 (43151)	Loss/tok 3.1054 (3.2979)	LR 1.250e-04
0: TRAIN [1][1660/7762]	Time 0.461 (0.326)	Data 1.02e-04 (2.88e-04)	Tok/s 51057 (43151)	Loss/tok 3.4506 (3.2974)	LR 1.250e-04
0: TRAIN [1][1670/7762]	Time 0.263 (0.326)	Data 1.05e-04 (2.87e-04)	Tok/s 38252 (43165)	Loss/tok 3.0647 (3.2976)	LR 1.250e-04
0: TRAIN [1][1680/7762]	Time 0.175 (0.325)	Data 1.31e-04 (2.86e-04)	Tok/s 29731 (43120)	Loss/tok 2.6285 (3.2966)	LR 1.250e-04
0: TRAIN [1][1690/7762]	Time 0.168 (0.325)	Data 9.42e-05 (2.85e-04)	Tok/s 31155 (43093)	Loss/tok 2.6748 (3.2957)	LR 1.250e-04
0: TRAIN [1][1700/7762]	Time 0.262 (0.325)	Data 1.01e-04 (2.84e-04)	Tok/s 39111 (43082)	Loss/tok 3.1125 (3.2952)	LR 1.250e-04
0: TRAIN [1][1710/7762]	Time 0.363 (0.325)	Data 1.03e-04 (2.83e-04)	Tok/s 45872 (43095)	Loss/tok 3.3543 (3.2954)	LR 1.250e-04
0: TRAIN [1][1720/7762]	Time 0.253 (0.325)	Data 1.08e-04 (2.82e-04)	Tok/s 40948 (43096)	Loss/tok 3.1730 (3.2952)	LR 1.250e-04
0: TRAIN [1][1730/7762]	Time 0.365 (0.325)	Data 1.06e-04 (2.81e-04)	Tok/s 45952 (43090)	Loss/tok 3.3686 (3.2950)	LR 1.250e-04
0: TRAIN [1][1740/7762]	Time 0.266 (0.325)	Data 1.01e-04 (2.80e-04)	Tok/s 39058 (43077)	Loss/tok 3.1703 (3.2945)	LR 1.250e-04
0: TRAIN [1][1750/7762]	Time 0.463 (0.325)	Data 1.15e-04 (2.79e-04)	Tok/s 50143 (43096)	Loss/tok 3.5081 (3.2963)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1760/7762]	Time 0.362 (0.325)	Data 1.02e-04 (2.78e-04)	Tok/s 46335 (43099)	Loss/tok 3.3685 (3.2961)	LR 1.250e-04
0: TRAIN [1][1770/7762]	Time 0.358 (0.325)	Data 1.03e-04 (2.77e-04)	Tok/s 46608 (43099)	Loss/tok 3.2486 (3.2959)	LR 1.250e-04
0: TRAIN [1][1780/7762]	Time 0.353 (0.325)	Data 1.04e-04 (2.76e-04)	Tok/s 48060 (43119)	Loss/tok 3.3205 (3.2959)	LR 1.250e-04
0: TRAIN [1][1790/7762]	Time 0.364 (0.326)	Data 9.87e-05 (2.75e-04)	Tok/s 45814 (43124)	Loss/tok 3.1683 (3.2956)	LR 1.250e-04
0: TRAIN [1][1800/7762]	Time 0.259 (0.325)	Data 1.02e-04 (2.74e-04)	Tok/s 38789 (43108)	Loss/tok 3.0821 (3.2950)	LR 1.250e-04
0: TRAIN [1][1810/7762]	Time 0.173 (0.325)	Data 1.07e-04 (2.73e-04)	Tok/s 30730 (43111)	Loss/tok 2.5779 (3.2954)	LR 1.250e-04
0: TRAIN [1][1820/7762]	Time 0.258 (0.325)	Data 1.00e-04 (2.72e-04)	Tok/s 41298 (43110)	Loss/tok 3.0681 (3.2951)	LR 1.250e-04
0: TRAIN [1][1830/7762]	Time 0.176 (0.325)	Data 1.05e-04 (2.71e-04)	Tok/s 29995 (43095)	Loss/tok 2.6479 (3.2948)	LR 1.250e-04
0: TRAIN [1][1840/7762]	Time 0.359 (0.325)	Data 1.08e-04 (2.70e-04)	Tok/s 46313 (43101)	Loss/tok 3.3168 (3.2951)	LR 1.250e-04
0: TRAIN [1][1850/7762]	Time 0.264 (0.325)	Data 1.06e-04 (2.69e-04)	Tok/s 39874 (43093)	Loss/tok 3.1233 (3.2945)	LR 1.250e-04
0: TRAIN [1][1860/7762]	Time 0.265 (0.325)	Data 1.01e-04 (2.68e-04)	Tok/s 38861 (43093)	Loss/tok 3.0679 (3.2944)	LR 1.250e-04
0: TRAIN [1][1870/7762]	Time 0.174 (0.325)	Data 1.04e-04 (2.67e-04)	Tok/s 30977 (43089)	Loss/tok 2.6726 (3.2944)	LR 1.250e-04
0: TRAIN [1][1880/7762]	Time 0.364 (0.325)	Data 1.05e-04 (2.67e-04)	Tok/s 46098 (43102)	Loss/tok 3.3259 (3.2944)	LR 1.250e-04
0: TRAIN [1][1890/7762]	Time 0.263 (0.325)	Data 1.01e-04 (2.66e-04)	Tok/s 38929 (43077)	Loss/tok 3.0686 (3.2939)	LR 1.250e-04
0: TRAIN [1][1900/7762]	Time 0.363 (0.325)	Data 9.73e-05 (2.65e-04)	Tok/s 46353 (43087)	Loss/tok 3.2912 (3.2944)	LR 1.250e-04
0: TRAIN [1][1910/7762]	Time 0.264 (0.325)	Data 1.03e-04 (2.64e-04)	Tok/s 39483 (43094)	Loss/tok 3.0995 (3.2943)	LR 1.250e-04
0: TRAIN [1][1920/7762]	Time 0.458 (0.326)	Data 1.04e-04 (2.63e-04)	Tok/s 51297 (43119)	Loss/tok 3.5306 (3.2950)	LR 1.250e-04
0: TRAIN [1][1930/7762]	Time 0.349 (0.325)	Data 1.04e-04 (2.63e-04)	Tok/s 47969 (43103)	Loss/tok 3.2788 (3.2949)	LR 1.250e-04
0: TRAIN [1][1940/7762]	Time 0.360 (0.326)	Data 9.99e-05 (2.62e-04)	Tok/s 46778 (43114)	Loss/tok 3.2924 (3.2955)	LR 1.250e-04
0: TRAIN [1][1950/7762]	Time 0.273 (0.326)	Data 1.18e-04 (2.61e-04)	Tok/s 38307 (43132)	Loss/tok 3.0338 (3.2961)	LR 1.250e-04
0: TRAIN [1][1960/7762]	Time 0.452 (0.326)	Data 1.02e-04 (2.60e-04)	Tok/s 51641 (43134)	Loss/tok 3.5646 (3.2959)	LR 1.250e-04
0: TRAIN [1][1970/7762]	Time 0.359 (0.326)	Data 1.07e-04 (2.59e-04)	Tok/s 46694 (43139)	Loss/tok 3.2822 (3.2956)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1980/7762]	Time 0.260 (0.326)	Data 1.02e-04 (2.59e-04)	Tok/s 40261 (43142)	Loss/tok 3.0669 (3.2959)	LR 1.250e-04
0: TRAIN [1][1990/7762]	Time 0.270 (0.326)	Data 9.85e-05 (2.58e-04)	Tok/s 37847 (43133)	Loss/tok 3.1617 (3.2956)	LR 1.250e-04
0: TRAIN [1][2000/7762]	Time 0.260 (0.326)	Data 1.08e-04 (2.57e-04)	Tok/s 39900 (43129)	Loss/tok 2.9562 (3.2952)	LR 1.250e-04
0: TRAIN [1][2010/7762]	Time 0.260 (0.326)	Data 1.03e-04 (2.56e-04)	Tok/s 39690 (43115)	Loss/tok 2.9747 (3.2948)	LR 1.250e-04
0: TRAIN [1][2020/7762]	Time 0.266 (0.326)	Data 1.13e-04 (2.56e-04)	Tok/s 38767 (43120)	Loss/tok 3.1126 (3.2951)	LR 1.250e-04
0: TRAIN [1][2030/7762]	Time 0.268 (0.326)	Data 1.04e-04 (2.55e-04)	Tok/s 39660 (43129)	Loss/tok 3.0226 (3.2959)	LR 1.250e-04
0: TRAIN [1][2040/7762]	Time 0.260 (0.326)	Data 1.04e-04 (2.54e-04)	Tok/s 39258 (43137)	Loss/tok 3.1005 (3.2961)	LR 1.250e-04
0: TRAIN [1][2050/7762]	Time 0.363 (0.326)	Data 1.03e-04 (2.53e-04)	Tok/s 46090 (43148)	Loss/tok 3.3455 (3.2964)	LR 1.250e-04
0: TRAIN [1][2060/7762]	Time 0.259 (0.326)	Data 1.06e-04 (2.53e-04)	Tok/s 40179 (43144)	Loss/tok 3.0373 (3.2968)	LR 1.250e-04
0: TRAIN [1][2070/7762]	Time 0.363 (0.327)	Data 1.04e-04 (2.52e-04)	Tok/s 46840 (43159)	Loss/tok 3.2084 (3.2970)	LR 1.250e-04
0: TRAIN [1][2080/7762]	Time 0.586 (0.327)	Data 1.04e-04 (2.51e-04)	Tok/s 50254 (43165)	Loss/tok 3.6567 (3.2974)	LR 1.250e-04
0: TRAIN [1][2090/7762]	Time 0.175 (0.327)	Data 1.03e-04 (2.50e-04)	Tok/s 29951 (43165)	Loss/tok 2.6452 (3.2976)	LR 1.250e-04
0: TRAIN [1][2100/7762]	Time 0.264 (0.327)	Data 1.07e-04 (2.50e-04)	Tok/s 38096 (43162)	Loss/tok 3.0968 (3.2973)	LR 1.250e-04
0: TRAIN [1][2110/7762]	Time 0.266 (0.327)	Data 1.03e-04 (2.49e-04)	Tok/s 37920 (43152)	Loss/tok 3.0593 (3.2969)	LR 1.250e-04
0: TRAIN [1][2120/7762]	Time 0.447 (0.326)	Data 9.89e-05 (2.48e-04)	Tok/s 51878 (43144)	Loss/tok 3.4351 (3.2965)	LR 1.250e-04
0: TRAIN [1][2130/7762]	Time 0.361 (0.327)	Data 1.01e-04 (2.48e-04)	Tok/s 46931 (43149)	Loss/tok 3.3207 (3.2970)	LR 1.250e-04
0: TRAIN [1][2140/7762]	Time 0.260 (0.327)	Data 1.06e-04 (2.47e-04)	Tok/s 39850 (43153)	Loss/tok 3.1538 (3.2972)	LR 1.250e-04
0: TRAIN [1][2150/7762]	Time 0.360 (0.327)	Data 1.04e-04 (2.46e-04)	Tok/s 46971 (43160)	Loss/tok 3.2013 (3.2971)	LR 1.250e-04
0: TRAIN [1][2160/7762]	Time 0.349 (0.327)	Data 9.80e-05 (2.46e-04)	Tok/s 48506 (43152)	Loss/tok 3.2316 (3.2969)	LR 1.250e-04
0: TRAIN [1][2170/7762]	Time 0.256 (0.326)	Data 9.73e-05 (2.45e-04)	Tok/s 40793 (43134)	Loss/tok 3.0374 (3.2963)	LR 1.250e-04
0: TRAIN [1][2180/7762]	Time 0.261 (0.326)	Data 1.02e-04 (2.44e-04)	Tok/s 39812 (43135)	Loss/tok 3.0238 (3.2961)	LR 1.250e-04
0: TRAIN [1][2190/7762]	Time 0.463 (0.326)	Data 1.05e-04 (2.44e-04)	Tok/s 50343 (43128)	Loss/tok 3.4074 (3.2958)	LR 1.250e-04
0: TRAIN [1][2200/7762]	Time 0.176 (0.326)	Data 1.07e-04 (2.43e-04)	Tok/s 29155 (43116)	Loss/tok 2.5437 (3.2954)	LR 1.250e-04
0: TRAIN [1][2210/7762]	Time 0.456 (0.326)	Data 1.06e-04 (2.43e-04)	Tok/s 51420 (43123)	Loss/tok 3.4791 (3.2956)	LR 1.250e-04
0: TRAIN [1][2220/7762]	Time 0.364 (0.326)	Data 1.03e-04 (2.42e-04)	Tok/s 46557 (43129)	Loss/tok 3.1774 (3.2954)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][2230/7762]	Time 0.255 (0.326)	Data 1.10e-04 (2.41e-04)	Tok/s 41062 (43141)	Loss/tok 3.0544 (3.2956)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][2240/7762]	Time 0.344 (0.327)	Data 1.02e-04 (2.41e-04)	Tok/s 48854 (43157)	Loss/tok 3.2367 (3.2962)	LR 1.250e-04
0: TRAIN [1][2250/7762]	Time 0.259 (0.326)	Data 1.01e-04 (2.40e-04)	Tok/s 40369 (43138)	Loss/tok 3.0170 (3.2957)	LR 1.250e-04
0: TRAIN [1][2260/7762]	Time 0.364 (0.326)	Data 1.04e-04 (2.40e-04)	Tok/s 46404 (43143)	Loss/tok 3.3183 (3.2958)	LR 1.250e-04
0: TRAIN [1][2270/7762]	Time 0.358 (0.326)	Data 1.05e-04 (2.39e-04)	Tok/s 47348 (43126)	Loss/tok 3.2125 (3.2955)	LR 1.250e-04
0: TRAIN [1][2280/7762]	Time 0.579 (0.327)	Data 1.00e-04 (2.38e-04)	Tok/s 51277 (43136)	Loss/tok 3.7459 (3.2962)	LR 1.250e-04
0: TRAIN [1][2290/7762]	Time 0.557 (0.327)	Data 1.04e-04 (2.38e-04)	Tok/s 53285 (43145)	Loss/tok 3.7104 (3.2967)	LR 1.250e-04
0: TRAIN [1][2300/7762]	Time 0.361 (0.327)	Data 1.07e-04 (2.37e-04)	Tok/s 47425 (43154)	Loss/tok 3.3281 (3.2967)	LR 1.250e-04
0: TRAIN [1][2310/7762]	Time 0.172 (0.327)	Data 1.01e-04 (2.37e-04)	Tok/s 31489 (43141)	Loss/tok 2.6917 (3.2967)	LR 1.250e-04
0: TRAIN [1][2320/7762]	Time 0.354 (0.327)	Data 1.07e-04 (2.36e-04)	Tok/s 46848 (43135)	Loss/tok 3.4444 (3.2966)	LR 1.250e-04
0: TRAIN [1][2330/7762]	Time 0.351 (0.326)	Data 1.23e-04 (2.36e-04)	Tok/s 48646 (43124)	Loss/tok 3.3041 (3.2963)	LR 1.250e-04
0: TRAIN [1][2340/7762]	Time 0.258 (0.326)	Data 1.03e-04 (2.35e-04)	Tok/s 39958 (43119)	Loss/tok 3.0578 (3.2960)	LR 1.250e-04
0: TRAIN [1][2350/7762]	Time 0.363 (0.326)	Data 9.73e-05 (2.35e-04)	Tok/s 46415 (43100)	Loss/tok 3.2546 (3.2955)	LR 1.250e-04
0: TRAIN [1][2360/7762]	Time 0.354 (0.326)	Data 1.19e-04 (2.34e-04)	Tok/s 46741 (43094)	Loss/tok 3.2738 (3.2954)	LR 1.250e-04
0: TRAIN [1][2370/7762]	Time 0.262 (0.326)	Data 1.02e-04 (2.33e-04)	Tok/s 39266 (43087)	Loss/tok 3.0500 (3.2949)	LR 1.250e-04
0: TRAIN [1][2380/7762]	Time 0.260 (0.326)	Data 1.03e-04 (2.33e-04)	Tok/s 39381 (43081)	Loss/tok 3.1283 (3.2952)	LR 1.250e-04
0: TRAIN [1][2390/7762]	Time 0.356 (0.326)	Data 9.89e-05 (2.32e-04)	Tok/s 47206 (43089)	Loss/tok 3.2088 (3.2952)	LR 1.250e-04
0: TRAIN [1][2400/7762]	Time 0.465 (0.326)	Data 1.04e-04 (2.32e-04)	Tok/s 49877 (43087)	Loss/tok 3.3947 (3.2950)	LR 1.250e-04
0: TRAIN [1][2410/7762]	Time 0.174 (0.326)	Data 1.02e-04 (2.31e-04)	Tok/s 31246 (43077)	Loss/tok 2.6646 (3.2945)	LR 1.250e-04
0: TRAIN [1][2420/7762]	Time 0.461 (0.326)	Data 1.04e-04 (2.31e-04)	Tok/s 51160 (43075)	Loss/tok 3.3231 (3.2942)	LR 1.250e-04
0: TRAIN [1][2430/7762]	Time 0.459 (0.326)	Data 9.78e-05 (2.30e-04)	Tok/s 50716 (43078)	Loss/tok 3.4014 (3.2943)	LR 1.250e-04
0: TRAIN [1][2440/7762]	Time 0.459 (0.326)	Data 1.72e-04 (2.30e-04)	Tok/s 51254 (43082)	Loss/tok 3.3090 (3.2943)	LR 1.250e-04
0: TRAIN [1][2450/7762]	Time 0.257 (0.326)	Data 1.09e-04 (2.29e-04)	Tok/s 39689 (43089)	Loss/tok 3.0552 (3.2949)	LR 1.250e-04
0: TRAIN [1][2460/7762]	Time 0.261 (0.326)	Data 1.16e-04 (2.29e-04)	Tok/s 38987 (43095)	Loss/tok 3.1193 (3.2948)	LR 1.250e-04
0: TRAIN [1][2470/7762]	Time 0.364 (0.326)	Data 1.07e-04 (2.28e-04)	Tok/s 46316 (43092)	Loss/tok 3.2124 (3.2949)	LR 1.250e-04
0: TRAIN [1][2480/7762]	Time 0.258 (0.326)	Data 1.73e-04 (2.28e-04)	Tok/s 39217 (43107)	Loss/tok 3.1603 (3.2954)	LR 1.250e-04
0: TRAIN [1][2490/7762]	Time 0.264 (0.326)	Data 1.03e-04 (2.27e-04)	Tok/s 39214 (43110)	Loss/tok 3.1514 (3.2951)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][2500/7762]	Time 0.463 (0.326)	Data 9.99e-05 (2.27e-04)	Tok/s 50085 (43110)	Loss/tok 3.4879 (3.2950)	LR 1.250e-04
0: TRAIN [1][2510/7762]	Time 0.264 (0.326)	Data 1.06e-04 (2.26e-04)	Tok/s 39567 (43104)	Loss/tok 3.1413 (3.2946)	LR 1.250e-04
0: TRAIN [1][2520/7762]	Time 0.360 (0.326)	Data 1.03e-04 (2.26e-04)	Tok/s 46872 (43099)	Loss/tok 3.3244 (3.2946)	LR 1.250e-04
0: TRAIN [1][2530/7762]	Time 0.261 (0.326)	Data 1.02e-04 (2.26e-04)	Tok/s 39846 (43086)	Loss/tok 3.0491 (3.2941)	LR 1.250e-04
0: TRAIN [1][2540/7762]	Time 0.265 (0.326)	Data 9.94e-05 (2.25e-04)	Tok/s 38931 (43072)	Loss/tok 3.0553 (3.2936)	LR 1.250e-04
0: TRAIN [1][2550/7762]	Time 0.262 (0.326)	Data 1.07e-04 (2.25e-04)	Tok/s 39900 (43060)	Loss/tok 2.9904 (3.2935)	LR 1.250e-04
0: TRAIN [1][2560/7762]	Time 0.359 (0.326)	Data 1.02e-04 (2.24e-04)	Tok/s 46649 (43053)	Loss/tok 3.2540 (3.2932)	LR 1.250e-04
0: TRAIN [1][2570/7762]	Time 0.256 (0.326)	Data 1.01e-04 (2.24e-04)	Tok/s 39974 (43052)	Loss/tok 3.0721 (3.2932)	LR 1.250e-04
0: TRAIN [1][2580/7762]	Time 0.261 (0.325)	Data 1.07e-04 (2.23e-04)	Tok/s 39438 (43042)	Loss/tok 3.1513 (3.2929)	LR 1.250e-04
0: TRAIN [1][2590/7762]	Time 0.259 (0.325)	Data 1.05e-04 (2.23e-04)	Tok/s 40183 (43028)	Loss/tok 3.0822 (3.2925)	LR 1.250e-04
0: TRAIN [1][2600/7762]	Time 0.353 (0.325)	Data 9.94e-05 (2.22e-04)	Tok/s 47205 (43024)	Loss/tok 3.1881 (3.2926)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][2610/7762]	Time 0.365 (0.325)	Data 1.02e-04 (2.22e-04)	Tok/s 46180 (43038)	Loss/tok 3.1901 (3.2929)	LR 1.250e-04
0: TRAIN [1][2620/7762]	Time 0.255 (0.325)	Data 1.03e-04 (2.21e-04)	Tok/s 40375 (43040)	Loss/tok 3.0920 (3.2932)	LR 1.250e-04
0: TRAIN [1][2630/7762]	Time 0.462 (0.325)	Data 9.99e-05 (2.21e-04)	Tok/s 50097 (43025)	Loss/tok 3.4588 (3.2929)	LR 1.250e-04
0: TRAIN [1][2640/7762]	Time 0.357 (0.325)	Data 1.03e-04 (2.21e-04)	Tok/s 47047 (43033)	Loss/tok 3.3511 (3.2933)	LR 1.250e-04
0: TRAIN [1][2650/7762]	Time 0.444 (0.325)	Data 9.94e-05 (2.20e-04)	Tok/s 53034 (43035)	Loss/tok 3.4750 (3.2934)	LR 1.250e-04
0: TRAIN [1][2660/7762]	Time 0.359 (0.325)	Data 1.04e-04 (2.20e-04)	Tok/s 46193 (43034)	Loss/tok 3.3873 (3.2934)	LR 1.250e-04
0: TRAIN [1][2670/7762]	Time 0.175 (0.325)	Data 9.54e-05 (2.19e-04)	Tok/s 30414 (43014)	Loss/tok 2.6920 (3.2930)	LR 1.250e-04
0: TRAIN [1][2680/7762]	Time 0.344 (0.325)	Data 1.03e-04 (2.19e-04)	Tok/s 48445 (43022)	Loss/tok 3.2539 (3.2929)	LR 1.250e-04
0: TRAIN [1][2690/7762]	Time 0.268 (0.325)	Data 1.02e-04 (2.18e-04)	Tok/s 38792 (43016)	Loss/tok 2.9978 (3.2929)	LR 1.250e-04
0: TRAIN [1][2700/7762]	Time 0.262 (0.325)	Data 1.03e-04 (2.18e-04)	Tok/s 39143 (43014)	Loss/tok 3.0579 (3.2930)	LR 1.250e-04
0: TRAIN [1][2710/7762]	Time 0.266 (0.326)	Data 1.04e-04 (2.18e-04)	Tok/s 38388 (43024)	Loss/tok 3.0813 (3.2935)	LR 1.250e-04
0: TRAIN [1][2720/7762]	Time 0.366 (0.326)	Data 1.05e-04 (2.17e-04)	Tok/s 45879 (43040)	Loss/tok 3.2716 (3.2939)	LR 1.250e-04
0: TRAIN [1][2730/7762]	Time 0.265 (0.326)	Data 1.05e-04 (2.17e-04)	Tok/s 38874 (43034)	Loss/tok 3.0308 (3.2936)	LR 1.250e-04
0: TRAIN [1][2740/7762]	Time 0.353 (0.326)	Data 9.85e-05 (2.16e-04)	Tok/s 47428 (43033)	Loss/tok 3.1693 (3.2935)	LR 1.250e-04
0: TRAIN [1][2750/7762]	Time 0.463 (0.326)	Data 1.03e-04 (2.16e-04)	Tok/s 50639 (43029)	Loss/tok 3.4773 (3.2934)	LR 1.250e-04
0: TRAIN [1][2760/7762]	Time 0.554 (0.326)	Data 1.00e-04 (2.15e-04)	Tok/s 54111 (43025)	Loss/tok 3.5437 (3.2933)	LR 1.250e-04
0: TRAIN [1][2770/7762]	Time 0.353 (0.325)	Data 1.02e-04 (2.15e-04)	Tok/s 47858 (43018)	Loss/tok 3.2785 (3.2929)	LR 1.250e-04
0: TRAIN [1][2780/7762]	Time 0.262 (0.325)	Data 1.01e-04 (2.15e-04)	Tok/s 39969 (43016)	Loss/tok 2.9830 (3.2926)	LR 1.250e-04
0: TRAIN [1][2790/7762]	Time 0.260 (0.326)	Data 1.06e-04 (2.14e-04)	Tok/s 39533 (43029)	Loss/tok 3.0469 (3.2926)	LR 1.250e-04
0: TRAIN [1][2800/7762]	Time 0.358 (0.326)	Data 1.00e-04 (2.14e-04)	Tok/s 46580 (43033)	Loss/tok 3.1912 (3.2928)	LR 1.250e-04
0: TRAIN [1][2810/7762]	Time 0.345 (0.326)	Data 1.14e-04 (2.13e-04)	Tok/s 49161 (43028)	Loss/tok 3.1932 (3.2926)	LR 1.250e-04
0: TRAIN [1][2820/7762]	Time 0.265 (0.325)	Data 1.24e-04 (2.13e-04)	Tok/s 38854 (43020)	Loss/tok 2.9867 (3.2922)	LR 1.250e-04
0: TRAIN [1][2830/7762]	Time 0.459 (0.325)	Data 1.06e-04 (2.13e-04)	Tok/s 49954 (43015)	Loss/tok 3.5273 (3.2921)	LR 1.250e-04
0: TRAIN [1][2840/7762]	Time 0.252 (0.325)	Data 1.03e-04 (2.12e-04)	Tok/s 40773 (43014)	Loss/tok 3.1187 (3.2918)	LR 1.250e-04
0: TRAIN [1][2850/7762]	Time 0.179 (0.325)	Data 1.09e-04 (2.12e-04)	Tok/s 29568 (43005)	Loss/tok 2.6504 (3.2916)	LR 1.250e-04
0: TRAIN [1][2860/7762]	Time 0.362 (0.325)	Data 1.02e-04 (2.12e-04)	Tok/s 46225 (43012)	Loss/tok 3.2498 (3.2918)	LR 1.250e-04
0: TRAIN [1][2870/7762]	Time 0.259 (0.325)	Data 1.05e-04 (2.11e-04)	Tok/s 39680 (43013)	Loss/tok 3.0744 (3.2916)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][2880/7762]	Time 0.575 (0.325)	Data 1.18e-04 (2.11e-04)	Tok/s 51887 (43025)	Loss/tok 3.5004 (3.2923)	LR 1.250e-04
0: TRAIN [1][2890/7762]	Time 0.266 (0.325)	Data 1.08e-04 (2.11e-04)	Tok/s 39305 (43019)	Loss/tok 3.1712 (3.2924)	LR 1.250e-04
0: TRAIN [1][2900/7762]	Time 0.177 (0.325)	Data 1.04e-04 (2.10e-04)	Tok/s 29639 (43023)	Loss/tok 2.5882 (3.2924)	LR 1.250e-04
0: TRAIN [1][2910/7762]	Time 0.353 (0.325)	Data 9.94e-05 (2.10e-04)	Tok/s 48064 (43020)	Loss/tok 3.2779 (3.2920)	LR 1.250e-04
0: TRAIN [1][2920/7762]	Time 0.261 (0.325)	Data 1.02e-04 (2.09e-04)	Tok/s 40347 (43008)	Loss/tok 3.1534 (3.2917)	LR 1.250e-04
0: TRAIN [1][2930/7762]	Time 0.368 (0.325)	Data 1.00e-04 (2.09e-04)	Tok/s 45139 (43016)	Loss/tok 3.3460 (3.2920)	LR 1.250e-04
0: TRAIN [1][2940/7762]	Time 0.266 (0.326)	Data 1.08e-04 (2.09e-04)	Tok/s 39139 (43026)	Loss/tok 3.1723 (3.2923)	LR 1.250e-04
0: TRAIN [1][2950/7762]	Time 0.267 (0.325)	Data 1.16e-04 (2.08e-04)	Tok/s 38075 (43019)	Loss/tok 3.0498 (3.2920)	LR 1.250e-04
0: TRAIN [1][2960/7762]	Time 0.265 (0.326)	Data 1.18e-04 (2.08e-04)	Tok/s 39180 (43018)	Loss/tok 2.9378 (3.2923)	LR 1.250e-04
0: TRAIN [1][2970/7762]	Time 0.267 (0.326)	Data 1.05e-04 (2.08e-04)	Tok/s 38607 (43023)	Loss/tok 3.0395 (3.2924)	LR 1.250e-04
0: TRAIN [1][2980/7762]	Time 0.176 (0.325)	Data 1.03e-04 (2.07e-04)	Tok/s 30190 (43012)	Loss/tok 2.5902 (3.2921)	LR 1.250e-04
0: TRAIN [1][2990/7762]	Time 0.366 (0.326)	Data 1.05e-04 (2.07e-04)	Tok/s 46084 (43019)	Loss/tok 3.2990 (3.2925)	LR 1.250e-04
0: TRAIN [1][3000/7762]	Time 0.262 (0.326)	Data 1.05e-04 (2.07e-04)	Tok/s 39164 (43019)	Loss/tok 3.0870 (3.2924)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][3010/7762]	Time 0.352 (0.326)	Data 1.08e-04 (2.06e-04)	Tok/s 47824 (43025)	Loss/tok 3.3075 (3.2928)	LR 1.250e-04
0: TRAIN [1][3020/7762]	Time 0.175 (0.326)	Data 1.05e-04 (2.06e-04)	Tok/s 30166 (43018)	Loss/tok 2.5097 (3.2927)	LR 1.250e-04
0: TRAIN [1][3030/7762]	Time 0.461 (0.326)	Data 1.03e-04 (2.06e-04)	Tok/s 50800 (43013)	Loss/tok 3.3867 (3.2924)	LR 1.250e-04
0: TRAIN [1][3040/7762]	Time 0.255 (0.326)	Data 1.04e-04 (2.05e-04)	Tok/s 39823 (43016)	Loss/tok 3.0344 (3.2929)	LR 1.250e-04
0: TRAIN [1][3050/7762]	Time 0.264 (0.326)	Data 1.18e-04 (2.05e-04)	Tok/s 38975 (43000)	Loss/tok 3.0693 (3.2926)	LR 1.250e-04
0: TRAIN [1][3060/7762]	Time 0.263 (0.326)	Data 1.04e-04 (2.05e-04)	Tok/s 38790 (43002)	Loss/tok 3.0290 (3.2924)	LR 1.250e-04
0: TRAIN [1][3070/7762]	Time 0.261 (0.326)	Data 1.03e-04 (2.04e-04)	Tok/s 40348 (42997)	Loss/tok 3.1397 (3.2923)	LR 1.250e-04
0: TRAIN [1][3080/7762]	Time 0.263 (0.325)	Data 1.04e-04 (2.04e-04)	Tok/s 38889 (42994)	Loss/tok 3.0554 (3.2922)	LR 1.250e-04
0: TRAIN [1][3090/7762]	Time 0.267 (0.325)	Data 1.25e-04 (2.04e-04)	Tok/s 39248 (42990)	Loss/tok 3.0533 (3.2919)	LR 1.250e-04
0: TRAIN [1][3100/7762]	Time 0.586 (0.325)	Data 1.01e-04 (2.03e-04)	Tok/s 50962 (42985)	Loss/tok 3.6955 (3.2918)	LR 1.250e-04
0: TRAIN [1][3110/7762]	Time 0.360 (0.325)	Data 1.03e-04 (2.03e-04)	Tok/s 46738 (42990)	Loss/tok 3.1990 (3.2918)	LR 1.250e-04
0: TRAIN [1][3120/7762]	Time 0.454 (0.325)	Data 1.22e-04 (2.03e-04)	Tok/s 51560 (42988)	Loss/tok 3.5585 (3.2917)	LR 1.250e-04
0: TRAIN [1][3130/7762]	Time 0.353 (0.325)	Data 1.01e-04 (2.02e-04)	Tok/s 47513 (42992)	Loss/tok 3.1984 (3.2914)	LR 1.250e-04
0: TRAIN [1][3140/7762]	Time 0.356 (0.325)	Data 1.04e-04 (2.02e-04)	Tok/s 46843 (42996)	Loss/tok 3.3857 (3.2914)	LR 1.250e-04
0: TRAIN [1][3150/7762]	Time 0.268 (0.325)	Data 1.03e-04 (2.02e-04)	Tok/s 38643 (42982)	Loss/tok 3.0984 (3.2909)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][3160/7762]	Time 0.170 (0.325)	Data 1.11e-04 (2.02e-04)	Tok/s 30931 (42976)	Loss/tok 2.6604 (3.2908)	LR 1.250e-04
0: TRAIN [1][3170/7762]	Time 0.451 (0.325)	Data 1.03e-04 (2.01e-04)	Tok/s 51798 (42976)	Loss/tok 3.3976 (3.2907)	LR 1.250e-04
0: TRAIN [1][3180/7762]	Time 0.254 (0.325)	Data 1.03e-04 (2.01e-04)	Tok/s 40362 (42979)	Loss/tok 2.8982 (3.2907)	LR 1.250e-04
0: TRAIN [1][3190/7762]	Time 0.261 (0.325)	Data 1.18e-04 (2.01e-04)	Tok/s 40094 (42975)	Loss/tok 3.0100 (3.2903)	LR 1.250e-04
0: TRAIN [1][3200/7762]	Time 0.260 (0.325)	Data 1.03e-04 (2.00e-04)	Tok/s 39979 (42979)	Loss/tok 3.2152 (3.2904)	LR 1.250e-04
0: TRAIN [1][3210/7762]	Time 0.347 (0.325)	Data 1.05e-04 (2.00e-04)	Tok/s 47743 (42992)	Loss/tok 3.3096 (3.2905)	LR 1.250e-04
0: TRAIN [1][3220/7762]	Time 0.346 (0.325)	Data 1.02e-04 (2.00e-04)	Tok/s 48356 (42993)	Loss/tok 3.2637 (3.2903)	LR 1.250e-04
0: TRAIN [1][3230/7762]	Time 0.452 (0.325)	Data 1.04e-04 (1.99e-04)	Tok/s 51573 (42990)	Loss/tok 3.4031 (3.2902)	LR 1.250e-04
0: TRAIN [1][3240/7762]	Time 0.265 (0.325)	Data 1.02e-04 (1.99e-04)	Tok/s 39471 (42977)	Loss/tok 2.9804 (3.2896)	LR 1.250e-04
0: TRAIN [1][3250/7762]	Time 0.364 (0.325)	Data 1.04e-04 (1.99e-04)	Tok/s 46595 (42988)	Loss/tok 3.2089 (3.2896)	LR 1.250e-04
0: TRAIN [1][3260/7762]	Time 0.175 (0.325)	Data 2.37e-04 (1.99e-04)	Tok/s 29656 (42985)	Loss/tok 2.7426 (3.2894)	LR 1.250e-04
0: TRAIN [1][3270/7762]	Time 0.359 (0.325)	Data 9.99e-05 (1.98e-04)	Tok/s 47208 (42982)	Loss/tok 3.3311 (3.2890)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][3280/7762]	Time 0.444 (0.325)	Data 9.87e-05 (1.98e-04)	Tok/s 52858 (42975)	Loss/tok 3.4358 (3.2887)	LR 1.250e-04
0: TRAIN [1][3290/7762]	Time 0.350 (0.325)	Data 1.03e-04 (1.98e-04)	Tok/s 48240 (42974)	Loss/tok 3.3292 (3.2888)	LR 1.250e-04
0: TRAIN [1][3300/7762]	Time 0.364 (0.325)	Data 1.03e-04 (1.97e-04)	Tok/s 46598 (42975)	Loss/tok 3.3305 (3.2891)	LR 1.250e-04
0: TRAIN [1][3310/7762]	Time 0.264 (0.325)	Data 1.04e-04 (1.97e-04)	Tok/s 38872 (42974)	Loss/tok 3.0090 (3.2891)	LR 1.250e-04
0: TRAIN [1][3320/7762]	Time 0.266 (0.325)	Data 1.03e-04 (1.97e-04)	Tok/s 39438 (42971)	Loss/tok 2.9591 (3.2891)	LR 1.250e-04
0: TRAIN [1][3330/7762]	Time 0.357 (0.325)	Data 1.04e-04 (1.97e-04)	Tok/s 46782 (42970)	Loss/tok 3.2576 (3.2890)	LR 1.250e-04
0: TRAIN [1][3340/7762]	Time 0.264 (0.325)	Data 1.04e-04 (1.96e-04)	Tok/s 40026 (42968)	Loss/tok 3.2061 (3.2890)	LR 1.250e-04
0: TRAIN [1][3350/7762]	Time 0.264 (0.325)	Data 1.10e-04 (1.96e-04)	Tok/s 39062 (42964)	Loss/tok 3.1271 (3.2888)	LR 1.250e-04
0: TRAIN [1][3360/7762]	Time 0.362 (0.325)	Data 1.03e-04 (1.96e-04)	Tok/s 46797 (42959)	Loss/tok 3.2084 (3.2885)	LR 1.250e-04
0: TRAIN [1][3370/7762]	Time 0.251 (0.325)	Data 1.20e-04 (1.96e-04)	Tok/s 41160 (42962)	Loss/tok 3.1218 (3.2883)	LR 1.250e-04
0: TRAIN [1][3380/7762]	Time 0.581 (0.325)	Data 9.75e-05 (1.95e-04)	Tok/s 51868 (42967)	Loss/tok 3.5673 (3.2884)	LR 1.250e-04
0: TRAIN [1][3390/7762]	Time 0.264 (0.325)	Data 1.06e-04 (1.95e-04)	Tok/s 38727 (42959)	Loss/tok 2.9899 (3.2881)	LR 1.250e-04
0: TRAIN [1][3400/7762]	Time 0.263 (0.324)	Data 1.02e-04 (1.95e-04)	Tok/s 39195 (42958)	Loss/tok 3.0964 (3.2878)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][3410/7762]	Time 0.451 (0.324)	Data 1.60e-04 (1.95e-04)	Tok/s 51257 (42959)	Loss/tok 3.5787 (3.2879)	LR 1.250e-04
0: TRAIN [1][3420/7762]	Time 0.363 (0.325)	Data 1.08e-04 (1.94e-04)	Tok/s 46417 (42967)	Loss/tok 3.3808 (3.2879)	LR 1.250e-04
0: TRAIN [1][3430/7762]	Time 0.265 (0.325)	Data 1.04e-04 (1.94e-04)	Tok/s 39239 (42969)	Loss/tok 3.0267 (3.2880)	LR 1.250e-04
0: TRAIN [1][3440/7762]	Time 0.348 (0.325)	Data 1.04e-04 (1.94e-04)	Tok/s 49104 (42975)	Loss/tok 3.2649 (3.2879)	LR 1.250e-04
0: TRAIN [1][3450/7762]	Time 0.349 (0.325)	Data 1.03e-04 (1.94e-04)	Tok/s 48080 (42976)	Loss/tok 3.2534 (3.2877)	LR 1.250e-04
0: TRAIN [1][3460/7762]	Time 0.351 (0.325)	Data 1.04e-04 (1.93e-04)	Tok/s 47761 (42982)	Loss/tok 3.3477 (3.2880)	LR 1.250e-04
0: TRAIN [1][3470/7762]	Time 0.462 (0.325)	Data 1.01e-04 (1.93e-04)	Tok/s 50483 (42982)	Loss/tok 3.4755 (3.2880)	LR 1.250e-04
0: TRAIN [1][3480/7762]	Time 0.589 (0.325)	Data 1.04e-04 (1.93e-04)	Tok/s 50340 (42985)	Loss/tok 3.7079 (3.2882)	LR 1.250e-04
0: TRAIN [1][3490/7762]	Time 0.583 (0.325)	Data 1.01e-04 (1.93e-04)	Tok/s 51756 (42981)	Loss/tok 3.5592 (3.2884)	LR 1.250e-04
0: TRAIN [1][3500/7762]	Time 0.587 (0.325)	Data 1.03e-04 (1.92e-04)	Tok/s 50891 (42984)	Loss/tok 3.6031 (3.2886)	LR 1.250e-04
0: TRAIN [1][3510/7762]	Time 0.251 (0.325)	Data 1.07e-04 (1.92e-04)	Tok/s 40651 (42984)	Loss/tok 3.0068 (3.2885)	LR 1.250e-04
0: TRAIN [1][3520/7762]	Time 0.461 (0.325)	Data 1.03e-04 (1.92e-04)	Tok/s 50176 (42988)	Loss/tok 3.4966 (3.2885)	LR 1.250e-04
0: TRAIN [1][3530/7762]	Time 0.172 (0.325)	Data 1.04e-04 (1.92e-04)	Tok/s 30798 (42982)	Loss/tok 2.6645 (3.2883)	LR 1.250e-04
0: TRAIN [1][3540/7762]	Time 0.172 (0.325)	Data 1.03e-04 (1.91e-04)	Tok/s 30295 (42981)	Loss/tok 2.5383 (3.2883)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][3550/7762]	Time 0.365 (0.325)	Data 1.04e-04 (1.91e-04)	Tok/s 45406 (42977)	Loss/tok 3.3075 (3.2881)	LR 1.250e-04
0: TRAIN [1][3560/7762]	Time 0.352 (0.325)	Data 1.03e-04 (1.91e-04)	Tok/s 48027 (42972)	Loss/tok 3.2610 (3.2878)	LR 1.250e-04
0: TRAIN [1][3570/7762]	Time 0.345 (0.325)	Data 1.07e-04 (1.91e-04)	Tok/s 48576 (42978)	Loss/tok 3.2931 (3.2878)	LR 1.250e-04
0: TRAIN [1][3580/7762]	Time 0.363 (0.325)	Data 1.06e-04 (1.90e-04)	Tok/s 46008 (42979)	Loss/tok 3.2594 (3.2879)	LR 1.250e-04
0: TRAIN [1][3590/7762]	Time 0.358 (0.325)	Data 1.04e-04 (1.90e-04)	Tok/s 46294 (42978)	Loss/tok 3.2756 (3.2878)	LR 1.250e-04
0: TRAIN [1][3600/7762]	Time 0.364 (0.325)	Data 1.18e-04 (1.90e-04)	Tok/s 46393 (42987)	Loss/tok 3.2759 (3.2879)	LR 1.250e-04
0: TRAIN [1][3610/7762]	Time 0.586 (0.325)	Data 1.18e-04 (1.90e-04)	Tok/s 50501 (43002)	Loss/tok 3.6804 (3.2883)	LR 1.250e-04
0: TRAIN [1][3620/7762]	Time 0.359 (0.325)	Data 1.01e-04 (1.90e-04)	Tok/s 47844 (43010)	Loss/tok 3.2307 (3.2884)	LR 1.250e-04
0: TRAIN [1][3630/7762]	Time 0.364 (0.325)	Data 1.02e-04 (1.89e-04)	Tok/s 46541 (43014)	Loss/tok 3.2038 (3.2885)	LR 1.250e-04
0: TRAIN [1][3640/7762]	Time 0.363 (0.325)	Data 1.06e-04 (1.89e-04)	Tok/s 46866 (43008)	Loss/tok 3.3068 (3.2883)	LR 1.250e-04
0: TRAIN [1][3650/7762]	Time 0.346 (0.325)	Data 1.08e-04 (1.89e-04)	Tok/s 48396 (43009)	Loss/tok 3.1714 (3.2881)	LR 1.250e-04
0: TRAIN [1][3660/7762]	Time 0.265 (0.325)	Data 1.07e-04 (1.89e-04)	Tok/s 39035 (43004)	Loss/tok 2.9411 (3.2878)	LR 1.250e-04
0: TRAIN [1][3670/7762]	Time 0.364 (0.325)	Data 1.04e-04 (1.88e-04)	Tok/s 46298 (43003)	Loss/tok 3.4102 (3.2878)	LR 1.250e-04
0: TRAIN [1][3680/7762]	Time 0.268 (0.325)	Data 1.03e-04 (1.88e-04)	Tok/s 37965 (42996)	Loss/tok 3.0983 (3.2874)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][3690/7762]	Time 0.261 (0.325)	Data 1.42e-04 (1.88e-04)	Tok/s 40229 (42990)	Loss/tok 3.1406 (3.2873)	LR 1.250e-04
0: TRAIN [1][3700/7762]	Time 0.584 (0.325)	Data 1.13e-04 (1.88e-04)	Tok/s 51896 (42993)	Loss/tok 3.5754 (3.2875)	LR 1.250e-04
0: TRAIN [1][3710/7762]	Time 0.263 (0.325)	Data 1.03e-04 (1.88e-04)	Tok/s 39313 (42993)	Loss/tok 3.0375 (3.2874)	LR 1.250e-04
0: TRAIN [1][3720/7762]	Time 0.359 (0.325)	Data 1.02e-04 (1.87e-04)	Tok/s 46146 (42995)	Loss/tok 3.3134 (3.2872)	LR 1.250e-04
0: TRAIN [1][3730/7762]	Time 0.582 (0.325)	Data 1.03e-04 (1.87e-04)	Tok/s 50675 (42997)	Loss/tok 3.7373 (3.2873)	LR 1.250e-04
0: TRAIN [1][3740/7762]	Time 0.455 (0.325)	Data 1.06e-04 (1.87e-04)	Tok/s 51252 (43002)	Loss/tok 3.5324 (3.2875)	LR 1.250e-04
0: TRAIN [1][3750/7762]	Time 0.457 (0.325)	Data 1.03e-04 (1.87e-04)	Tok/s 50674 (43010)	Loss/tok 3.4570 (3.2876)	LR 1.250e-04
0: TRAIN [1][3760/7762]	Time 0.264 (0.325)	Data 1.06e-04 (1.87e-04)	Tok/s 39922 (43007)	Loss/tok 3.0872 (3.2875)	LR 1.250e-04
0: TRAIN [1][3770/7762]	Time 0.356 (0.325)	Data 1.05e-04 (1.86e-04)	Tok/s 47316 (43007)	Loss/tok 3.2330 (3.2872)	LR 1.250e-04
0: TRAIN [1][3780/7762]	Time 0.460 (0.325)	Data 1.02e-04 (1.86e-04)	Tok/s 50553 (43003)	Loss/tok 3.4328 (3.2871)	LR 1.250e-04
0: TRAIN [1][3790/7762]	Time 0.353 (0.325)	Data 1.03e-04 (1.86e-04)	Tok/s 47062 (43005)	Loss/tok 3.3840 (3.2870)	LR 1.250e-04
0: TRAIN [1][3800/7762]	Time 0.358 (0.325)	Data 1.01e-04 (1.86e-04)	Tok/s 47063 (43003)	Loss/tok 3.3664 (3.2868)	LR 1.250e-04
0: TRAIN [1][3810/7762]	Time 0.173 (0.325)	Data 9.99e-05 (1.85e-04)	Tok/s 30542 (43000)	Loss/tok 2.6006 (3.2868)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][3820/7762]	Time 0.260 (0.325)	Data 1.02e-04 (1.85e-04)	Tok/s 39229 (43003)	Loss/tok 3.1130 (3.2867)	LR 1.250e-04
0: TRAIN [1][3830/7762]	Time 0.177 (0.325)	Data 9.92e-05 (1.85e-04)	Tok/s 29895 (43005)	Loss/tok 2.6701 (3.2866)	LR 1.250e-04
0: TRAIN [1][3840/7762]	Time 0.174 (0.325)	Data 1.02e-04 (1.85e-04)	Tok/s 30189 (42995)	Loss/tok 2.6415 (3.2863)	LR 1.250e-04
0: TRAIN [1][3850/7762]	Time 0.262 (0.325)	Data 9.82e-05 (1.85e-04)	Tok/s 38648 (42985)	Loss/tok 3.0255 (3.2861)	LR 1.250e-04
0: TRAIN [1][3860/7762]	Time 0.588 (0.325)	Data 9.99e-05 (1.84e-04)	Tok/s 50303 (42993)	Loss/tok 3.6988 (3.2864)	LR 1.250e-04
0: TRAIN [1][3870/7762]	Time 0.358 (0.325)	Data 9.92e-05 (1.84e-04)	Tok/s 46987 (42986)	Loss/tok 3.3166 (3.2862)	LR 1.250e-04
0: TRAIN [1][3880/7762]	Time 0.366 (0.325)	Data 1.03e-04 (1.84e-04)	Tok/s 45529 (42997)	Loss/tok 3.3520 (3.2867)	LR 1.250e-04
0: TRAIN [1][3890/7762]	Time 0.260 (0.325)	Data 1.03e-04 (1.84e-04)	Tok/s 40062 (42996)	Loss/tok 3.1869 (3.2867)	LR 1.250e-04
0: TRAIN [1][3900/7762]	Time 0.569 (0.325)	Data 1.05e-04 (1.84e-04)	Tok/s 52067 (43002)	Loss/tok 3.6175 (3.2871)	LR 1.250e-04
0: TRAIN [1][3910/7762]	Time 0.268 (0.325)	Data 9.89e-05 (1.83e-04)	Tok/s 37866 (42998)	Loss/tok 3.0860 (3.2869)	LR 1.250e-04
0: TRAIN [1][3920/7762]	Time 0.260 (0.325)	Data 1.02e-04 (1.83e-04)	Tok/s 40122 (42994)	Loss/tok 3.0323 (3.2870)	LR 1.250e-04
0: TRAIN [1][3930/7762]	Time 0.178 (0.325)	Data 1.02e-04 (1.83e-04)	Tok/s 30522 (42988)	Loss/tok 2.6509 (3.2869)	LR 1.250e-04
0: TRAIN [1][3940/7762]	Time 0.350 (0.325)	Data 1.12e-04 (1.83e-04)	Tok/s 47998 (42981)	Loss/tok 3.2134 (3.2866)	LR 1.250e-04
0: TRAIN [1][3950/7762]	Time 0.267 (0.325)	Data 1.18e-04 (1.83e-04)	Tok/s 37853 (42974)	Loss/tok 3.1043 (3.2865)	LR 1.250e-04
0: TRAIN [1][3960/7762]	Time 0.364 (0.325)	Data 1.06e-04 (1.82e-04)	Tok/s 46145 (42969)	Loss/tok 3.1759 (3.2862)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][3970/7762]	Time 0.357 (0.324)	Data 1.01e-04 (1.82e-04)	Tok/s 46920 (42962)	Loss/tok 3.3700 (3.2859)	LR 1.250e-04
0: TRAIN [1][3980/7762]	Time 0.264 (0.324)	Data 1.02e-04 (1.82e-04)	Tok/s 39399 (42962)	Loss/tok 3.0951 (3.2859)	LR 1.250e-04
0: TRAIN [1][3990/7762]	Time 0.457 (0.324)	Data 1.62e-04 (1.82e-04)	Tok/s 50561 (42965)	Loss/tok 3.4732 (3.2860)	LR 1.250e-04
0: TRAIN [1][4000/7762]	Time 0.446 (0.325)	Data 1.03e-04 (1.82e-04)	Tok/s 52428 (42965)	Loss/tok 3.5382 (3.2861)	LR 1.250e-04
0: TRAIN [1][4010/7762]	Time 0.269 (0.325)	Data 1.02e-04 (1.81e-04)	Tok/s 38230 (42967)	Loss/tok 3.1374 (3.2861)	LR 1.250e-04
0: TRAIN [1][4020/7762]	Time 0.262 (0.325)	Data 1.02e-04 (1.81e-04)	Tok/s 39023 (42973)	Loss/tok 3.1283 (3.2862)	LR 1.250e-04
0: TRAIN [1][4030/7762]	Time 0.254 (0.325)	Data 1.00e-04 (1.81e-04)	Tok/s 40512 (42972)	Loss/tok 3.1365 (3.2861)	LR 1.250e-04
0: TRAIN [1][4040/7762]	Time 0.362 (0.325)	Data 1.03e-04 (1.81e-04)	Tok/s 47295 (42972)	Loss/tok 3.1346 (3.2859)	LR 1.250e-04
0: TRAIN [1][4050/7762]	Time 0.265 (0.325)	Data 1.03e-04 (1.81e-04)	Tok/s 38662 (42973)	Loss/tok 3.0563 (3.2861)	LR 1.250e-04
0: TRAIN [1][4060/7762]	Time 0.360 (0.325)	Data 1.06e-04 (1.80e-04)	Tok/s 46850 (42967)	Loss/tok 3.2465 (3.2860)	LR 1.250e-04
0: TRAIN [1][4070/7762]	Time 0.457 (0.325)	Data 1.04e-04 (1.80e-04)	Tok/s 51137 (42974)	Loss/tok 3.5036 (3.2862)	LR 1.250e-04
0: TRAIN [1][4080/7762]	Time 0.366 (0.325)	Data 1.03e-04 (1.80e-04)	Tok/s 46122 (42979)	Loss/tok 3.2782 (3.2863)	LR 1.250e-04
0: TRAIN [1][4090/7762]	Time 0.268 (0.325)	Data 1.03e-04 (1.80e-04)	Tok/s 37886 (42979)	Loss/tok 3.1414 (3.2862)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][4100/7762]	Time 0.430 (0.325)	Data 1.14e-04 (1.80e-04)	Tok/s 54905 (42990)	Loss/tok 3.4198 (3.2863)	LR 1.250e-04
0: TRAIN [1][4110/7762]	Time 0.366 (0.325)	Data 1.04e-04 (1.80e-04)	Tok/s 46160 (42985)	Loss/tok 3.3087 (3.2861)	LR 1.250e-04
0: TRAIN [1][4120/7762]	Time 0.464 (0.325)	Data 1.03e-04 (1.79e-04)	Tok/s 50723 (42994)	Loss/tok 3.4121 (3.2863)	LR 1.250e-04
0: TRAIN [1][4130/7762]	Time 0.344 (0.325)	Data 9.42e-05 (1.79e-04)	Tok/s 48352 (42996)	Loss/tok 3.3393 (3.2864)	LR 1.250e-04
0: TRAIN [1][4140/7762]	Time 0.453 (0.325)	Data 1.03e-04 (1.79e-04)	Tok/s 51193 (42996)	Loss/tok 3.3811 (3.2863)	LR 1.250e-04
0: TRAIN [1][4150/7762]	Time 0.263 (0.325)	Data 1.20e-04 (1.79e-04)	Tok/s 39724 (43003)	Loss/tok 3.0166 (3.2862)	LR 1.250e-04
0: TRAIN [1][4160/7762]	Time 0.261 (0.325)	Data 1.07e-04 (1.79e-04)	Tok/s 40760 (43003)	Loss/tok 2.9858 (3.2862)	LR 1.250e-04
0: TRAIN [1][4170/7762]	Time 0.363 (0.325)	Data 1.77e-04 (1.79e-04)	Tok/s 46637 (43005)	Loss/tok 3.2773 (3.2863)	LR 1.250e-04
0: TRAIN [1][4180/7762]	Time 0.261 (0.325)	Data 1.03e-04 (1.78e-04)	Tok/s 40170 (43009)	Loss/tok 3.0422 (3.2864)	LR 1.250e-04
0: TRAIN [1][4190/7762]	Time 0.175 (0.325)	Data 1.18e-04 (1.78e-04)	Tok/s 30363 (43005)	Loss/tok 2.7202 (3.2864)	LR 1.250e-04
0: TRAIN [1][4200/7762]	Time 0.261 (0.325)	Data 1.03e-04 (1.78e-04)	Tok/s 38493 (42996)	Loss/tok 3.0165 (3.2863)	LR 1.250e-04
0: TRAIN [1][4210/7762]	Time 0.574 (0.325)	Data 1.01e-04 (1.78e-04)	Tok/s 52169 (43004)	Loss/tok 3.5205 (3.2866)	LR 1.250e-04
0: TRAIN [1][4220/7762]	Time 0.364 (0.325)	Data 1.06e-04 (1.78e-04)	Tok/s 46729 (43006)	Loss/tok 3.3668 (3.2865)	LR 1.250e-04
0: TRAIN [1][4230/7762]	Time 0.354 (0.325)	Data 1.03e-04 (1.78e-04)	Tok/s 47269 (43003)	Loss/tok 3.2346 (3.2863)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][4240/7762]	Time 0.438 (0.325)	Data 1.06e-04 (1.77e-04)	Tok/s 53253 (43003)	Loss/tok 3.5624 (3.2862)	LR 1.250e-04
0: TRAIN [1][4250/7762]	Time 0.263 (0.325)	Data 1.04e-04 (1.77e-04)	Tok/s 38977 (43000)	Loss/tok 3.1075 (3.2862)	LR 1.250e-04
0: TRAIN [1][4260/7762]	Time 0.355 (0.325)	Data 9.80e-05 (1.77e-04)	Tok/s 46995 (42993)	Loss/tok 3.2377 (3.2859)	LR 1.250e-04
0: TRAIN [1][4270/7762]	Time 0.361 (0.325)	Data 1.01e-04 (1.77e-04)	Tok/s 46545 (42992)	Loss/tok 3.2565 (3.2857)	LR 1.250e-04
0: TRAIN [1][4280/7762]	Time 0.261 (0.325)	Data 1.09e-04 (1.77e-04)	Tok/s 38582 (42991)	Loss/tok 3.2251 (3.2857)	LR 1.250e-04
0: TRAIN [1][4290/7762]	Time 0.253 (0.325)	Data 9.82e-05 (1.77e-04)	Tok/s 40594 (42986)	Loss/tok 3.1193 (3.2856)	LR 1.250e-04
0: TRAIN [1][4300/7762]	Time 0.585 (0.325)	Data 1.02e-04 (1.76e-04)	Tok/s 50688 (42990)	Loss/tok 3.6057 (3.2857)	LR 1.250e-04
0: TRAIN [1][4310/7762]	Time 0.360 (0.325)	Data 1.02e-04 (1.76e-04)	Tok/s 46598 (42991)	Loss/tok 3.2194 (3.2856)	LR 1.250e-04
0: TRAIN [1][4320/7762]	Time 0.265 (0.325)	Data 1.27e-04 (1.76e-04)	Tok/s 39017 (42993)	Loss/tok 3.0350 (3.2856)	LR 1.250e-04
0: TRAIN [1][4330/7762]	Time 0.252 (0.325)	Data 1.08e-04 (1.76e-04)	Tok/s 41084 (42993)	Loss/tok 3.1537 (3.2854)	LR 1.250e-04
0: TRAIN [1][4340/7762]	Time 0.259 (0.325)	Data 1.02e-04 (1.76e-04)	Tok/s 39436 (42987)	Loss/tok 3.1188 (3.2852)	LR 1.250e-04
0: TRAIN [1][4350/7762]	Time 0.266 (0.324)	Data 1.21e-04 (1.76e-04)	Tok/s 39343 (42984)	Loss/tok 2.9032 (3.2849)	LR 1.250e-04
0: TRAIN [1][4360/7762]	Time 0.264 (0.324)	Data 9.82e-05 (1.75e-04)	Tok/s 39480 (42982)	Loss/tok 2.8505 (3.2850)	LR 1.250e-04
0: TRAIN [1][4370/7762]	Time 0.263 (0.324)	Data 1.02e-04 (1.75e-04)	Tok/s 39183 (42981)	Loss/tok 3.0037 (3.2847)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][4380/7762]	Time 0.261 (0.324)	Data 1.05e-04 (1.75e-04)	Tok/s 39372 (42980)	Loss/tok 3.0426 (3.2846)	LR 1.250e-04
0: TRAIN [1][4390/7762]	Time 0.265 (0.324)	Data 9.97e-05 (1.75e-04)	Tok/s 39015 (42976)	Loss/tok 3.0863 (3.2844)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][4400/7762]	Time 0.253 (0.324)	Data 1.20e-04 (1.75e-04)	Tok/s 41012 (42977)	Loss/tok 3.0674 (3.2844)	LR 1.250e-04
0: TRAIN [1][4410/7762]	Time 0.367 (0.324)	Data 1.04e-04 (1.75e-04)	Tok/s 45898 (42976)	Loss/tok 3.2582 (3.2843)	LR 1.250e-04
0: TRAIN [1][4420/7762]	Time 0.574 (0.324)	Data 1.03e-04 (1.74e-04)	Tok/s 51661 (42982)	Loss/tok 3.7674 (3.2846)	LR 1.250e-04
0: TRAIN [1][4430/7762]	Time 0.362 (0.324)	Data 1.03e-04 (1.74e-04)	Tok/s 46419 (42985)	Loss/tok 3.2382 (3.2846)	LR 1.250e-04
0: TRAIN [1][4440/7762]	Time 0.264 (0.325)	Data 1.07e-04 (1.74e-04)	Tok/s 39489 (42988)	Loss/tok 3.0266 (3.2847)	LR 1.250e-04
0: TRAIN [1][4450/7762]	Time 0.263 (0.325)	Data 9.82e-05 (1.74e-04)	Tok/s 39294 (42991)	Loss/tok 2.9955 (3.2848)	LR 1.250e-04
0: TRAIN [1][4460/7762]	Time 0.265 (0.325)	Data 9.97e-05 (1.74e-04)	Tok/s 38658 (42990)	Loss/tok 3.1105 (3.2846)	LR 1.250e-04
0: TRAIN [1][4470/7762]	Time 0.267 (0.325)	Data 1.02e-04 (1.74e-04)	Tok/s 38379 (42988)	Loss/tok 3.1206 (3.2846)	LR 1.250e-04
0: TRAIN [1][4480/7762]	Time 0.260 (0.324)	Data 1.03e-04 (1.73e-04)	Tok/s 39926 (42984)	Loss/tok 3.1813 (3.2844)	LR 1.250e-04
0: TRAIN [1][4490/7762]	Time 0.259 (0.324)	Data 1.03e-04 (1.73e-04)	Tok/s 39308 (42978)	Loss/tok 3.0993 (3.2842)	LR 1.250e-04
0: TRAIN [1][4500/7762]	Time 0.459 (0.324)	Data 1.04e-04 (1.73e-04)	Tok/s 50792 (42977)	Loss/tok 3.4946 (3.2841)	LR 1.250e-04
0: TRAIN [1][4510/7762]	Time 0.454 (0.324)	Data 1.05e-04 (1.73e-04)	Tok/s 51888 (42981)	Loss/tok 3.3161 (3.2841)	LR 1.250e-04
0: TRAIN [1][4520/7762]	Time 0.359 (0.324)	Data 1.07e-04 (1.73e-04)	Tok/s 46871 (42983)	Loss/tok 3.2250 (3.2842)	LR 1.250e-04
0: TRAIN [1][4530/7762]	Time 0.579 (0.325)	Data 1.03e-04 (1.73e-04)	Tok/s 51654 (42987)	Loss/tok 3.6579 (3.2843)	LR 1.250e-04
0: TRAIN [1][4540/7762]	Time 0.255 (0.324)	Data 1.06e-04 (1.73e-04)	Tok/s 40316 (42986)	Loss/tok 3.0711 (3.2842)	LR 1.250e-04
0: TRAIN [1][4550/7762]	Time 0.463 (0.325)	Data 1.04e-04 (1.72e-04)	Tok/s 50520 (42993)	Loss/tok 3.4499 (3.2846)	LR 1.250e-04
0: TRAIN [1][4560/7762]	Time 0.264 (0.325)	Data 1.03e-04 (1.72e-04)	Tok/s 38969 (42989)	Loss/tok 3.1760 (3.2843)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][4570/7762]	Time 0.364 (0.325)	Data 1.05e-04 (1.72e-04)	Tok/s 46579 (42990)	Loss/tok 3.3644 (3.2844)	LR 1.250e-04
0: TRAIN [1][4580/7762]	Time 0.460 (0.325)	Data 1.08e-04 (1.72e-04)	Tok/s 51015 (42997)	Loss/tok 3.4058 (3.2846)	LR 1.250e-04
0: TRAIN [1][4590/7762]	Time 0.173 (0.325)	Data 1.11e-04 (1.72e-04)	Tok/s 30796 (42996)	Loss/tok 2.6610 (3.2847)	LR 1.250e-04
0: TRAIN [1][4600/7762]	Time 0.264 (0.325)	Data 1.01e-04 (1.72e-04)	Tok/s 38904 (42993)	Loss/tok 3.1321 (3.2845)	LR 1.250e-04
0: TRAIN [1][4610/7762]	Time 0.264 (0.325)	Data 1.07e-04 (1.72e-04)	Tok/s 39155 (42995)	Loss/tok 3.0263 (3.2846)	LR 1.250e-04
0: TRAIN [1][4620/7762]	Time 0.352 (0.325)	Data 1.05e-04 (1.71e-04)	Tok/s 47231 (42992)	Loss/tok 3.3105 (3.2845)	LR 1.250e-04
0: TRAIN [1][4630/7762]	Time 0.269 (0.325)	Data 1.08e-04 (1.71e-04)	Tok/s 39098 (42990)	Loss/tok 3.0463 (3.2844)	LR 1.250e-04
0: TRAIN [1][4640/7762]	Time 0.177 (0.325)	Data 1.08e-04 (1.71e-04)	Tok/s 30043 (42986)	Loss/tok 2.6404 (3.2843)	LR 1.250e-04
0: TRAIN [1][4650/7762]	Time 0.262 (0.325)	Data 1.30e-04 (1.71e-04)	Tok/s 39187 (42985)	Loss/tok 3.0889 (3.2841)	LR 1.250e-04
0: TRAIN [1][4660/7762]	Time 0.588 (0.325)	Data 1.08e-04 (1.71e-04)	Tok/s 50812 (42990)	Loss/tok 3.6663 (3.2842)	LR 1.250e-04
0: TRAIN [1][4670/7762]	Time 0.364 (0.325)	Data 1.03e-04 (1.71e-04)	Tok/s 46229 (42986)	Loss/tok 3.2259 (3.2841)	LR 1.250e-04
0: TRAIN [1][4680/7762]	Time 0.566 (0.325)	Data 1.04e-04 (1.71e-04)	Tok/s 52781 (42988)	Loss/tok 3.6434 (3.2842)	LR 1.250e-04
0: TRAIN [1][4690/7762]	Time 0.175 (0.325)	Data 1.05e-04 (1.71e-04)	Tok/s 29937 (42989)	Loss/tok 2.6937 (3.2841)	LR 1.250e-04
0: TRAIN [1][4700/7762]	Time 0.266 (0.325)	Data 1.03e-04 (1.70e-04)	Tok/s 38312 (42989)	Loss/tok 3.1083 (3.2841)	LR 1.250e-04
0: TRAIN [1][4710/7762]	Time 0.569 (0.325)	Data 9.94e-05 (1.70e-04)	Tok/s 52429 (42988)	Loss/tok 3.6157 (3.2843)	LR 1.250e-04
0: TRAIN [1][4720/7762]	Time 0.257 (0.325)	Data 1.03e-04 (1.70e-04)	Tok/s 40532 (42980)	Loss/tok 3.1100 (3.2840)	LR 1.250e-04
0: TRAIN [1][4730/7762]	Time 0.450 (0.325)	Data 1.02e-04 (1.70e-04)	Tok/s 51879 (42976)	Loss/tok 3.3708 (3.2841)	LR 1.250e-04
0: TRAIN [1][4740/7762]	Time 0.462 (0.325)	Data 1.05e-04 (1.70e-04)	Tok/s 50046 (42978)	Loss/tok 3.4817 (3.2841)	LR 1.250e-04
0: TRAIN [1][4750/7762]	Time 0.174 (0.325)	Data 1.06e-04 (1.70e-04)	Tok/s 30241 (42980)	Loss/tok 2.6460 (3.2842)	LR 1.250e-04
0: TRAIN [1][4760/7762]	Time 0.451 (0.325)	Data 1.06e-04 (1.70e-04)	Tok/s 51806 (42980)	Loss/tok 3.5139 (3.2841)	LR 1.250e-04
0: TRAIN [1][4770/7762]	Time 0.267 (0.324)	Data 1.02e-04 (1.69e-04)	Tok/s 39077 (42975)	Loss/tok 2.9827 (3.2839)	LR 1.250e-04
0: TRAIN [1][4780/7762]	Time 0.261 (0.324)	Data 1.03e-04 (1.69e-04)	Tok/s 39693 (42973)	Loss/tok 3.0667 (3.2837)	LR 1.250e-04
0: TRAIN [1][4790/7762]	Time 0.362 (0.324)	Data 1.03e-04 (1.69e-04)	Tok/s 45998 (42975)	Loss/tok 3.2325 (3.2837)	LR 1.250e-04
0: TRAIN [1][4800/7762]	Time 0.258 (0.324)	Data 1.03e-04 (1.69e-04)	Tok/s 40212 (42972)	Loss/tok 3.1156 (3.2835)	LR 1.250e-04
0: TRAIN [1][4810/7762]	Time 0.267 (0.324)	Data 9.58e-05 (1.69e-04)	Tok/s 37977 (42972)	Loss/tok 3.1060 (3.2834)	LR 1.250e-04
0: TRAIN [1][4820/7762]	Time 0.268 (0.324)	Data 1.04e-04 (1.69e-04)	Tok/s 38123 (42973)	Loss/tok 3.1177 (3.2835)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][4830/7762]	Time 0.366 (0.324)	Data 1.02e-04 (1.69e-04)	Tok/s 46340 (42980)	Loss/tok 3.3784 (3.2836)	LR 1.250e-04
0: TRAIN [1][4840/7762]	Time 0.365 (0.325)	Data 1.03e-04 (1.68e-04)	Tok/s 45742 (42987)	Loss/tok 3.3232 (3.2839)	LR 1.250e-04
0: TRAIN [1][4850/7762]	Time 0.345 (0.325)	Data 1.04e-04 (1.68e-04)	Tok/s 49104 (42988)	Loss/tok 3.3277 (3.2840)	LR 1.250e-04
0: TRAIN [1][4860/7762]	Time 0.353 (0.325)	Data 1.04e-04 (1.68e-04)	Tok/s 47837 (42990)	Loss/tok 3.3013 (3.2841)	LR 1.250e-04
0: TRAIN [1][4870/7762]	Time 0.176 (0.325)	Data 1.03e-04 (1.68e-04)	Tok/s 30066 (42984)	Loss/tok 2.7016 (3.2840)	LR 1.250e-04
0: TRAIN [1][4880/7762]	Time 0.464 (0.325)	Data 1.06e-04 (1.68e-04)	Tok/s 50337 (42986)	Loss/tok 3.5621 (3.2840)	LR 1.250e-04
0: TRAIN [1][4890/7762]	Time 0.363 (0.325)	Data 1.11e-04 (1.68e-04)	Tok/s 46678 (42990)	Loss/tok 3.3834 (3.2839)	LR 1.250e-04
0: TRAIN [1][4900/7762]	Time 0.263 (0.325)	Data 1.34e-04 (1.68e-04)	Tok/s 39438 (42988)	Loss/tok 3.1446 (3.2841)	LR 1.250e-04
0: TRAIN [1][4910/7762]	Time 0.357 (0.325)	Data 1.01e-04 (1.68e-04)	Tok/s 46551 (42991)	Loss/tok 3.1794 (3.2840)	LR 1.250e-04
0: TRAIN [1][4920/7762]	Time 0.257 (0.325)	Data 1.02e-04 (1.67e-04)	Tok/s 39723 (42988)	Loss/tok 2.9962 (3.2837)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][4930/7762]	Time 0.347 (0.325)	Data 9.94e-05 (1.67e-04)	Tok/s 47677 (42988)	Loss/tok 3.3534 (3.2840)	LR 1.250e-04
0: TRAIN [1][4940/7762]	Time 0.360 (0.325)	Data 1.27e-04 (1.67e-04)	Tok/s 46092 (42984)	Loss/tok 3.2912 (3.2838)	LR 1.250e-04
0: TRAIN [1][4950/7762]	Time 0.358 (0.325)	Data 1.03e-04 (1.67e-04)	Tok/s 46970 (42992)	Loss/tok 3.2874 (3.2839)	LR 1.250e-04
0: TRAIN [1][4960/7762]	Time 0.269 (0.325)	Data 1.03e-04 (1.67e-04)	Tok/s 38271 (42988)	Loss/tok 3.1208 (3.2837)	LR 1.250e-04
0: TRAIN [1][4970/7762]	Time 0.265 (0.325)	Data 1.01e-04 (1.67e-04)	Tok/s 39320 (42986)	Loss/tok 3.1406 (3.2835)	LR 1.250e-04
0: TRAIN [1][4980/7762]	Time 0.251 (0.324)	Data 1.01e-04 (1.67e-04)	Tok/s 41523 (42982)	Loss/tok 3.0177 (3.2833)	LR 1.250e-04
0: TRAIN [1][4990/7762]	Time 0.464 (0.325)	Data 9.97e-05 (1.67e-04)	Tok/s 50335 (42985)	Loss/tok 3.3036 (3.2834)	LR 1.250e-04
0: TRAIN [1][5000/7762]	Time 0.442 (0.325)	Data 1.00e-04 (1.66e-04)	Tok/s 52674 (42983)	Loss/tok 3.3388 (3.2833)	LR 1.250e-04
0: TRAIN [1][5010/7762]	Time 0.268 (0.325)	Data 1.18e-04 (1.66e-04)	Tok/s 38592 (42987)	Loss/tok 3.0248 (3.2834)	LR 1.250e-04
0: TRAIN [1][5020/7762]	Time 0.465 (0.325)	Data 1.03e-04 (1.66e-04)	Tok/s 49860 (42988)	Loss/tok 3.4256 (3.2836)	LR 1.250e-04
0: TRAIN [1][5030/7762]	Time 0.264 (0.325)	Data 1.00e-04 (1.66e-04)	Tok/s 39797 (42987)	Loss/tok 2.9592 (3.2837)	LR 1.250e-04
0: TRAIN [1][5040/7762]	Time 0.262 (0.325)	Data 1.04e-04 (1.66e-04)	Tok/s 39266 (42988)	Loss/tok 3.0777 (3.2836)	LR 1.250e-04
0: TRAIN [1][5050/7762]	Time 0.255 (0.325)	Data 1.06e-04 (1.66e-04)	Tok/s 40721 (42989)	Loss/tok 3.0888 (3.2836)	LR 1.250e-04
0: TRAIN [1][5060/7762]	Time 0.365 (0.325)	Data 9.99e-05 (1.66e-04)	Tok/s 45979 (42987)	Loss/tok 3.2253 (3.2836)	LR 1.250e-04
0: TRAIN [1][5070/7762]	Time 0.175 (0.325)	Data 1.01e-04 (1.66e-04)	Tok/s 30019 (42985)	Loss/tok 2.6723 (3.2834)	LR 1.250e-04
0: TRAIN [1][5080/7762]	Time 0.447 (0.325)	Data 1.19e-04 (1.65e-04)	Tok/s 52073 (42981)	Loss/tok 3.4753 (3.2832)	LR 1.250e-04
0: TRAIN [1][5090/7762]	Time 0.171 (0.325)	Data 1.03e-04 (1.65e-04)	Tok/s 30983 (42981)	Loss/tok 2.5933 (3.2831)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][5100/7762]	Time 0.247 (0.325)	Data 1.71e-04 (1.65e-04)	Tok/s 41641 (42982)	Loss/tok 3.1309 (3.2830)	LR 1.250e-04
0: TRAIN [1][5110/7762]	Time 0.457 (0.325)	Data 1.04e-04 (1.65e-04)	Tok/s 51177 (42978)	Loss/tok 3.3971 (3.2829)	LR 1.250e-04
0: TRAIN [1][5120/7762]	Time 0.253 (0.324)	Data 1.17e-04 (1.65e-04)	Tok/s 40978 (42977)	Loss/tok 3.1954 (3.2828)	LR 1.250e-04
0: TRAIN [1][5130/7762]	Time 0.358 (0.324)	Data 9.97e-05 (1.65e-04)	Tok/s 46560 (42974)	Loss/tok 3.2156 (3.2826)	LR 1.250e-04
0: TRAIN [1][5140/7762]	Time 0.456 (0.325)	Data 1.03e-04 (1.65e-04)	Tok/s 50976 (42978)	Loss/tok 3.4545 (3.2827)	LR 1.250e-04
0: TRAIN [1][5150/7762]	Time 0.261 (0.325)	Data 9.99e-05 (1.65e-04)	Tok/s 39310 (42978)	Loss/tok 2.9673 (3.2826)	LR 1.250e-04
0: TRAIN [1][5160/7762]	Time 0.170 (0.324)	Data 1.15e-04 (1.65e-04)	Tok/s 31408 (42969)	Loss/tok 2.6565 (3.2825)	LR 1.250e-04
0: TRAIN [1][5170/7762]	Time 0.437 (0.324)	Data 1.06e-04 (1.64e-04)	Tok/s 52751 (42970)	Loss/tok 3.4939 (3.2826)	LR 1.250e-04
0: TRAIN [1][5180/7762]	Time 0.460 (0.325)	Data 1.01e-04 (1.64e-04)	Tok/s 51327 (42974)	Loss/tok 3.3686 (3.2827)	LR 1.250e-04
0: TRAIN [1][5190/7762]	Time 0.262 (0.324)	Data 1.03e-04 (1.64e-04)	Tok/s 39207 (42967)	Loss/tok 3.0959 (3.2824)	LR 1.250e-04
0: TRAIN [1][5200/7762]	Time 0.173 (0.324)	Data 1.02e-04 (1.64e-04)	Tok/s 29861 (42964)	Loss/tok 2.6871 (3.2823)	LR 1.250e-04
0: TRAIN [1][5210/7762]	Time 0.359 (0.324)	Data 1.04e-04 (1.64e-04)	Tok/s 46523 (42971)	Loss/tok 3.3155 (3.2826)	LR 1.250e-04
0: TRAIN [1][5220/7762]	Time 0.352 (0.325)	Data 1.18e-04 (1.64e-04)	Tok/s 48247 (42979)	Loss/tok 3.3220 (3.2828)	LR 1.250e-04
0: TRAIN [1][5230/7762]	Time 0.354 (0.325)	Data 9.97e-05 (1.64e-04)	Tok/s 46727 (42981)	Loss/tok 3.3689 (3.2828)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][5240/7762]	Time 0.358 (0.325)	Data 1.01e-04 (1.64e-04)	Tok/s 47145 (42984)	Loss/tok 3.2999 (3.2830)	LR 1.250e-04
0: TRAIN [1][5250/7762]	Time 0.354 (0.325)	Data 1.08e-04 (1.64e-04)	Tok/s 48315 (42982)	Loss/tok 3.2858 (3.2831)	LR 1.250e-04
0: TRAIN [1][5260/7762]	Time 0.255 (0.325)	Data 9.99e-05 (1.63e-04)	Tok/s 40724 (42982)	Loss/tok 3.0318 (3.2829)	LR 1.250e-04
0: TRAIN [1][5270/7762]	Time 0.362 (0.325)	Data 1.01e-04 (1.63e-04)	Tok/s 46521 (42978)	Loss/tok 3.2506 (3.2829)	LR 1.250e-04
0: TRAIN [1][5280/7762]	Time 0.179 (0.325)	Data 1.05e-04 (1.63e-04)	Tok/s 30272 (42972)	Loss/tok 2.7891 (3.2827)	LR 1.250e-04
0: TRAIN [1][5290/7762]	Time 0.437 (0.325)	Data 1.05e-04 (1.63e-04)	Tok/s 52986 (42971)	Loss/tok 3.4855 (3.2827)	LR 1.250e-04
0: TRAIN [1][5300/7762]	Time 0.361 (0.325)	Data 1.03e-04 (1.63e-04)	Tok/s 46172 (42974)	Loss/tok 3.2168 (3.2826)	LR 1.250e-04
0: TRAIN [1][5310/7762]	Time 0.264 (0.325)	Data 1.04e-04 (1.63e-04)	Tok/s 39042 (42975)	Loss/tok 2.9887 (3.2827)	LR 1.250e-04
0: TRAIN [1][5320/7762]	Time 0.459 (0.325)	Data 1.06e-04 (1.63e-04)	Tok/s 50720 (42978)	Loss/tok 3.5396 (3.2827)	LR 1.250e-04
0: TRAIN [1][5330/7762]	Time 0.260 (0.325)	Data 1.61e-04 (1.63e-04)	Tok/s 39668 (42976)	Loss/tok 3.0859 (3.2825)	LR 1.250e-04
0: TRAIN [1][5340/7762]	Time 0.466 (0.325)	Data 1.05e-04 (1.63e-04)	Tok/s 50370 (42982)	Loss/tok 3.4755 (3.2826)	LR 1.250e-04
0: TRAIN [1][5350/7762]	Time 0.362 (0.325)	Data 1.03e-04 (1.62e-04)	Tok/s 46815 (42980)	Loss/tok 3.2519 (3.2824)	LR 1.250e-04
0: TRAIN [1][5360/7762]	Time 0.257 (0.325)	Data 1.03e-04 (1.62e-04)	Tok/s 40317 (42975)	Loss/tok 3.0388 (3.2822)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][5370/7762]	Time 0.348 (0.325)	Data 1.19e-04 (1.62e-04)	Tok/s 48186 (42976)	Loss/tok 3.3907 (3.2823)	LR 1.250e-04
0: TRAIN [1][5380/7762]	Time 0.361 (0.325)	Data 1.03e-04 (1.62e-04)	Tok/s 47366 (42978)	Loss/tok 3.2571 (3.2824)	LR 1.250e-04
0: TRAIN [1][5390/7762]	Time 0.265 (0.325)	Data 1.03e-04 (1.62e-04)	Tok/s 38942 (42983)	Loss/tok 3.0306 (3.2825)	LR 1.250e-04
0: TRAIN [1][5400/7762]	Time 0.171 (0.325)	Data 1.02e-04 (1.62e-04)	Tok/s 30580 (42981)	Loss/tok 2.5308 (3.2826)	LR 1.250e-04
0: TRAIN [1][5410/7762]	Time 0.360 (0.325)	Data 1.07e-04 (1.62e-04)	Tok/s 46867 (42986)	Loss/tok 3.3556 (3.2826)	LR 1.250e-04
0: TRAIN [1][5420/7762]	Time 0.255 (0.325)	Data 1.03e-04 (1.62e-04)	Tok/s 39965 (42986)	Loss/tok 3.0705 (3.2826)	LR 1.250e-04
0: TRAIN [1][5430/7762]	Time 0.263 (0.325)	Data 1.06e-04 (1.62e-04)	Tok/s 38679 (42987)	Loss/tok 3.0804 (3.2827)	LR 1.250e-04
0: TRAIN [1][5440/7762]	Time 0.269 (0.325)	Data 1.01e-04 (1.62e-04)	Tok/s 37698 (42989)	Loss/tok 3.1205 (3.2829)	LR 1.250e-04
0: TRAIN [1][5450/7762]	Time 0.364 (0.325)	Data 1.04e-04 (1.61e-04)	Tok/s 46779 (42990)	Loss/tok 3.3849 (3.2829)	LR 1.250e-04
0: TRAIN [1][5460/7762]	Time 0.174 (0.325)	Data 1.02e-04 (1.61e-04)	Tok/s 30650 (42984)	Loss/tok 2.6871 (3.2827)	LR 1.250e-04
0: TRAIN [1][5470/7762]	Time 0.582 (0.325)	Data 1.07e-04 (1.61e-04)	Tok/s 51122 (42986)	Loss/tok 3.5801 (3.2828)	LR 1.250e-04
0: TRAIN [1][5480/7762]	Time 0.266 (0.325)	Data 1.02e-04 (1.61e-04)	Tok/s 38848 (42983)	Loss/tok 3.0745 (3.2826)	LR 1.250e-04
0: TRAIN [1][5490/7762]	Time 0.272 (0.325)	Data 1.02e-04 (1.61e-04)	Tok/s 36833 (42978)	Loss/tok 3.1481 (3.2824)	LR 1.250e-04
0: TRAIN [1][5500/7762]	Time 0.367 (0.325)	Data 1.03e-04 (1.61e-04)	Tok/s 45816 (42982)	Loss/tok 3.2049 (3.2826)	LR 1.250e-04
0: TRAIN [1][5510/7762]	Time 0.177 (0.325)	Data 1.07e-04 (1.61e-04)	Tok/s 30047 (42981)	Loss/tok 2.6981 (3.2825)	LR 1.250e-04
0: TRAIN [1][5520/7762]	Time 0.367 (0.325)	Data 1.08e-04 (1.61e-04)	Tok/s 45302 (42982)	Loss/tok 3.2823 (3.2825)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][5530/7762]	Time 0.358 (0.325)	Data 1.06e-04 (1.61e-04)	Tok/s 47222 (42987)	Loss/tok 3.1601 (3.2828)	LR 1.250e-04
0: TRAIN [1][5540/7762]	Time 0.345 (0.325)	Data 1.05e-04 (1.61e-04)	Tok/s 49441 (42991)	Loss/tok 3.1962 (3.2828)	LR 1.250e-04
0: TRAIN [1][5550/7762]	Time 0.359 (0.325)	Data 1.01e-04 (1.60e-04)	Tok/s 46641 (42988)	Loss/tok 3.3041 (3.2827)	LR 1.250e-04
0: TRAIN [1][5560/7762]	Time 0.464 (0.325)	Data 1.03e-04 (1.60e-04)	Tok/s 50562 (42987)	Loss/tok 3.3605 (3.2826)	LR 1.250e-04
0: TRAIN [1][5570/7762]	Time 0.261 (0.325)	Data 1.03e-04 (1.60e-04)	Tok/s 39305 (42985)	Loss/tok 3.1140 (3.2825)	LR 1.250e-04
0: TRAIN [1][5580/7762]	Time 0.178 (0.325)	Data 1.10e-04 (1.60e-04)	Tok/s 30438 (42989)	Loss/tok 2.7103 (3.2825)	LR 1.250e-04
0: TRAIN [1][5590/7762]	Time 0.269 (0.325)	Data 1.04e-04 (1.60e-04)	Tok/s 38482 (42987)	Loss/tok 3.0835 (3.2825)	LR 1.250e-04
0: TRAIN [1][5600/7762]	Time 0.261 (0.325)	Data 1.25e-04 (1.60e-04)	Tok/s 39493 (42983)	Loss/tok 3.1049 (3.2823)	LR 1.250e-04
0: TRAIN [1][5610/7762]	Time 0.259 (0.325)	Data 1.05e-04 (1.60e-04)	Tok/s 39084 (42984)	Loss/tok 3.0234 (3.2826)	LR 1.250e-04
0: TRAIN [1][5620/7762]	Time 0.266 (0.325)	Data 1.38e-04 (1.60e-04)	Tok/s 39122 (42981)	Loss/tok 3.0360 (3.2825)	LR 1.250e-04
0: TRAIN [1][5630/7762]	Time 0.357 (0.325)	Data 1.04e-04 (1.60e-04)	Tok/s 46906 (42979)	Loss/tok 3.3269 (3.2823)	LR 1.250e-04
0: TRAIN [1][5640/7762]	Time 0.363 (0.325)	Data 1.02e-04 (1.60e-04)	Tok/s 45572 (42979)	Loss/tok 3.1418 (3.2822)	LR 1.250e-04
0: TRAIN [1][5650/7762]	Time 0.559 (0.325)	Data 1.22e-04 (1.59e-04)	Tok/s 54468 (42979)	Loss/tok 3.5831 (3.2823)	LR 1.250e-04
0: TRAIN [1][5660/7762]	Time 0.265 (0.325)	Data 9.92e-05 (1.59e-04)	Tok/s 39324 (42976)	Loss/tok 3.1239 (3.2821)	LR 1.250e-04
0: TRAIN [1][5670/7762]	Time 0.258 (0.325)	Data 1.04e-04 (1.59e-04)	Tok/s 40424 (42976)	Loss/tok 2.9480 (3.2820)	LR 1.250e-04
0: TRAIN [1][5680/7762]	Time 0.454 (0.325)	Data 1.03e-04 (1.59e-04)	Tok/s 51225 (42976)	Loss/tok 3.4666 (3.2819)	LR 1.250e-04
0: TRAIN [1][5690/7762]	Time 0.361 (0.325)	Data 1.07e-04 (1.59e-04)	Tok/s 45673 (42982)	Loss/tok 3.3747 (3.2823)	LR 1.250e-04
0: TRAIN [1][5700/7762]	Time 0.265 (0.325)	Data 9.89e-05 (1.59e-04)	Tok/s 39113 (42977)	Loss/tok 2.9698 (3.2821)	LR 1.250e-04
0: TRAIN [1][5710/7762]	Time 0.457 (0.325)	Data 1.09e-04 (1.59e-04)	Tok/s 51341 (42985)	Loss/tok 3.5006 (3.2824)	LR 1.250e-04
0: TRAIN [1][5720/7762]	Time 0.263 (0.325)	Data 9.80e-05 (1.59e-04)	Tok/s 39701 (42985)	Loss/tok 3.0600 (3.2825)	LR 1.250e-04
0: TRAIN [1][5730/7762]	Time 0.347 (0.325)	Data 1.03e-04 (1.59e-04)	Tok/s 48584 (42989)	Loss/tok 3.2861 (3.2824)	LR 1.250e-04
0: TRAIN [1][5740/7762]	Time 0.178 (0.325)	Data 1.14e-04 (1.59e-04)	Tok/s 28832 (42985)	Loss/tok 2.6773 (3.2822)	LR 1.250e-04
0: TRAIN [1][5750/7762]	Time 0.361 (0.325)	Data 1.01e-04 (1.59e-04)	Tok/s 46803 (42985)	Loss/tok 3.2468 (3.2821)	LR 1.250e-04
0: TRAIN [1][5760/7762]	Time 0.357 (0.325)	Data 1.27e-04 (1.58e-04)	Tok/s 47602 (42983)	Loss/tok 3.3053 (3.2819)	LR 1.250e-04
0: TRAIN [1][5770/7762]	Time 0.173 (0.325)	Data 1.11e-04 (1.58e-04)	Tok/s 30527 (42988)	Loss/tok 2.7985 (3.2820)	LR 1.250e-04
0: TRAIN [1][5780/7762]	Time 0.261 (0.325)	Data 1.06e-04 (1.58e-04)	Tok/s 40288 (42988)	Loss/tok 3.1992 (3.2821)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][5790/7762]	Time 0.343 (0.325)	Data 1.04e-04 (1.58e-04)	Tok/s 49465 (42994)	Loss/tok 3.2919 (3.2822)	LR 1.250e-04
0: TRAIN [1][5800/7762]	Time 0.267 (0.325)	Data 9.97e-05 (1.58e-04)	Tok/s 38341 (42989)	Loss/tok 3.0525 (3.2820)	LR 1.250e-04
0: TRAIN [1][5810/7762]	Time 0.464 (0.325)	Data 1.07e-04 (1.58e-04)	Tok/s 49914 (42996)	Loss/tok 3.3485 (3.2820)	LR 1.250e-04
0: TRAIN [1][5820/7762]	Time 0.355 (0.325)	Data 1.10e-04 (1.58e-04)	Tok/s 47311 (42997)	Loss/tok 3.3294 (3.2820)	LR 1.250e-04
0: TRAIN [1][5830/7762]	Time 0.583 (0.325)	Data 1.02e-04 (1.58e-04)	Tok/s 51292 (43003)	Loss/tok 3.7208 (3.2823)	LR 1.250e-04
0: TRAIN [1][5840/7762]	Time 0.266 (0.325)	Data 1.05e-04 (1.58e-04)	Tok/s 39346 (43007)	Loss/tok 3.0604 (3.2824)	LR 1.250e-04
0: TRAIN [1][5850/7762]	Time 0.588 (0.325)	Data 1.19e-04 (1.58e-04)	Tok/s 49915 (43009)	Loss/tok 3.7857 (3.2824)	LR 1.250e-04
0: TRAIN [1][5860/7762]	Time 0.365 (0.325)	Data 1.01e-04 (1.58e-04)	Tok/s 46039 (43009)	Loss/tok 3.2702 (3.2823)	LR 1.250e-04
0: TRAIN [1][5870/7762]	Time 0.363 (0.325)	Data 1.04e-04 (1.57e-04)	Tok/s 46291 (43010)	Loss/tok 3.3429 (3.2823)	LR 1.250e-04
0: TRAIN [1][5880/7762]	Time 0.260 (0.325)	Data 1.02e-04 (1.57e-04)	Tok/s 40045 (43012)	Loss/tok 3.0792 (3.2822)	LR 1.250e-04
0: TRAIN [1][5890/7762]	Time 0.464 (0.325)	Data 1.02e-04 (1.57e-04)	Tok/s 51187 (43012)	Loss/tok 3.5071 (3.2822)	LR 1.250e-04
0: TRAIN [1][5900/7762]	Time 0.366 (0.325)	Data 1.03e-04 (1.57e-04)	Tok/s 46125 (43019)	Loss/tok 3.2097 (3.2824)	LR 1.250e-04
0: TRAIN [1][5910/7762]	Time 0.258 (0.325)	Data 1.18e-04 (1.57e-04)	Tok/s 40917 (43015)	Loss/tok 3.0321 (3.2824)	LR 1.250e-04
0: TRAIN [1][5920/7762]	Time 0.342 (0.325)	Data 1.15e-04 (1.57e-04)	Tok/s 49895 (43020)	Loss/tok 3.3392 (3.2823)	LR 1.250e-04
0: TRAIN [1][5930/7762]	Time 0.267 (0.325)	Data 1.03e-04 (1.57e-04)	Tok/s 38862 (43020)	Loss/tok 3.0748 (3.2822)	LR 1.250e-04
0: TRAIN [1][5940/7762]	Time 0.268 (0.325)	Data 1.03e-04 (1.57e-04)	Tok/s 38261 (43018)	Loss/tok 3.1378 (3.2821)	LR 1.250e-04
0: TRAIN [1][5950/7762]	Time 0.346 (0.325)	Data 1.06e-04 (1.57e-04)	Tok/s 48005 (43018)	Loss/tok 3.3377 (3.2820)	LR 1.250e-04
0: TRAIN [1][5960/7762]	Time 0.464 (0.325)	Data 2.35e-04 (1.57e-04)	Tok/s 49923 (43018)	Loss/tok 3.4616 (3.2821)	LR 1.250e-04
0: TRAIN [1][5970/7762]	Time 0.465 (0.325)	Data 1.04e-04 (1.57e-04)	Tok/s 49732 (43019)	Loss/tok 3.5051 (3.2821)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][5980/7762]	Time 0.261 (0.325)	Data 1.00e-04 (1.57e-04)	Tok/s 39734 (43022)	Loss/tok 3.1200 (3.2823)	LR 1.250e-04
0: TRAIN [1][5990/7762]	Time 0.268 (0.325)	Data 1.07e-04 (1.56e-04)	Tok/s 39653 (43024)	Loss/tok 3.1246 (3.2825)	LR 1.250e-04
0: TRAIN [1][6000/7762]	Time 0.256 (0.325)	Data 1.04e-04 (1.56e-04)	Tok/s 39957 (43021)	Loss/tok 3.0887 (3.2823)	LR 1.250e-04
0: TRAIN [1][6010/7762]	Time 0.364 (0.325)	Data 1.06e-04 (1.56e-04)	Tok/s 46981 (43025)	Loss/tok 3.1886 (3.2823)	LR 1.250e-04
0: TRAIN [1][6020/7762]	Time 0.465 (0.325)	Data 1.03e-04 (1.56e-04)	Tok/s 50694 (43030)	Loss/tok 3.2915 (3.2824)	LR 1.250e-04
0: TRAIN [1][6030/7762]	Time 0.171 (0.325)	Data 1.04e-04 (1.56e-04)	Tok/s 30681 (43027)	Loss/tok 2.5853 (3.2823)	LR 1.250e-04
0: TRAIN [1][6040/7762]	Time 0.357 (0.325)	Data 1.03e-04 (1.56e-04)	Tok/s 46880 (43034)	Loss/tok 3.2450 (3.2823)	LR 1.250e-04
0: TRAIN [1][6050/7762]	Time 0.256 (0.325)	Data 9.97e-05 (1.56e-04)	Tok/s 40326 (43025)	Loss/tok 3.0861 (3.2820)	LR 1.250e-04
0: TRAIN [1][6060/7762]	Time 0.355 (0.325)	Data 1.04e-04 (1.56e-04)	Tok/s 47882 (43027)	Loss/tok 3.1861 (3.2820)	LR 1.250e-04
0: TRAIN [1][6070/7762]	Time 0.257 (0.325)	Data 1.03e-04 (1.56e-04)	Tok/s 39137 (43032)	Loss/tok 3.0805 (3.2821)	LR 1.250e-04
0: TRAIN [1][6080/7762]	Time 0.359 (0.325)	Data 1.03e-04 (1.56e-04)	Tok/s 47216 (43032)	Loss/tok 3.2591 (3.2821)	LR 1.250e-04
0: TRAIN [1][6090/7762]	Time 0.263 (0.325)	Data 1.01e-04 (1.56e-04)	Tok/s 39259 (43031)	Loss/tok 3.0970 (3.2819)	LR 1.250e-04
0: TRAIN [1][6100/7762]	Time 0.460 (0.325)	Data 1.18e-04 (1.56e-04)	Tok/s 51223 (43038)	Loss/tok 3.2940 (3.2820)	LR 1.250e-04
0: TRAIN [1][6110/7762]	Time 0.177 (0.325)	Data 1.03e-04 (1.55e-04)	Tok/s 29538 (43038)	Loss/tok 2.6833 (3.2820)	LR 1.250e-04
0: TRAIN [1][6120/7762]	Time 0.264 (0.325)	Data 1.05e-04 (1.55e-04)	Tok/s 39378 (43035)	Loss/tok 3.0861 (3.2818)	LR 1.250e-04
0: TRAIN [1][6130/7762]	Time 0.351 (0.325)	Data 1.00e-04 (1.55e-04)	Tok/s 47263 (43038)	Loss/tok 3.2901 (3.2818)	LR 1.250e-04
0: TRAIN [1][6140/7762]	Time 0.171 (0.325)	Data 1.10e-04 (1.55e-04)	Tok/s 31128 (43036)	Loss/tok 2.6539 (3.2816)	LR 1.250e-04
0: TRAIN [1][6150/7762]	Time 0.261 (0.325)	Data 1.10e-04 (1.55e-04)	Tok/s 39404 (43034)	Loss/tok 3.0309 (3.2815)	LR 1.250e-04
0: TRAIN [1][6160/7762]	Time 0.342 (0.325)	Data 1.02e-04 (1.55e-04)	Tok/s 48317 (43038)	Loss/tok 3.3460 (3.2816)	LR 1.250e-04
0: TRAIN [1][6170/7762]	Time 0.463 (0.325)	Data 1.01e-04 (1.55e-04)	Tok/s 51278 (43037)	Loss/tok 3.3257 (3.2815)	LR 1.250e-04
0: TRAIN [1][6180/7762]	Time 0.259 (0.325)	Data 9.89e-05 (1.55e-04)	Tok/s 39176 (43027)	Loss/tok 2.9997 (3.2812)	LR 1.250e-04
0: TRAIN [1][6190/7762]	Time 0.464 (0.325)	Data 1.06e-04 (1.55e-04)	Tok/s 50379 (43025)	Loss/tok 3.3530 (3.2811)	LR 1.250e-04
0: TRAIN [1][6200/7762]	Time 0.273 (0.325)	Data 9.94e-05 (1.55e-04)	Tok/s 37186 (43023)	Loss/tok 3.0680 (3.2810)	LR 1.250e-04
0: TRAIN [1][6210/7762]	Time 0.464 (0.325)	Data 1.02e-04 (1.55e-04)	Tok/s 50998 (43028)	Loss/tok 3.3933 (3.2813)	LR 1.250e-04
0: TRAIN [1][6220/7762]	Time 0.265 (0.325)	Data 1.05e-04 (1.55e-04)	Tok/s 39094 (43034)	Loss/tok 3.1395 (3.2814)	LR 1.250e-04
0: TRAIN [1][6230/7762]	Time 0.362 (0.325)	Data 1.01e-04 (1.54e-04)	Tok/s 47069 (43034)	Loss/tok 3.2765 (3.2813)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][6240/7762]	Time 0.262 (0.325)	Data 1.03e-04 (1.54e-04)	Tok/s 39241 (43033)	Loss/tok 3.0297 (3.2812)	LR 1.250e-04
0: TRAIN [1][6250/7762]	Time 0.174 (0.325)	Data 2.38e-04 (1.54e-04)	Tok/s 29837 (43030)	Loss/tok 2.6103 (3.2810)	LR 1.250e-04
0: TRAIN [1][6260/7762]	Time 0.259 (0.325)	Data 1.04e-04 (1.54e-04)	Tok/s 39991 (43032)	Loss/tok 3.1189 (3.2809)	LR 1.250e-04
0: TRAIN [1][6270/7762]	Time 0.362 (0.325)	Data 9.97e-05 (1.54e-04)	Tok/s 46222 (43028)	Loss/tok 3.2905 (3.2807)	LR 1.250e-04
0: TRAIN [1][6280/7762]	Time 0.259 (0.325)	Data 1.02e-04 (1.54e-04)	Tok/s 39435 (43030)	Loss/tok 3.1749 (3.2809)	LR 1.250e-04
0: TRAIN [1][6290/7762]	Time 0.254 (0.325)	Data 1.10e-04 (1.54e-04)	Tok/s 40629 (43028)	Loss/tok 3.0341 (3.2807)	LR 1.250e-04
0: TRAIN [1][6300/7762]	Time 0.363 (0.325)	Data 1.09e-04 (1.54e-04)	Tok/s 45779 (43031)	Loss/tok 3.1941 (3.2807)	LR 1.250e-04
0: TRAIN [1][6310/7762]	Time 0.366 (0.325)	Data 1.03e-04 (1.54e-04)	Tok/s 45593 (43035)	Loss/tok 3.2789 (3.2808)	LR 1.250e-04
0: TRAIN [1][6320/7762]	Time 0.263 (0.325)	Data 1.15e-04 (1.54e-04)	Tok/s 39220 (43038)	Loss/tok 2.9530 (3.2807)	LR 1.250e-04
0: TRAIN [1][6330/7762]	Time 0.360 (0.325)	Data 1.03e-04 (1.54e-04)	Tok/s 46361 (43036)	Loss/tok 3.1765 (3.2806)	LR 1.250e-04
0: TRAIN [1][6340/7762]	Time 0.260 (0.325)	Data 1.20e-04 (1.54e-04)	Tok/s 39487 (43034)	Loss/tok 3.0847 (3.2805)	LR 1.250e-04
0: TRAIN [1][6350/7762]	Time 0.262 (0.325)	Data 1.02e-04 (1.54e-04)	Tok/s 39140 (43029)	Loss/tok 3.0549 (3.2804)	LR 1.250e-04
0: TRAIN [1][6360/7762]	Time 0.261 (0.325)	Data 1.08e-04 (1.53e-04)	Tok/s 39416 (43030)	Loss/tok 2.9911 (3.2802)	LR 1.250e-04
0: TRAIN [1][6370/7762]	Time 0.259 (0.325)	Data 9.85e-05 (1.53e-04)	Tok/s 40186 (43022)	Loss/tok 2.9949 (3.2800)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][6380/7762]	Time 0.177 (0.325)	Data 1.04e-04 (1.53e-04)	Tok/s 30324 (43022)	Loss/tok 2.7147 (3.2799)	LR 1.250e-04
0: TRAIN [1][6390/7762]	Time 0.271 (0.325)	Data 1.03e-04 (1.53e-04)	Tok/s 38340 (43020)	Loss/tok 3.1371 (3.2798)	LR 1.250e-04
0: TRAIN [1][6400/7762]	Time 0.358 (0.325)	Data 1.02e-04 (1.53e-04)	Tok/s 47483 (43023)	Loss/tok 3.2650 (3.2799)	LR 1.250e-04
0: TRAIN [1][6410/7762]	Time 0.360 (0.325)	Data 1.03e-04 (1.53e-04)	Tok/s 46883 (43026)	Loss/tok 3.3104 (3.2799)	LR 1.250e-04
0: TRAIN [1][6420/7762]	Time 0.362 (0.325)	Data 1.03e-04 (1.53e-04)	Tok/s 46768 (43028)	Loss/tok 3.2532 (3.2802)	LR 1.250e-04
0: TRAIN [1][6430/7762]	Time 0.261 (0.325)	Data 1.03e-04 (1.53e-04)	Tok/s 39361 (43026)	Loss/tok 3.0898 (3.2800)	LR 1.250e-04
0: TRAIN [1][6440/7762]	Time 0.462 (0.325)	Data 1.05e-04 (1.53e-04)	Tok/s 50625 (43025)	Loss/tok 3.4393 (3.2799)	LR 1.250e-04
0: TRAIN [1][6450/7762]	Time 0.260 (0.325)	Data 1.17e-04 (1.53e-04)	Tok/s 39064 (43026)	Loss/tok 2.9486 (3.2799)	LR 1.250e-04
0: TRAIN [1][6460/7762]	Time 0.466 (0.325)	Data 1.03e-04 (1.53e-04)	Tok/s 50137 (43026)	Loss/tok 3.3563 (3.2798)	LR 1.250e-04
0: TRAIN [1][6470/7762]	Time 0.344 (0.325)	Data 1.05e-04 (1.53e-04)	Tok/s 48783 (43029)	Loss/tok 3.2583 (3.2798)	LR 1.250e-04
0: TRAIN [1][6480/7762]	Time 0.456 (0.325)	Data 9.70e-05 (1.53e-04)	Tok/s 51433 (43026)	Loss/tok 3.3395 (3.2797)	LR 1.250e-04
0: TRAIN [1][6490/7762]	Time 0.364 (0.325)	Data 1.05e-04 (1.53e-04)	Tok/s 46149 (43024)	Loss/tok 3.1239 (3.2795)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][6500/7762]	Time 0.453 (0.325)	Data 1.04e-04 (1.53e-04)	Tok/s 51243 (43026)	Loss/tok 3.4803 (3.2796)	LR 1.250e-04
0: TRAIN [1][6510/7762]	Time 0.266 (0.325)	Data 1.24e-04 (1.52e-04)	Tok/s 38583 (43024)	Loss/tok 2.9998 (3.2795)	LR 1.250e-04
0: TRAIN [1][6520/7762]	Time 0.351 (0.325)	Data 1.07e-04 (1.52e-04)	Tok/s 47888 (43028)	Loss/tok 3.3738 (3.2796)	LR 1.250e-04
0: TRAIN [1][6530/7762]	Time 0.257 (0.325)	Data 1.02e-04 (1.52e-04)	Tok/s 39913 (43028)	Loss/tok 3.0412 (3.2795)	LR 1.250e-04
0: TRAIN [1][6540/7762]	Time 0.263 (0.325)	Data 1.02e-04 (1.52e-04)	Tok/s 39302 (43031)	Loss/tok 3.0809 (3.2795)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][6550/7762]	Time 0.464 (0.325)	Data 1.17e-04 (1.52e-04)	Tok/s 50437 (43032)	Loss/tok 3.4837 (3.2796)	LR 1.250e-04
0: TRAIN [1][6560/7762]	Time 0.462 (0.325)	Data 1.23e-04 (1.52e-04)	Tok/s 50763 (43032)	Loss/tok 3.4433 (3.2797)	LR 1.250e-04
0: TRAIN [1][6570/7762]	Time 0.264 (0.325)	Data 1.01e-04 (1.52e-04)	Tok/s 38277 (43030)	Loss/tok 3.0587 (3.2796)	LR 1.250e-04
0: TRAIN [1][6580/7762]	Time 0.350 (0.325)	Data 1.06e-04 (1.52e-04)	Tok/s 47868 (43029)	Loss/tok 3.2775 (3.2796)	LR 1.250e-04
0: TRAIN [1][6590/7762]	Time 0.357 (0.325)	Data 9.99e-05 (1.52e-04)	Tok/s 45883 (43031)	Loss/tok 3.3350 (3.2795)	LR 1.250e-04
0: TRAIN [1][6600/7762]	Time 0.265 (0.325)	Data 1.03e-04 (1.52e-04)	Tok/s 39540 (43029)	Loss/tok 3.0842 (3.2794)	LR 1.250e-04
0: TRAIN [1][6610/7762]	Time 0.265 (0.325)	Data 1.03e-04 (1.52e-04)	Tok/s 38653 (43029)	Loss/tok 3.0846 (3.2795)	LR 1.250e-04
0: TRAIN [1][6620/7762]	Time 0.355 (0.325)	Data 1.22e-04 (1.52e-04)	Tok/s 47350 (43033)	Loss/tok 3.3357 (3.2796)	LR 1.250e-04
0: TRAIN [1][6630/7762]	Time 0.460 (0.325)	Data 1.02e-04 (1.52e-04)	Tok/s 50586 (43033)	Loss/tok 3.4720 (3.2796)	LR 1.250e-04
0: TRAIN [1][6640/7762]	Time 0.266 (0.325)	Data 1.03e-04 (1.52e-04)	Tok/s 38751 (43035)	Loss/tok 2.9548 (3.2797)	LR 1.250e-04
0: TRAIN [1][6650/7762]	Time 0.254 (0.325)	Data 1.05e-04 (1.51e-04)	Tok/s 40958 (43035)	Loss/tok 3.0742 (3.2796)	LR 1.250e-04
0: TRAIN [1][6660/7762]	Time 0.363 (0.325)	Data 1.02e-04 (1.51e-04)	Tok/s 46806 (43034)	Loss/tok 3.2598 (3.2796)	LR 1.250e-04
0: TRAIN [1][6670/7762]	Time 0.259 (0.325)	Data 1.06e-04 (1.51e-04)	Tok/s 39573 (43033)	Loss/tok 3.0137 (3.2795)	LR 1.250e-04
0: TRAIN [1][6680/7762]	Time 0.260 (0.325)	Data 1.02e-04 (1.51e-04)	Tok/s 40398 (43036)	Loss/tok 2.9973 (3.2796)	LR 1.250e-04
0: TRAIN [1][6690/7762]	Time 0.261 (0.325)	Data 1.03e-04 (1.51e-04)	Tok/s 39231 (43034)	Loss/tok 3.1450 (3.2795)	LR 1.250e-04
0: TRAIN [1][6700/7762]	Time 0.265 (0.325)	Data 1.10e-04 (1.51e-04)	Tok/s 39050 (43033)	Loss/tok 3.0884 (3.2795)	LR 1.250e-04
0: TRAIN [1][6710/7762]	Time 0.462 (0.325)	Data 1.06e-04 (1.51e-04)	Tok/s 50784 (43037)	Loss/tok 3.4886 (3.2796)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][6720/7762]	Time 0.340 (0.325)	Data 1.10e-04 (1.51e-04)	Tok/s 49472 (43040)	Loss/tok 3.2177 (3.2797)	LR 1.250e-04
0: TRAIN [1][6730/7762]	Time 0.360 (0.325)	Data 1.05e-04 (1.51e-04)	Tok/s 47017 (43042)	Loss/tok 3.2460 (3.2797)	LR 1.250e-04
0: TRAIN [1][6740/7762]	Time 0.263 (0.325)	Data 1.19e-04 (1.51e-04)	Tok/s 39034 (43039)	Loss/tok 3.1694 (3.2796)	LR 1.250e-04
0: TRAIN [1][6750/7762]	Time 0.265 (0.325)	Data 1.09e-04 (1.51e-04)	Tok/s 39319 (43042)	Loss/tok 2.9289 (3.2798)	LR 1.250e-04
0: TRAIN [1][6760/7762]	Time 0.462 (0.325)	Data 1.03e-04 (1.51e-04)	Tok/s 50232 (43039)	Loss/tok 3.5380 (3.2798)	LR 1.250e-04
0: TRAIN [1][6770/7762]	Time 0.263 (0.325)	Data 1.02e-04 (1.51e-04)	Tok/s 39540 (43041)	Loss/tok 2.9894 (3.2797)	LR 1.250e-04
0: TRAIN [1][6780/7762]	Time 0.462 (0.325)	Data 1.06e-04 (1.51e-04)	Tok/s 50030 (43041)	Loss/tok 3.4342 (3.2796)	LR 1.250e-04
0: TRAIN [1][6790/7762]	Time 0.361 (0.325)	Data 1.01e-04 (1.51e-04)	Tok/s 47711 (43041)	Loss/tok 3.2429 (3.2796)	LR 1.250e-04
0: TRAIN [1][6800/7762]	Time 0.260 (0.325)	Data 1.19e-04 (1.50e-04)	Tok/s 41229 (43042)	Loss/tok 2.9813 (3.2797)	LR 1.250e-04
0: TRAIN [1][6810/7762]	Time 0.176 (0.325)	Data 1.47e-04 (1.50e-04)	Tok/s 30440 (43040)	Loss/tok 2.7355 (3.2796)	LR 1.250e-04
0: TRAIN [1][6820/7762]	Time 0.354 (0.325)	Data 1.04e-04 (1.50e-04)	Tok/s 47739 (43040)	Loss/tok 3.3024 (3.2796)	LR 1.250e-04
0: TRAIN [1][6830/7762]	Time 0.268 (0.325)	Data 1.01e-04 (1.50e-04)	Tok/s 38636 (43042)	Loss/tok 3.0164 (3.2797)	LR 1.250e-04
0: TRAIN [1][6840/7762]	Time 0.263 (0.326)	Data 1.16e-04 (1.50e-04)	Tok/s 39368 (43045)	Loss/tok 3.0287 (3.2798)	LR 1.250e-04
0: TRAIN [1][6850/7762]	Time 0.265 (0.326)	Data 1.05e-04 (1.50e-04)	Tok/s 38408 (43044)	Loss/tok 3.0824 (3.2798)	LR 1.250e-04
0: TRAIN [1][6860/7762]	Time 0.365 (0.326)	Data 1.01e-04 (1.50e-04)	Tok/s 45539 (43046)	Loss/tok 3.2768 (3.2797)	LR 1.250e-04
0: TRAIN [1][6870/7762]	Time 0.175 (0.326)	Data 1.04e-04 (1.50e-04)	Tok/s 30351 (43046)	Loss/tok 2.6503 (3.2798)	LR 1.250e-04
0: TRAIN [1][6880/7762]	Time 0.256 (0.325)	Data 1.01e-04 (1.50e-04)	Tok/s 40311 (43045)	Loss/tok 3.0597 (3.2797)	LR 1.250e-04
0: TRAIN [1][6890/7762]	Time 0.253 (0.325)	Data 1.03e-04 (1.50e-04)	Tok/s 40958 (43047)	Loss/tok 3.0159 (3.2796)	LR 1.250e-04
0: TRAIN [1][6900/7762]	Time 0.457 (0.326)	Data 1.01e-04 (1.50e-04)	Tok/s 51144 (43050)	Loss/tok 3.3176 (3.2797)	LR 1.250e-04
0: TRAIN [1][6910/7762]	Time 0.364 (0.325)	Data 1.04e-04 (1.50e-04)	Tok/s 46969 (43047)	Loss/tok 3.2116 (3.2795)	LR 1.250e-04
0: TRAIN [1][6920/7762]	Time 0.345 (0.326)	Data 1.01e-04 (1.50e-04)	Tok/s 48632 (43053)	Loss/tok 3.1704 (3.2796)	LR 1.250e-04
0: TRAIN [1][6930/7762]	Time 0.267 (0.326)	Data 1.04e-04 (1.50e-04)	Tok/s 38773 (43055)	Loss/tok 3.0561 (3.2796)	LR 1.250e-04
0: TRAIN [1][6940/7762]	Time 0.443 (0.326)	Data 1.02e-04 (1.50e-04)	Tok/s 52598 (43055)	Loss/tok 3.4584 (3.2796)	LR 1.250e-04
0: TRAIN [1][6950/7762]	Time 0.367 (0.326)	Data 1.05e-04 (1.50e-04)	Tok/s 46343 (43054)	Loss/tok 3.2858 (3.2795)	LR 1.250e-04
0: TRAIN [1][6960/7762]	Time 0.265 (0.326)	Data 1.00e-04 (1.49e-04)	Tok/s 39129 (43052)	Loss/tok 3.0905 (3.2794)	LR 1.250e-04
0: TRAIN [1][6970/7762]	Time 0.361 (0.325)	Data 1.03e-04 (1.49e-04)	Tok/s 46282 (43047)	Loss/tok 3.1849 (3.2793)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][6980/7762]	Time 0.364 (0.325)	Data 1.01e-04 (1.49e-04)	Tok/s 46839 (43046)	Loss/tok 3.2295 (3.2792)	LR 1.250e-04
0: TRAIN [1][6990/7762]	Time 0.262 (0.325)	Data 1.07e-04 (1.49e-04)	Tok/s 39542 (43047)	Loss/tok 3.1347 (3.2792)	LR 1.250e-04
0: TRAIN [1][7000/7762]	Time 0.365 (0.325)	Data 2.37e-04 (1.49e-04)	Tok/s 45322 (43050)	Loss/tok 3.3239 (3.2792)	LR 1.250e-04
0: TRAIN [1][7010/7762]	Time 0.462 (0.326)	Data 1.02e-04 (1.49e-04)	Tok/s 50831 (43051)	Loss/tok 3.3581 (3.2793)	LR 1.250e-04
0: TRAIN [1][7020/7762]	Time 0.592 (0.326)	Data 1.04e-04 (1.49e-04)	Tok/s 50653 (43049)	Loss/tok 3.4386 (3.2793)	LR 1.250e-04
0: TRAIN [1][7030/7762]	Time 0.363 (0.326)	Data 1.05e-04 (1.49e-04)	Tok/s 46500 (43053)	Loss/tok 3.2115 (3.2793)	LR 1.250e-04
0: TRAIN [1][7040/7762]	Time 0.266 (0.326)	Data 1.02e-04 (1.49e-04)	Tok/s 38721 (43057)	Loss/tok 2.9791 (3.2795)	LR 1.250e-04
0: TRAIN [1][7050/7762]	Time 0.585 (0.326)	Data 1.19e-04 (1.49e-04)	Tok/s 50939 (43059)	Loss/tok 3.5623 (3.2795)	LR 1.250e-04
0: TRAIN [1][7060/7762]	Time 0.265 (0.326)	Data 1.04e-04 (1.49e-04)	Tok/s 39076 (43062)	Loss/tok 3.0368 (3.2794)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][7070/7762]	Time 0.256 (0.326)	Data 1.11e-04 (1.49e-04)	Tok/s 40582 (43061)	Loss/tok 2.9543 (3.2794)	LR 1.250e-04
0: TRAIN [1][7080/7762]	Time 0.344 (0.326)	Data 1.08e-04 (1.49e-04)	Tok/s 48585 (43062)	Loss/tok 3.3547 (3.2794)	LR 1.250e-04
0: TRAIN [1][7090/7762]	Time 0.264 (0.326)	Data 1.03e-04 (1.49e-04)	Tok/s 40348 (43060)	Loss/tok 3.0743 (3.2793)	LR 1.250e-04
0: TRAIN [1][7100/7762]	Time 0.351 (0.326)	Data 1.04e-04 (1.49e-04)	Tok/s 47961 (43057)	Loss/tok 3.2579 (3.2795)	LR 1.250e-04
0: TRAIN [1][7110/7762]	Time 0.358 (0.326)	Data 1.05e-04 (1.49e-04)	Tok/s 46903 (43061)	Loss/tok 3.1764 (3.2795)	LR 1.250e-04
0: TRAIN [1][7120/7762]	Time 0.463 (0.326)	Data 1.00e-04 (1.49e-04)	Tok/s 50549 (43064)	Loss/tok 3.4783 (3.2795)	LR 1.250e-04
0: TRAIN [1][7130/7762]	Time 0.456 (0.326)	Data 1.06e-04 (1.48e-04)	Tok/s 51590 (43064)	Loss/tok 3.3157 (3.2794)	LR 1.250e-04
0: TRAIN [1][7140/7762]	Time 0.581 (0.326)	Data 1.01e-04 (1.48e-04)	Tok/s 51084 (43069)	Loss/tok 3.5700 (3.2798)	LR 1.250e-04
0: TRAIN [1][7150/7762]	Time 0.259 (0.326)	Data 1.07e-04 (1.48e-04)	Tok/s 41068 (43071)	Loss/tok 3.1312 (3.2799)	LR 1.250e-04
0: TRAIN [1][7160/7762]	Time 0.589 (0.326)	Data 1.02e-04 (1.48e-04)	Tok/s 50770 (43073)	Loss/tok 3.6102 (3.2800)	LR 1.250e-04
0: TRAIN [1][7170/7762]	Time 0.253 (0.326)	Data 9.89e-05 (1.48e-04)	Tok/s 40064 (43069)	Loss/tok 3.0616 (3.2798)	LR 1.250e-04
0: TRAIN [1][7180/7762]	Time 0.266 (0.326)	Data 9.97e-05 (1.48e-04)	Tok/s 39154 (43071)	Loss/tok 3.0605 (3.2800)	LR 1.250e-04
0: TRAIN [1][7190/7762]	Time 0.263 (0.326)	Data 1.13e-04 (1.48e-04)	Tok/s 39060 (43074)	Loss/tok 3.1248 (3.2800)	LR 1.250e-04
0: TRAIN [1][7200/7762]	Time 0.175 (0.326)	Data 1.14e-04 (1.48e-04)	Tok/s 30722 (43071)	Loss/tok 2.6256 (3.2799)	LR 1.250e-04
0: TRAIN [1][7210/7762]	Time 0.261 (0.326)	Data 1.08e-04 (1.48e-04)	Tok/s 39869 (43070)	Loss/tok 3.1504 (3.2800)	LR 1.250e-04
0: TRAIN [1][7220/7762]	Time 0.365 (0.326)	Data 9.92e-05 (1.48e-04)	Tok/s 46316 (43069)	Loss/tok 3.2746 (3.2801)	LR 1.250e-04
0: TRAIN [1][7230/7762]	Time 0.361 (0.326)	Data 1.02e-04 (1.48e-04)	Tok/s 46547 (43065)	Loss/tok 3.3440 (3.2799)	LR 1.250e-04
0: TRAIN [1][7240/7762]	Time 0.255 (0.326)	Data 9.94e-05 (1.48e-04)	Tok/s 40830 (43059)	Loss/tok 3.1350 (3.2797)	LR 1.250e-04
0: TRAIN [1][7250/7762]	Time 0.252 (0.326)	Data 1.15e-04 (1.48e-04)	Tok/s 40016 (43059)	Loss/tok 3.1177 (3.2797)	LR 1.250e-04
0: TRAIN [1][7260/7762]	Time 0.261 (0.326)	Data 1.10e-04 (1.48e-04)	Tok/s 39369 (43055)	Loss/tok 3.1132 (3.2795)	LR 1.250e-04
0: TRAIN [1][7270/7762]	Time 0.264 (0.326)	Data 9.99e-05 (1.48e-04)	Tok/s 39079 (43049)	Loss/tok 2.9988 (3.2792)	LR 1.250e-04
0: TRAIN [1][7280/7762]	Time 0.258 (0.326)	Data 9.73e-05 (1.48e-04)	Tok/s 39288 (43047)	Loss/tok 3.1200 (3.2792)	LR 1.250e-04
0: TRAIN [1][7290/7762]	Time 0.552 (0.326)	Data 1.06e-04 (1.47e-04)	Tok/s 55012 (43050)	Loss/tok 3.5101 (3.2793)	LR 1.250e-04
0: TRAIN [1][7300/7762]	Time 0.364 (0.326)	Data 1.05e-04 (1.47e-04)	Tok/s 46627 (43051)	Loss/tok 3.1335 (3.2792)	LR 1.250e-04
0: TRAIN [1][7310/7762]	Time 0.583 (0.326)	Data 1.07e-04 (1.47e-04)	Tok/s 51898 (43054)	Loss/tok 3.6698 (3.2793)	LR 1.250e-04
0: TRAIN [1][7320/7762]	Time 0.256 (0.326)	Data 1.07e-04 (1.47e-04)	Tok/s 41169 (43055)	Loss/tok 3.0055 (3.2794)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][7330/7762]	Time 0.270 (0.326)	Data 1.19e-04 (1.47e-04)	Tok/s 37294 (43058)	Loss/tok 3.0863 (3.2795)	LR 1.250e-04
0: TRAIN [1][7340/7762]	Time 0.260 (0.326)	Data 1.09e-04 (1.47e-04)	Tok/s 39149 (43058)	Loss/tok 3.0067 (3.2795)	LR 1.250e-04
0: TRAIN [1][7350/7762]	Time 0.365 (0.326)	Data 1.01e-04 (1.47e-04)	Tok/s 45816 (43060)	Loss/tok 3.2107 (3.2795)	LR 1.250e-04
0: TRAIN [1][7360/7762]	Time 0.458 (0.326)	Data 1.03e-04 (1.47e-04)	Tok/s 51436 (43059)	Loss/tok 3.3890 (3.2794)	LR 1.250e-04
0: TRAIN [1][7370/7762]	Time 0.444 (0.326)	Data 1.02e-04 (1.47e-04)	Tok/s 51820 (43065)	Loss/tok 3.4127 (3.2796)	LR 1.250e-04
0: TRAIN [1][7380/7762]	Time 0.460 (0.326)	Data 9.97e-05 (1.47e-04)	Tok/s 50318 (43062)	Loss/tok 3.4913 (3.2796)	LR 1.250e-04
0: TRAIN [1][7390/7762]	Time 0.264 (0.326)	Data 1.06e-04 (1.47e-04)	Tok/s 38849 (43058)	Loss/tok 3.0646 (3.2794)	LR 1.250e-04
0: TRAIN [1][7400/7762]	Time 0.264 (0.326)	Data 1.03e-04 (1.47e-04)	Tok/s 39038 (43061)	Loss/tok 3.1034 (3.2795)	LR 1.250e-04
0: TRAIN [1][7410/7762]	Time 0.265 (0.326)	Data 1.04e-04 (1.47e-04)	Tok/s 38619 (43063)	Loss/tok 3.0955 (3.2795)	LR 1.250e-04
0: TRAIN [1][7420/7762]	Time 0.582 (0.326)	Data 1.02e-04 (1.47e-04)	Tok/s 51096 (43063)	Loss/tok 3.6854 (3.2796)	LR 1.250e-04
0: TRAIN [1][7430/7762]	Time 0.593 (0.326)	Data 1.02e-04 (1.47e-04)	Tok/s 50203 (43067)	Loss/tok 3.6252 (3.2798)	LR 1.250e-04
0: TRAIN [1][7440/7762]	Time 0.260 (0.326)	Data 1.05e-04 (1.47e-04)	Tok/s 39978 (43068)	Loss/tok 3.0289 (3.2799)	LR 1.250e-04
0: TRAIN [1][7450/7762]	Time 0.174 (0.326)	Data 1.02e-04 (1.47e-04)	Tok/s 30283 (43065)	Loss/tok 2.6660 (3.2798)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [1][7460/7762]	Time 0.163 (0.326)	Data 1.09e-04 (1.47e-04)	Tok/s 31856 (43062)	Loss/tok 2.7008 (3.2796)	LR 1.250e-04
0: TRAIN [1][7470/7762]	Time 0.173 (0.326)	Data 1.03e-04 (1.46e-04)	Tok/s 30677 (43060)	Loss/tok 2.6090 (3.2796)	LR 1.250e-04
0: TRAIN [1][7480/7762]	Time 0.457 (0.326)	Data 1.04e-04 (1.46e-04)	Tok/s 51641 (43060)	Loss/tok 3.2722 (3.2795)	LR 1.250e-04
0: TRAIN [1][7490/7762]	Time 0.363 (0.326)	Data 9.94e-05 (1.46e-04)	Tok/s 46535 (43063)	Loss/tok 3.2640 (3.2795)	LR 1.250e-04
0: TRAIN [1][7500/7762]	Time 0.354 (0.326)	Data 1.07e-04 (1.46e-04)	Tok/s 47620 (43065)	Loss/tok 3.2778 (3.2798)	LR 1.250e-04
0: TRAIN [1][7510/7762]	Time 0.461 (0.326)	Data 1.04e-04 (1.46e-04)	Tok/s 49937 (43070)	Loss/tok 3.4599 (3.2800)	LR 1.250e-04
0: TRAIN [1][7520/7762]	Time 0.364 (0.326)	Data 1.03e-04 (1.46e-04)	Tok/s 46175 (43070)	Loss/tok 3.2980 (3.2799)	LR 1.250e-04
0: TRAIN [1][7530/7762]	Time 0.263 (0.326)	Data 1.00e-04 (1.46e-04)	Tok/s 39037 (43072)	Loss/tok 3.0847 (3.2799)	LR 1.250e-04
0: TRAIN [1][7540/7762]	Time 0.366 (0.326)	Data 1.06e-04 (1.46e-04)	Tok/s 46011 (43074)	Loss/tok 3.2375 (3.2800)	LR 1.250e-04
0: TRAIN [1][7550/7762]	Time 0.356 (0.326)	Data 1.01e-04 (1.46e-04)	Tok/s 46781 (43073)	Loss/tok 3.2626 (3.2800)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][7560/7762]	Time 0.590 (0.326)	Data 1.09e-04 (1.46e-04)	Tok/s 50191 (43077)	Loss/tok 3.7384 (3.2803)	LR 1.250e-04
0: TRAIN [1][7570/7762]	Time 0.260 (0.326)	Data 1.06e-04 (1.46e-04)	Tok/s 39954 (43076)	Loss/tok 3.0330 (3.2804)	LR 1.250e-04
0: TRAIN [1][7580/7762]	Time 0.178 (0.326)	Data 1.04e-04 (1.46e-04)	Tok/s 29877 (43073)	Loss/tok 2.7340 (3.2802)	LR 1.250e-04
0: TRAIN [1][7590/7762]	Time 0.262 (0.326)	Data 1.06e-04 (1.46e-04)	Tok/s 39750 (43071)	Loss/tok 2.9504 (3.2801)	LR 1.250e-04
0: TRAIN [1][7600/7762]	Time 0.358 (0.326)	Data 1.05e-04 (1.46e-04)	Tok/s 47167 (43073)	Loss/tok 3.2352 (3.2801)	LR 1.250e-04
0: TRAIN [1][7610/7762]	Time 0.261 (0.326)	Data 1.03e-04 (1.46e-04)	Tok/s 39920 (43070)	Loss/tok 3.0563 (3.2800)	LR 1.250e-04
0: TRAIN [1][7620/7762]	Time 0.264 (0.326)	Data 1.03e-04 (1.46e-04)	Tok/s 38658 (43069)	Loss/tok 3.0569 (3.2800)	LR 1.250e-04
0: TRAIN [1][7630/7762]	Time 0.461 (0.326)	Data 1.01e-04 (1.46e-04)	Tok/s 51434 (43070)	Loss/tok 3.3217 (3.2802)	LR 1.250e-04
0: TRAIN [1][7640/7762]	Time 0.461 (0.326)	Data 1.05e-04 (1.46e-04)	Tok/s 50557 (43067)	Loss/tok 3.4008 (3.2800)	LR 1.250e-04
0: TRAIN [1][7650/7762]	Time 0.439 (0.326)	Data 1.00e-04 (1.46e-04)	Tok/s 53266 (43066)	Loss/tok 3.4759 (3.2801)	LR 1.250e-04
0: TRAIN [1][7660/7762]	Time 0.363 (0.326)	Data 1.08e-04 (1.45e-04)	Tok/s 46183 (43068)	Loss/tok 3.3529 (3.2803)	LR 1.250e-04
0: TRAIN [1][7670/7762]	Time 0.358 (0.326)	Data 1.06e-04 (1.45e-04)	Tok/s 46916 (43069)	Loss/tok 3.3082 (3.2802)	LR 1.250e-04
0: TRAIN [1][7680/7762]	Time 0.342 (0.326)	Data 1.00e-04 (1.45e-04)	Tok/s 50111 (43067)	Loss/tok 3.2348 (3.2801)	LR 1.250e-04
0: TRAIN [1][7690/7762]	Time 0.365 (0.326)	Data 1.04e-04 (1.45e-04)	Tok/s 45891 (43065)	Loss/tok 3.2223 (3.2800)	LR 1.250e-04
0: TRAIN [1][7700/7762]	Time 0.452 (0.326)	Data 1.03e-04 (1.45e-04)	Tok/s 51927 (43064)	Loss/tok 3.3421 (3.2799)	LR 1.250e-04
0: TRAIN [1][7710/7762]	Time 0.264 (0.326)	Data 1.07e-04 (1.45e-04)	Tok/s 38550 (43063)	Loss/tok 3.0994 (3.2799)	LR 1.250e-04
0: TRAIN [1][7720/7762]	Time 0.177 (0.326)	Data 1.02e-04 (1.45e-04)	Tok/s 29494 (43061)	Loss/tok 2.6160 (3.2800)	LR 1.250e-04
0: TRAIN [1][7730/7762]	Time 0.353 (0.326)	Data 1.04e-04 (1.45e-04)	Tok/s 47377 (43060)	Loss/tok 3.2940 (3.2799)	LR 1.250e-04
0: TRAIN [1][7740/7762]	Time 0.266 (0.326)	Data 1.05e-04 (1.45e-04)	Tok/s 39626 (43061)	Loss/tok 2.9684 (3.2799)	LR 1.250e-04
0: TRAIN [1][7750/7762]	Time 0.455 (0.326)	Data 1.08e-04 (1.45e-04)	Tok/s 50824 (43061)	Loss/tok 3.4926 (3.2799)	LR 1.250e-04
0: TRAIN [1][7760/7762]	Time 0.463 (0.326)	Data 1.49e-02 (1.47e-04)	Tok/s 50195 (43058)	Loss/tok 3.4200 (3.2798)	LR 1.250e-04
:::MLL 1573750919.910 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1573750919.911 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/12]	Time 0.861 (0.861)	Decoder iters 128.0 (128.0)	Tok/s 19201 (19201)
0: TEST [1][10/12]	Time 0.125 (0.293)	Decoder iters 29.0 (50.2)	Tok/s 29639 (29804)
0: Running moses detokenizer
0: BLEU(score=22.619049297510763, counts=[36300, 17677, 9828, 5712], totals=[65502, 62499, 59497, 56500], precisions=[55.41815517083448, 28.283652538440617, 16.51847992335748, 10.109734513274336], bp=1.0, sys_len=65502, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1573750925.534 eval_accuracy: {"value": 22.62, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1573750925.535 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.2812	Test BLEU: 22.62
0: Performance: Epoch: 1	Training: 86129 Tok/s
0: Finished epoch 1
:::MLL 1573750925.535 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1573750925.535 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1573750925.536 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 4206667747
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][0/7762]	Time 0.669 (0.669)	Data 3.08e-01 (3.08e-01)	Tok/s 25002 (25002)	Loss/tok 3.1672 (3.1672)	LR 1.250e-04
0: TRAIN [2][10/7762]	Time 0.260 (0.304)	Data 1.03e-04 (2.81e-02)	Tok/s 40134 (38954)	Loss/tok 3.0827 (3.1549)	LR 1.250e-04
0: TRAIN [2][20/7762]	Time 0.254 (0.303)	Data 1.03e-04 (1.48e-02)	Tok/s 40359 (41292)	Loss/tok 2.9998 (3.1608)	LR 1.250e-04
0: TRAIN [2][30/7762]	Time 0.345 (0.307)	Data 1.11e-04 (1.00e-02)	Tok/s 49029 (42088)	Loss/tok 3.1787 (3.1722)	LR 1.250e-04
0: TRAIN [2][40/7762]	Time 0.265 (0.303)	Data 1.01e-04 (7.61e-03)	Tok/s 39762 (41992)	Loss/tok 3.1099 (3.1723)	LR 1.250e-04
0: TRAIN [2][50/7762]	Time 0.263 (0.312)	Data 1.00e-04 (6.14e-03)	Tok/s 39958 (42577)	Loss/tok 3.0389 (3.2036)	LR 1.250e-04
0: TRAIN [2][60/7762]	Time 0.353 (0.317)	Data 1.03e-04 (5.15e-03)	Tok/s 47958 (43197)	Loss/tok 3.3053 (3.2080)	LR 1.250e-04
0: TRAIN [2][70/7762]	Time 0.363 (0.321)	Data 1.07e-04 (4.44e-03)	Tok/s 45613 (43463)	Loss/tok 3.1187 (3.2187)	LR 1.250e-04
0: TRAIN [2][80/7762]	Time 0.453 (0.325)	Data 1.05e-04 (3.90e-03)	Tok/s 52143 (43714)	Loss/tok 3.3835 (3.2228)	LR 1.250e-04
0: TRAIN [2][90/7762]	Time 0.352 (0.323)	Data 1.07e-04 (3.49e-03)	Tok/s 47677 (43645)	Loss/tok 3.2465 (3.2177)	LR 1.250e-04
0: TRAIN [2][100/7762]	Time 0.361 (0.326)	Data 1.10e-04 (3.15e-03)	Tok/s 46967 (43821)	Loss/tok 3.3139 (3.2302)	LR 1.250e-04
0: TRAIN [2][110/7762]	Time 0.365 (0.327)	Data 1.21e-04 (2.88e-03)	Tok/s 46673 (43764)	Loss/tok 3.1657 (3.2282)	LR 1.250e-04
0: TRAIN [2][120/7762]	Time 0.261 (0.327)	Data 1.08e-04 (2.65e-03)	Tok/s 39115 (43723)	Loss/tok 3.0541 (3.2319)	LR 1.250e-04
0: TRAIN [2][130/7762]	Time 0.178 (0.324)	Data 9.97e-05 (2.45e-03)	Tok/s 28866 (43462)	Loss/tok 2.6533 (3.2246)	LR 1.250e-04
0: TRAIN [2][140/7762]	Time 0.344 (0.326)	Data 1.02e-04 (2.29e-03)	Tok/s 48902 (43557)	Loss/tok 3.2049 (3.2273)	LR 1.250e-04
0: TRAIN [2][150/7762]	Time 0.264 (0.323)	Data 1.02e-04 (2.14e-03)	Tok/s 38947 (43391)	Loss/tok 3.0548 (3.2210)	LR 1.250e-04
0: TRAIN [2][160/7762]	Time 0.260 (0.321)	Data 1.04e-04 (2.02e-03)	Tok/s 39534 (43241)	Loss/tok 3.0419 (3.2139)	LR 1.250e-04
0: TRAIN [2][170/7762]	Time 0.452 (0.321)	Data 1.06e-04 (1.90e-03)	Tok/s 52206 (43228)	Loss/tok 3.3557 (3.2134)	LR 1.250e-04
0: TRAIN [2][180/7762]	Time 0.357 (0.322)	Data 1.04e-04 (1.81e-03)	Tok/s 46920 (43319)	Loss/tok 3.2289 (3.2138)	LR 1.250e-04
0: TRAIN [2][190/7762]	Time 0.365 (0.322)	Data 1.04e-04 (1.72e-03)	Tok/s 46159 (43337)	Loss/tok 3.1275 (3.2132)	LR 1.250e-04
0: TRAIN [2][200/7762]	Time 0.584 (0.322)	Data 1.05e-04 (1.64e-03)	Tok/s 51218 (43284)	Loss/tok 3.5678 (3.2145)	LR 1.250e-04
0: TRAIN [2][210/7762]	Time 0.363 (0.321)	Data 1.08e-04 (1.56e-03)	Tok/s 46363 (43174)	Loss/tok 3.2530 (3.2096)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][220/7762]	Time 0.366 (0.322)	Data 1.07e-04 (1.50e-03)	Tok/s 45315 (43233)	Loss/tok 3.2759 (3.2132)	LR 1.250e-04
0: TRAIN [2][230/7762]	Time 0.258 (0.322)	Data 1.03e-04 (1.44e-03)	Tok/s 39852 (43240)	Loss/tok 3.0153 (3.2131)	LR 1.250e-04
0: TRAIN [2][240/7762]	Time 0.366 (0.322)	Data 1.05e-04 (1.38e-03)	Tok/s 46223 (43264)	Loss/tok 3.1679 (3.2131)	LR 1.250e-04
0: TRAIN [2][250/7762]	Time 0.450 (0.322)	Data 1.03e-04 (1.33e-03)	Tok/s 52649 (43283)	Loss/tok 3.2332 (3.2111)	LR 1.250e-04
0: TRAIN [2][260/7762]	Time 0.353 (0.321)	Data 1.04e-04 (1.28e-03)	Tok/s 47992 (43208)	Loss/tok 3.2383 (3.2113)	LR 1.250e-04
0: TRAIN [2][270/7762]	Time 0.176 (0.322)	Data 1.05e-04 (1.24e-03)	Tok/s 30847 (43233)	Loss/tok 2.6249 (3.2120)	LR 1.250e-04
0: TRAIN [2][280/7762]	Time 0.266 (0.322)	Data 1.02e-04 (1.20e-03)	Tok/s 38750 (43250)	Loss/tok 3.0012 (3.2116)	LR 1.250e-04
0: TRAIN [2][290/7762]	Time 0.263 (0.321)	Data 1.09e-04 (1.16e-03)	Tok/s 39463 (43242)	Loss/tok 3.0325 (3.2103)	LR 1.250e-04
0: TRAIN [2][300/7762]	Time 0.265 (0.322)	Data 1.23e-04 (1.13e-03)	Tok/s 39123 (43216)	Loss/tok 3.1008 (3.2138)	LR 1.250e-04
0: TRAIN [2][310/7762]	Time 0.352 (0.322)	Data 9.75e-05 (1.10e-03)	Tok/s 48121 (43215)	Loss/tok 3.1944 (3.2155)	LR 1.250e-04
0: TRAIN [2][320/7762]	Time 0.362 (0.321)	Data 9.85e-05 (1.06e-03)	Tok/s 46264 (43087)	Loss/tok 3.3604 (3.2120)	LR 1.250e-04
0: TRAIN [2][330/7762]	Time 0.359 (0.321)	Data 1.03e-04 (1.04e-03)	Tok/s 46755 (43148)	Loss/tok 3.1762 (3.2136)	LR 1.250e-04
0: TRAIN [2][340/7762]	Time 0.561 (0.321)	Data 1.02e-04 (1.01e-03)	Tok/s 52595 (43146)	Loss/tok 3.7017 (3.2151)	LR 1.250e-04
0: TRAIN [2][350/7762]	Time 0.363 (0.322)	Data 1.04e-04 (9.82e-04)	Tok/s 46533 (43140)	Loss/tok 3.2627 (3.2162)	LR 1.250e-04
0: TRAIN [2][360/7762]	Time 0.462 (0.321)	Data 9.97e-05 (9.58e-04)	Tok/s 50798 (43089)	Loss/tok 3.3298 (3.2150)	LR 1.250e-04
0: TRAIN [2][370/7762]	Time 0.179 (0.320)	Data 1.03e-04 (9.35e-04)	Tok/s 29979 (43016)	Loss/tok 2.5733 (3.2143)	LR 1.250e-04
0: TRAIN [2][380/7762]	Time 0.461 (0.321)	Data 1.04e-04 (9.13e-04)	Tok/s 50847 (43005)	Loss/tok 3.3195 (3.2131)	LR 1.250e-04
0: TRAIN [2][390/7762]	Time 0.456 (0.321)	Data 1.23e-04 (8.92e-04)	Tok/s 51103 (43052)	Loss/tok 3.4509 (3.2135)	LR 1.250e-04
0: TRAIN [2][400/7762]	Time 0.354 (0.319)	Data 9.58e-05 (8.73e-04)	Tok/s 47881 (42931)	Loss/tok 3.2022 (3.2100)	LR 1.250e-04
0: TRAIN [2][410/7762]	Time 0.178 (0.318)	Data 1.01e-04 (8.54e-04)	Tok/s 29557 (42863)	Loss/tok 2.6311 (3.2081)	LR 1.250e-04
0: TRAIN [2][420/7762]	Time 0.266 (0.319)	Data 1.05e-04 (8.36e-04)	Tok/s 38480 (42882)	Loss/tok 3.0227 (3.2099)	LR 1.250e-04
0: TRAIN [2][430/7762]	Time 0.260 (0.319)	Data 1.05e-04 (8.19e-04)	Tok/s 40890 (42903)	Loss/tok 3.0734 (3.2090)	LR 1.250e-04
0: TRAIN [2][440/7762]	Time 0.263 (0.319)	Data 9.85e-05 (8.03e-04)	Tok/s 40288 (42876)	Loss/tok 3.0277 (3.2086)	LR 1.250e-04
0: TRAIN [2][450/7762]	Time 0.461 (0.319)	Data 1.05e-04 (7.88e-04)	Tok/s 51252 (42885)	Loss/tok 3.2386 (3.2087)	LR 1.250e-04
0: TRAIN [2][460/7762]	Time 0.344 (0.319)	Data 1.02e-04 (7.73e-04)	Tok/s 48965 (42906)	Loss/tok 3.2205 (3.2091)	LR 1.250e-04
0: TRAIN [2][470/7762]	Time 0.267 (0.320)	Data 1.07e-04 (7.59e-04)	Tok/s 38094 (42938)	Loss/tok 3.0860 (3.2129)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][480/7762]	Time 0.458 (0.320)	Data 1.06e-04 (7.45e-04)	Tok/s 50976 (42884)	Loss/tok 3.3665 (3.2108)	LR 1.250e-04
0: TRAIN [2][490/7762]	Time 0.261 (0.319)	Data 1.06e-04 (7.32e-04)	Tok/s 39899 (42855)	Loss/tok 3.2574 (3.2092)	LR 1.250e-04
0: TRAIN [2][500/7762]	Time 0.446 (0.319)	Data 1.05e-04 (7.20e-04)	Tok/s 52426 (42856)	Loss/tok 3.4034 (3.2088)	LR 1.250e-04
0: TRAIN [2][510/7762]	Time 0.260 (0.319)	Data 1.05e-04 (7.08e-04)	Tok/s 39570 (42862)	Loss/tok 2.9979 (3.2111)	LR 1.250e-04
0: TRAIN [2][520/7762]	Time 0.266 (0.319)	Data 9.94e-05 (6.96e-04)	Tok/s 39616 (42830)	Loss/tok 3.0878 (3.2094)	LR 1.250e-04
0: TRAIN [2][530/7762]	Time 0.257 (0.319)	Data 9.92e-05 (6.85e-04)	Tok/s 39825 (42851)	Loss/tok 3.1096 (3.2093)	LR 1.250e-04
0: TRAIN [2][540/7762]	Time 0.263 (0.318)	Data 1.21e-04 (6.74e-04)	Tok/s 38900 (42837)	Loss/tok 3.0315 (3.2083)	LR 1.250e-04
0: TRAIN [2][550/7762]	Time 0.254 (0.318)	Data 1.04e-04 (6.64e-04)	Tok/s 41269 (42848)	Loss/tok 2.9978 (3.2076)	LR 1.250e-04
0: TRAIN [2][560/7762]	Time 0.363 (0.319)	Data 1.01e-04 (6.54e-04)	Tok/s 45960 (42871)	Loss/tok 3.1946 (3.2107)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][570/7762]	Time 0.257 (0.320)	Data 1.08e-04 (6.44e-04)	Tok/s 39719 (42865)	Loss/tok 3.1146 (3.2126)	LR 1.250e-04
0: TRAIN [2][580/7762]	Time 0.574 (0.321)	Data 1.04e-04 (6.35e-04)	Tok/s 51537 (42894)	Loss/tok 3.6104 (3.2152)	LR 1.250e-04
0: TRAIN [2][590/7762]	Time 0.368 (0.320)	Data 1.01e-04 (6.26e-04)	Tok/s 45605 (42868)	Loss/tok 3.3823 (3.2146)	LR 1.250e-04
0: TRAIN [2][600/7762]	Time 0.254 (0.320)	Data 1.03e-04 (6.17e-04)	Tok/s 40124 (42858)	Loss/tok 3.0205 (3.2138)	LR 1.250e-04
0: TRAIN [2][610/7762]	Time 0.358 (0.320)	Data 1.01e-04 (6.09e-04)	Tok/s 47392 (42857)	Loss/tok 3.1868 (3.2130)	LR 1.250e-04
0: TRAIN [2][620/7762]	Time 0.260 (0.321)	Data 1.01e-04 (6.01e-04)	Tok/s 39462 (42905)	Loss/tok 2.8983 (3.2145)	LR 1.250e-04
0: TRAIN [2][630/7762]	Time 0.265 (0.320)	Data 1.05e-04 (5.93e-04)	Tok/s 38252 (42864)	Loss/tok 3.0362 (3.2129)	LR 1.250e-04
0: TRAIN [2][640/7762]	Time 0.462 (0.321)	Data 1.04e-04 (5.85e-04)	Tok/s 50454 (42882)	Loss/tok 3.4404 (3.2138)	LR 1.250e-04
0: TRAIN [2][650/7762]	Time 0.351 (0.320)	Data 1.02e-04 (5.78e-04)	Tok/s 48335 (42841)	Loss/tok 3.3232 (3.2124)	LR 1.250e-04
0: TRAIN [2][660/7762]	Time 0.366 (0.320)	Data 1.09e-04 (5.71e-04)	Tok/s 45427 (42872)	Loss/tok 3.2109 (3.2119)	LR 1.250e-04
0: TRAIN [2][670/7762]	Time 0.263 (0.320)	Data 1.01e-04 (5.64e-04)	Tok/s 39959 (42873)	Loss/tok 3.0486 (3.2125)	LR 1.250e-04
0: TRAIN [2][680/7762]	Time 0.266 (0.320)	Data 1.03e-04 (5.57e-04)	Tok/s 38375 (42863)	Loss/tok 3.0373 (3.2123)	LR 1.250e-04
0: TRAIN [2][690/7762]	Time 0.175 (0.319)	Data 1.02e-04 (5.51e-04)	Tok/s 29690 (42830)	Loss/tok 2.6802 (3.2111)	LR 1.250e-04
0: TRAIN [2][700/7762]	Time 0.365 (0.320)	Data 1.25e-04 (5.45e-04)	Tok/s 46283 (42855)	Loss/tok 3.1890 (3.2113)	LR 1.250e-04
0: TRAIN [2][710/7762]	Time 0.174 (0.320)	Data 1.04e-04 (5.39e-04)	Tok/s 29725 (42871)	Loss/tok 2.5615 (3.2128)	LR 1.250e-04
0: TRAIN [2][720/7762]	Time 0.351 (0.320)	Data 1.08e-04 (5.33e-04)	Tok/s 47409 (42861)	Loss/tok 3.3299 (3.2126)	LR 1.250e-04
0: TRAIN [2][730/7762]	Time 0.265 (0.320)	Data 1.07e-04 (5.27e-04)	Tok/s 39044 (42843)	Loss/tok 3.0387 (3.2126)	LR 1.250e-04
0: TRAIN [2][740/7762]	Time 0.365 (0.320)	Data 1.03e-04 (5.21e-04)	Tok/s 45725 (42856)	Loss/tok 3.1657 (3.2133)	LR 1.250e-04
0: TRAIN [2][750/7762]	Time 0.174 (0.320)	Data 1.02e-04 (5.15e-04)	Tok/s 29525 (42849)	Loss/tok 2.6613 (3.2130)	LR 1.250e-04
0: TRAIN [2][760/7762]	Time 0.464 (0.320)	Data 1.04e-04 (5.10e-04)	Tok/s 49912 (42876)	Loss/tok 3.3914 (3.2125)	LR 1.250e-04
0: TRAIN [2][770/7762]	Time 0.362 (0.320)	Data 1.02e-04 (5.05e-04)	Tok/s 45296 (42858)	Loss/tok 3.2070 (3.2112)	LR 1.250e-04
0: TRAIN [2][780/7762]	Time 0.260 (0.320)	Data 1.04e-04 (5.00e-04)	Tok/s 39755 (42900)	Loss/tok 3.0529 (3.2119)	LR 1.250e-04
0: TRAIN [2][790/7762]	Time 0.463 (0.321)	Data 1.03e-04 (4.95e-04)	Tok/s 49789 (42923)	Loss/tok 3.4007 (3.2133)	LR 1.250e-04
0: TRAIN [2][800/7762]	Time 0.176 (0.320)	Data 1.08e-04 (4.90e-04)	Tok/s 29588 (42891)	Loss/tok 2.6014 (3.2121)	LR 1.250e-04
0: TRAIN [2][810/7762]	Time 0.463 (0.320)	Data 1.26e-04 (4.86e-04)	Tok/s 49982 (42906)	Loss/tok 3.5028 (3.2127)	LR 1.250e-04
0: TRAIN [2][820/7762]	Time 0.261 (0.321)	Data 1.05e-04 (4.81e-04)	Tok/s 39508 (42934)	Loss/tok 3.0627 (3.2139)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][830/7762]	Time 0.253 (0.321)	Data 1.04e-04 (4.76e-04)	Tok/s 40603 (42969)	Loss/tok 3.2152 (3.2158)	LR 1.250e-04
0: TRAIN [2][840/7762]	Time 0.265 (0.321)	Data 1.02e-04 (4.72e-04)	Tok/s 39147 (42952)	Loss/tok 3.0555 (3.2151)	LR 1.250e-04
0: TRAIN [2][850/7762]	Time 0.174 (0.321)	Data 1.03e-04 (4.68e-04)	Tok/s 29951 (42948)	Loss/tok 2.6357 (3.2147)	LR 1.250e-04
0: TRAIN [2][860/7762]	Time 0.365 (0.322)	Data 1.18e-04 (4.64e-04)	Tok/s 46221 (42976)	Loss/tok 3.2772 (3.2160)	LR 1.250e-04
0: TRAIN [2][870/7762]	Time 0.265 (0.321)	Data 1.06e-04 (4.59e-04)	Tok/s 39261 (42943)	Loss/tok 3.0583 (3.2148)	LR 1.250e-04
0: TRAIN [2][880/7762]	Time 0.261 (0.321)	Data 1.01e-04 (4.55e-04)	Tok/s 39701 (42930)	Loss/tok 2.9516 (3.2156)	LR 1.250e-04
0: TRAIN [2][890/7762]	Time 0.265 (0.321)	Data 1.02e-04 (4.51e-04)	Tok/s 38498 (42947)	Loss/tok 2.9579 (3.2157)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][900/7762]	Time 0.162 (0.321)	Data 1.14e-04 (4.48e-04)	Tok/s 32669 (42939)	Loss/tok 2.6262 (3.2162)	LR 1.250e-04
0: TRAIN [2][910/7762]	Time 0.465 (0.321)	Data 1.03e-04 (4.44e-04)	Tok/s 50795 (42944)	Loss/tok 3.3385 (3.2168)	LR 1.250e-04
0: TRAIN [2][920/7762]	Time 0.364 (0.321)	Data 1.02e-04 (4.40e-04)	Tok/s 46111 (42948)	Loss/tok 3.2148 (3.2164)	LR 1.250e-04
0: TRAIN [2][930/7762]	Time 0.256 (0.321)	Data 1.02e-04 (4.36e-04)	Tok/s 40203 (42965)	Loss/tok 2.8830 (3.2160)	LR 1.250e-04
0: TRAIN [2][940/7762]	Time 0.176 (0.322)	Data 1.03e-04 (4.33e-04)	Tok/s 29050 (42985)	Loss/tok 2.5072 (3.2183)	LR 1.250e-04
0: TRAIN [2][950/7762]	Time 0.465 (0.322)	Data 1.05e-04 (4.30e-04)	Tok/s 49112 (42991)	Loss/tok 3.3857 (3.2185)	LR 1.250e-04
0: TRAIN [2][960/7762]	Time 0.457 (0.322)	Data 1.05e-04 (4.26e-04)	Tok/s 50812 (42996)	Loss/tok 3.4163 (3.2184)	LR 1.250e-04
0: TRAIN [2][970/7762]	Time 0.450 (0.322)	Data 9.82e-05 (4.23e-04)	Tok/s 51696 (43007)	Loss/tok 3.4148 (3.2190)	LR 1.250e-04
0: TRAIN [2][980/7762]	Time 0.268 (0.322)	Data 1.09e-04 (4.20e-04)	Tok/s 37617 (43014)	Loss/tok 3.0773 (3.2195)	LR 1.250e-04
0: TRAIN [2][990/7762]	Time 0.448 (0.323)	Data 1.06e-04 (4.17e-04)	Tok/s 52666 (43073)	Loss/tok 3.3483 (3.2208)	LR 1.250e-04
0: TRAIN [2][1000/7762]	Time 0.363 (0.324)	Data 1.03e-04 (4.14e-04)	Tok/s 46541 (43087)	Loss/tok 3.1654 (3.2210)	LR 1.250e-04
0: TRAIN [2][1010/7762]	Time 0.360 (0.323)	Data 1.01e-04 (4.10e-04)	Tok/s 46784 (43089)	Loss/tok 3.2296 (3.2203)	LR 1.250e-04
0: TRAIN [2][1020/7762]	Time 0.359 (0.324)	Data 1.03e-04 (4.07e-04)	Tok/s 45896 (43103)	Loss/tok 3.3591 (3.2213)	LR 1.250e-04
0: TRAIN [2][1030/7762]	Time 0.265 (0.324)	Data 1.06e-04 (4.05e-04)	Tok/s 38444 (43093)	Loss/tok 3.1037 (3.2215)	LR 1.250e-04
0: TRAIN [2][1040/7762]	Time 0.178 (0.323)	Data 1.10e-04 (4.02e-04)	Tok/s 30143 (43073)	Loss/tok 2.6233 (3.2207)	LR 1.250e-04
0: TRAIN [2][1050/7762]	Time 0.265 (0.324)	Data 1.04e-04 (3.99e-04)	Tok/s 38254 (43077)	Loss/tok 3.0300 (3.2217)	LR 1.250e-04
0: TRAIN [2][1060/7762]	Time 0.264 (0.324)	Data 1.04e-04 (3.96e-04)	Tok/s 39324 (43076)	Loss/tok 3.0475 (3.2219)	LR 1.250e-04
0: TRAIN [2][1070/7762]	Time 0.260 (0.323)	Data 9.73e-05 (3.93e-04)	Tok/s 39286 (43052)	Loss/tok 3.0705 (3.2219)	LR 1.250e-04
0: TRAIN [2][1080/7762]	Time 0.365 (0.324)	Data 1.01e-04 (3.91e-04)	Tok/s 46037 (43066)	Loss/tok 3.2361 (3.2225)	LR 1.250e-04
0: TRAIN [2][1090/7762]	Time 0.358 (0.324)	Data 1.06e-04 (3.88e-04)	Tok/s 47126 (43054)	Loss/tok 3.1339 (3.2218)	LR 1.250e-04
0: TRAIN [2][1100/7762]	Time 0.266 (0.323)	Data 1.08e-04 (3.85e-04)	Tok/s 39145 (43035)	Loss/tok 3.0755 (3.2218)	LR 1.250e-04
0: TRAIN [2][1110/7762]	Time 0.268 (0.323)	Data 1.02e-04 (3.83e-04)	Tok/s 38095 (43033)	Loss/tok 3.0501 (3.2218)	LR 1.250e-04
0: TRAIN [2][1120/7762]	Time 0.363 (0.323)	Data 1.26e-04 (3.81e-04)	Tok/s 46610 (43048)	Loss/tok 3.3660 (3.2214)	LR 1.250e-04
0: TRAIN [2][1130/7762]	Time 0.349 (0.324)	Data 1.03e-04 (3.78e-04)	Tok/s 47932 (43049)	Loss/tok 3.1812 (3.2220)	LR 1.250e-04
0: TRAIN [2][1140/7762]	Time 0.584 (0.324)	Data 1.06e-04 (3.76e-04)	Tok/s 50766 (43071)	Loss/tok 3.5596 (3.2238)	LR 1.250e-04
0: TRAIN [2][1150/7762]	Time 0.346 (0.324)	Data 1.03e-04 (3.73e-04)	Tok/s 48012 (43083)	Loss/tok 3.3156 (3.2243)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1160/7762]	Time 0.364 (0.325)	Data 1.03e-04 (3.71e-04)	Tok/s 45963 (43110)	Loss/tok 3.2763 (3.2257)	LR 1.250e-04
0: TRAIN [2][1170/7762]	Time 0.350 (0.325)	Data 1.04e-04 (3.69e-04)	Tok/s 48117 (43110)	Loss/tok 3.1404 (3.2253)	LR 1.250e-04
0: TRAIN [2][1180/7762]	Time 0.558 (0.325)	Data 1.03e-04 (3.67e-04)	Tok/s 53376 (43115)	Loss/tok 3.5714 (3.2257)	LR 1.250e-04
0: TRAIN [2][1190/7762]	Time 0.354 (0.325)	Data 1.04e-04 (3.64e-04)	Tok/s 47269 (43114)	Loss/tok 3.2611 (3.2259)	LR 1.250e-04
0: TRAIN [2][1200/7762]	Time 0.352 (0.325)	Data 1.05e-04 (3.62e-04)	Tok/s 47645 (43102)	Loss/tok 3.1836 (3.2255)	LR 1.250e-04
0: TRAIN [2][1210/7762]	Time 0.364 (0.325)	Data 1.19e-04 (3.60e-04)	Tok/s 45634 (43097)	Loss/tok 3.1383 (3.2249)	LR 1.250e-04
0: TRAIN [2][1220/7762]	Time 0.355 (0.325)	Data 1.06e-04 (3.58e-04)	Tok/s 46710 (43122)	Loss/tok 3.2478 (3.2262)	LR 1.250e-04
0: TRAIN [2][1230/7762]	Time 0.177 (0.325)	Data 1.15e-04 (3.56e-04)	Tok/s 29915 (43088)	Loss/tok 2.5921 (3.2250)	LR 1.250e-04
0: TRAIN [2][1240/7762]	Time 0.264 (0.325)	Data 1.10e-04 (3.54e-04)	Tok/s 38569 (43116)	Loss/tok 3.1504 (3.2262)	LR 1.250e-04
0: TRAIN [2][1250/7762]	Time 0.260 (0.325)	Data 1.08e-04 (3.52e-04)	Tok/s 39412 (43108)	Loss/tok 2.9875 (3.2267)	LR 1.250e-04
0: TRAIN [2][1260/7762]	Time 0.176 (0.326)	Data 1.03e-04 (3.50e-04)	Tok/s 29087 (43106)	Loss/tok 2.6764 (3.2280)	LR 1.250e-04
0: TRAIN [2][1270/7762]	Time 0.586 (0.326)	Data 1.04e-04 (3.48e-04)	Tok/s 50275 (43107)	Loss/tok 3.5457 (3.2287)	LR 1.250e-04
0: TRAIN [2][1280/7762]	Time 0.450 (0.326)	Data 1.10e-04 (3.46e-04)	Tok/s 51790 (43086)	Loss/tok 3.4984 (3.2286)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1290/7762]	Time 0.593 (0.326)	Data 1.04e-04 (3.44e-04)	Tok/s 50293 (43110)	Loss/tok 3.5602 (3.2301)	LR 1.250e-04
0: TRAIN [2][1300/7762]	Time 0.263 (0.326)	Data 1.04e-04 (3.43e-04)	Tok/s 38989 (43117)	Loss/tok 3.0908 (3.2303)	LR 1.250e-04
0: TRAIN [2][1310/7762]	Time 0.452 (0.326)	Data 1.02e-04 (3.41e-04)	Tok/s 50401 (43089)	Loss/tok 3.4416 (3.2298)	LR 1.250e-04
0: TRAIN [2][1320/7762]	Time 0.259 (0.326)	Data 1.01e-04 (3.39e-04)	Tok/s 40447 (43075)	Loss/tok 3.0128 (3.2291)	LR 1.250e-04
0: TRAIN [2][1330/7762]	Time 0.260 (0.326)	Data 1.03e-04 (3.37e-04)	Tok/s 39409 (43093)	Loss/tok 2.9648 (3.2287)	LR 1.250e-04
0: TRAIN [2][1340/7762]	Time 0.261 (0.326)	Data 1.02e-04 (3.36e-04)	Tok/s 39780 (43095)	Loss/tok 3.0556 (3.2294)	LR 1.250e-04
0: TRAIN [2][1350/7762]	Time 0.259 (0.326)	Data 1.04e-04 (3.34e-04)	Tok/s 40724 (43079)	Loss/tok 3.0914 (3.2287)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1360/7762]	Time 0.264 (0.326)	Data 1.21e-04 (3.32e-04)	Tok/s 38982 (43089)	Loss/tok 2.9448 (3.2294)	LR 1.250e-04
0: TRAIN [2][1370/7762]	Time 0.172 (0.326)	Data 1.08e-04 (3.31e-04)	Tok/s 30485 (43104)	Loss/tok 2.5102 (3.2300)	LR 1.250e-04
0: TRAIN [2][1380/7762]	Time 0.461 (0.326)	Data 9.80e-05 (3.29e-04)	Tok/s 50127 (43100)	Loss/tok 3.2789 (3.2298)	LR 1.250e-04
0: TRAIN [2][1390/7762]	Time 0.357 (0.326)	Data 1.11e-04 (3.27e-04)	Tok/s 47255 (43114)	Loss/tok 3.2258 (3.2299)	LR 1.250e-04
0: TRAIN [2][1400/7762]	Time 0.262 (0.326)	Data 1.05e-04 (3.26e-04)	Tok/s 39625 (43117)	Loss/tok 2.9990 (3.2299)	LR 1.250e-04
0: TRAIN [2][1410/7762]	Time 0.265 (0.326)	Data 1.01e-04 (3.24e-04)	Tok/s 38411 (43112)	Loss/tok 3.0213 (3.2303)	LR 1.250e-04
0: TRAIN [2][1420/7762]	Time 0.343 (0.326)	Data 1.02e-04 (3.23e-04)	Tok/s 49618 (43101)	Loss/tok 3.1392 (3.2300)	LR 1.250e-04
0: TRAIN [2][1430/7762]	Time 0.363 (0.326)	Data 9.82e-05 (3.21e-04)	Tok/s 46203 (43099)	Loss/tok 3.2347 (3.2303)	LR 1.250e-04
0: TRAIN [2][1440/7762]	Time 0.253 (0.326)	Data 1.02e-04 (3.20e-04)	Tok/s 41301 (43103)	Loss/tok 3.0613 (3.2304)	LR 1.250e-04
0: TRAIN [2][1450/7762]	Time 0.349 (0.326)	Data 1.06e-04 (3.18e-04)	Tok/s 48318 (43110)	Loss/tok 3.2314 (3.2304)	LR 1.250e-04
0: TRAIN [2][1460/7762]	Time 0.358 (0.327)	Data 1.00e-04 (3.17e-04)	Tok/s 47078 (43134)	Loss/tok 3.1357 (3.2310)	LR 1.250e-04
0: TRAIN [2][1470/7762]	Time 0.362 (0.327)	Data 1.03e-04 (3.15e-04)	Tok/s 45983 (43127)	Loss/tok 3.2527 (3.2308)	LR 1.250e-04
0: TRAIN [2][1480/7762]	Time 0.263 (0.327)	Data 1.03e-04 (3.14e-04)	Tok/s 39945 (43129)	Loss/tok 3.1092 (3.2307)	LR 1.250e-04
0: TRAIN [2][1490/7762]	Time 0.267 (0.326)	Data 1.02e-04 (3.12e-04)	Tok/s 39315 (43123)	Loss/tok 3.0797 (3.2304)	LR 1.250e-04
0: TRAIN [2][1500/7762]	Time 0.364 (0.326)	Data 1.05e-04 (3.11e-04)	Tok/s 45574 (43128)	Loss/tok 3.3043 (3.2306)	LR 1.250e-04
0: TRAIN [2][1510/7762]	Time 0.264 (0.327)	Data 1.03e-04 (3.10e-04)	Tok/s 38530 (43127)	Loss/tok 3.0881 (3.2307)	LR 1.250e-04
0: TRAIN [2][1520/7762]	Time 0.259 (0.327)	Data 1.04e-04 (3.08e-04)	Tok/s 39900 (43141)	Loss/tok 3.0113 (3.2304)	LR 1.250e-04
0: TRAIN [2][1530/7762]	Time 0.178 (0.327)	Data 1.07e-04 (3.07e-04)	Tok/s 29673 (43135)	Loss/tok 2.6768 (3.2304)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1540/7762]	Time 0.256 (0.326)	Data 1.04e-04 (3.06e-04)	Tok/s 39086 (43114)	Loss/tok 3.0069 (3.2299)	LR 1.250e-04
0: TRAIN [2][1550/7762]	Time 0.266 (0.326)	Data 1.05e-04 (3.04e-04)	Tok/s 37848 (43109)	Loss/tok 3.1709 (3.2300)	LR 1.250e-04
0: TRAIN [2][1560/7762]	Time 0.362 (0.326)	Data 1.03e-04 (3.03e-04)	Tok/s 46779 (43114)	Loss/tok 3.2593 (3.2302)	LR 1.250e-04
0: TRAIN [2][1570/7762]	Time 0.265 (0.327)	Data 1.09e-04 (3.02e-04)	Tok/s 38860 (43131)	Loss/tok 3.0338 (3.2313)	LR 1.250e-04
0: TRAIN [2][1580/7762]	Time 0.177 (0.327)	Data 9.87e-05 (3.01e-04)	Tok/s 29849 (43112)	Loss/tok 2.5783 (3.2312)	LR 1.250e-04
0: TRAIN [2][1590/7762]	Time 0.356 (0.327)	Data 1.08e-04 (2.99e-04)	Tok/s 47365 (43118)	Loss/tok 3.2199 (3.2314)	LR 1.250e-04
0: TRAIN [2][1600/7762]	Time 0.255 (0.327)	Data 1.02e-04 (2.98e-04)	Tok/s 40854 (43106)	Loss/tok 3.0647 (3.2313)	LR 1.250e-04
0: TRAIN [2][1610/7762]	Time 0.260 (0.327)	Data 1.03e-04 (2.97e-04)	Tok/s 39293 (43100)	Loss/tok 3.0787 (3.2312)	LR 1.250e-04
0: TRAIN [2][1620/7762]	Time 0.260 (0.326)	Data 9.68e-05 (2.96e-04)	Tok/s 40642 (43081)	Loss/tok 3.0010 (3.2305)	LR 1.250e-04
0: TRAIN [2][1630/7762]	Time 0.460 (0.326)	Data 1.06e-04 (2.95e-04)	Tok/s 51766 (43072)	Loss/tok 3.3114 (3.2306)	LR 1.250e-04
0: TRAIN [2][1640/7762]	Time 0.348 (0.326)	Data 1.07e-04 (2.94e-04)	Tok/s 48326 (43058)	Loss/tok 3.2329 (3.2300)	LR 1.250e-04
0: TRAIN [2][1650/7762]	Time 0.261 (0.326)	Data 1.09e-04 (2.92e-04)	Tok/s 40086 (43047)	Loss/tok 3.0208 (3.2294)	LR 1.250e-04
0: TRAIN [2][1660/7762]	Time 0.260 (0.326)	Data 1.06e-04 (2.91e-04)	Tok/s 40653 (43046)	Loss/tok 3.0483 (3.2294)	LR 1.250e-04
0: TRAIN [2][1670/7762]	Time 0.267 (0.325)	Data 1.04e-04 (2.90e-04)	Tok/s 38461 (43030)	Loss/tok 3.0055 (3.2289)	LR 1.250e-04
0: TRAIN [2][1680/7762]	Time 0.266 (0.325)	Data 1.00e-04 (2.89e-04)	Tok/s 38738 (43044)	Loss/tok 2.9826 (3.2290)	LR 1.250e-04
0: TRAIN [2][1690/7762]	Time 0.460 (0.326)	Data 1.03e-04 (2.88e-04)	Tok/s 50804 (43068)	Loss/tok 3.4422 (3.2304)	LR 1.250e-04
0: TRAIN [2][1700/7762]	Time 0.351 (0.326)	Data 9.89e-05 (2.87e-04)	Tok/s 47852 (43058)	Loss/tok 3.2114 (3.2304)	LR 1.250e-04
0: TRAIN [2][1710/7762]	Time 0.263 (0.326)	Data 2.47e-04 (2.86e-04)	Tok/s 39922 (43047)	Loss/tok 2.9544 (3.2299)	LR 1.250e-04
0: TRAIN [2][1720/7762]	Time 0.353 (0.326)	Data 1.19e-04 (2.85e-04)	Tok/s 47511 (43056)	Loss/tok 3.2230 (3.2298)	LR 1.250e-04
0: TRAIN [2][1730/7762]	Time 0.261 (0.326)	Data 1.03e-04 (2.84e-04)	Tok/s 40008 (43053)	Loss/tok 3.0588 (3.2295)	LR 1.250e-04
0: TRAIN [2][1740/7762]	Time 0.263 (0.326)	Data 1.03e-04 (2.83e-04)	Tok/s 39795 (43059)	Loss/tok 3.0143 (3.2294)	LR 1.250e-04
0: TRAIN [2][1750/7762]	Time 0.357 (0.326)	Data 1.03e-04 (2.82e-04)	Tok/s 47184 (43070)	Loss/tok 3.2801 (3.2295)	LR 1.250e-04
0: TRAIN [2][1760/7762]	Time 0.256 (0.326)	Data 1.06e-04 (2.81e-04)	Tok/s 39716 (43084)	Loss/tok 3.0173 (3.2301)	LR 1.250e-04
0: TRAIN [2][1770/7762]	Time 0.357 (0.327)	Data 1.07e-04 (2.80e-04)	Tok/s 47340 (43108)	Loss/tok 3.2384 (3.2315)	LR 1.250e-04
0: TRAIN [2][1780/7762]	Time 0.469 (0.327)	Data 1.26e-04 (2.79e-04)	Tok/s 50297 (43136)	Loss/tok 3.3658 (3.2326)	LR 1.250e-04
0: TRAIN [2][1790/7762]	Time 0.365 (0.327)	Data 1.01e-04 (2.78e-04)	Tok/s 46551 (43128)	Loss/tok 3.1910 (3.2320)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1800/7762]	Time 0.262 (0.327)	Data 1.07e-04 (2.77e-04)	Tok/s 39948 (43144)	Loss/tok 3.0667 (3.2322)	LR 1.250e-04
0: TRAIN [2][1810/7762]	Time 0.265 (0.327)	Data 1.08e-04 (2.76e-04)	Tok/s 39318 (43157)	Loss/tok 3.0543 (3.2325)	LR 1.250e-04
0: TRAIN [2][1820/7762]	Time 0.265 (0.327)	Data 1.08e-04 (2.75e-04)	Tok/s 40236 (43166)	Loss/tok 3.1093 (3.2325)	LR 1.250e-04
0: TRAIN [2][1830/7762]	Time 0.262 (0.327)	Data 1.13e-04 (2.74e-04)	Tok/s 39270 (43169)	Loss/tok 3.0084 (3.2324)	LR 1.250e-04
0: TRAIN [2][1840/7762]	Time 0.267 (0.327)	Data 1.07e-04 (2.73e-04)	Tok/s 39182 (43170)	Loss/tok 2.9539 (3.2324)	LR 1.250e-04
0: TRAIN [2][1850/7762]	Time 0.265 (0.327)	Data 1.08e-04 (2.72e-04)	Tok/s 39014 (43152)	Loss/tok 2.9627 (3.2318)	LR 1.250e-04
0: TRAIN [2][1860/7762]	Time 0.259 (0.327)	Data 1.09e-04 (2.72e-04)	Tok/s 39887 (43162)	Loss/tok 3.0334 (3.2321)	LR 1.250e-04
0: TRAIN [2][1870/7762]	Time 0.255 (0.327)	Data 1.07e-04 (2.71e-04)	Tok/s 42029 (43146)	Loss/tok 3.1442 (3.2319)	LR 1.250e-04
0: TRAIN [2][1880/7762]	Time 0.364 (0.327)	Data 1.05e-04 (2.70e-04)	Tok/s 45268 (43151)	Loss/tok 3.3236 (3.2322)	LR 1.250e-04
0: TRAIN [2][1890/7762]	Time 0.176 (0.327)	Data 2.38e-04 (2.69e-04)	Tok/s 30044 (43132)	Loss/tok 2.6179 (3.2319)	LR 1.250e-04
0: TRAIN [2][1900/7762]	Time 0.264 (0.327)	Data 1.05e-04 (2.68e-04)	Tok/s 39486 (43127)	Loss/tok 2.9491 (3.2319)	LR 1.250e-04
0: TRAIN [2][1910/7762]	Time 0.344 (0.327)	Data 1.03e-04 (2.67e-04)	Tok/s 49156 (43139)	Loss/tok 3.2530 (3.2318)	LR 1.250e-04
0: TRAIN [2][1920/7762]	Time 0.264 (0.327)	Data 1.08e-04 (2.67e-04)	Tok/s 39236 (43149)	Loss/tok 2.9861 (3.2319)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1930/7762]	Time 0.461 (0.327)	Data 1.16e-04 (2.66e-04)	Tok/s 51490 (43143)	Loss/tok 3.3006 (3.2317)	LR 1.250e-04
0: TRAIN [2][1940/7762]	Time 0.343 (0.327)	Data 1.21e-04 (2.65e-04)	Tok/s 48861 (43135)	Loss/tok 3.2545 (3.2316)	LR 1.250e-04
0: TRAIN [2][1950/7762]	Time 0.366 (0.327)	Data 1.06e-04 (2.64e-04)	Tok/s 45616 (43141)	Loss/tok 3.1065 (3.2319)	LR 1.250e-04
0: TRAIN [2][1960/7762]	Time 0.365 (0.327)	Data 1.03e-04 (2.63e-04)	Tok/s 46492 (43124)	Loss/tok 3.2786 (3.2315)	LR 1.250e-04
0: TRAIN [2][1970/7762]	Time 0.583 (0.327)	Data 1.01e-04 (2.62e-04)	Tok/s 51052 (43124)	Loss/tok 3.6199 (3.2317)	LR 1.250e-04
0: TRAIN [2][1980/7762]	Time 0.176 (0.327)	Data 1.04e-04 (2.62e-04)	Tok/s 29982 (43132)	Loss/tok 2.5704 (3.2323)	LR 1.250e-04
0: TRAIN [2][1990/7762]	Time 0.266 (0.327)	Data 1.05e-04 (2.61e-04)	Tok/s 39991 (43127)	Loss/tok 3.0653 (3.2322)	LR 1.250e-04
0: TRAIN [2][2000/7762]	Time 0.264 (0.327)	Data 1.09e-04 (2.60e-04)	Tok/s 39348 (43122)	Loss/tok 2.9520 (3.2318)	LR 1.250e-04
0: TRAIN [2][2010/7762]	Time 0.343 (0.327)	Data 1.16e-04 (2.59e-04)	Tok/s 49718 (43112)	Loss/tok 3.1992 (3.2313)	LR 1.250e-04
0: TRAIN [2][2020/7762]	Time 0.268 (0.327)	Data 1.08e-04 (2.59e-04)	Tok/s 38893 (43102)	Loss/tok 2.9523 (3.2308)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2030/7762]	Time 0.356 (0.327)	Data 1.05e-04 (2.58e-04)	Tok/s 46460 (43108)	Loss/tok 3.2589 (3.2309)	LR 1.250e-04
0: TRAIN [2][2040/7762]	Time 0.355 (0.327)	Data 1.02e-04 (2.57e-04)	Tok/s 47901 (43109)	Loss/tok 3.2352 (3.2311)	LR 1.250e-04
0: TRAIN [2][2050/7762]	Time 0.260 (0.327)	Data 1.00e-04 (2.56e-04)	Tok/s 39234 (43097)	Loss/tok 2.9812 (3.2306)	LR 1.250e-04
0: TRAIN [2][2060/7762]	Time 0.364 (0.326)	Data 1.06e-04 (2.56e-04)	Tok/s 46587 (43099)	Loss/tok 3.2333 (3.2303)	LR 1.250e-04
0: TRAIN [2][2070/7762]	Time 0.176 (0.326)	Data 1.06e-04 (2.55e-04)	Tok/s 29240 (43087)	Loss/tok 2.5864 (3.2298)	LR 1.250e-04
0: TRAIN [2][2080/7762]	Time 0.364 (0.326)	Data 1.19e-04 (2.54e-04)	Tok/s 45482 (43082)	Loss/tok 3.2630 (3.2297)	LR 1.250e-04
0: TRAIN [2][2090/7762]	Time 0.174 (0.326)	Data 1.03e-04 (2.53e-04)	Tok/s 30364 (43066)	Loss/tok 2.6255 (3.2291)	LR 1.250e-04
0: TRAIN [2][2100/7762]	Time 0.445 (0.326)	Data 1.08e-04 (2.53e-04)	Tok/s 52558 (43085)	Loss/tok 3.2728 (3.2291)	LR 1.250e-04
0: TRAIN [2][2110/7762]	Time 0.463 (0.326)	Data 1.03e-04 (2.52e-04)	Tok/s 51184 (43068)	Loss/tok 3.3360 (3.2286)	LR 1.250e-04
0: TRAIN [2][2120/7762]	Time 0.266 (0.326)	Data 1.06e-04 (2.51e-04)	Tok/s 38856 (43068)	Loss/tok 2.9839 (3.2286)	LR 1.250e-04
0: TRAIN [2][2130/7762]	Time 0.439 (0.326)	Data 1.21e-04 (2.51e-04)	Tok/s 53867 (43089)	Loss/tok 3.3759 (3.2291)	LR 1.250e-04
0: TRAIN [2][2140/7762]	Time 0.463 (0.326)	Data 1.03e-04 (2.50e-04)	Tok/s 49745 (43080)	Loss/tok 3.5367 (3.2295)	LR 1.250e-04
0: TRAIN [2][2150/7762]	Time 0.265 (0.326)	Data 9.82e-05 (2.49e-04)	Tok/s 39254 (43054)	Loss/tok 2.9930 (3.2291)	LR 1.250e-04
0: TRAIN [2][2160/7762]	Time 0.268 (0.326)	Data 1.02e-04 (2.49e-04)	Tok/s 39029 (43057)	Loss/tok 3.0219 (3.2293)	LR 1.250e-04
0: TRAIN [2][2170/7762]	Time 0.588 (0.326)	Data 1.35e-04 (2.48e-04)	Tok/s 50241 (43075)	Loss/tok 3.5705 (3.2300)	LR 1.250e-04
0: TRAIN [2][2180/7762]	Time 0.180 (0.326)	Data 1.11e-04 (2.47e-04)	Tok/s 29799 (43067)	Loss/tok 2.4662 (3.2298)	LR 1.250e-04
0: TRAIN [2][2190/7762]	Time 0.461 (0.326)	Data 1.08e-04 (2.47e-04)	Tok/s 51031 (43061)	Loss/tok 3.3899 (3.2298)	LR 1.250e-04
0: TRAIN [2][2200/7762]	Time 0.265 (0.326)	Data 1.02e-04 (2.46e-04)	Tok/s 38528 (43058)	Loss/tok 2.9364 (3.2301)	LR 1.250e-04
0: TRAIN [2][2210/7762]	Time 0.176 (0.326)	Data 1.00e-04 (2.46e-04)	Tok/s 30361 (43050)	Loss/tok 2.7371 (3.2298)	LR 1.250e-04
0: TRAIN [2][2220/7762]	Time 0.267 (0.326)	Data 1.35e-04 (2.45e-04)	Tok/s 38567 (43051)	Loss/tok 3.0715 (3.2298)	LR 1.250e-04
0: TRAIN [2][2230/7762]	Time 0.258 (0.326)	Data 1.00e-04 (2.44e-04)	Tok/s 40419 (43056)	Loss/tok 3.0868 (3.2298)	LR 1.250e-04
0: TRAIN [2][2240/7762]	Time 0.263 (0.326)	Data 1.04e-04 (2.44e-04)	Tok/s 38817 (43064)	Loss/tok 3.0314 (3.2302)	LR 1.250e-04
0: TRAIN [2][2250/7762]	Time 0.344 (0.326)	Data 1.06e-04 (2.43e-04)	Tok/s 48421 (43073)	Loss/tok 3.2408 (3.2302)	LR 1.250e-04
0: TRAIN [2][2260/7762]	Time 0.174 (0.326)	Data 1.04e-04 (2.42e-04)	Tok/s 29595 (43051)	Loss/tok 2.6373 (3.2297)	LR 1.250e-04
0: TRAIN [2][2270/7762]	Time 0.464 (0.326)	Data 9.94e-05 (2.42e-04)	Tok/s 50065 (43047)	Loss/tok 3.5157 (3.2294)	LR 1.250e-04
0: TRAIN [2][2280/7762]	Time 0.265 (0.326)	Data 1.06e-04 (2.41e-04)	Tok/s 38216 (43041)	Loss/tok 3.0547 (3.2291)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][2290/7762]	Time 0.261 (0.326)	Data 1.04e-04 (2.41e-04)	Tok/s 39144 (43032)	Loss/tok 3.0126 (3.2291)	LR 1.250e-04
0: TRAIN [2][2300/7762]	Time 0.266 (0.326)	Data 1.43e-04 (2.40e-04)	Tok/s 38355 (43037)	Loss/tok 3.0468 (3.2290)	LR 1.250e-04
0: TRAIN [2][2310/7762]	Time 0.355 (0.326)	Data 1.00e-04 (2.40e-04)	Tok/s 46922 (43031)	Loss/tok 3.3320 (3.2288)	LR 1.250e-04
0: TRAIN [2][2320/7762]	Time 0.260 (0.326)	Data 1.04e-04 (2.39e-04)	Tok/s 40576 (43033)	Loss/tok 3.1262 (3.2286)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2330/7762]	Time 0.458 (0.326)	Data 1.19e-04 (2.38e-04)	Tok/s 50574 (43034)	Loss/tok 3.4092 (3.2289)	LR 1.250e-04
0: TRAIN [2][2340/7762]	Time 0.265 (0.326)	Data 9.78e-05 (2.38e-04)	Tok/s 39516 (43030)	Loss/tok 3.0848 (3.2288)	LR 1.250e-04
0: TRAIN [2][2350/7762]	Time 0.359 (0.326)	Data 1.04e-04 (2.37e-04)	Tok/s 47192 (43028)	Loss/tok 3.2927 (3.2286)	LR 1.250e-04
0: TRAIN [2][2360/7762]	Time 0.262 (0.326)	Data 1.07e-04 (2.37e-04)	Tok/s 39217 (43030)	Loss/tok 2.9173 (3.2283)	LR 1.250e-04
0: TRAIN [2][2370/7762]	Time 0.172 (0.326)	Data 1.01e-04 (2.36e-04)	Tok/s 30332 (43037)	Loss/tok 2.5552 (3.2289)	LR 1.250e-04
0: TRAIN [2][2380/7762]	Time 0.178 (0.326)	Data 1.02e-04 (2.36e-04)	Tok/s 29295 (43021)	Loss/tok 2.7539 (3.2285)	LR 1.250e-04
0: TRAIN [2][2390/7762]	Time 0.264 (0.326)	Data 9.92e-05 (2.35e-04)	Tok/s 38814 (43026)	Loss/tok 3.0041 (3.2284)	LR 1.250e-04
0: TRAIN [2][2400/7762]	Time 0.351 (0.326)	Data 1.18e-04 (2.35e-04)	Tok/s 47912 (43022)	Loss/tok 3.1596 (3.2283)	LR 1.250e-04
0: TRAIN [2][2410/7762]	Time 0.458 (0.325)	Data 1.04e-04 (2.34e-04)	Tok/s 50841 (43013)	Loss/tok 3.4025 (3.2280)	LR 1.250e-04
0: TRAIN [2][2420/7762]	Time 0.258 (0.325)	Data 1.40e-04 (2.34e-04)	Tok/s 40037 (43015)	Loss/tok 3.0221 (3.2281)	LR 1.250e-04
0: TRAIN [2][2430/7762]	Time 0.364 (0.325)	Data 9.73e-05 (2.33e-04)	Tok/s 45695 (43015)	Loss/tok 3.1677 (3.2282)	LR 1.250e-04
0: TRAIN [2][2440/7762]	Time 0.262 (0.325)	Data 1.03e-04 (2.32e-04)	Tok/s 39685 (43016)	Loss/tok 3.0441 (3.2279)	LR 1.250e-04
0: TRAIN [2][2450/7762]	Time 0.358 (0.326)	Data 1.04e-04 (2.32e-04)	Tok/s 47061 (43027)	Loss/tok 3.2438 (3.2278)	LR 1.250e-04
0: TRAIN [2][2460/7762]	Time 0.263 (0.325)	Data 9.73e-05 (2.31e-04)	Tok/s 39167 (43016)	Loss/tok 3.1848 (3.2275)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2470/7762]	Time 0.583 (0.325)	Data 1.05e-04 (2.31e-04)	Tok/s 51496 (43008)	Loss/tok 3.5014 (3.2273)	LR 1.250e-04
0: TRAIN [2][2480/7762]	Time 0.361 (0.325)	Data 1.07e-04 (2.31e-04)	Tok/s 46815 (43016)	Loss/tok 3.2423 (3.2272)	LR 1.250e-04
0: TRAIN [2][2490/7762]	Time 0.257 (0.325)	Data 1.03e-04 (2.30e-04)	Tok/s 41465 (43009)	Loss/tok 3.0047 (3.2269)	LR 1.250e-04
0: TRAIN [2][2500/7762]	Time 0.263 (0.325)	Data 1.17e-04 (2.30e-04)	Tok/s 39285 (43005)	Loss/tok 3.2167 (3.2265)	LR 1.250e-04
0: TRAIN [2][2510/7762]	Time 0.361 (0.325)	Data 1.01e-04 (2.29e-04)	Tok/s 45836 (43008)	Loss/tok 3.2347 (3.2263)	LR 1.250e-04
0: TRAIN [2][2520/7762]	Time 0.458 (0.325)	Data 1.02e-04 (2.29e-04)	Tok/s 51283 (43006)	Loss/tok 3.3532 (3.2261)	LR 1.250e-04
0: TRAIN [2][2530/7762]	Time 0.459 (0.325)	Data 1.02e-04 (2.28e-04)	Tok/s 51712 (43012)	Loss/tok 3.4141 (3.2263)	LR 1.250e-04
0: TRAIN [2][2540/7762]	Time 0.178 (0.325)	Data 1.07e-04 (2.28e-04)	Tok/s 30156 (43024)	Loss/tok 2.6785 (3.2268)	LR 1.250e-04
0: TRAIN [2][2550/7762]	Time 0.359 (0.325)	Data 1.16e-04 (2.27e-04)	Tok/s 47173 (43021)	Loss/tok 3.2791 (3.2270)	LR 1.250e-04
0: TRAIN [2][2560/7762]	Time 0.264 (0.325)	Data 1.03e-04 (2.27e-04)	Tok/s 39625 (43023)	Loss/tok 2.9876 (3.2273)	LR 1.250e-04
0: TRAIN [2][2570/7762]	Time 0.259 (0.325)	Data 1.34e-04 (2.26e-04)	Tok/s 40951 (43018)	Loss/tok 2.9228 (3.2270)	LR 1.250e-04
0: TRAIN [2][2580/7762]	Time 0.363 (0.325)	Data 1.05e-04 (2.26e-04)	Tok/s 45780 (43009)	Loss/tok 3.2595 (3.2268)	LR 1.250e-04
0: TRAIN [2][2590/7762]	Time 0.268 (0.325)	Data 1.03e-04 (2.25e-04)	Tok/s 38014 (43015)	Loss/tok 2.9526 (3.2268)	LR 1.250e-04
0: TRAIN [2][2600/7762]	Time 0.265 (0.325)	Data 1.18e-04 (2.25e-04)	Tok/s 38183 (43008)	Loss/tok 3.0159 (3.2266)	LR 1.250e-04
0: TRAIN [2][2610/7762]	Time 0.175 (0.325)	Data 1.05e-04 (2.24e-04)	Tok/s 30613 (43006)	Loss/tok 2.5399 (3.2265)	LR 1.250e-04
0: TRAIN [2][2620/7762]	Time 0.578 (0.325)	Data 1.08e-04 (2.24e-04)	Tok/s 51452 (43016)	Loss/tok 3.5714 (3.2268)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2630/7762]	Time 0.351 (0.325)	Data 9.97e-05 (2.24e-04)	Tok/s 47264 (43014)	Loss/tok 3.2170 (3.2269)	LR 1.250e-04
0: TRAIN [2][2640/7762]	Time 0.259 (0.325)	Data 1.23e-04 (2.23e-04)	Tok/s 39915 (43014)	Loss/tok 3.0678 (3.2271)	LR 1.250e-04
0: TRAIN [2][2650/7762]	Time 0.265 (0.325)	Data 1.04e-04 (2.23e-04)	Tok/s 39011 (43018)	Loss/tok 3.1173 (3.2272)	LR 1.250e-04
0: TRAIN [2][2660/7762]	Time 0.453 (0.325)	Data 1.21e-04 (2.22e-04)	Tok/s 51245 (43013)	Loss/tok 3.5372 (3.2272)	LR 1.250e-04
0: TRAIN [2][2670/7762]	Time 0.174 (0.325)	Data 1.04e-04 (2.22e-04)	Tok/s 30372 (43009)	Loss/tok 2.6104 (3.2273)	LR 1.250e-04
0: TRAIN [2][2680/7762]	Time 0.359 (0.326)	Data 2.39e-04 (2.21e-04)	Tok/s 46752 (43019)	Loss/tok 3.3325 (3.2280)	LR 1.250e-04
0: TRAIN [2][2690/7762]	Time 0.589 (0.326)	Data 1.07e-04 (2.21e-04)	Tok/s 50220 (43029)	Loss/tok 3.5767 (3.2284)	LR 1.250e-04
0: TRAIN [2][2700/7762]	Time 0.253 (0.326)	Data 1.05e-04 (2.21e-04)	Tok/s 40326 (43044)	Loss/tok 2.9325 (3.2289)	LR 1.250e-04
0: TRAIN [2][2710/7762]	Time 0.265 (0.326)	Data 9.99e-05 (2.20e-04)	Tok/s 38997 (43038)	Loss/tok 2.9371 (3.2285)	LR 1.250e-04
0: TRAIN [2][2720/7762]	Time 0.262 (0.326)	Data 1.08e-04 (2.20e-04)	Tok/s 39971 (43042)	Loss/tok 2.9401 (3.2289)	LR 1.250e-04
0: TRAIN [2][2730/7762]	Time 0.262 (0.326)	Data 1.01e-04 (2.19e-04)	Tok/s 39604 (43045)	Loss/tok 2.9940 (3.2288)	LR 1.250e-04
0: TRAIN [2][2740/7762]	Time 0.264 (0.326)	Data 1.24e-04 (2.19e-04)	Tok/s 38521 (43041)	Loss/tok 3.0398 (3.2288)	LR 1.250e-04
0: TRAIN [2][2750/7762]	Time 0.453 (0.326)	Data 1.05e-04 (2.19e-04)	Tok/s 51770 (43045)	Loss/tok 3.4266 (3.2290)	LR 1.250e-04
0: TRAIN [2][2760/7762]	Time 0.367 (0.326)	Data 1.03e-04 (2.18e-04)	Tok/s 45706 (43048)	Loss/tok 3.2823 (3.2289)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2770/7762]	Time 0.588 (0.326)	Data 1.04e-04 (2.18e-04)	Tok/s 50780 (43046)	Loss/tok 3.5668 (3.2290)	LR 1.250e-04
0: TRAIN [2][2780/7762]	Time 0.362 (0.326)	Data 1.11e-04 (2.17e-04)	Tok/s 45877 (43049)	Loss/tok 3.2489 (3.2290)	LR 1.250e-04
0: TRAIN [2][2790/7762]	Time 0.354 (0.326)	Data 1.04e-04 (2.17e-04)	Tok/s 48363 (43048)	Loss/tok 3.2359 (3.2287)	LR 1.250e-04
0: TRAIN [2][2800/7762]	Time 0.267 (0.326)	Data 1.02e-04 (2.17e-04)	Tok/s 39852 (43050)	Loss/tok 2.9954 (3.2285)	LR 1.250e-04
0: TRAIN [2][2810/7762]	Time 0.354 (0.326)	Data 1.01e-04 (2.16e-04)	Tok/s 47327 (43049)	Loss/tok 3.2268 (3.2285)	LR 1.250e-04
0: TRAIN [2][2820/7762]	Time 0.268 (0.326)	Data 1.08e-04 (2.16e-04)	Tok/s 38276 (43041)	Loss/tok 2.9515 (3.2282)	LR 1.250e-04
0: TRAIN [2][2830/7762]	Time 0.350 (0.326)	Data 1.03e-04 (2.15e-04)	Tok/s 47817 (43034)	Loss/tok 3.1074 (3.2278)	LR 1.250e-04
0: TRAIN [2][2840/7762]	Time 0.264 (0.326)	Data 1.04e-04 (2.15e-04)	Tok/s 39072 (43028)	Loss/tok 2.9771 (3.2276)	LR 1.250e-04
0: TRAIN [2][2850/7762]	Time 0.256 (0.326)	Data 1.13e-04 (2.15e-04)	Tok/s 39467 (43023)	Loss/tok 3.1058 (3.2276)	LR 1.250e-04
0: TRAIN [2][2860/7762]	Time 0.265 (0.326)	Data 1.02e-04 (2.14e-04)	Tok/s 38826 (43025)	Loss/tok 2.9357 (3.2274)	LR 1.250e-04
0: TRAIN [2][2870/7762]	Time 0.367 (0.326)	Data 1.04e-04 (2.14e-04)	Tok/s 45359 (43026)	Loss/tok 3.1951 (3.2273)	LR 1.250e-04
0: TRAIN [2][2880/7762]	Time 0.586 (0.326)	Data 1.07e-04 (2.14e-04)	Tok/s 50228 (43028)	Loss/tok 3.6382 (3.2278)	LR 1.250e-04
0: TRAIN [2][2890/7762]	Time 0.462 (0.326)	Data 1.05e-04 (2.13e-04)	Tok/s 49858 (43036)	Loss/tok 3.3905 (3.2283)	LR 1.250e-04
0: TRAIN [2][2900/7762]	Time 0.261 (0.326)	Data 1.03e-04 (2.13e-04)	Tok/s 39774 (43030)	Loss/tok 3.0687 (3.2280)	LR 1.250e-04
0: TRAIN [2][2910/7762]	Time 0.364 (0.326)	Data 1.02e-04 (2.12e-04)	Tok/s 46980 (43026)	Loss/tok 3.2193 (3.2278)	LR 1.250e-04
0: TRAIN [2][2920/7762]	Time 0.264 (0.326)	Data 1.06e-04 (2.12e-04)	Tok/s 38927 (43036)	Loss/tok 3.1033 (3.2283)	LR 1.250e-04
0: TRAIN [2][2930/7762]	Time 0.265 (0.326)	Data 1.06e-04 (2.12e-04)	Tok/s 39537 (43030)	Loss/tok 3.0188 (3.2282)	LR 1.250e-04
0: TRAIN [2][2940/7762]	Time 0.339 (0.326)	Data 1.00e-04 (2.11e-04)	Tok/s 49590 (43028)	Loss/tok 3.1521 (3.2283)	LR 1.250e-04
0: TRAIN [2][2950/7762]	Time 0.446 (0.326)	Data 1.06e-04 (2.11e-04)	Tok/s 52125 (43029)	Loss/tok 3.4689 (3.2282)	LR 1.250e-04
0: TRAIN [2][2960/7762]	Time 0.264 (0.326)	Data 1.02e-04 (2.11e-04)	Tok/s 38859 (43019)	Loss/tok 2.9716 (3.2279)	LR 1.250e-04
0: TRAIN [2][2970/7762]	Time 0.173 (0.326)	Data 1.13e-04 (2.10e-04)	Tok/s 30544 (43014)	Loss/tok 2.6193 (3.2283)	LR 1.250e-04
0: TRAIN [2][2980/7762]	Time 0.268 (0.326)	Data 1.09e-04 (2.10e-04)	Tok/s 39141 (43021)	Loss/tok 3.0468 (3.2285)	LR 1.250e-04
0: TRAIN [2][2990/7762]	Time 0.264 (0.326)	Data 1.09e-04 (2.09e-04)	Tok/s 39364 (43025)	Loss/tok 3.0421 (3.2286)	LR 1.250e-04
0: TRAIN [2][3000/7762]	Time 0.256 (0.326)	Data 1.05e-04 (2.09e-04)	Tok/s 40299 (43028)	Loss/tok 2.9614 (3.2290)	LR 1.250e-04
0: TRAIN [2][3010/7762]	Time 0.356 (0.326)	Data 9.87e-05 (2.09e-04)	Tok/s 46789 (43018)	Loss/tok 3.3078 (3.2288)	LR 1.250e-04
0: TRAIN [2][3020/7762]	Time 0.269 (0.326)	Data 1.04e-04 (2.08e-04)	Tok/s 38053 (43014)	Loss/tok 3.0951 (3.2286)	LR 1.250e-04
0: TRAIN [2][3030/7762]	Time 0.265 (0.326)	Data 1.05e-04 (2.08e-04)	Tok/s 39954 (43016)	Loss/tok 3.0408 (3.2284)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3040/7762]	Time 0.260 (0.326)	Data 1.16e-04 (2.08e-04)	Tok/s 39821 (43018)	Loss/tok 3.0770 (3.2285)	LR 1.250e-04
0: TRAIN [2][3050/7762]	Time 0.464 (0.326)	Data 1.24e-04 (2.07e-04)	Tok/s 51299 (43013)	Loss/tok 3.2544 (3.2283)	LR 1.250e-04
0: TRAIN [2][3060/7762]	Time 0.351 (0.326)	Data 1.15e-04 (2.07e-04)	Tok/s 47597 (43013)	Loss/tok 3.1280 (3.2283)	LR 1.250e-04
0: TRAIN [2][3070/7762]	Time 0.437 (0.326)	Data 1.28e-04 (2.07e-04)	Tok/s 54535 (43028)	Loss/tok 3.3683 (3.2290)	LR 1.250e-04
0: TRAIN [2][3080/7762]	Time 0.366 (0.326)	Data 1.06e-04 (2.06e-04)	Tok/s 46052 (43032)	Loss/tok 3.1854 (3.2291)	LR 1.250e-04
0: TRAIN [2][3090/7762]	Time 0.363 (0.326)	Data 1.07e-04 (2.06e-04)	Tok/s 46869 (43031)	Loss/tok 3.1585 (3.2291)	LR 1.250e-04
0: TRAIN [2][3100/7762]	Time 0.266 (0.326)	Data 1.07e-04 (2.06e-04)	Tok/s 37757 (43026)	Loss/tok 2.9619 (3.2289)	LR 1.250e-04
0: TRAIN [2][3110/7762]	Time 0.365 (0.326)	Data 1.06e-04 (2.06e-04)	Tok/s 46669 (43037)	Loss/tok 3.2512 (3.2295)	LR 1.250e-04
0: TRAIN [2][3120/7762]	Time 0.343 (0.326)	Data 1.05e-04 (2.05e-04)	Tok/s 48982 (43037)	Loss/tok 3.3678 (3.2297)	LR 1.250e-04
0: TRAIN [2][3130/7762]	Time 0.363 (0.326)	Data 1.07e-04 (2.05e-04)	Tok/s 45965 (43043)	Loss/tok 3.2214 (3.2297)	LR 1.250e-04
0: TRAIN [2][3140/7762]	Time 0.258 (0.326)	Data 1.04e-04 (2.05e-04)	Tok/s 39919 (43041)	Loss/tok 2.9111 (3.2296)	LR 1.250e-04
0: TRAIN [2][3150/7762]	Time 0.267 (0.326)	Data 1.03e-04 (2.04e-04)	Tok/s 38583 (43039)	Loss/tok 2.9375 (3.2293)	LR 1.250e-04
0: TRAIN [2][3160/7762]	Time 0.353 (0.326)	Data 1.07e-04 (2.04e-04)	Tok/s 47090 (43034)	Loss/tok 3.2563 (3.2291)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3170/7762]	Time 0.261 (0.326)	Data 1.05e-04 (2.04e-04)	Tok/s 39684 (43030)	Loss/tok 2.9834 (3.2289)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3180/7762]	Time 0.264 (0.326)	Data 1.18e-04 (2.03e-04)	Tok/s 38828 (43033)	Loss/tok 3.0094 (3.2289)	LR 1.250e-04
0: TRAIN [2][3190/7762]	Time 0.353 (0.326)	Data 1.06e-04 (2.03e-04)	Tok/s 47422 (43034)	Loss/tok 3.2327 (3.2291)	LR 1.250e-04
0: TRAIN [2][3200/7762]	Time 0.259 (0.326)	Data 1.04e-04 (2.03e-04)	Tok/s 39504 (43031)	Loss/tok 3.0141 (3.2289)	LR 1.250e-04
0: TRAIN [2][3210/7762]	Time 0.263 (0.326)	Data 1.01e-04 (2.02e-04)	Tok/s 39029 (43031)	Loss/tok 3.0514 (3.2288)	LR 1.250e-04
0: TRAIN [2][3220/7762]	Time 0.360 (0.326)	Data 1.07e-04 (2.02e-04)	Tok/s 46134 (43040)	Loss/tok 3.1969 (3.2290)	LR 1.250e-04
0: TRAIN [2][3230/7762]	Time 0.254 (0.326)	Data 1.01e-04 (2.02e-04)	Tok/s 40652 (43044)	Loss/tok 3.0431 (3.2290)	LR 1.250e-04
0: TRAIN [2][3240/7762]	Time 0.446 (0.326)	Data 1.01e-04 (2.01e-04)	Tok/s 52707 (43053)	Loss/tok 3.3498 (3.2292)	LR 1.250e-04
0: TRAIN [2][3250/7762]	Time 0.457 (0.326)	Data 9.97e-05 (2.01e-04)	Tok/s 50847 (43046)	Loss/tok 3.3138 (3.2289)	LR 1.250e-04
0: TRAIN [2][3260/7762]	Time 0.265 (0.326)	Data 1.04e-04 (2.01e-04)	Tok/s 38810 (43037)	Loss/tok 2.9347 (3.2286)	LR 1.250e-04
0: TRAIN [2][3270/7762]	Time 0.341 (0.326)	Data 1.06e-04 (2.01e-04)	Tok/s 50072 (43042)	Loss/tok 3.0857 (3.2285)	LR 1.250e-04
0: TRAIN [2][3280/7762]	Time 0.450 (0.326)	Data 1.08e-04 (2.00e-04)	Tok/s 52065 (43040)	Loss/tok 3.4386 (3.2285)	LR 1.250e-04
0: TRAIN [2][3290/7762]	Time 0.366 (0.326)	Data 9.97e-05 (2.00e-04)	Tok/s 46307 (43035)	Loss/tok 3.1403 (3.2284)	LR 1.250e-04
0: TRAIN [2][3300/7762]	Time 0.357 (0.326)	Data 1.01e-04 (2.00e-04)	Tok/s 46904 (43034)	Loss/tok 3.2049 (3.2281)	LR 1.250e-04
0: TRAIN [2][3310/7762]	Time 0.361 (0.326)	Data 1.03e-04 (1.99e-04)	Tok/s 46652 (43040)	Loss/tok 3.2395 (3.2283)	LR 1.250e-04
0: TRAIN [2][3320/7762]	Time 0.362 (0.326)	Data 1.06e-04 (1.99e-04)	Tok/s 46634 (43037)	Loss/tok 3.1982 (3.2283)	LR 1.250e-04
0: TRAIN [2][3330/7762]	Time 0.359 (0.326)	Data 1.04e-04 (1.99e-04)	Tok/s 46589 (43032)	Loss/tok 3.1340 (3.2280)	LR 1.250e-04
0: TRAIN [2][3340/7762]	Time 0.578 (0.326)	Data 1.02e-04 (1.99e-04)	Tok/s 51272 (43037)	Loss/tok 3.7010 (3.2284)	LR 1.250e-04
0: TRAIN [2][3350/7762]	Time 0.364 (0.326)	Data 1.08e-04 (1.98e-04)	Tok/s 46124 (43040)	Loss/tok 3.2605 (3.2283)	LR 1.250e-04
0: TRAIN [2][3360/7762]	Time 0.267 (0.326)	Data 1.03e-04 (1.98e-04)	Tok/s 38181 (43030)	Loss/tok 3.0828 (3.2280)	LR 1.250e-04
0: TRAIN [2][3370/7762]	Time 0.437 (0.326)	Data 1.02e-04 (1.98e-04)	Tok/s 53773 (43036)	Loss/tok 3.2296 (3.2280)	LR 1.250e-04
0: TRAIN [2][3380/7762]	Time 0.264 (0.326)	Data 1.04e-04 (1.97e-04)	Tok/s 39984 (43028)	Loss/tok 3.1182 (3.2278)	LR 1.250e-04
0: TRAIN [2][3390/7762]	Time 0.582 (0.326)	Data 1.05e-04 (1.97e-04)	Tok/s 51418 (43033)	Loss/tok 3.5185 (3.2279)	LR 1.250e-04
0: TRAIN [2][3400/7762]	Time 0.177 (0.326)	Data 1.08e-04 (1.97e-04)	Tok/s 30038 (43033)	Loss/tok 2.6343 (3.2280)	LR 1.250e-04
0: TRAIN [2][3410/7762]	Time 0.261 (0.326)	Data 1.08e-04 (1.97e-04)	Tok/s 39429 (43041)	Loss/tok 2.9953 (3.2282)	LR 1.250e-04
0: TRAIN [2][3420/7762]	Time 0.451 (0.326)	Data 1.17e-04 (1.96e-04)	Tok/s 50757 (43037)	Loss/tok 3.4870 (3.2280)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3430/7762]	Time 0.588 (0.326)	Data 1.02e-04 (1.96e-04)	Tok/s 49807 (43039)	Loss/tok 3.6991 (3.2283)	LR 1.250e-04
0: TRAIN [2][3440/7762]	Time 0.178 (0.326)	Data 1.02e-04 (1.96e-04)	Tok/s 30085 (43041)	Loss/tok 2.7015 (3.2286)	LR 1.250e-04
0: TRAIN [2][3450/7762]	Time 0.357 (0.326)	Data 1.05e-04 (1.96e-04)	Tok/s 46994 (43048)	Loss/tok 3.1532 (3.2288)	LR 1.250e-04
0: TRAIN [2][3460/7762]	Time 0.360 (0.326)	Data 1.01e-04 (1.95e-04)	Tok/s 45952 (43051)	Loss/tok 3.1718 (3.2287)	LR 1.250e-04
0: TRAIN [2][3470/7762]	Time 0.366 (0.326)	Data 1.03e-04 (1.95e-04)	Tok/s 46380 (43053)	Loss/tok 3.2016 (3.2285)	LR 1.250e-04
0: TRAIN [2][3480/7762]	Time 0.269 (0.326)	Data 9.73e-05 (1.95e-04)	Tok/s 38285 (43050)	Loss/tok 3.0265 (3.2283)	LR 1.250e-04
0: TRAIN [2][3490/7762]	Time 0.361 (0.326)	Data 1.21e-04 (1.95e-04)	Tok/s 45905 (43050)	Loss/tok 3.2836 (3.2283)	LR 1.250e-04
0: TRAIN [2][3500/7762]	Time 0.266 (0.326)	Data 1.02e-04 (1.94e-04)	Tok/s 38973 (43047)	Loss/tok 2.9890 (3.2281)	LR 1.250e-04
0: TRAIN [2][3510/7762]	Time 0.364 (0.326)	Data 1.01e-04 (1.94e-04)	Tok/s 46113 (43042)	Loss/tok 3.2834 (3.2279)	LR 1.250e-04
0: TRAIN [2][3520/7762]	Time 0.463 (0.326)	Data 1.03e-04 (1.94e-04)	Tok/s 50928 (43036)	Loss/tok 3.4458 (3.2277)	LR 1.250e-04
0: TRAIN [2][3530/7762]	Time 0.467 (0.326)	Data 1.08e-04 (1.94e-04)	Tok/s 49483 (43048)	Loss/tok 3.4945 (3.2283)	LR 1.250e-04
0: TRAIN [2][3540/7762]	Time 0.266 (0.326)	Data 1.02e-04 (1.93e-04)	Tok/s 38525 (43043)	Loss/tok 2.9867 (3.2281)	LR 1.250e-04
0: TRAIN [2][3550/7762]	Time 0.357 (0.326)	Data 1.03e-04 (1.93e-04)	Tok/s 46324 (43044)	Loss/tok 3.2965 (3.2278)	LR 1.250e-04
0: TRAIN [2][3560/7762]	Time 0.264 (0.326)	Data 1.03e-04 (1.93e-04)	Tok/s 38828 (43032)	Loss/tok 2.9607 (3.2274)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3570/7762]	Time 0.460 (0.326)	Data 1.04e-04 (1.93e-04)	Tok/s 51207 (43037)	Loss/tok 3.4471 (3.2274)	LR 1.250e-04
0: TRAIN [2][3580/7762]	Time 0.360 (0.326)	Data 1.08e-04 (1.92e-04)	Tok/s 46716 (43040)	Loss/tok 3.3200 (3.2276)	LR 1.250e-04
0: TRAIN [2][3590/7762]	Time 0.176 (0.326)	Data 1.02e-04 (1.92e-04)	Tok/s 30313 (43038)	Loss/tok 2.7204 (3.2275)	LR 1.250e-04
0: TRAIN [2][3600/7762]	Time 0.356 (0.326)	Data 1.05e-04 (1.92e-04)	Tok/s 46597 (43041)	Loss/tok 3.2749 (3.2275)	LR 1.250e-04
0: TRAIN [2][3610/7762]	Time 0.265 (0.326)	Data 1.04e-04 (1.92e-04)	Tok/s 39086 (43045)	Loss/tok 3.0013 (3.2274)	LR 1.250e-04
0: TRAIN [2][3620/7762]	Time 0.358 (0.326)	Data 1.03e-04 (1.91e-04)	Tok/s 47176 (43043)	Loss/tok 3.2084 (3.2273)	LR 1.250e-04
0: TRAIN [2][3630/7762]	Time 0.262 (0.326)	Data 1.08e-04 (1.91e-04)	Tok/s 39194 (43043)	Loss/tok 3.0522 (3.2273)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3640/7762]	Time 0.451 (0.326)	Data 1.21e-04 (1.91e-04)	Tok/s 51171 (43047)	Loss/tok 3.4731 (3.2277)	LR 1.250e-04
0: TRAIN [2][3650/7762]	Time 0.270 (0.326)	Data 1.16e-04 (1.91e-04)	Tok/s 38229 (43044)	Loss/tok 3.0525 (3.2275)	LR 1.250e-04
0: TRAIN [2][3660/7762]	Time 0.263 (0.326)	Data 1.01e-04 (1.90e-04)	Tok/s 40081 (43033)	Loss/tok 3.0382 (3.2272)	LR 1.250e-04
0: TRAIN [2][3670/7762]	Time 0.263 (0.326)	Data 1.01e-04 (1.90e-04)	Tok/s 39576 (43032)	Loss/tok 3.1285 (3.2271)	LR 1.250e-04
0: TRAIN [2][3680/7762]	Time 0.357 (0.326)	Data 1.03e-04 (1.90e-04)	Tok/s 46935 (43034)	Loss/tok 3.2291 (3.2271)	LR 1.250e-04
0: TRAIN [2][3690/7762]	Time 0.358 (0.326)	Data 1.14e-04 (1.90e-04)	Tok/s 46632 (43033)	Loss/tok 3.2608 (3.2269)	LR 1.250e-04
0: TRAIN [2][3700/7762]	Time 0.586 (0.326)	Data 1.11e-04 (1.90e-04)	Tok/s 51244 (43040)	Loss/tok 3.5499 (3.2270)	LR 1.250e-04
0: TRAIN [2][3710/7762]	Time 0.258 (0.326)	Data 1.08e-04 (1.89e-04)	Tok/s 39817 (43036)	Loss/tok 2.9850 (3.2269)	LR 1.250e-04
0: TRAIN [2][3720/7762]	Time 0.359 (0.326)	Data 1.02e-04 (1.89e-04)	Tok/s 46467 (43041)	Loss/tok 3.1736 (3.2269)	LR 1.250e-04
0: TRAIN [2][3730/7762]	Time 0.260 (0.326)	Data 1.09e-04 (1.89e-04)	Tok/s 39788 (43046)	Loss/tok 3.0013 (3.2272)	LR 1.250e-04
0: TRAIN [2][3740/7762]	Time 0.586 (0.326)	Data 1.04e-04 (1.89e-04)	Tok/s 50496 (43048)	Loss/tok 3.6574 (3.2275)	LR 1.250e-04
0: TRAIN [2][3750/7762]	Time 0.360 (0.326)	Data 1.08e-04 (1.88e-04)	Tok/s 46110 (43046)	Loss/tok 3.2018 (3.2272)	LR 1.250e-04
0: TRAIN [2][3760/7762]	Time 0.361 (0.326)	Data 1.02e-04 (1.88e-04)	Tok/s 46798 (43039)	Loss/tok 3.3686 (3.2270)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3770/7762]	Time 0.267 (0.326)	Data 9.89e-05 (1.88e-04)	Tok/s 38212 (43044)	Loss/tok 3.0825 (3.2274)	LR 1.250e-04
0: TRAIN [2][3780/7762]	Time 0.263 (0.326)	Data 1.19e-04 (1.88e-04)	Tok/s 39186 (43050)	Loss/tok 2.9814 (3.2277)	LR 1.250e-04
0: TRAIN [2][3790/7762]	Time 0.350 (0.326)	Data 1.04e-04 (1.88e-04)	Tok/s 47984 (43054)	Loss/tok 3.2040 (3.2276)	LR 1.250e-04
0: TRAIN [2][3800/7762]	Time 0.465 (0.326)	Data 1.09e-04 (1.87e-04)	Tok/s 49881 (43064)	Loss/tok 3.3549 (3.2279)	LR 1.250e-04
0: TRAIN [2][3810/7762]	Time 0.365 (0.326)	Data 9.94e-05 (1.87e-04)	Tok/s 46165 (43062)	Loss/tok 3.2376 (3.2278)	LR 1.250e-04
0: TRAIN [2][3820/7762]	Time 0.259 (0.326)	Data 1.06e-04 (1.87e-04)	Tok/s 40840 (43058)	Loss/tok 3.0077 (3.2278)	LR 1.250e-04
0: TRAIN [2][3830/7762]	Time 0.366 (0.326)	Data 1.07e-04 (1.87e-04)	Tok/s 46196 (43069)	Loss/tok 3.1172 (3.2281)	LR 1.250e-04
0: TRAIN [2][3840/7762]	Time 0.563 (0.326)	Data 1.05e-04 (1.87e-04)	Tok/s 52566 (43068)	Loss/tok 3.4612 (3.2282)	LR 1.250e-04
0: TRAIN [2][3850/7762]	Time 0.260 (0.326)	Data 1.04e-04 (1.86e-04)	Tok/s 40072 (43073)	Loss/tok 3.0279 (3.2284)	LR 1.250e-04
0: TRAIN [2][3860/7762]	Time 0.358 (0.326)	Data 1.03e-04 (1.86e-04)	Tok/s 46758 (43076)	Loss/tok 3.3774 (3.2284)	LR 1.250e-04
0: TRAIN [2][3870/7762]	Time 0.449 (0.326)	Data 1.05e-04 (1.86e-04)	Tok/s 51989 (43082)	Loss/tok 3.3470 (3.2283)	LR 1.250e-04
0: TRAIN [2][3880/7762]	Time 0.174 (0.326)	Data 1.02e-04 (1.86e-04)	Tok/s 29997 (43078)	Loss/tok 2.6429 (3.2285)	LR 1.250e-04
0: TRAIN [2][3890/7762]	Time 0.365 (0.326)	Data 1.02e-04 (1.86e-04)	Tok/s 45658 (43076)	Loss/tok 3.3222 (3.2283)	LR 1.250e-04
0: TRAIN [2][3900/7762]	Time 0.267 (0.326)	Data 1.01e-04 (1.85e-04)	Tok/s 38118 (43072)	Loss/tok 2.9596 (3.2281)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3910/7762]	Time 0.359 (0.326)	Data 1.04e-04 (1.85e-04)	Tok/s 46816 (43082)	Loss/tok 3.2548 (3.2283)	LR 1.250e-04
0: TRAIN [2][3920/7762]	Time 0.261 (0.326)	Data 1.03e-04 (1.85e-04)	Tok/s 40044 (43081)	Loss/tok 3.0288 (3.2281)	LR 1.250e-04
0: TRAIN [2][3930/7762]	Time 0.265 (0.326)	Data 1.03e-04 (1.85e-04)	Tok/s 39200 (43078)	Loss/tok 3.1798 (3.2282)	LR 1.250e-04
0: TRAIN [2][3940/7762]	Time 0.355 (0.326)	Data 1.04e-04 (1.84e-04)	Tok/s 46993 (43077)	Loss/tok 3.2401 (3.2280)	LR 1.250e-04
0: TRAIN [2][3950/7762]	Time 0.455 (0.326)	Data 1.14e-04 (1.84e-04)	Tok/s 51289 (43081)	Loss/tok 3.4351 (3.2281)	LR 1.250e-04
0: TRAIN [2][3960/7762]	Time 0.261 (0.326)	Data 1.03e-04 (1.84e-04)	Tok/s 39742 (43088)	Loss/tok 3.1070 (3.2282)	LR 1.250e-04
0: TRAIN [2][3970/7762]	Time 0.361 (0.326)	Data 1.09e-04 (1.84e-04)	Tok/s 46452 (43097)	Loss/tok 3.3274 (3.2284)	LR 1.250e-04
0: TRAIN [2][3980/7762]	Time 0.362 (0.326)	Data 9.92e-05 (1.84e-04)	Tok/s 46222 (43097)	Loss/tok 3.2644 (3.2285)	LR 1.250e-04
0: TRAIN [2][3990/7762]	Time 0.178 (0.326)	Data 1.04e-04 (1.84e-04)	Tok/s 30304 (43092)	Loss/tok 2.6112 (3.2284)	LR 1.250e-04
0: TRAIN [2][4000/7762]	Time 0.174 (0.326)	Data 1.04e-04 (1.83e-04)	Tok/s 30545 (43092)	Loss/tok 2.5900 (3.2282)	LR 1.250e-04
0: TRAIN [2][4010/7762]	Time 0.568 (0.326)	Data 9.99e-05 (1.83e-04)	Tok/s 52027 (43093)	Loss/tok 3.6120 (3.2282)	LR 1.250e-04
0: TRAIN [2][4020/7762]	Time 0.347 (0.326)	Data 1.05e-04 (1.83e-04)	Tok/s 48869 (43095)	Loss/tok 3.1312 (3.2280)	LR 1.250e-04
0: TRAIN [2][4030/7762]	Time 0.266 (0.326)	Data 1.03e-04 (1.83e-04)	Tok/s 38529 (43090)	Loss/tok 3.0285 (3.2278)	LR 1.250e-04
0: TRAIN [2][4040/7762]	Time 0.253 (0.326)	Data 1.08e-04 (1.83e-04)	Tok/s 40940 (43095)	Loss/tok 2.9620 (3.2279)	LR 1.250e-04
0: TRAIN [2][4050/7762]	Time 0.177 (0.326)	Data 1.07e-04 (1.82e-04)	Tok/s 29674 (43099)	Loss/tok 2.5849 (3.2280)	LR 1.250e-04
0: TRAIN [2][4060/7762]	Time 0.368 (0.326)	Data 1.06e-04 (1.82e-04)	Tok/s 46162 (43105)	Loss/tok 3.3277 (3.2282)	LR 1.250e-04
0: TRAIN [2][4070/7762]	Time 0.261 (0.326)	Data 1.05e-04 (1.82e-04)	Tok/s 39709 (43108)	Loss/tok 3.0672 (3.2281)	LR 1.250e-04
0: TRAIN [2][4080/7762]	Time 0.253 (0.327)	Data 1.17e-04 (1.82e-04)	Tok/s 40532 (43113)	Loss/tok 3.0618 (3.2281)	LR 1.250e-04
0: TRAIN [2][4090/7762]	Time 0.262 (0.327)	Data 1.00e-04 (1.82e-04)	Tok/s 40522 (43116)	Loss/tok 2.9820 (3.2282)	LR 1.250e-04
0: TRAIN [2][4100/7762]	Time 0.266 (0.327)	Data 1.03e-04 (1.81e-04)	Tok/s 38120 (43120)	Loss/tok 3.1366 (3.2282)	LR 1.250e-04
0: TRAIN [2][4110/7762]	Time 0.355 (0.326)	Data 1.00e-04 (1.81e-04)	Tok/s 47089 (43116)	Loss/tok 3.2189 (3.2280)	LR 1.250e-04
0: TRAIN [2][4120/7762]	Time 0.253 (0.327)	Data 1.08e-04 (1.81e-04)	Tok/s 41455 (43123)	Loss/tok 3.0681 (3.2283)	LR 1.250e-04
0: TRAIN [2][4130/7762]	Time 0.269 (0.327)	Data 1.01e-04 (1.81e-04)	Tok/s 38420 (43120)	Loss/tok 3.0641 (3.2281)	LR 1.250e-04
0: TRAIN [2][4140/7762]	Time 0.358 (0.326)	Data 1.02e-04 (1.81e-04)	Tok/s 45959 (43109)	Loss/tok 3.2201 (3.2278)	LR 1.250e-04
0: TRAIN [2][4150/7762]	Time 0.361 (0.326)	Data 1.03e-04 (1.81e-04)	Tok/s 46144 (43117)	Loss/tok 3.3165 (3.2282)	LR 1.250e-04
0: TRAIN [2][4160/7762]	Time 0.253 (0.326)	Data 1.03e-04 (1.80e-04)	Tok/s 40640 (43118)	Loss/tok 2.9599 (3.2281)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][4170/7762]	Time 0.260 (0.326)	Data 1.00e-04 (1.80e-04)	Tok/s 39136 (43113)	Loss/tok 3.0410 (3.2280)	LR 1.250e-04
0: TRAIN [2][4180/7762]	Time 0.262 (0.326)	Data 1.04e-04 (1.80e-04)	Tok/s 38940 (43110)	Loss/tok 2.9776 (3.2278)	LR 1.250e-04
0: TRAIN [2][4190/7762]	Time 0.361 (0.326)	Data 1.02e-04 (1.80e-04)	Tok/s 46300 (43107)	Loss/tok 3.1255 (3.2277)	LR 1.250e-04
0: TRAIN [2][4200/7762]	Time 0.586 (0.326)	Data 1.02e-04 (1.80e-04)	Tok/s 50712 (43108)	Loss/tok 3.4338 (3.2277)	LR 1.250e-04
0: TRAIN [2][4210/7762]	Time 0.361 (0.326)	Data 1.04e-04 (1.79e-04)	Tok/s 46656 (43112)	Loss/tok 3.2009 (3.2279)	LR 1.250e-04
0: TRAIN [2][4220/7762]	Time 0.259 (0.326)	Data 1.01e-04 (1.79e-04)	Tok/s 39421 (43110)	Loss/tok 2.9746 (3.2279)	LR 1.250e-04
0: TRAIN [2][4230/7762]	Time 0.262 (0.326)	Data 1.05e-04 (1.79e-04)	Tok/s 39056 (43106)	Loss/tok 3.0487 (3.2277)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][4240/7762]	Time 0.256 (0.326)	Data 1.12e-04 (1.79e-04)	Tok/s 40832 (43104)	Loss/tok 2.8544 (3.2276)	LR 1.250e-04
0: TRAIN [2][4250/7762]	Time 0.358 (0.326)	Data 1.08e-04 (1.79e-04)	Tok/s 47237 (43109)	Loss/tok 3.3088 (3.2280)	LR 1.250e-04
0: TRAIN [2][4260/7762]	Time 0.360 (0.326)	Data 1.03e-04 (1.79e-04)	Tok/s 46123 (43116)	Loss/tok 3.2411 (3.2280)	LR 1.250e-04
0: TRAIN [2][4270/7762]	Time 0.358 (0.326)	Data 1.03e-04 (1.78e-04)	Tok/s 46868 (43113)	Loss/tok 3.3073 (3.2279)	LR 1.250e-04
0: TRAIN [2][4280/7762]	Time 0.263 (0.326)	Data 1.07e-04 (1.78e-04)	Tok/s 39234 (43108)	Loss/tok 2.9539 (3.2277)	LR 1.250e-04
0: TRAIN [2][4290/7762]	Time 0.268 (0.326)	Data 1.04e-04 (1.78e-04)	Tok/s 38715 (43110)	Loss/tok 2.9246 (3.2276)	LR 1.250e-04
0: TRAIN [2][4300/7762]	Time 0.264 (0.326)	Data 1.05e-04 (1.78e-04)	Tok/s 39601 (43107)	Loss/tok 3.1107 (3.2276)	LR 1.250e-04
0: TRAIN [2][4310/7762]	Time 0.262 (0.326)	Data 1.09e-04 (1.78e-04)	Tok/s 38262 (43108)	Loss/tok 3.0706 (3.2276)	LR 1.250e-04
0: TRAIN [2][4320/7762]	Time 0.464 (0.326)	Data 9.99e-05 (1.78e-04)	Tok/s 49873 (43114)	Loss/tok 3.5110 (3.2277)	LR 1.250e-04
0: TRAIN [2][4330/7762]	Time 0.258 (0.326)	Data 9.61e-05 (1.77e-04)	Tok/s 39767 (43113)	Loss/tok 3.0598 (3.2276)	LR 1.250e-04
0: TRAIN [2][4340/7762]	Time 0.260 (0.326)	Data 1.04e-04 (1.77e-04)	Tok/s 39514 (43111)	Loss/tok 3.0513 (3.2275)	LR 1.250e-04
0: TRAIN [2][4350/7762]	Time 0.177 (0.326)	Data 1.02e-04 (1.77e-04)	Tok/s 30156 (43106)	Loss/tok 2.6257 (3.2274)	LR 1.250e-04
0: TRAIN [2][4360/7762]	Time 0.177 (0.326)	Data 1.06e-04 (1.77e-04)	Tok/s 29575 (43104)	Loss/tok 2.6361 (3.2273)	LR 1.250e-04
0: TRAIN [2][4370/7762]	Time 0.261 (0.326)	Data 1.06e-04 (1.77e-04)	Tok/s 38457 (43105)	Loss/tok 3.0596 (3.2272)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][4380/7762]	Time 0.465 (0.326)	Data 1.03e-04 (1.77e-04)	Tok/s 50078 (43106)	Loss/tok 3.4676 (3.2273)	LR 1.250e-04
0: TRAIN [2][4390/7762]	Time 0.262 (0.326)	Data 1.21e-04 (1.76e-04)	Tok/s 39666 (43102)	Loss/tok 2.9434 (3.2271)	LR 1.250e-04
0: TRAIN [2][4400/7762]	Time 0.361 (0.326)	Data 1.05e-04 (1.76e-04)	Tok/s 46849 (43097)	Loss/tok 3.1745 (3.2269)	LR 1.250e-04
0: TRAIN [2][4410/7762]	Time 0.255 (0.326)	Data 1.03e-04 (1.76e-04)	Tok/s 41881 (43102)	Loss/tok 3.1415 (3.2271)	LR 1.250e-04
0: TRAIN [2][4420/7762]	Time 0.264 (0.326)	Data 1.57e-04 (1.76e-04)	Tok/s 39483 (43100)	Loss/tok 3.0033 (3.2273)	LR 1.250e-04
0: TRAIN [2][4430/7762]	Time 0.264 (0.326)	Data 9.85e-05 (1.76e-04)	Tok/s 38875 (43094)	Loss/tok 2.9933 (3.2273)	LR 1.250e-04
0: TRAIN [2][4440/7762]	Time 0.253 (0.326)	Data 9.78e-05 (1.76e-04)	Tok/s 40860 (43087)	Loss/tok 3.1477 (3.2271)	LR 1.250e-04
0: TRAIN [2][4450/7762]	Time 0.255 (0.326)	Data 1.04e-04 (1.75e-04)	Tok/s 40354 (43086)	Loss/tok 2.9955 (3.2270)	LR 1.250e-04
0: TRAIN [2][4460/7762]	Time 0.352 (0.326)	Data 1.07e-04 (1.75e-04)	Tok/s 46950 (43094)	Loss/tok 3.2972 (3.2272)	LR 1.250e-04
0: TRAIN [2][4470/7762]	Time 0.464 (0.326)	Data 1.05e-04 (1.75e-04)	Tok/s 49770 (43097)	Loss/tok 3.5405 (3.2277)	LR 1.250e-04
0: TRAIN [2][4480/7762]	Time 0.351 (0.326)	Data 1.05e-04 (1.75e-04)	Tok/s 47335 (43103)	Loss/tok 3.2469 (3.2282)	LR 1.250e-04
0: TRAIN [2][4490/7762]	Time 0.588 (0.326)	Data 1.06e-04 (1.75e-04)	Tok/s 50286 (43104)	Loss/tok 3.5013 (3.2284)	LR 1.250e-04
0: TRAIN [2][4500/7762]	Time 0.266 (0.326)	Data 1.03e-04 (1.75e-04)	Tok/s 38448 (43102)	Loss/tok 3.0834 (3.2284)	LR 1.250e-04
0: TRAIN [2][4510/7762]	Time 0.266 (0.326)	Data 1.06e-04 (1.74e-04)	Tok/s 38289 (43096)	Loss/tok 2.9656 (3.2282)	LR 1.250e-04
0: TRAIN [2][4520/7762]	Time 0.366 (0.326)	Data 1.18e-04 (1.74e-04)	Tok/s 46618 (43095)	Loss/tok 3.2815 (3.2281)	LR 1.250e-04
0: TRAIN [2][4530/7762]	Time 0.365 (0.326)	Data 1.05e-04 (1.74e-04)	Tok/s 46357 (43094)	Loss/tok 3.1319 (3.2281)	LR 1.250e-04
0: TRAIN [2][4540/7762]	Time 0.261 (0.326)	Data 1.14e-04 (1.74e-04)	Tok/s 39283 (43087)	Loss/tok 3.0905 (3.2280)	LR 1.250e-04
0: TRAIN [2][4550/7762]	Time 0.261 (0.326)	Data 1.07e-04 (1.74e-04)	Tok/s 39889 (43089)	Loss/tok 2.9902 (3.2282)	LR 1.250e-04
0: TRAIN [2][4560/7762]	Time 0.264 (0.326)	Data 1.21e-04 (1.74e-04)	Tok/s 39391 (43093)	Loss/tok 3.0290 (3.2284)	LR 1.250e-04
0: TRAIN [2][4570/7762]	Time 0.268 (0.326)	Data 1.08e-04 (1.74e-04)	Tok/s 39066 (43096)	Loss/tok 3.2147 (3.2285)	LR 1.250e-04
0: TRAIN [2][4580/7762]	Time 0.173 (0.326)	Data 9.97e-05 (1.73e-04)	Tok/s 30938 (43083)	Loss/tok 2.4977 (3.2282)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][4590/7762]	Time 0.259 (0.326)	Data 1.03e-04 (1.73e-04)	Tok/s 39781 (43090)	Loss/tok 3.0619 (3.2286)	LR 1.250e-04
0: TRAIN [2][4600/7762]	Time 0.177 (0.326)	Data 1.05e-04 (1.73e-04)	Tok/s 30208 (43086)	Loss/tok 2.7047 (3.2285)	LR 1.250e-04
0: TRAIN [2][4610/7762]	Time 0.177 (0.326)	Data 1.04e-04 (1.73e-04)	Tok/s 29989 (43091)	Loss/tok 2.6395 (3.2287)	LR 1.250e-04
0: TRAIN [2][4620/7762]	Time 0.362 (0.326)	Data 1.04e-04 (1.73e-04)	Tok/s 46273 (43083)	Loss/tok 3.3127 (3.2286)	LR 1.250e-04
0: TRAIN [2][4630/7762]	Time 0.259 (0.326)	Data 1.05e-04 (1.73e-04)	Tok/s 39858 (43088)	Loss/tok 2.9324 (3.2288)	LR 1.250e-04
0: TRAIN [2][4640/7762]	Time 0.258 (0.326)	Data 1.02e-04 (1.73e-04)	Tok/s 39615 (43082)	Loss/tok 2.8916 (3.2286)	LR 1.250e-04
0: TRAIN [2][4650/7762]	Time 0.265 (0.326)	Data 1.05e-04 (1.72e-04)	Tok/s 38052 (43083)	Loss/tok 3.1622 (3.2287)	LR 1.250e-04
0: TRAIN [2][4660/7762]	Time 0.252 (0.326)	Data 1.04e-04 (1.72e-04)	Tok/s 40443 (43082)	Loss/tok 3.0864 (3.2286)	LR 1.250e-04
0: TRAIN [2][4670/7762]	Time 0.255 (0.326)	Data 1.02e-04 (1.72e-04)	Tok/s 40615 (43084)	Loss/tok 3.1410 (3.2286)	LR 1.250e-04
0: TRAIN [2][4680/7762]	Time 0.256 (0.326)	Data 1.08e-04 (1.72e-04)	Tok/s 40313 (43076)	Loss/tok 2.9796 (3.2285)	LR 1.250e-04
0: TRAIN [2][4690/7762]	Time 0.174 (0.326)	Data 1.08e-04 (1.72e-04)	Tok/s 30882 (43078)	Loss/tok 2.5567 (3.2285)	LR 1.250e-04
0: TRAIN [2][4700/7762]	Time 0.342 (0.326)	Data 1.17e-04 (1.72e-04)	Tok/s 49323 (43076)	Loss/tok 3.2460 (3.2284)	LR 1.250e-04
0: TRAIN [2][4710/7762]	Time 0.357 (0.326)	Data 1.08e-04 (1.72e-04)	Tok/s 46909 (43087)	Loss/tok 3.3232 (3.2289)	LR 1.250e-04
0: TRAIN [2][4720/7762]	Time 0.357 (0.326)	Data 9.97e-05 (1.71e-04)	Tok/s 46404 (43086)	Loss/tok 3.3550 (3.2288)	LR 1.250e-04
0: TRAIN [2][4730/7762]	Time 0.589 (0.326)	Data 1.03e-04 (1.71e-04)	Tok/s 50457 (43093)	Loss/tok 3.5725 (3.2290)	LR 1.250e-04
0: TRAIN [2][4740/7762]	Time 0.362 (0.327)	Data 1.05e-04 (1.71e-04)	Tok/s 46673 (43097)	Loss/tok 3.2757 (3.2291)	LR 1.250e-04
0: TRAIN [2][4750/7762]	Time 0.168 (0.327)	Data 1.07e-04 (1.71e-04)	Tok/s 30697 (43098)	Loss/tok 2.5359 (3.2291)	LR 1.250e-04
0: TRAIN [2][4760/7762]	Time 0.366 (0.327)	Data 1.06e-04 (1.71e-04)	Tok/s 46124 (43097)	Loss/tok 3.1629 (3.2290)	LR 1.250e-04
0: TRAIN [2][4770/7762]	Time 0.357 (0.327)	Data 1.07e-04 (1.71e-04)	Tok/s 46551 (43101)	Loss/tok 3.2606 (3.2291)	LR 1.250e-04
0: TRAIN [2][4780/7762]	Time 0.177 (0.327)	Data 1.01e-04 (1.71e-04)	Tok/s 29261 (43097)	Loss/tok 2.5555 (3.2291)	LR 1.250e-04
0: TRAIN [2][4790/7762]	Time 0.357 (0.327)	Data 1.03e-04 (1.71e-04)	Tok/s 47088 (43093)	Loss/tok 3.0999 (3.2290)	LR 1.250e-04
0: TRAIN [2][4800/7762]	Time 0.263 (0.326)	Data 1.03e-04 (1.70e-04)	Tok/s 38296 (43092)	Loss/tok 3.0451 (3.2288)	LR 1.250e-04
0: TRAIN [2][4810/7762]	Time 0.453 (0.326)	Data 1.02e-04 (1.70e-04)	Tok/s 50999 (43089)	Loss/tok 3.5445 (3.2288)	LR 1.250e-04
0: TRAIN [2][4820/7762]	Time 0.258 (0.326)	Data 1.04e-04 (1.70e-04)	Tok/s 39981 (43086)	Loss/tok 2.8850 (3.2288)	LR 1.250e-04
0: TRAIN [2][4830/7762]	Time 0.267 (0.326)	Data 1.03e-04 (1.70e-04)	Tok/s 38496 (43079)	Loss/tok 2.9432 (3.2286)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][4840/7762]	Time 0.581 (0.326)	Data 1.20e-04 (1.70e-04)	Tok/s 51311 (43084)	Loss/tok 3.6740 (3.2287)	LR 1.250e-04
0: TRAIN [2][4850/7762]	Time 0.345 (0.326)	Data 1.04e-04 (1.70e-04)	Tok/s 48803 (43084)	Loss/tok 3.2695 (3.2285)	LR 1.250e-04
0: TRAIN [2][4860/7762]	Time 0.268 (0.326)	Data 9.94e-05 (1.70e-04)	Tok/s 37800 (43083)	Loss/tok 3.1589 (3.2284)	LR 1.250e-04
0: TRAIN [2][4870/7762]	Time 0.353 (0.326)	Data 1.00e-04 (1.69e-04)	Tok/s 47076 (43079)	Loss/tok 3.2016 (3.2283)	LR 1.250e-04
0: TRAIN [2][4880/7762]	Time 0.177 (0.326)	Data 1.03e-04 (1.69e-04)	Tok/s 29241 (43077)	Loss/tok 2.6488 (3.2281)	LR 1.250e-04
0: TRAIN [2][4890/7762]	Time 0.343 (0.326)	Data 2.44e-04 (1.69e-04)	Tok/s 49480 (43085)	Loss/tok 3.1763 (3.2283)	LR 1.250e-04
0: TRAIN [2][4900/7762]	Time 0.175 (0.326)	Data 1.02e-04 (1.69e-04)	Tok/s 29938 (43083)	Loss/tok 2.7047 (3.2281)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][4910/7762]	Time 0.352 (0.326)	Data 1.09e-04 (1.69e-04)	Tok/s 48165 (43086)	Loss/tok 3.2749 (3.2281)	LR 1.250e-04
0: TRAIN [2][4920/7762]	Time 0.361 (0.326)	Data 1.07e-04 (1.69e-04)	Tok/s 46016 (43084)	Loss/tok 3.2808 (3.2279)	LR 1.250e-04
0: TRAIN [2][4930/7762]	Time 0.572 (0.326)	Data 1.03e-04 (1.69e-04)	Tok/s 51951 (43083)	Loss/tok 3.5699 (3.2280)	LR 1.250e-04
0: TRAIN [2][4940/7762]	Time 0.263 (0.326)	Data 1.08e-04 (1.69e-04)	Tok/s 39659 (43091)	Loss/tok 3.1676 (3.2281)	LR 1.250e-04
0: TRAIN [2][4950/7762]	Time 0.360 (0.326)	Data 1.04e-04 (1.68e-04)	Tok/s 46636 (43092)	Loss/tok 3.1863 (3.2282)	LR 1.250e-04
0: TRAIN [2][4960/7762]	Time 0.175 (0.326)	Data 1.09e-04 (1.68e-04)	Tok/s 29666 (43083)	Loss/tok 2.4975 (3.2282)	LR 1.250e-04
0: TRAIN [2][4970/7762]	Time 0.265 (0.326)	Data 1.04e-04 (1.68e-04)	Tok/s 39024 (43081)	Loss/tok 2.9951 (3.2280)	LR 1.250e-04
0: TRAIN [2][4980/7762]	Time 0.364 (0.326)	Data 1.02e-04 (1.68e-04)	Tok/s 46726 (43079)	Loss/tok 3.1816 (3.2278)	LR 1.250e-04
0: TRAIN [2][4990/7762]	Time 0.581 (0.326)	Data 1.02e-04 (1.68e-04)	Tok/s 51746 (43079)	Loss/tok 3.5698 (3.2277)	LR 1.250e-04
0: TRAIN [2][5000/7762]	Time 0.268 (0.326)	Data 1.07e-04 (1.68e-04)	Tok/s 37729 (43083)	Loss/tok 3.1490 (3.2277)	LR 1.250e-04
0: TRAIN [2][5010/7762]	Time 0.463 (0.326)	Data 1.03e-04 (1.68e-04)	Tok/s 50304 (43088)	Loss/tok 3.2671 (3.2279)	LR 1.250e-04
0: TRAIN [2][5020/7762]	Time 0.361 (0.326)	Data 2.38e-04 (1.68e-04)	Tok/s 46651 (43092)	Loss/tok 3.1838 (3.2281)	LR 1.250e-04
0: TRAIN [2][5030/7762]	Time 0.253 (0.326)	Data 1.08e-04 (1.67e-04)	Tok/s 40915 (43086)	Loss/tok 2.9403 (3.2279)	LR 1.250e-04
0: TRAIN [2][5040/7762]	Time 0.261 (0.326)	Data 1.04e-04 (1.67e-04)	Tok/s 39952 (43083)	Loss/tok 3.0515 (3.2277)	LR 1.250e-04
0: TRAIN [2][5050/7762]	Time 0.264 (0.326)	Data 1.03e-04 (1.67e-04)	Tok/s 38882 (43082)	Loss/tok 3.0753 (3.2275)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][5060/7762]	Time 0.265 (0.326)	Data 1.00e-04 (1.67e-04)	Tok/s 39173 (43082)	Loss/tok 3.0783 (3.2275)	LR 1.250e-04
0: TRAIN [2][5070/7762]	Time 0.450 (0.326)	Data 1.05e-04 (1.67e-04)	Tok/s 51464 (43091)	Loss/tok 3.3678 (3.2280)	LR 1.250e-04
0: TRAIN [2][5080/7762]	Time 0.580 (0.326)	Data 1.01e-04 (1.67e-04)	Tok/s 52067 (43091)	Loss/tok 3.5977 (3.2280)	LR 1.250e-04
0: TRAIN [2][5090/7762]	Time 0.263 (0.326)	Data 1.06e-04 (1.67e-04)	Tok/s 40187 (43097)	Loss/tok 2.9455 (3.2282)	LR 1.250e-04
0: TRAIN [2][5100/7762]	Time 0.561 (0.326)	Data 1.06e-04 (1.67e-04)	Tok/s 54063 (43097)	Loss/tok 3.5186 (3.2282)	LR 1.250e-04
0: TRAIN [2][5110/7762]	Time 0.364 (0.326)	Data 1.18e-04 (1.66e-04)	Tok/s 46619 (43094)	Loss/tok 3.2909 (3.2282)	LR 1.250e-04
0: TRAIN [2][5120/7762]	Time 0.265 (0.326)	Data 1.05e-04 (1.66e-04)	Tok/s 38844 (43084)	Loss/tok 2.9760 (3.2280)	LR 1.250e-04
0: TRAIN [2][5130/7762]	Time 0.556 (0.326)	Data 1.07e-04 (1.66e-04)	Tok/s 53512 (43086)	Loss/tok 3.5893 (3.2281)	LR 1.250e-04
0: TRAIN [2][5140/7762]	Time 0.583 (0.326)	Data 1.04e-04 (1.66e-04)	Tok/s 51464 (43091)	Loss/tok 3.6098 (3.2286)	LR 1.250e-04
0: TRAIN [2][5150/7762]	Time 0.260 (0.326)	Data 1.04e-04 (1.66e-04)	Tok/s 40684 (43090)	Loss/tok 2.9998 (3.2286)	LR 1.250e-04
0: TRAIN [2][5160/7762]	Time 0.258 (0.326)	Data 1.08e-04 (1.66e-04)	Tok/s 39743 (43090)	Loss/tok 3.0494 (3.2285)	LR 1.250e-04
0: TRAIN [2][5170/7762]	Time 0.170 (0.326)	Data 1.12e-04 (1.66e-04)	Tok/s 30141 (43085)	Loss/tok 2.6415 (3.2284)	LR 1.250e-04
0: TRAIN [2][5180/7762]	Time 0.463 (0.326)	Data 1.09e-04 (1.66e-04)	Tok/s 49940 (43087)	Loss/tok 3.4169 (3.2283)	LR 1.250e-04
0: TRAIN [2][5190/7762]	Time 0.462 (0.326)	Data 1.06e-04 (1.66e-04)	Tok/s 50533 (43089)	Loss/tok 3.3564 (3.2284)	LR 1.250e-04
0: TRAIN [2][5200/7762]	Time 0.268 (0.326)	Data 1.05e-04 (1.65e-04)	Tok/s 38000 (43082)	Loss/tok 2.9765 (3.2282)	LR 1.250e-04
0: TRAIN [2][5210/7762]	Time 0.359 (0.326)	Data 1.08e-04 (1.65e-04)	Tok/s 46514 (43081)	Loss/tok 3.1019 (3.2281)	LR 1.250e-04
0: TRAIN [2][5220/7762]	Time 0.584 (0.326)	Data 1.04e-04 (1.65e-04)	Tok/s 51208 (43087)	Loss/tok 3.6373 (3.2284)	LR 1.250e-04
0: TRAIN [2][5230/7762]	Time 0.360 (0.326)	Data 1.05e-04 (1.65e-04)	Tok/s 47051 (43081)	Loss/tok 3.0816 (3.2282)	LR 1.250e-04
0: TRAIN [2][5240/7762]	Time 0.177 (0.326)	Data 1.03e-04 (1.65e-04)	Tok/s 29867 (43081)	Loss/tok 2.4927 (3.2283)	LR 1.250e-04
0: TRAIN [2][5250/7762]	Time 0.178 (0.326)	Data 1.06e-04 (1.65e-04)	Tok/s 30251 (43076)	Loss/tok 2.6404 (3.2281)	LR 1.250e-04
0: TRAIN [2][5260/7762]	Time 0.264 (0.326)	Data 1.03e-04 (1.65e-04)	Tok/s 40195 (43073)	Loss/tok 3.0459 (3.2279)	LR 1.250e-04
0: TRAIN [2][5270/7762]	Time 0.341 (0.326)	Data 1.17e-04 (1.65e-04)	Tok/s 49020 (43079)	Loss/tok 3.2525 (3.2280)	LR 1.250e-04
0: TRAIN [2][5280/7762]	Time 0.266 (0.326)	Data 1.04e-04 (1.65e-04)	Tok/s 38511 (43081)	Loss/tok 2.9867 (3.2281)	LR 1.250e-04
0: TRAIN [2][5290/7762]	Time 0.463 (0.326)	Data 1.03e-04 (1.64e-04)	Tok/s 50779 (43082)	Loss/tok 3.4026 (3.2281)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][5300/7762]	Time 0.584 (0.326)	Data 1.05e-04 (1.64e-04)	Tok/s 51519 (43087)	Loss/tok 3.5651 (3.2284)	LR 1.250e-04
0: TRAIN [2][5310/7762]	Time 0.260 (0.326)	Data 1.22e-04 (1.64e-04)	Tok/s 40035 (43085)	Loss/tok 3.0125 (3.2283)	LR 1.250e-04
0: TRAIN [2][5320/7762]	Time 0.348 (0.326)	Data 1.04e-04 (1.64e-04)	Tok/s 48262 (43091)	Loss/tok 3.2178 (3.2284)	LR 1.250e-04
0: TRAIN [2][5330/7762]	Time 0.454 (0.327)	Data 1.05e-04 (1.64e-04)	Tok/s 52147 (43094)	Loss/tok 3.4382 (3.2285)	LR 1.250e-04
0: TRAIN [2][5340/7762]	Time 0.262 (0.326)	Data 1.00e-04 (1.64e-04)	Tok/s 39668 (43088)	Loss/tok 3.1421 (3.2283)	LR 1.250e-04
0: TRAIN [2][5350/7762]	Time 0.367 (0.326)	Data 1.12e-04 (1.64e-04)	Tok/s 45803 (43085)	Loss/tok 3.2869 (3.2282)	LR 1.250e-04
0: TRAIN [2][5360/7762]	Time 0.462 (0.326)	Data 1.02e-04 (1.64e-04)	Tok/s 50474 (43091)	Loss/tok 3.3784 (3.2283)	LR 1.250e-04
0: TRAIN [2][5370/7762]	Time 0.444 (0.326)	Data 1.08e-04 (1.64e-04)	Tok/s 52643 (43094)	Loss/tok 3.4537 (3.2284)	LR 1.250e-04
0: TRAIN [2][5380/7762]	Time 0.463 (0.327)	Data 1.22e-04 (1.63e-04)	Tok/s 50037 (43099)	Loss/tok 3.3989 (3.2284)	LR 1.250e-04
0: TRAIN [2][5390/7762]	Time 0.352 (0.327)	Data 1.09e-04 (1.63e-04)	Tok/s 47893 (43097)	Loss/tok 3.4027 (3.2284)	LR 1.250e-04
0: TRAIN [2][5400/7762]	Time 0.365 (0.326)	Data 1.03e-04 (1.63e-04)	Tok/s 46298 (43098)	Loss/tok 3.1809 (3.2282)	LR 1.250e-04
0: TRAIN [2][5410/7762]	Time 0.365 (0.326)	Data 1.03e-04 (1.63e-04)	Tok/s 46045 (43094)	Loss/tok 3.1588 (3.2281)	LR 1.250e-04
0: TRAIN [2][5420/7762]	Time 0.452 (0.326)	Data 1.30e-04 (1.63e-04)	Tok/s 51512 (43098)	Loss/tok 3.3072 (3.2283)	LR 1.250e-04
0: TRAIN [2][5430/7762]	Time 0.365 (0.326)	Data 1.05e-04 (1.63e-04)	Tok/s 45991 (43096)	Loss/tok 3.2738 (3.2283)	LR 1.250e-04
0: TRAIN [2][5440/7762]	Time 0.176 (0.327)	Data 1.10e-04 (1.63e-04)	Tok/s 29809 (43099)	Loss/tok 2.6310 (3.2284)	LR 1.250e-04
0: TRAIN [2][5450/7762]	Time 0.355 (0.327)	Data 1.07e-04 (1.63e-04)	Tok/s 47998 (43103)	Loss/tok 3.2319 (3.2287)	LR 1.250e-04
0: TRAIN [2][5460/7762]	Time 0.260 (0.327)	Data 1.18e-04 (1.63e-04)	Tok/s 40519 (43106)	Loss/tok 3.0018 (3.2287)	LR 1.250e-04
0: TRAIN [2][5470/7762]	Time 0.178 (0.327)	Data 1.36e-04 (1.63e-04)	Tok/s 29604 (43104)	Loss/tok 2.6631 (3.2286)	LR 1.250e-04
0: TRAIN [2][5480/7762]	Time 0.436 (0.327)	Data 1.07e-04 (1.62e-04)	Tok/s 53756 (43107)	Loss/tok 3.3954 (3.2287)	LR 1.250e-04
0: TRAIN [2][5490/7762]	Time 0.358 (0.327)	Data 1.04e-04 (1.62e-04)	Tok/s 46370 (43104)	Loss/tok 3.2298 (3.2288)	LR 1.250e-04
0: TRAIN [2][5500/7762]	Time 0.268 (0.327)	Data 1.03e-04 (1.62e-04)	Tok/s 37976 (43101)	Loss/tok 3.0571 (3.2286)	LR 1.250e-04
0: TRAIN [2][5510/7762]	Time 0.451 (0.327)	Data 1.10e-04 (1.62e-04)	Tok/s 51662 (43100)	Loss/tok 3.3421 (3.2285)	LR 1.250e-04
0: TRAIN [2][5520/7762]	Time 0.586 (0.327)	Data 1.04e-04 (1.62e-04)	Tok/s 51330 (43103)	Loss/tok 3.5010 (3.2286)	LR 1.250e-04
0: TRAIN [2][5530/7762]	Time 0.360 (0.327)	Data 1.19e-04 (1.62e-04)	Tok/s 46960 (43104)	Loss/tok 3.3088 (3.2285)	LR 1.250e-04
0: TRAIN [2][5540/7762]	Time 0.582 (0.327)	Data 1.07e-04 (1.62e-04)	Tok/s 50711 (43112)	Loss/tok 3.7216 (3.2289)	LR 1.250e-04
0: TRAIN [2][5550/7762]	Time 0.265 (0.327)	Data 1.07e-04 (1.62e-04)	Tok/s 38236 (43113)	Loss/tok 2.9690 (3.2289)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][5560/7762]	Time 0.365 (0.327)	Data 1.04e-04 (1.62e-04)	Tok/s 45270 (43115)	Loss/tok 3.2051 (3.2288)	LR 1.250e-04
0: TRAIN [2][5570/7762]	Time 0.260 (0.327)	Data 9.99e-05 (1.62e-04)	Tok/s 40139 (43110)	Loss/tok 3.0146 (3.2286)	LR 1.250e-04
0: TRAIN [2][5580/7762]	Time 0.267 (0.327)	Data 1.05e-04 (1.61e-04)	Tok/s 38653 (43114)	Loss/tok 3.0700 (3.2285)	LR 1.250e-04
0: TRAIN [2][5590/7762]	Time 0.265 (0.327)	Data 1.09e-04 (1.61e-04)	Tok/s 38970 (43110)	Loss/tok 3.0343 (3.2284)	LR 1.250e-04
0: TRAIN [2][5600/7762]	Time 0.265 (0.327)	Data 1.04e-04 (1.61e-04)	Tok/s 39640 (43111)	Loss/tok 3.0391 (3.2283)	LR 1.250e-04
0: TRAIN [2][5610/7762]	Time 0.365 (0.327)	Data 1.02e-04 (1.61e-04)	Tok/s 45468 (43109)	Loss/tok 3.3332 (3.2282)	LR 1.250e-04
0: TRAIN [2][5620/7762]	Time 0.345 (0.327)	Data 1.02e-04 (1.61e-04)	Tok/s 48218 (43112)	Loss/tok 3.3907 (3.2282)	LR 1.250e-04
0: TRAIN [2][5630/7762]	Time 0.269 (0.326)	Data 1.01e-04 (1.61e-04)	Tok/s 38859 (43106)	Loss/tok 2.8921 (3.2280)	LR 1.250e-04
0: TRAIN [2][5640/7762]	Time 0.250 (0.326)	Data 1.04e-04 (1.61e-04)	Tok/s 41970 (43096)	Loss/tok 3.0948 (3.2279)	LR 1.250e-04
0: TRAIN [2][5650/7762]	Time 0.461 (0.326)	Data 1.10e-04 (1.61e-04)	Tok/s 50844 (43100)	Loss/tok 3.3998 (3.2281)	LR 1.250e-04
0: TRAIN [2][5660/7762]	Time 0.463 (0.327)	Data 1.05e-04 (1.61e-04)	Tok/s 49916 (43102)	Loss/tok 3.4321 (3.2281)	LR 1.250e-04
0: TRAIN [2][5670/7762]	Time 0.364 (0.327)	Data 1.20e-04 (1.61e-04)	Tok/s 46345 (43103)	Loss/tok 3.1374 (3.2282)	LR 1.250e-04
0: TRAIN [2][5680/7762]	Time 0.256 (0.326)	Data 1.01e-04 (1.60e-04)	Tok/s 41755 (43099)	Loss/tok 2.8732 (3.2280)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][5690/7762]	Time 0.266 (0.327)	Data 1.20e-04 (1.60e-04)	Tok/s 38361 (43104)	Loss/tok 3.1103 (3.2281)	LR 1.250e-04
0: TRAIN [2][5700/7762]	Time 0.464 (0.327)	Data 1.06e-04 (1.60e-04)	Tok/s 49653 (43106)	Loss/tok 3.4251 (3.2286)	LR 1.250e-04
0: TRAIN [2][5710/7762]	Time 0.174 (0.327)	Data 1.03e-04 (1.60e-04)	Tok/s 30935 (43104)	Loss/tok 2.6656 (3.2286)	LR 1.250e-04
0: TRAIN [2][5720/7762]	Time 0.267 (0.327)	Data 1.04e-04 (1.60e-04)	Tok/s 38951 (43099)	Loss/tok 2.9807 (3.2284)	LR 1.250e-04
0: TRAIN [2][5730/7762]	Time 0.264 (0.327)	Data 1.03e-04 (1.60e-04)	Tok/s 38023 (43101)	Loss/tok 2.9602 (3.2285)	LR 1.250e-04
0: TRAIN [2][5740/7762]	Time 0.360 (0.327)	Data 1.07e-04 (1.60e-04)	Tok/s 46612 (43104)	Loss/tok 3.2701 (3.2285)	LR 1.250e-04
0: TRAIN [2][5750/7762]	Time 0.260 (0.327)	Data 1.03e-04 (1.60e-04)	Tok/s 39632 (43103)	Loss/tok 2.9386 (3.2285)	LR 1.250e-04
0: TRAIN [2][5760/7762]	Time 0.254 (0.326)	Data 9.78e-05 (1.60e-04)	Tok/s 40437 (43094)	Loss/tok 3.0393 (3.2282)	LR 1.250e-04
0: TRAIN [2][5770/7762]	Time 0.265 (0.327)	Data 1.05e-04 (1.60e-04)	Tok/s 38573 (43098)	Loss/tok 3.0998 (3.2284)	LR 1.250e-04
0: TRAIN [2][5780/7762]	Time 0.180 (0.326)	Data 1.01e-04 (1.60e-04)	Tok/s 29599 (43093)	Loss/tok 2.5396 (3.2285)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][5790/7762]	Time 0.265 (0.327)	Data 1.06e-04 (1.59e-04)	Tok/s 38812 (43098)	Loss/tok 3.0207 (3.2287)	LR 1.250e-04
0: TRAIN [2][5800/7762]	Time 0.267 (0.327)	Data 1.21e-04 (1.59e-04)	Tok/s 38466 (43098)	Loss/tok 3.0917 (3.2287)	LR 1.250e-04
0: TRAIN [2][5810/7762]	Time 0.265 (0.327)	Data 1.01e-04 (1.59e-04)	Tok/s 39069 (43100)	Loss/tok 3.0354 (3.2288)	LR 1.250e-04
0: TRAIN [2][5820/7762]	Time 0.460 (0.327)	Data 1.09e-04 (1.59e-04)	Tok/s 50726 (43105)	Loss/tok 3.3483 (3.2290)	LR 1.250e-04
0: TRAIN [2][5830/7762]	Time 0.353 (0.327)	Data 1.05e-04 (1.59e-04)	Tok/s 47618 (43110)	Loss/tok 3.2889 (3.2291)	LR 1.250e-04
0: TRAIN [2][5840/7762]	Time 0.173 (0.327)	Data 1.01e-04 (1.59e-04)	Tok/s 31179 (43109)	Loss/tok 2.6447 (3.2291)	LR 1.250e-04
0: TRAIN [2][5850/7762]	Time 0.466 (0.327)	Data 1.06e-04 (1.59e-04)	Tok/s 50039 (43111)	Loss/tok 3.4370 (3.2290)	LR 1.250e-04
0: TRAIN [2][5860/7762]	Time 0.452 (0.327)	Data 1.08e-04 (1.59e-04)	Tok/s 51910 (43116)	Loss/tok 3.3515 (3.2292)	LR 1.250e-04
0: TRAIN [2][5870/7762]	Time 0.262 (0.327)	Data 1.03e-04 (1.59e-04)	Tok/s 39743 (43114)	Loss/tok 2.9877 (3.2292)	LR 1.250e-04
0: TRAIN [2][5880/7762]	Time 0.265 (0.327)	Data 1.10e-04 (1.59e-04)	Tok/s 39089 (43117)	Loss/tok 2.9582 (3.2293)	LR 1.250e-04
0: TRAIN [2][5890/7762]	Time 0.354 (0.327)	Data 1.00e-04 (1.59e-04)	Tok/s 46904 (43118)	Loss/tok 3.2647 (3.2297)	LR 1.250e-04
0: TRAIN [2][5900/7762]	Time 0.263 (0.327)	Data 1.04e-04 (1.58e-04)	Tok/s 39777 (43117)	Loss/tok 2.9458 (3.2297)	LR 1.250e-04
0: TRAIN [2][5910/7762]	Time 0.269 (0.327)	Data 1.18e-04 (1.58e-04)	Tok/s 38468 (43116)	Loss/tok 2.9929 (3.2296)	LR 1.250e-04
0: TRAIN [2][5920/7762]	Time 0.452 (0.327)	Data 1.05e-04 (1.58e-04)	Tok/s 51329 (43120)	Loss/tok 3.6225 (3.2298)	LR 1.250e-04
0: TRAIN [2][5930/7762]	Time 0.364 (0.327)	Data 1.04e-04 (1.58e-04)	Tok/s 45382 (43124)	Loss/tok 3.2507 (3.2300)	LR 1.250e-04
0: TRAIN [2][5940/7762]	Time 0.588 (0.327)	Data 1.00e-04 (1.58e-04)	Tok/s 50600 (43129)	Loss/tok 3.6308 (3.2301)	LR 1.250e-04
0: TRAIN [2][5950/7762]	Time 0.259 (0.327)	Data 1.05e-04 (1.58e-04)	Tok/s 40173 (43128)	Loss/tok 3.0645 (3.2301)	LR 1.250e-04
0: TRAIN [2][5960/7762]	Time 0.267 (0.327)	Data 1.02e-04 (1.58e-04)	Tok/s 38614 (43126)	Loss/tok 3.0655 (3.2299)	LR 1.250e-04
0: TRAIN [2][5970/7762]	Time 0.359 (0.327)	Data 1.07e-04 (1.58e-04)	Tok/s 46281 (43127)	Loss/tok 3.2991 (3.2299)	LR 1.250e-04
0: TRAIN [2][5980/7762]	Time 0.351 (0.327)	Data 1.05e-04 (1.58e-04)	Tok/s 48217 (43124)	Loss/tok 3.2107 (3.2297)	LR 1.250e-04
0: TRAIN [2][5990/7762]	Time 0.258 (0.327)	Data 1.01e-04 (1.58e-04)	Tok/s 40082 (43121)	Loss/tok 3.0960 (3.2296)	LR 1.250e-04
0: TRAIN [2][6000/7762]	Time 0.362 (0.327)	Data 1.08e-04 (1.58e-04)	Tok/s 45990 (43123)	Loss/tok 3.2294 (3.2296)	LR 1.250e-04
0: TRAIN [2][6010/7762]	Time 0.263 (0.327)	Data 1.05e-04 (1.57e-04)	Tok/s 39213 (43123)	Loss/tok 2.9395 (3.2295)	LR 1.250e-04
0: TRAIN [2][6020/7762]	Time 0.261 (0.327)	Data 1.05e-04 (1.57e-04)	Tok/s 40061 (43121)	Loss/tok 3.0499 (3.2294)	LR 1.250e-04
0: TRAIN [2][6030/7762]	Time 0.262 (0.327)	Data 1.03e-04 (1.57e-04)	Tok/s 39089 (43122)	Loss/tok 3.0540 (3.2295)	LR 1.250e-04
0: TRAIN [2][6040/7762]	Time 0.252 (0.327)	Data 1.03e-04 (1.57e-04)	Tok/s 41084 (43122)	Loss/tok 2.9720 (3.2296)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][6050/7762]	Time 0.361 (0.327)	Data 1.04e-04 (1.57e-04)	Tok/s 47018 (43129)	Loss/tok 3.1396 (3.2298)	LR 1.250e-04
0: TRAIN [2][6060/7762]	Time 0.364 (0.327)	Data 1.03e-04 (1.57e-04)	Tok/s 46469 (43130)	Loss/tok 3.0975 (3.2297)	LR 1.250e-04
0: TRAIN [2][6070/7762]	Time 0.459 (0.327)	Data 1.04e-04 (1.57e-04)	Tok/s 50740 (43135)	Loss/tok 3.4661 (3.2298)	LR 1.250e-04
0: TRAIN [2][6080/7762]	Time 0.254 (0.327)	Data 1.01e-04 (1.57e-04)	Tok/s 40542 (43134)	Loss/tok 3.0327 (3.2296)	LR 1.250e-04
0: TRAIN [2][6090/7762]	Time 0.359 (0.327)	Data 1.05e-04 (1.57e-04)	Tok/s 46789 (43135)	Loss/tok 3.1876 (3.2295)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][6100/7762]	Time 0.360 (0.327)	Data 1.04e-04 (1.57e-04)	Tok/s 46427 (43138)	Loss/tok 3.2702 (3.2297)	LR 1.250e-04
0: TRAIN [2][6110/7762]	Time 0.453 (0.327)	Data 1.11e-04 (1.57e-04)	Tok/s 51913 (43140)	Loss/tok 3.3659 (3.2298)	LR 1.250e-04
0: TRAIN [2][6120/7762]	Time 0.465 (0.327)	Data 1.07e-04 (1.57e-04)	Tok/s 50320 (43139)	Loss/tok 3.5637 (3.2299)	LR 1.250e-04
0: TRAIN [2][6130/7762]	Time 0.359 (0.327)	Data 1.05e-04 (1.56e-04)	Tok/s 47065 (43142)	Loss/tok 3.2301 (3.2299)	LR 1.250e-04
0: TRAIN [2][6140/7762]	Time 0.468 (0.327)	Data 1.02e-04 (1.56e-04)	Tok/s 49533 (43143)	Loss/tok 3.4300 (3.2298)	LR 1.250e-04
0: TRAIN [2][6150/7762]	Time 0.268 (0.327)	Data 1.04e-04 (1.56e-04)	Tok/s 37795 (43134)	Loss/tok 3.0319 (3.2296)	LR 1.250e-04
0: TRAIN [2][6160/7762]	Time 0.451 (0.327)	Data 1.06e-04 (1.56e-04)	Tok/s 51389 (43139)	Loss/tok 3.3620 (3.2297)	LR 1.250e-04
0: TRAIN [2][6170/7762]	Time 0.461 (0.327)	Data 1.20e-04 (1.56e-04)	Tok/s 50811 (43135)	Loss/tok 3.3481 (3.2296)	LR 1.250e-04
0: TRAIN [2][6180/7762]	Time 0.264 (0.327)	Data 1.03e-04 (1.56e-04)	Tok/s 39830 (43136)	Loss/tok 3.1001 (3.2296)	LR 1.250e-04
0: TRAIN [2][6190/7762]	Time 0.263 (0.327)	Data 1.15e-04 (1.56e-04)	Tok/s 38646 (43132)	Loss/tok 2.9981 (3.2295)	LR 1.250e-04
0: TRAIN [2][6200/7762]	Time 0.263 (0.327)	Data 1.09e-04 (1.56e-04)	Tok/s 38649 (43132)	Loss/tok 3.0851 (3.2295)	LR 1.250e-04
0: TRAIN [2][6210/7762]	Time 0.454 (0.327)	Data 1.21e-04 (1.56e-04)	Tok/s 51300 (43132)	Loss/tok 3.4542 (3.2296)	LR 1.250e-04
0: TRAIN [2][6220/7762]	Time 0.568 (0.327)	Data 1.05e-04 (1.56e-04)	Tok/s 52286 (43128)	Loss/tok 3.6165 (3.2296)	LR 1.250e-04
0: TRAIN [2][6230/7762]	Time 0.361 (0.327)	Data 1.05e-04 (1.56e-04)	Tok/s 46589 (43130)	Loss/tok 3.1454 (3.2295)	LR 1.250e-04
0: TRAIN [2][6240/7762]	Time 0.176 (0.327)	Data 1.04e-04 (1.56e-04)	Tok/s 29926 (43127)	Loss/tok 2.7097 (3.2293)	LR 1.250e-04
0: TRAIN [2][6250/7762]	Time 0.460 (0.327)	Data 1.07e-04 (1.55e-04)	Tok/s 51174 (43126)	Loss/tok 3.2558 (3.2292)	LR 1.250e-04
0: TRAIN [2][6260/7762]	Time 0.170 (0.327)	Data 9.99e-05 (1.55e-04)	Tok/s 29664 (43119)	Loss/tok 2.5515 (3.2291)	LR 1.250e-04
0: TRAIN [2][6270/7762]	Time 0.263 (0.327)	Data 1.04e-04 (1.55e-04)	Tok/s 39022 (43123)	Loss/tok 3.0213 (3.2291)	LR 1.250e-04
0: TRAIN [2][6280/7762]	Time 0.264 (0.327)	Data 1.31e-04 (1.55e-04)	Tok/s 38218 (43120)	Loss/tok 2.9877 (3.2290)	LR 1.250e-04
0: TRAIN [2][6290/7762]	Time 0.462 (0.327)	Data 1.06e-04 (1.55e-04)	Tok/s 50080 (43115)	Loss/tok 3.4440 (3.2290)	LR 1.250e-04
0: TRAIN [2][6300/7762]	Time 0.260 (0.327)	Data 1.02e-04 (1.55e-04)	Tok/s 39612 (43115)	Loss/tok 2.9830 (3.2289)	LR 1.250e-04
0: TRAIN [2][6310/7762]	Time 0.557 (0.327)	Data 1.02e-04 (1.55e-04)	Tok/s 53760 (43118)	Loss/tok 3.5681 (3.2291)	LR 1.250e-04
0: TRAIN [2][6320/7762]	Time 0.264 (0.327)	Data 1.03e-04 (1.55e-04)	Tok/s 39039 (43117)	Loss/tok 2.9611 (3.2290)	LR 1.250e-04
0: TRAIN [2][6330/7762]	Time 0.262 (0.327)	Data 1.03e-04 (1.55e-04)	Tok/s 38827 (43115)	Loss/tok 2.9504 (3.2289)	LR 1.250e-04
0: TRAIN [2][6340/7762]	Time 0.264 (0.327)	Data 1.03e-04 (1.55e-04)	Tok/s 39645 (43115)	Loss/tok 3.1083 (3.2289)	LR 1.250e-04
0: TRAIN [2][6350/7762]	Time 0.263 (0.327)	Data 1.01e-04 (1.55e-04)	Tok/s 38171 (43107)	Loss/tok 3.0928 (3.2286)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][6360/7762]	Time 0.267 (0.327)	Data 1.01e-04 (1.55e-04)	Tok/s 39044 (43108)	Loss/tok 2.9823 (3.2285)	LR 1.250e-04
0: TRAIN [2][6370/7762]	Time 0.266 (0.327)	Data 1.06e-04 (1.55e-04)	Tok/s 38816 (43110)	Loss/tok 2.8795 (3.2285)	LR 1.250e-04
0: TRAIN [2][6380/7762]	Time 0.463 (0.327)	Data 1.00e-04 (1.54e-04)	Tok/s 50934 (43112)	Loss/tok 3.3249 (3.2284)	LR 1.250e-04
0: TRAIN [2][6390/7762]	Time 0.266 (0.327)	Data 1.09e-04 (1.54e-04)	Tok/s 39614 (43109)	Loss/tok 2.9290 (3.2284)	LR 1.250e-04
0: TRAIN [2][6400/7762]	Time 0.465 (0.327)	Data 1.04e-04 (1.54e-04)	Tok/s 50520 (43113)	Loss/tok 3.4190 (3.2285)	LR 1.250e-04
0: TRAIN [2][6410/7762]	Time 0.266 (0.327)	Data 1.03e-04 (1.54e-04)	Tok/s 39227 (43114)	Loss/tok 2.9294 (3.2286)	LR 1.250e-04
0: TRAIN [2][6420/7762]	Time 0.176 (0.327)	Data 1.02e-04 (1.54e-04)	Tok/s 29759 (43109)	Loss/tok 2.6855 (3.2284)	LR 1.250e-04
0: TRAIN [2][6430/7762]	Time 0.174 (0.327)	Data 1.03e-04 (1.54e-04)	Tok/s 30279 (43108)	Loss/tok 2.6108 (3.2283)	LR 1.250e-04
0: TRAIN [2][6440/7762]	Time 0.265 (0.327)	Data 1.02e-04 (1.54e-04)	Tok/s 38592 (43105)	Loss/tok 3.1053 (3.2282)	LR 1.250e-04
0: TRAIN [2][6450/7762]	Time 0.265 (0.327)	Data 9.89e-05 (1.54e-04)	Tok/s 39229 (43103)	Loss/tok 3.0301 (3.2281)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][6460/7762]	Time 0.451 (0.327)	Data 1.16e-04 (1.54e-04)	Tok/s 51535 (43107)	Loss/tok 3.4152 (3.2283)	LR 1.250e-04
0: TRAIN [2][6470/7762]	Time 0.254 (0.327)	Data 1.64e-04 (1.54e-04)	Tok/s 40214 (43110)	Loss/tok 3.0649 (3.2285)	LR 1.250e-04
0: TRAIN [2][6480/7762]	Time 0.344 (0.327)	Data 1.07e-04 (1.54e-04)	Tok/s 49199 (43112)	Loss/tok 3.1415 (3.2285)	LR 1.250e-04
0: TRAIN [2][6490/7762]	Time 0.267 (0.327)	Data 1.03e-04 (1.54e-04)	Tok/s 37601 (43108)	Loss/tok 3.0865 (3.2284)	LR 1.250e-04
0: TRAIN [2][6500/7762]	Time 0.269 (0.327)	Data 1.08e-04 (1.54e-04)	Tok/s 38517 (43107)	Loss/tok 2.9666 (3.2285)	LR 1.250e-04
0: TRAIN [2][6510/7762]	Time 0.253 (0.327)	Data 1.21e-04 (1.53e-04)	Tok/s 40520 (43111)	Loss/tok 3.0901 (3.2287)	LR 1.250e-04
0: TRAIN [2][6520/7762]	Time 0.462 (0.327)	Data 1.04e-04 (1.53e-04)	Tok/s 51025 (43108)	Loss/tok 3.3459 (3.2286)	LR 1.250e-04
0: TRAIN [2][6530/7762]	Time 0.434 (0.327)	Data 1.02e-04 (1.53e-04)	Tok/s 53288 (43105)	Loss/tok 3.4013 (3.2285)	LR 1.250e-04
0: TRAIN [2][6540/7762]	Time 0.263 (0.327)	Data 9.85e-05 (1.53e-04)	Tok/s 39623 (43104)	Loss/tok 2.9810 (3.2287)	LR 1.250e-04
0: TRAIN [2][6550/7762]	Time 0.463 (0.327)	Data 1.02e-04 (1.53e-04)	Tok/s 50639 (43106)	Loss/tok 3.4489 (3.2288)	LR 1.250e-04
0: TRAIN [2][6560/7762]	Time 0.361 (0.327)	Data 1.09e-04 (1.53e-04)	Tok/s 46984 (43111)	Loss/tok 3.2343 (3.2289)	LR 1.250e-04
0: TRAIN [2][6570/7762]	Time 0.257 (0.327)	Data 9.58e-05 (1.53e-04)	Tok/s 40475 (43108)	Loss/tok 2.9900 (3.2288)	LR 1.250e-04
0: TRAIN [2][6580/7762]	Time 0.266 (0.327)	Data 1.03e-04 (1.53e-04)	Tok/s 38694 (43107)	Loss/tok 3.1148 (3.2288)	LR 1.250e-04
0: TRAIN [2][6590/7762]	Time 0.261 (0.327)	Data 1.03e-04 (1.53e-04)	Tok/s 39796 (43107)	Loss/tok 2.9873 (3.2287)	LR 1.250e-04
0: TRAIN [2][6600/7762]	Time 0.254 (0.327)	Data 1.21e-04 (1.53e-04)	Tok/s 40455 (43103)	Loss/tok 3.1302 (3.2285)	LR 1.250e-04
0: TRAIN [2][6610/7762]	Time 0.255 (0.327)	Data 1.36e-04 (1.53e-04)	Tok/s 39911 (43103)	Loss/tok 3.0305 (3.2284)	LR 1.250e-04
0: TRAIN [2][6620/7762]	Time 0.263 (0.327)	Data 1.04e-04 (1.53e-04)	Tok/s 39009 (43104)	Loss/tok 3.0975 (3.2284)	LR 1.250e-04
0: TRAIN [2][6630/7762]	Time 0.266 (0.327)	Data 1.06e-04 (1.53e-04)	Tok/s 38520 (43103)	Loss/tok 3.0940 (3.2285)	LR 1.250e-04
0: TRAIN [2][6640/7762]	Time 0.258 (0.327)	Data 1.01e-04 (1.52e-04)	Tok/s 39469 (43099)	Loss/tok 2.9233 (3.2283)	LR 1.250e-04
0: TRAIN [2][6650/7762]	Time 0.265 (0.327)	Data 1.05e-04 (1.52e-04)	Tok/s 39479 (43101)	Loss/tok 3.0675 (3.2283)	LR 1.250e-04
0: TRAIN [2][6660/7762]	Time 0.264 (0.327)	Data 1.04e-04 (1.52e-04)	Tok/s 38033 (43101)	Loss/tok 3.0241 (3.2284)	LR 1.250e-04
0: TRAIN [2][6670/7762]	Time 0.263 (0.327)	Data 1.01e-04 (1.52e-04)	Tok/s 39810 (43099)	Loss/tok 3.0382 (3.2283)	LR 1.250e-04
0: TRAIN [2][6680/7762]	Time 0.175 (0.327)	Data 1.06e-04 (1.52e-04)	Tok/s 30813 (43098)	Loss/tok 2.5407 (3.2285)	LR 1.250e-04
0: TRAIN [2][6690/7762]	Time 0.364 (0.327)	Data 1.06e-04 (1.52e-04)	Tok/s 45667 (43098)	Loss/tok 3.2101 (3.2284)	LR 1.250e-04
0: TRAIN [2][6700/7762]	Time 0.265 (0.327)	Data 1.04e-04 (1.52e-04)	Tok/s 38012 (43101)	Loss/tok 3.0762 (3.2285)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][6710/7762]	Time 0.443 (0.327)	Data 1.02e-04 (1.52e-04)	Tok/s 51844 (43103)	Loss/tok 3.5277 (3.2286)	LR 1.250e-04
0: TRAIN [2][6720/7762]	Time 0.174 (0.327)	Data 1.04e-04 (1.52e-04)	Tok/s 30958 (43099)	Loss/tok 2.6231 (3.2285)	LR 1.250e-04
0: TRAIN [2][6730/7762]	Time 0.254 (0.327)	Data 1.00e-04 (1.52e-04)	Tok/s 40438 (43091)	Loss/tok 3.1017 (3.2283)	LR 1.250e-04
0: TRAIN [2][6740/7762]	Time 0.349 (0.327)	Data 1.04e-04 (1.52e-04)	Tok/s 47765 (43090)	Loss/tok 3.2702 (3.2283)	LR 1.250e-04
0: TRAIN [2][6750/7762]	Time 0.464 (0.327)	Data 9.94e-05 (1.52e-04)	Tok/s 50839 (43091)	Loss/tok 3.3154 (3.2285)	LR 1.250e-04
0: TRAIN [2][6760/7762]	Time 0.560 (0.327)	Data 1.05e-04 (1.52e-04)	Tok/s 53688 (43093)	Loss/tok 3.5775 (3.2287)	LR 1.250e-04
0: TRAIN [2][6770/7762]	Time 0.174 (0.327)	Data 1.06e-04 (1.52e-04)	Tok/s 29935 (43085)	Loss/tok 2.5467 (3.2285)	LR 1.250e-04
0: TRAIN [2][6780/7762]	Time 0.173 (0.327)	Data 1.11e-04 (1.52e-04)	Tok/s 30938 (43085)	Loss/tok 2.5911 (3.2285)	LR 1.250e-04
0: TRAIN [2][6790/7762]	Time 0.264 (0.326)	Data 1.06e-04 (1.51e-04)	Tok/s 38873 (43081)	Loss/tok 2.9302 (3.2283)	LR 1.250e-04
0: TRAIN [2][6800/7762]	Time 0.264 (0.326)	Data 1.08e-04 (1.51e-04)	Tok/s 38662 (43079)	Loss/tok 2.9671 (3.2283)	LR 1.250e-04
0: TRAIN [2][6810/7762]	Time 0.266 (0.326)	Data 1.10e-04 (1.51e-04)	Tok/s 38512 (43078)	Loss/tok 3.0012 (3.2282)	LR 1.250e-04
0: TRAIN [2][6820/7762]	Time 0.268 (0.326)	Data 1.04e-04 (1.51e-04)	Tok/s 38316 (43080)	Loss/tok 2.9368 (3.2281)	LR 1.250e-04
0: TRAIN [2][6830/7762]	Time 0.345 (0.326)	Data 1.17e-04 (1.51e-04)	Tok/s 47850 (43080)	Loss/tok 3.1865 (3.2280)	LR 1.250e-04
0: TRAIN [2][6840/7762]	Time 0.365 (0.326)	Data 2.44e-04 (1.51e-04)	Tok/s 46099 (43083)	Loss/tok 3.2976 (3.2281)	LR 1.250e-04
0: TRAIN [2][6850/7762]	Time 0.253 (0.326)	Data 1.08e-04 (1.51e-04)	Tok/s 40718 (43081)	Loss/tok 3.1384 (3.2280)	LR 1.250e-04
0: TRAIN [2][6860/7762]	Time 0.264 (0.326)	Data 1.03e-04 (1.51e-04)	Tok/s 38007 (43081)	Loss/tok 3.0377 (3.2280)	LR 1.250e-04
0: TRAIN [2][6870/7762]	Time 0.260 (0.326)	Data 1.03e-04 (1.51e-04)	Tok/s 39640 (43078)	Loss/tok 3.0250 (3.2279)	LR 1.250e-04
0: TRAIN [2][6880/7762]	Time 0.450 (0.326)	Data 1.03e-04 (1.51e-04)	Tok/s 51577 (43079)	Loss/tok 3.4048 (3.2278)	LR 1.250e-04
0: TRAIN [2][6890/7762]	Time 0.260 (0.326)	Data 1.05e-04 (1.51e-04)	Tok/s 40387 (43077)	Loss/tok 2.9901 (3.2276)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][6900/7762]	Time 0.258 (0.326)	Data 1.15e-04 (1.51e-04)	Tok/s 39074 (43081)	Loss/tok 2.9840 (3.2277)	LR 1.250e-04
0: TRAIN [2][6910/7762]	Time 0.466 (0.326)	Data 1.11e-04 (1.51e-04)	Tok/s 49794 (43086)	Loss/tok 3.3036 (3.2279)	LR 1.250e-04
0: TRAIN [2][6920/7762]	Time 0.460 (0.326)	Data 1.05e-04 (1.51e-04)	Tok/s 50921 (43083)	Loss/tok 3.3667 (3.2278)	LR 1.250e-04
0: TRAIN [2][6930/7762]	Time 0.254 (0.326)	Data 1.05e-04 (1.51e-04)	Tok/s 40965 (43084)	Loss/tok 2.9810 (3.2279)	LR 1.250e-04
0: TRAIN [2][6940/7762]	Time 0.265 (0.326)	Data 1.02e-04 (1.51e-04)	Tok/s 39061 (43084)	Loss/tok 3.0717 (3.2278)	LR 1.250e-04
0: TRAIN [2][6950/7762]	Time 0.362 (0.326)	Data 1.06e-04 (1.50e-04)	Tok/s 47247 (43084)	Loss/tok 3.2454 (3.2278)	LR 1.250e-04
0: TRAIN [2][6960/7762]	Time 0.360 (0.326)	Data 1.03e-04 (1.50e-04)	Tok/s 46565 (43088)	Loss/tok 3.3101 (3.2279)	LR 1.250e-04
0: TRAIN [2][6970/7762]	Time 0.349 (0.326)	Data 1.03e-04 (1.50e-04)	Tok/s 48122 (43091)	Loss/tok 3.2134 (3.2279)	LR 1.250e-04
0: TRAIN [2][6980/7762]	Time 0.363 (0.326)	Data 1.00e-04 (1.50e-04)	Tok/s 46660 (43090)	Loss/tok 3.1759 (3.2278)	LR 1.250e-04
0: TRAIN [2][6990/7762]	Time 0.461 (0.326)	Data 1.00e-04 (1.50e-04)	Tok/s 50994 (43088)	Loss/tok 3.2739 (3.2277)	LR 1.250e-04
0: TRAIN [2][7000/7762]	Time 0.579 (0.326)	Data 1.19e-04 (1.50e-04)	Tok/s 51231 (43090)	Loss/tok 3.5377 (3.2278)	LR 1.250e-04
0: TRAIN [2][7010/7762]	Time 0.570 (0.326)	Data 1.03e-04 (1.50e-04)	Tok/s 52815 (43089)	Loss/tok 3.5453 (3.2277)	LR 1.250e-04
0: TRAIN [2][7020/7762]	Time 0.264 (0.326)	Data 1.19e-04 (1.50e-04)	Tok/s 38499 (43090)	Loss/tok 3.0249 (3.2277)	LR 1.250e-04
0: TRAIN [2][7030/7762]	Time 0.357 (0.326)	Data 1.06e-04 (1.50e-04)	Tok/s 47126 (43092)	Loss/tok 3.2383 (3.2276)	LR 1.250e-04
0: TRAIN [2][7040/7762]	Time 0.585 (0.327)	Data 1.05e-04 (1.50e-04)	Tok/s 51148 (43092)	Loss/tok 3.5984 (3.2277)	LR 1.250e-04
0: TRAIN [2][7050/7762]	Time 0.266 (0.327)	Data 1.22e-04 (1.50e-04)	Tok/s 39139 (43097)	Loss/tok 2.9127 (3.2278)	LR 1.250e-04
0: TRAIN [2][7060/7762]	Time 0.267 (0.327)	Data 9.99e-05 (1.50e-04)	Tok/s 38986 (43097)	Loss/tok 2.9758 (3.2276)	LR 1.250e-04
0: TRAIN [2][7070/7762]	Time 0.257 (0.327)	Data 1.12e-04 (1.50e-04)	Tok/s 40079 (43096)	Loss/tok 2.9192 (3.2275)	LR 1.250e-04
0: TRAIN [2][7080/7762]	Time 0.567 (0.327)	Data 9.97e-05 (1.50e-04)	Tok/s 52040 (43097)	Loss/tok 3.6467 (3.2275)	LR 1.250e-04
0: TRAIN [2][7090/7762]	Time 0.260 (0.327)	Data 9.97e-05 (1.50e-04)	Tok/s 40315 (43098)	Loss/tok 2.9932 (3.2275)	LR 1.250e-04
0: TRAIN [2][7100/7762]	Time 0.362 (0.326)	Data 1.02e-04 (1.50e-04)	Tok/s 46291 (43092)	Loss/tok 3.1971 (3.2273)	LR 1.250e-04
0: TRAIN [2][7110/7762]	Time 0.177 (0.326)	Data 1.06e-04 (1.50e-04)	Tok/s 29973 (43094)	Loss/tok 2.8188 (3.2273)	LR 1.250e-04
0: TRAIN [2][7120/7762]	Time 0.367 (0.326)	Data 1.23e-04 (1.49e-04)	Tok/s 45858 (43091)	Loss/tok 3.2653 (3.2272)	LR 1.250e-04
0: TRAIN [2][7130/7762]	Time 0.459 (0.326)	Data 1.06e-04 (1.49e-04)	Tok/s 50911 (43092)	Loss/tok 3.4427 (3.2273)	LR 1.250e-04
0: TRAIN [2][7140/7762]	Time 0.365 (0.326)	Data 1.20e-04 (1.49e-04)	Tok/s 45773 (43091)	Loss/tok 3.3117 (3.2273)	LR 1.250e-04
0: TRAIN [2][7150/7762]	Time 0.364 (0.326)	Data 9.94e-05 (1.49e-04)	Tok/s 45743 (43091)	Loss/tok 3.2005 (3.2272)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][7160/7762]	Time 0.362 (0.326)	Data 9.97e-05 (1.49e-04)	Tok/s 46067 (43092)	Loss/tok 3.1798 (3.2272)	LR 1.250e-04
0: TRAIN [2][7170/7762]	Time 0.263 (0.326)	Data 1.02e-04 (1.49e-04)	Tok/s 39496 (43089)	Loss/tok 3.0328 (3.2272)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][7180/7762]	Time 0.257 (0.326)	Data 1.02e-04 (1.49e-04)	Tok/s 41004 (43085)	Loss/tok 3.0254 (3.2271)	LR 1.250e-04
0: TRAIN [2][7190/7762]	Time 0.174 (0.326)	Data 1.06e-04 (1.49e-04)	Tok/s 29923 (43086)	Loss/tok 2.5322 (3.2272)	LR 1.250e-04
0: TRAIN [2][7200/7762]	Time 0.588 (0.326)	Data 1.06e-04 (1.49e-04)	Tok/s 49987 (43088)	Loss/tok 3.6092 (3.2274)	LR 1.250e-04
0: TRAIN [2][7210/7762]	Time 0.366 (0.326)	Data 1.03e-04 (1.49e-04)	Tok/s 45735 (43088)	Loss/tok 3.2033 (3.2273)	LR 1.250e-04
0: TRAIN [2][7220/7762]	Time 0.361 (0.326)	Data 1.05e-04 (1.49e-04)	Tok/s 46266 (43089)	Loss/tok 3.2676 (3.2273)	LR 1.250e-04
0: TRAIN [2][7230/7762]	Time 0.261 (0.326)	Data 1.21e-04 (1.49e-04)	Tok/s 39350 (43087)	Loss/tok 3.0965 (3.2272)	LR 1.250e-04
0: TRAIN [2][7240/7762]	Time 0.265 (0.326)	Data 1.03e-04 (1.49e-04)	Tok/s 38857 (43084)	Loss/tok 3.0379 (3.2272)	LR 1.250e-04
0: TRAIN [2][7250/7762]	Time 0.266 (0.326)	Data 2.36e-04 (1.49e-04)	Tok/s 39362 (43083)	Loss/tok 3.0367 (3.2272)	LR 1.250e-04
0: TRAIN [2][7260/7762]	Time 0.264 (0.326)	Data 1.03e-04 (1.49e-04)	Tok/s 39109 (43084)	Loss/tok 2.9868 (3.2272)	LR 1.250e-04
0: TRAIN [2][7270/7762]	Time 0.262 (0.326)	Data 1.03e-04 (1.49e-04)	Tok/s 40390 (43083)	Loss/tok 3.0629 (3.2271)	LR 1.250e-04
0: TRAIN [2][7280/7762]	Time 0.265 (0.326)	Data 1.09e-04 (1.49e-04)	Tok/s 38923 (43086)	Loss/tok 3.0044 (3.2272)	LR 1.250e-04
0: TRAIN [2][7290/7762]	Time 0.362 (0.326)	Data 1.04e-04 (1.48e-04)	Tok/s 47136 (43082)	Loss/tok 3.2327 (3.2270)	LR 1.250e-04
0: TRAIN [2][7300/7762]	Time 0.261 (0.326)	Data 1.07e-04 (1.48e-04)	Tok/s 39137 (43084)	Loss/tok 3.0244 (3.2270)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][7310/7762]	Time 0.358 (0.326)	Data 9.70e-05 (1.48e-04)	Tok/s 46522 (43083)	Loss/tok 3.2760 (3.2270)	LR 1.250e-04
0: TRAIN [2][7320/7762]	Time 0.268 (0.326)	Data 1.05e-04 (1.48e-04)	Tok/s 38792 (43081)	Loss/tok 3.0419 (3.2270)	LR 1.250e-04
0: TRAIN [2][7330/7762]	Time 0.348 (0.326)	Data 1.02e-04 (1.48e-04)	Tok/s 48990 (43078)	Loss/tok 3.1762 (3.2268)	LR 1.250e-04
0: TRAIN [2][7340/7762]	Time 0.364 (0.326)	Data 1.15e-04 (1.48e-04)	Tok/s 45572 (43079)	Loss/tok 3.3644 (3.2269)	LR 1.250e-04
0: TRAIN [2][7350/7762]	Time 0.265 (0.326)	Data 1.04e-04 (1.48e-04)	Tok/s 39295 (43083)	Loss/tok 3.1354 (3.2271)	LR 1.250e-04
0: TRAIN [2][7360/7762]	Time 0.264 (0.326)	Data 1.19e-04 (1.48e-04)	Tok/s 38637 (43083)	Loss/tok 2.9294 (3.2271)	LR 1.250e-04
0: TRAIN [2][7370/7762]	Time 0.262 (0.326)	Data 1.07e-04 (1.48e-04)	Tok/s 40253 (43082)	Loss/tok 3.0259 (3.2271)	LR 1.250e-04
0: TRAIN [2][7380/7762]	Time 0.353 (0.326)	Data 1.19e-04 (1.48e-04)	Tok/s 47542 (43082)	Loss/tok 3.2551 (3.2271)	LR 1.250e-04
0: TRAIN [2][7390/7762]	Time 0.365 (0.326)	Data 1.03e-04 (1.48e-04)	Tok/s 45893 (43082)	Loss/tok 3.3443 (3.2272)	LR 1.250e-04
0: TRAIN [2][7400/7762]	Time 0.359 (0.326)	Data 1.02e-04 (1.48e-04)	Tok/s 47231 (43081)	Loss/tok 3.2163 (3.2272)	LR 1.250e-04
0: TRAIN [2][7410/7762]	Time 0.263 (0.326)	Data 1.11e-04 (1.48e-04)	Tok/s 40440 (43083)	Loss/tok 2.9839 (3.2274)	LR 1.250e-04
0: TRAIN [2][7420/7762]	Time 0.464 (0.326)	Data 1.19e-04 (1.48e-04)	Tok/s 49831 (43086)	Loss/tok 3.2710 (3.2274)	LR 1.250e-04
0: TRAIN [2][7430/7762]	Time 0.262 (0.326)	Data 2.43e-04 (1.48e-04)	Tok/s 38800 (43084)	Loss/tok 3.1381 (3.2274)	LR 1.250e-04
0: TRAIN [2][7440/7762]	Time 0.461 (0.326)	Data 1.02e-04 (1.48e-04)	Tok/s 50222 (43083)	Loss/tok 3.4786 (3.2274)	LR 1.250e-04
0: TRAIN [2][7450/7762]	Time 0.363 (0.326)	Data 1.04e-04 (1.48e-04)	Tok/s 45545 (43084)	Loss/tok 3.4766 (3.2276)	LR 1.250e-04
0: TRAIN [2][7460/7762]	Time 0.365 (0.326)	Data 1.03e-04 (1.48e-04)	Tok/s 45913 (43082)	Loss/tok 3.1430 (3.2276)	LR 1.250e-04
0: TRAIN [2][7470/7762]	Time 0.357 (0.326)	Data 2.39e-04 (1.47e-04)	Tok/s 46955 (43084)	Loss/tok 3.2314 (3.2277)	LR 1.250e-04
0: TRAIN [2][7480/7762]	Time 0.458 (0.327)	Data 1.04e-04 (1.47e-04)	Tok/s 50943 (43087)	Loss/tok 3.2380 (3.2278)	LR 1.250e-04
0: TRAIN [2][7490/7762]	Time 0.344 (0.327)	Data 1.24e-04 (1.47e-04)	Tok/s 48953 (43092)	Loss/tok 3.1268 (3.2279)	LR 1.250e-04
0: TRAIN [2][7500/7762]	Time 0.261 (0.327)	Data 9.56e-05 (1.47e-04)	Tok/s 39689 (43088)	Loss/tok 2.9569 (3.2277)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][7510/7762]	Time 0.177 (0.327)	Data 1.04e-04 (1.47e-04)	Tok/s 29291 (43087)	Loss/tok 2.6469 (3.2278)	LR 1.250e-04
0: TRAIN [2][7520/7762]	Time 0.356 (0.327)	Data 1.03e-04 (1.47e-04)	Tok/s 46794 (43089)	Loss/tok 3.1784 (3.2278)	LR 1.250e-04
0: TRAIN [2][7530/7762]	Time 0.261 (0.326)	Data 1.06e-04 (1.47e-04)	Tok/s 39751 (43084)	Loss/tok 2.9102 (3.2277)	LR 1.250e-04
0: TRAIN [2][7540/7762]	Time 0.255 (0.326)	Data 1.16e-04 (1.47e-04)	Tok/s 40317 (43085)	Loss/tok 2.9965 (3.2277)	LR 1.250e-04
0: TRAIN [2][7550/7762]	Time 0.356 (0.326)	Data 9.97e-05 (1.47e-04)	Tok/s 47106 (43081)	Loss/tok 3.3278 (3.2276)	LR 1.250e-04
0: TRAIN [2][7560/7762]	Time 0.175 (0.326)	Data 1.03e-04 (1.47e-04)	Tok/s 30640 (43080)	Loss/tok 2.5983 (3.2276)	LR 1.250e-04
0: TRAIN [2][7570/7762]	Time 0.358 (0.326)	Data 1.02e-04 (1.47e-04)	Tok/s 46724 (43081)	Loss/tok 3.2690 (3.2276)	LR 1.250e-04
0: TRAIN [2][7580/7762]	Time 0.361 (0.326)	Data 1.01e-04 (1.47e-04)	Tok/s 46489 (43076)	Loss/tok 3.3342 (3.2275)	LR 1.250e-04
0: TRAIN [2][7590/7762]	Time 0.353 (0.326)	Data 1.02e-04 (1.47e-04)	Tok/s 47775 (43079)	Loss/tok 3.2486 (3.2276)	LR 1.250e-04
0: TRAIN [2][7600/7762]	Time 0.354 (0.326)	Data 1.03e-04 (1.47e-04)	Tok/s 47432 (43081)	Loss/tok 3.3456 (3.2276)	LR 1.250e-04
0: TRAIN [2][7610/7762]	Time 0.466 (0.326)	Data 1.19e-04 (1.47e-04)	Tok/s 49761 (43080)	Loss/tok 3.4557 (3.2276)	LR 1.250e-04
0: TRAIN [2][7620/7762]	Time 0.265 (0.326)	Data 1.03e-04 (1.47e-04)	Tok/s 38541 (43078)	Loss/tok 2.9855 (3.2275)	LR 1.250e-04
0: TRAIN [2][7630/7762]	Time 0.175 (0.326)	Data 1.02e-04 (1.47e-04)	Tok/s 29944 (43074)	Loss/tok 2.6023 (3.2274)	LR 1.250e-04
0: TRAIN [2][7640/7762]	Time 0.260 (0.326)	Data 1.03e-04 (1.47e-04)	Tok/s 39579 (43072)	Loss/tok 3.0876 (3.2273)	LR 1.250e-04
0: TRAIN [2][7650/7762]	Time 0.258 (0.326)	Data 1.02e-04 (1.46e-04)	Tok/s 40670 (43069)	Loss/tok 2.9664 (3.2271)	LR 1.250e-04
0: TRAIN [2][7660/7762]	Time 0.355 (0.326)	Data 1.15e-04 (1.46e-04)	Tok/s 47235 (43069)	Loss/tok 3.2237 (3.2272)	LR 1.250e-04
0: TRAIN [2][7670/7762]	Time 0.357 (0.326)	Data 1.00e-04 (1.46e-04)	Tok/s 47658 (43065)	Loss/tok 3.2919 (3.2271)	LR 1.250e-04
0: TRAIN [2][7680/7762]	Time 0.460 (0.326)	Data 1.05e-04 (1.46e-04)	Tok/s 50110 (43066)	Loss/tok 3.5123 (3.2271)	LR 1.250e-04
0: TRAIN [2][7690/7762]	Time 0.261 (0.326)	Data 1.05e-04 (1.46e-04)	Tok/s 39589 (43066)	Loss/tok 2.9900 (3.2272)	LR 1.250e-04
0: TRAIN [2][7700/7762]	Time 0.362 (0.326)	Data 1.07e-04 (1.46e-04)	Tok/s 46114 (43066)	Loss/tok 3.2293 (3.2272)	LR 1.250e-04
0: TRAIN [2][7710/7762]	Time 0.265 (0.326)	Data 1.01e-04 (1.46e-04)	Tok/s 39225 (43066)	Loss/tok 3.0116 (3.2273)	LR 1.250e-04
0: TRAIN [2][7720/7762]	Time 0.256 (0.326)	Data 1.04e-04 (1.46e-04)	Tok/s 40625 (43067)	Loss/tok 2.8317 (3.2273)	LR 1.250e-04
0: TRAIN [2][7730/7762]	Time 0.269 (0.326)	Data 1.05e-04 (1.46e-04)	Tok/s 37754 (43066)	Loss/tok 3.1170 (3.2273)	LR 1.250e-04
0: TRAIN [2][7740/7762]	Time 0.262 (0.326)	Data 1.01e-04 (1.46e-04)	Tok/s 39545 (43065)	Loss/tok 2.9787 (3.2273)	LR 1.250e-04
0: TRAIN [2][7750/7762]	Time 0.351 (0.326)	Data 1.03e-04 (1.46e-04)	Tok/s 47784 (43064)	Loss/tok 3.2346 (3.2273)	LR 1.250e-04
0: TRAIN [2][7760/7762]	Time 0.266 (0.326)	Data 1.49e-02 (1.48e-04)	Tok/s 39546 (43061)	Loss/tok 3.0323 (3.2271)	LR 1.250e-04
:::MLL 1573753458.677 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1573753458.678 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/12]	Time 0.808 (0.808)	Decoder iters 108.0 (108.0)	Tok/s 20394 (20394)
0: TEST [2][10/12]	Time 0.117 (0.290)	Decoder iters 25.0 (49.2)	Tok/s 31724 (29772)
0: Running moses detokenizer
0: BLEU(score=22.872129378099892, counts=[36357, 17776, 9927, 5753], totals=[65196, 62193, 59191, 56194], precisions=[55.76569114669611, 28.581994758252538, 16.771130746228312, 10.237747802256468], bp=1.0, sys_len=65196, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1573753464.208 eval_accuracy: {"value": 22.87, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1573753464.208 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.2269	Test BLEU: 22.87
0: Performance: Epoch: 2	Training: 86122 Tok/s
0: Finished epoch 2
:::MLL 1573753464.208 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1573753464.209 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1573753464.209 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 3249966159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][0/7762]	Time 0.493 (0.493)	Data 3.28e-01 (3.28e-01)	Tok/s 10735 (10735)	Loss/tok 2.6689 (2.6689)	LR 1.250e-04
0: TRAIN [3][10/7762]	Time 0.356 (0.376)	Data 1.05e-04 (2.99e-02)	Tok/s 46961 (43149)	Loss/tok 3.1447 (3.2600)	LR 1.250e-04
0: TRAIN [3][20/7762]	Time 0.359 (0.390)	Data 1.09e-04 (1.57e-02)	Tok/s 46787 (45842)	Loss/tok 3.0786 (3.3035)	LR 1.250e-04
0: TRAIN [3][30/7762]	Time 0.448 (0.383)	Data 1.05e-04 (1.07e-02)	Tok/s 51584 (46100)	Loss/tok 3.3850 (3.2908)	LR 1.250e-04
0: TRAIN [3][40/7762]	Time 0.357 (0.367)	Data 1.06e-04 (8.10e-03)	Tok/s 46775 (45565)	Loss/tok 3.1119 (3.2599)	LR 1.250e-04
0: TRAIN [3][50/7762]	Time 0.265 (0.352)	Data 9.97e-05 (6.53e-03)	Tok/s 39235 (44760)	Loss/tok 3.0434 (3.2380)	LR 1.250e-04
0: TRAIN [3][60/7762]	Time 0.360 (0.353)	Data 1.03e-04 (5.48e-03)	Tok/s 47091 (45042)	Loss/tok 3.1891 (3.2307)	LR 1.250e-04
0: TRAIN [3][70/7762]	Time 0.362 (0.352)	Data 1.14e-04 (4.72e-03)	Tok/s 46448 (45126)	Loss/tok 3.2354 (3.2304)	LR 1.250e-04
0: TRAIN [3][80/7762]	Time 0.354 (0.350)	Data 1.01e-04 (4.15e-03)	Tok/s 48007 (44893)	Loss/tok 3.1939 (3.2257)	LR 1.250e-04
0: TRAIN [3][90/7762]	Time 0.358 (0.348)	Data 1.01e-04 (3.71e-03)	Tok/s 46999 (44726)	Loss/tok 3.1334 (3.2214)	LR 1.250e-04
0: TRAIN [3][100/7762]	Time 0.572 (0.344)	Data 1.03e-04 (3.35e-03)	Tok/s 51812 (44256)	Loss/tok 3.6531 (3.2273)	LR 1.250e-04
0: TRAIN [3][110/7762]	Time 0.452 (0.341)	Data 9.97e-05 (3.06e-03)	Tok/s 51455 (43926)	Loss/tok 3.3462 (3.2214)	LR 1.250e-04
0: TRAIN [3][120/7762]	Time 0.360 (0.336)	Data 1.01e-04 (2.81e-03)	Tok/s 47319 (43740)	Loss/tok 3.2061 (3.2124)	LR 1.250e-04
0: TRAIN [3][130/7762]	Time 0.451 (0.337)	Data 1.02e-04 (2.61e-03)	Tok/s 52726 (43850)	Loss/tok 3.3357 (3.2161)	LR 1.250e-04
0: TRAIN [3][140/7762]	Time 0.265 (0.336)	Data 1.02e-04 (2.43e-03)	Tok/s 38289 (43846)	Loss/tok 3.0884 (3.2132)	LR 1.250e-04
0: TRAIN [3][150/7762]	Time 0.586 (0.339)	Data 1.04e-04 (2.28e-03)	Tok/s 50357 (43964)	Loss/tok 3.6788 (3.2201)	LR 1.250e-04
0: TRAIN [3][160/7762]	Time 0.358 (0.339)	Data 1.05e-04 (2.14e-03)	Tok/s 46904 (44070)	Loss/tok 3.1790 (3.2153)	LR 1.250e-04
0: TRAIN [3][170/7762]	Time 0.358 (0.337)	Data 1.01e-04 (2.02e-03)	Tok/s 46515 (43860)	Loss/tok 3.2288 (3.2140)	LR 1.250e-04
0: TRAIN [3][180/7762]	Time 0.264 (0.338)	Data 9.94e-05 (1.92e-03)	Tok/s 38560 (43813)	Loss/tok 2.8818 (3.2167)	LR 1.250e-04
0: TRAIN [3][190/7762]	Time 0.352 (0.335)	Data 1.04e-04 (1.82e-03)	Tok/s 47720 (43640)	Loss/tok 3.1060 (3.2110)	LR 1.250e-04
0: TRAIN [3][200/7762]	Time 0.450 (0.332)	Data 1.08e-04 (1.74e-03)	Tok/s 52127 (43431)	Loss/tok 3.3791 (3.2072)	LR 1.250e-04
0: TRAIN [3][210/7762]	Time 0.266 (0.328)	Data 9.92e-05 (1.66e-03)	Tok/s 39889 (43229)	Loss/tok 3.0114 (3.2020)	LR 1.250e-04
0: TRAIN [3][220/7762]	Time 0.173 (0.328)	Data 1.02e-04 (1.59e-03)	Tok/s 31408 (43194)	Loss/tok 2.7399 (3.2017)	LR 1.250e-04
0: TRAIN [3][230/7762]	Time 0.357 (0.327)	Data 1.03e-04 (1.52e-03)	Tok/s 47283 (43118)	Loss/tok 3.1628 (3.1966)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][240/7762]	Time 0.462 (0.327)	Data 1.07e-04 (1.47e-03)	Tok/s 50017 (43122)	Loss/tok 3.3677 (3.1953)	LR 1.250e-04
0: TRAIN [3][250/7762]	Time 0.360 (0.326)	Data 1.03e-04 (1.41e-03)	Tok/s 47009 (43004)	Loss/tok 3.1646 (3.1940)	LR 1.250e-04
0: TRAIN [3][260/7762]	Time 0.264 (0.324)	Data 1.08e-04 (1.36e-03)	Tok/s 38409 (42872)	Loss/tok 3.1140 (3.1893)	LR 1.250e-04
0: TRAIN [3][270/7762]	Time 0.258 (0.322)	Data 9.78e-05 (1.32e-03)	Tok/s 39847 (42784)	Loss/tok 3.1188 (3.1855)	LR 1.250e-04
0: TRAIN [3][280/7762]	Time 0.261 (0.321)	Data 9.97e-05 (1.27e-03)	Tok/s 39218 (42691)	Loss/tok 3.0234 (3.1834)	LR 1.250e-04
0: TRAIN [3][290/7762]	Time 0.352 (0.321)	Data 1.08e-04 (1.23e-03)	Tok/s 47654 (42726)	Loss/tok 3.1280 (3.1836)	LR 1.250e-04
0: TRAIN [3][300/7762]	Time 0.357 (0.321)	Data 1.15e-04 (1.19e-03)	Tok/s 47136 (42669)	Loss/tok 3.1844 (3.1847)	LR 1.250e-04
0: TRAIN [3][310/7762]	Time 0.582 (0.320)	Data 9.94e-05 (1.16e-03)	Tok/s 51788 (42617)	Loss/tok 3.4933 (3.1838)	LR 1.250e-04
0: TRAIN [3][320/7762]	Time 0.261 (0.325)	Data 1.23e-04 (1.13e-03)	Tok/s 40315 (42746)	Loss/tok 2.8893 (3.1978)	LR 1.250e-04
0: TRAIN [3][330/7762]	Time 0.364 (0.325)	Data 9.87e-05 (1.10e-03)	Tok/s 46256 (42812)	Loss/tok 3.1173 (3.1959)	LR 1.250e-04
0: TRAIN [3][340/7762]	Time 0.264 (0.324)	Data 1.06e-04 (1.07e-03)	Tok/s 38812 (42804)	Loss/tok 3.0714 (3.1952)	LR 1.250e-04
0: TRAIN [3][350/7762]	Time 0.255 (0.324)	Data 1.03e-04 (1.04e-03)	Tok/s 40180 (42760)	Loss/tok 3.0194 (3.1934)	LR 1.250e-04
0: TRAIN [3][360/7762]	Time 0.257 (0.323)	Data 9.99e-05 (1.01e-03)	Tok/s 40123 (42711)	Loss/tok 2.9865 (3.1911)	LR 1.250e-04
0: TRAIN [3][370/7762]	Time 0.264 (0.322)	Data 1.01e-04 (9.89e-04)	Tok/s 38657 (42666)	Loss/tok 3.0222 (3.1898)	LR 1.250e-04
0: TRAIN [3][380/7762]	Time 0.350 (0.322)	Data 1.20e-04 (9.66e-04)	Tok/s 47886 (42688)	Loss/tok 3.1504 (3.1893)	LR 1.250e-04
0: TRAIN [3][390/7762]	Time 0.359 (0.322)	Data 1.00e-04 (9.44e-04)	Tok/s 46019 (42713)	Loss/tok 3.1972 (3.1877)	LR 1.250e-04
0: TRAIN [3][400/7762]	Time 0.263 (0.323)	Data 1.56e-04 (9.23e-04)	Tok/s 39102 (42764)	Loss/tok 2.9798 (3.1882)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][410/7762]	Time 0.459 (0.325)	Data 1.06e-04 (9.03e-04)	Tok/s 50742 (42865)	Loss/tok 3.2996 (3.1923)	LR 1.250e-04
0: TRAIN [3][420/7762]	Time 0.363 (0.323)	Data 1.02e-04 (8.84e-04)	Tok/s 46200 (42779)	Loss/tok 3.2117 (3.1898)	LR 1.250e-04
0: TRAIN [3][430/7762]	Time 0.174 (0.323)	Data 1.04e-04 (8.66e-04)	Tok/s 30418 (42759)	Loss/tok 2.5554 (3.1888)	LR 1.250e-04
0: TRAIN [3][440/7762]	Time 0.261 (0.323)	Data 1.09e-04 (8.49e-04)	Tok/s 39980 (42782)	Loss/tok 2.9012 (3.1887)	LR 1.250e-04
0: TRAIN [3][450/7762]	Time 0.261 (0.323)	Data 1.08e-04 (8.33e-04)	Tok/s 39509 (42835)	Loss/tok 3.0078 (3.1875)	LR 1.250e-04
0: TRAIN [3][460/7762]	Time 0.259 (0.324)	Data 1.07e-04 (8.17e-04)	Tok/s 38972 (42835)	Loss/tok 3.0177 (3.1895)	LR 1.250e-04
0: TRAIN [3][470/7762]	Time 0.264 (0.323)	Data 1.00e-04 (8.02e-04)	Tok/s 38829 (42793)	Loss/tok 2.9969 (3.1881)	LR 1.250e-04
0: TRAIN [3][480/7762]	Time 0.264 (0.323)	Data 1.02e-04 (7.87e-04)	Tok/s 38559 (42777)	Loss/tok 2.9677 (3.1864)	LR 1.250e-04
0: TRAIN [3][490/7762]	Time 0.452 (0.323)	Data 1.03e-04 (7.73e-04)	Tok/s 51555 (42779)	Loss/tok 3.3624 (3.1860)	LR 1.250e-04
0: TRAIN [3][500/7762]	Time 0.364 (0.323)	Data 1.02e-04 (7.60e-04)	Tok/s 46178 (42780)	Loss/tok 3.2476 (3.1862)	LR 1.250e-04
0: TRAIN [3][510/7762]	Time 0.342 (0.323)	Data 1.06e-04 (7.47e-04)	Tok/s 49210 (42797)	Loss/tok 3.1144 (3.1863)	LR 1.250e-04
0: TRAIN [3][520/7762]	Time 0.354 (0.322)	Data 1.03e-04 (7.35e-04)	Tok/s 47315 (42740)	Loss/tok 3.1167 (3.1837)	LR 1.250e-04
0: TRAIN [3][530/7762]	Time 0.176 (0.321)	Data 1.05e-04 (7.23e-04)	Tok/s 30026 (42706)	Loss/tok 2.6432 (3.1819)	LR 1.250e-04
0: TRAIN [3][540/7762]	Time 0.360 (0.322)	Data 1.03e-04 (7.12e-04)	Tok/s 47140 (42735)	Loss/tok 3.0908 (3.1825)	LR 1.250e-04
0: TRAIN [3][550/7762]	Time 0.367 (0.322)	Data 1.03e-04 (7.01e-04)	Tok/s 45572 (42762)	Loss/tok 3.2874 (3.1842)	LR 1.250e-04
0: TRAIN [3][560/7762]	Time 0.263 (0.323)	Data 1.08e-04 (6.90e-04)	Tok/s 39832 (42794)	Loss/tok 3.0679 (3.1855)	LR 1.250e-04
0: TRAIN [3][570/7762]	Time 0.368 (0.322)	Data 1.10e-04 (6.80e-04)	Tok/s 45789 (42767)	Loss/tok 3.2042 (3.1845)	LR 1.250e-04
0: TRAIN [3][580/7762]	Time 0.177 (0.323)	Data 1.06e-04 (6.70e-04)	Tok/s 29909 (42794)	Loss/tok 2.6222 (3.1869)	LR 1.250e-04
0: TRAIN [3][590/7762]	Time 0.265 (0.323)	Data 1.02e-04 (6.61e-04)	Tok/s 38714 (42777)	Loss/tok 2.9885 (3.1858)	LR 1.250e-04
0: TRAIN [3][600/7762]	Time 0.466 (0.324)	Data 1.06e-04 (6.51e-04)	Tok/s 49837 (42850)	Loss/tok 3.2812 (3.1890)	LR 1.250e-04
0: TRAIN [3][610/7762]	Time 0.368 (0.326)	Data 1.05e-04 (6.43e-04)	Tok/s 45439 (42913)	Loss/tok 3.1977 (3.1923)	LR 1.250e-04
0: TRAIN [3][620/7762]	Time 0.368 (0.326)	Data 1.21e-04 (6.34e-04)	Tok/s 45771 (42939)	Loss/tok 3.2214 (3.1936)	LR 1.250e-04
0: TRAIN [3][630/7762]	Time 0.266 (0.326)	Data 1.07e-04 (6.26e-04)	Tok/s 38296 (42937)	Loss/tok 2.9279 (3.1938)	LR 1.250e-04
0: TRAIN [3][640/7762]	Time 0.455 (0.326)	Data 1.04e-04 (6.17e-04)	Tok/s 51699 (42941)	Loss/tok 3.2611 (3.1934)	LR 1.250e-04
0: TRAIN [3][650/7762]	Time 0.175 (0.327)	Data 1.20e-04 (6.10e-04)	Tok/s 30162 (42957)	Loss/tok 2.5909 (3.1947)	LR 1.250e-04
0: TRAIN [3][660/7762]	Time 0.251 (0.326)	Data 1.05e-04 (6.02e-04)	Tok/s 41485 (42922)	Loss/tok 2.9867 (3.1941)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][670/7762]	Time 0.585 (0.326)	Data 1.43e-04 (5.95e-04)	Tok/s 51608 (42928)	Loss/tok 3.4641 (3.1941)	LR 1.250e-04
0: TRAIN [3][680/7762]	Time 0.265 (0.326)	Data 1.07e-04 (5.87e-04)	Tok/s 39197 (42939)	Loss/tok 3.1087 (3.1940)	LR 1.250e-04
0: TRAIN [3][690/7762]	Time 0.265 (0.327)	Data 1.03e-04 (5.80e-04)	Tok/s 38677 (42973)	Loss/tok 2.9932 (3.1957)	LR 1.250e-04
0: TRAIN [3][700/7762]	Time 0.344 (0.327)	Data 1.03e-04 (5.74e-04)	Tok/s 48501 (42988)	Loss/tok 3.1715 (3.1958)	LR 1.250e-04
0: TRAIN [3][710/7762]	Time 0.266 (0.327)	Data 1.03e-04 (5.67e-04)	Tok/s 39012 (42994)	Loss/tok 3.0144 (3.1952)	LR 1.250e-04
0: TRAIN [3][720/7762]	Time 0.365 (0.327)	Data 1.01e-04 (5.61e-04)	Tok/s 46405 (42982)	Loss/tok 3.2135 (3.1940)	LR 1.250e-04
0: TRAIN [3][730/7762]	Time 0.458 (0.327)	Data 1.25e-04 (5.55e-04)	Tok/s 50989 (42990)	Loss/tok 3.3197 (3.1947)	LR 1.250e-04
0: TRAIN [3][740/7762]	Time 0.263 (0.327)	Data 1.02e-04 (5.49e-04)	Tok/s 39520 (42954)	Loss/tok 2.9886 (3.1935)	LR 1.250e-04
0: TRAIN [3][750/7762]	Time 0.359 (0.327)	Data 1.05e-04 (5.43e-04)	Tok/s 45977 (42958)	Loss/tok 3.1874 (3.1934)	LR 1.250e-04
0: TRAIN [3][760/7762]	Time 0.266 (0.327)	Data 1.08e-04 (5.37e-04)	Tok/s 38860 (42972)	Loss/tok 3.0511 (3.1932)	LR 1.250e-04
0: TRAIN [3][770/7762]	Time 0.351 (0.327)	Data 1.02e-04 (5.31e-04)	Tok/s 48097 (42992)	Loss/tok 3.2833 (3.1934)	LR 1.250e-04
0: TRAIN [3][780/7762]	Time 0.265 (0.326)	Data 1.03e-04 (5.26e-04)	Tok/s 38917 (42963)	Loss/tok 3.0450 (3.1920)	LR 1.250e-04
0: TRAIN [3][790/7762]	Time 0.268 (0.325)	Data 1.04e-04 (5.20e-04)	Tok/s 38351 (42892)	Loss/tok 3.0559 (3.1902)	LR 1.250e-04
0: TRAIN [3][800/7762]	Time 0.267 (0.326)	Data 1.08e-04 (5.15e-04)	Tok/s 39101 (42924)	Loss/tok 2.8950 (3.1907)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][810/7762]	Time 0.259 (0.326)	Data 1.08e-04 (5.10e-04)	Tok/s 39007 (42941)	Loss/tok 2.9905 (3.1921)	LR 1.250e-04
0: TRAIN [3][820/7762]	Time 0.361 (0.327)	Data 1.04e-04 (5.05e-04)	Tok/s 46833 (42993)	Loss/tok 3.2058 (3.1928)	LR 1.250e-04
0: TRAIN [3][830/7762]	Time 0.348 (0.327)	Data 1.04e-04 (5.00e-04)	Tok/s 48881 (43002)	Loss/tok 3.1976 (3.1923)	LR 1.250e-04
0: TRAIN [3][840/7762]	Time 0.268 (0.327)	Data 1.01e-04 (4.96e-04)	Tok/s 38043 (43008)	Loss/tok 2.8921 (3.1927)	LR 1.250e-04
0: TRAIN [3][850/7762]	Time 0.440 (0.327)	Data 9.97e-05 (4.91e-04)	Tok/s 53957 (43012)	Loss/tok 3.3149 (3.1925)	LR 1.250e-04
0: TRAIN [3][860/7762]	Time 0.452 (0.327)	Data 1.09e-04 (4.87e-04)	Tok/s 51834 (42993)	Loss/tok 3.2875 (3.1914)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][870/7762]	Time 0.451 (0.328)	Data 1.06e-04 (4.82e-04)	Tok/s 52320 (43051)	Loss/tok 3.3345 (3.1950)	LR 1.250e-04
0: TRAIN [3][880/7762]	Time 0.562 (0.328)	Data 1.01e-04 (4.78e-04)	Tok/s 52987 (43028)	Loss/tok 3.5465 (3.1943)	LR 1.250e-04
0: TRAIN [3][890/7762]	Time 0.365 (0.328)	Data 1.95e-04 (4.74e-04)	Tok/s 45894 (43046)	Loss/tok 3.2670 (3.1953)	LR 1.250e-04
0: TRAIN [3][900/7762]	Time 0.178 (0.327)	Data 1.01e-04 (4.70e-04)	Tok/s 29420 (43011)	Loss/tok 2.5814 (3.1945)	LR 1.250e-04
0: TRAIN [3][910/7762]	Time 0.451 (0.327)	Data 9.99e-05 (4.66e-04)	Tok/s 51766 (42961)	Loss/tok 3.3685 (3.1932)	LR 1.250e-04
0: TRAIN [3][920/7762]	Time 0.261 (0.327)	Data 1.02e-04 (4.62e-04)	Tok/s 39277 (42989)	Loss/tok 2.9636 (3.1930)	LR 1.250e-04
0: TRAIN [3][930/7762]	Time 0.550 (0.327)	Data 1.01e-04 (4.58e-04)	Tok/s 54082 (43001)	Loss/tok 3.5397 (3.1935)	LR 1.250e-04
0: TRAIN [3][940/7762]	Time 0.342 (0.327)	Data 1.01e-04 (4.54e-04)	Tok/s 49096 (42995)	Loss/tok 3.1522 (3.1935)	LR 1.250e-04
0: TRAIN [3][950/7762]	Time 0.264 (0.327)	Data 1.08e-04 (4.51e-04)	Tok/s 38629 (42972)	Loss/tok 3.0304 (3.1926)	LR 1.250e-04
0: TRAIN [3][960/7762]	Time 0.262 (0.327)	Data 1.09e-04 (4.47e-04)	Tok/s 39248 (42992)	Loss/tok 2.9232 (3.1941)	LR 1.250e-04
0: TRAIN [3][970/7762]	Time 0.362 (0.327)	Data 1.01e-04 (4.44e-04)	Tok/s 46017 (42989)	Loss/tok 3.1138 (3.1937)	LR 1.250e-04
0: TRAIN [3][980/7762]	Time 0.260 (0.326)	Data 1.15e-04 (4.40e-04)	Tok/s 38840 (42961)	Loss/tok 3.0424 (3.1930)	LR 1.250e-04
0: TRAIN [3][990/7762]	Time 0.263 (0.326)	Data 1.02e-04 (4.37e-04)	Tok/s 39449 (42936)	Loss/tok 2.9222 (3.1918)	LR 1.250e-04
0: TRAIN [3][1000/7762]	Time 0.264 (0.326)	Data 1.01e-04 (4.34e-04)	Tok/s 38234 (42966)	Loss/tok 3.0310 (3.1929)	LR 1.250e-04
0: TRAIN [3][1010/7762]	Time 0.558 (0.327)	Data 1.04e-04 (4.30e-04)	Tok/s 53017 (42981)	Loss/tok 3.5470 (3.1936)	LR 1.250e-04
0: TRAIN [3][1020/7762]	Time 0.262 (0.326)	Data 1.06e-04 (4.27e-04)	Tok/s 38869 (42957)	Loss/tok 2.9057 (3.1939)	LR 1.250e-04
0: TRAIN [3][1030/7762]	Time 0.268 (0.327)	Data 1.03e-04 (4.24e-04)	Tok/s 37937 (42990)	Loss/tok 2.9621 (3.1942)	LR 1.250e-04
0: TRAIN [3][1040/7762]	Time 0.364 (0.327)	Data 1.03e-04 (4.21e-04)	Tok/s 45713 (42983)	Loss/tok 3.2726 (3.1938)	LR 1.250e-04
0: TRAIN [3][1050/7762]	Time 0.446 (0.327)	Data 1.07e-04 (4.18e-04)	Tok/s 51930 (42994)	Loss/tok 3.2813 (3.1943)	LR 1.250e-04
0: TRAIN [3][1060/7762]	Time 0.460 (0.327)	Data 1.29e-04 (4.15e-04)	Tok/s 50474 (43004)	Loss/tok 3.4074 (3.1950)	LR 1.250e-04
0: TRAIN [3][1070/7762]	Time 0.363 (0.327)	Data 9.89e-05 (4.12e-04)	Tok/s 46062 (43024)	Loss/tok 3.1359 (3.1949)	LR 1.250e-04
0: TRAIN [3][1080/7762]	Time 0.268 (0.327)	Data 1.09e-04 (4.09e-04)	Tok/s 37966 (43017)	Loss/tok 3.1372 (3.1953)	LR 1.250e-04
0: TRAIN [3][1090/7762]	Time 0.177 (0.327)	Data 1.02e-04 (4.07e-04)	Tok/s 29963 (43013)	Loss/tok 2.6148 (3.1954)	LR 1.250e-04
0: TRAIN [3][1100/7762]	Time 0.589 (0.327)	Data 1.11e-04 (4.04e-04)	Tok/s 50997 (43001)	Loss/tok 3.4424 (3.1956)	LR 1.250e-04
0: TRAIN [3][1110/7762]	Time 0.455 (0.327)	Data 1.04e-04 (4.01e-04)	Tok/s 51226 (43008)	Loss/tok 3.2432 (3.1959)	LR 1.250e-04
0: TRAIN [3][1120/7762]	Time 0.462 (0.327)	Data 1.08e-04 (3.98e-04)	Tok/s 50902 (43003)	Loss/tok 3.3503 (3.1958)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][1130/7762]	Time 0.354 (0.328)	Data 1.19e-04 (3.96e-04)	Tok/s 47115 (43024)	Loss/tok 3.2632 (3.1960)	LR 1.250e-04
0: TRAIN [3][1140/7762]	Time 0.362 (0.328)	Data 1.14e-04 (3.93e-04)	Tok/s 46442 (43052)	Loss/tok 3.2557 (3.1967)	LR 1.250e-04
0: TRAIN [3][1150/7762]	Time 0.351 (0.328)	Data 1.01e-04 (3.91e-04)	Tok/s 47470 (43057)	Loss/tok 3.1728 (3.1967)	LR 1.250e-04
0: TRAIN [3][1160/7762]	Time 0.440 (0.328)	Data 1.04e-04 (3.88e-04)	Tok/s 52882 (43084)	Loss/tok 3.4943 (3.1976)	LR 1.250e-04
0: TRAIN [3][1170/7762]	Time 0.174 (0.328)	Data 1.07e-04 (3.86e-04)	Tok/s 30847 (43093)	Loss/tok 2.5705 (3.1979)	LR 1.250e-04
0: TRAIN [3][1180/7762]	Time 0.347 (0.329)	Data 1.01e-04 (3.84e-04)	Tok/s 48975 (43116)	Loss/tok 3.1101 (3.1983)	LR 1.250e-04
0: TRAIN [3][1190/7762]	Time 0.176 (0.328)	Data 9.87e-05 (3.81e-04)	Tok/s 30052 (43088)	Loss/tok 2.5184 (3.1977)	LR 1.250e-04
0: TRAIN [3][1200/7762]	Time 0.176 (0.328)	Data 1.03e-04 (3.79e-04)	Tok/s 30139 (43080)	Loss/tok 2.5970 (3.1971)	LR 1.250e-04
0: TRAIN [3][1210/7762]	Time 0.363 (0.328)	Data 1.08e-04 (3.77e-04)	Tok/s 46076 (43076)	Loss/tok 3.2919 (3.1967)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1220/7762]	Time 0.450 (0.329)	Data 1.05e-04 (3.74e-04)	Tok/s 52104 (43109)	Loss/tok 3.3749 (3.1976)	LR 1.250e-04
0: TRAIN [3][1230/7762]	Time 0.263 (0.328)	Data 1.07e-04 (3.72e-04)	Tok/s 38775 (43086)	Loss/tok 3.1029 (3.1975)	LR 1.250e-04
0: TRAIN [3][1240/7762]	Time 0.259 (0.329)	Data 1.12e-04 (3.70e-04)	Tok/s 39493 (43094)	Loss/tok 3.0023 (3.1985)	LR 1.250e-04
0: TRAIN [3][1250/7762]	Time 0.363 (0.328)	Data 9.66e-05 (3.68e-04)	Tok/s 45833 (43087)	Loss/tok 3.2124 (3.1980)	LR 1.250e-04
0: TRAIN [3][1260/7762]	Time 0.172 (0.328)	Data 1.04e-04 (3.66e-04)	Tok/s 30532 (43078)	Loss/tok 2.6219 (3.1975)	LR 1.250e-04
0: TRAIN [3][1270/7762]	Time 0.353 (0.328)	Data 1.04e-04 (3.64e-04)	Tok/s 47739 (43072)	Loss/tok 3.0889 (3.1966)	LR 1.250e-04
0: TRAIN [3][1280/7762]	Time 0.265 (0.328)	Data 1.01e-04 (3.62e-04)	Tok/s 38840 (43088)	Loss/tok 3.0084 (3.1971)	LR 1.250e-04
0: TRAIN [3][1290/7762]	Time 0.175 (0.328)	Data 9.87e-05 (3.60e-04)	Tok/s 29816 (43082)	Loss/tok 2.6015 (3.1973)	LR 1.250e-04
0: TRAIN [3][1300/7762]	Time 0.175 (0.327)	Data 1.07e-04 (3.58e-04)	Tok/s 31127 (43044)	Loss/tok 2.6734 (3.1962)	LR 1.250e-04
0: TRAIN [3][1310/7762]	Time 0.358 (0.327)	Data 1.02e-04 (3.56e-04)	Tok/s 46690 (43049)	Loss/tok 3.2564 (3.1963)	LR 1.250e-04
0: TRAIN [3][1320/7762]	Time 0.175 (0.327)	Data 1.04e-04 (3.54e-04)	Tok/s 30867 (43025)	Loss/tok 2.5917 (3.1954)	LR 1.250e-04
0: TRAIN [3][1330/7762]	Time 0.561 (0.327)	Data 9.94e-05 (3.52e-04)	Tok/s 53519 (42993)	Loss/tok 3.5655 (3.1949)	LR 1.250e-04
0: TRAIN [3][1340/7762]	Time 0.359 (0.327)	Data 1.06e-04 (3.50e-04)	Tok/s 46948 (42990)	Loss/tok 3.2335 (3.1957)	LR 1.250e-04
0: TRAIN [3][1350/7762]	Time 0.264 (0.327)	Data 1.06e-04 (3.49e-04)	Tok/s 39988 (43019)	Loss/tok 3.1516 (3.1971)	LR 1.250e-04
0: TRAIN [3][1360/7762]	Time 0.262 (0.328)	Data 1.02e-04 (3.47e-04)	Tok/s 39320 (43033)	Loss/tok 2.9971 (3.1981)	LR 1.250e-04
0: TRAIN [3][1370/7762]	Time 0.269 (0.328)	Data 1.08e-04 (3.45e-04)	Tok/s 38953 (43036)	Loss/tok 2.9525 (3.1978)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1380/7762]	Time 0.254 (0.328)	Data 1.03e-04 (3.43e-04)	Tok/s 40942 (43029)	Loss/tok 3.0297 (3.1975)	LR 1.250e-04
0: TRAIN [3][1390/7762]	Time 0.172 (0.328)	Data 1.03e-04 (3.42e-04)	Tok/s 31064 (43031)	Loss/tok 2.5030 (3.1986)	LR 1.250e-04
0: TRAIN [3][1400/7762]	Time 0.266 (0.328)	Data 1.14e-04 (3.40e-04)	Tok/s 38570 (43030)	Loss/tok 3.0306 (3.1985)	LR 1.250e-04
0: TRAIN [3][1410/7762]	Time 0.464 (0.328)	Data 1.07e-04 (3.38e-04)	Tok/s 50171 (43040)	Loss/tok 3.3814 (3.1990)	LR 1.250e-04
0: TRAIN [3][1420/7762]	Time 0.450 (0.328)	Data 1.23e-04 (3.37e-04)	Tok/s 50976 (43052)	Loss/tok 3.4490 (3.1996)	LR 1.250e-04
0: TRAIN [3][1430/7762]	Time 0.265 (0.328)	Data 1.06e-04 (3.35e-04)	Tok/s 39206 (43033)	Loss/tok 3.0180 (3.1990)	LR 1.250e-04
0: TRAIN [3][1440/7762]	Time 0.265 (0.328)	Data 1.02e-04 (3.34e-04)	Tok/s 38610 (43042)	Loss/tok 3.2019 (3.1992)	LR 1.250e-04
0: TRAIN [3][1450/7762]	Time 0.257 (0.327)	Data 9.94e-05 (3.32e-04)	Tok/s 39535 (43002)	Loss/tok 3.0747 (3.1983)	LR 1.250e-04
0: TRAIN [3][1460/7762]	Time 0.266 (0.327)	Data 1.05e-04 (3.31e-04)	Tok/s 39215 (42998)	Loss/tok 2.9908 (3.1984)	LR 1.250e-04
0: TRAIN [3][1470/7762]	Time 0.569 (0.328)	Data 1.16e-04 (3.29e-04)	Tok/s 52143 (43008)	Loss/tok 3.5870 (3.1990)	LR 1.250e-04
0: TRAIN [3][1480/7762]	Time 0.460 (0.328)	Data 1.05e-04 (3.28e-04)	Tok/s 51089 (43017)	Loss/tok 3.3289 (3.1988)	LR 1.250e-04
0: TRAIN [3][1490/7762]	Time 0.361 (0.328)	Data 1.20e-04 (3.26e-04)	Tok/s 45898 (43025)	Loss/tok 3.1222 (3.1987)	LR 1.250e-04
0: TRAIN [3][1500/7762]	Time 0.266 (0.327)	Data 1.08e-04 (3.25e-04)	Tok/s 38799 (42996)	Loss/tok 2.9074 (3.1978)	LR 1.250e-04
0: TRAIN [3][1510/7762]	Time 0.353 (0.327)	Data 1.07e-04 (3.23e-04)	Tok/s 47053 (42986)	Loss/tok 3.0770 (3.1970)	LR 1.250e-04
0: TRAIN [3][1520/7762]	Time 0.364 (0.327)	Data 1.04e-04 (3.22e-04)	Tok/s 46434 (42965)	Loss/tok 3.2319 (3.1962)	LR 1.250e-04
0: TRAIN [3][1530/7762]	Time 0.459 (0.327)	Data 1.05e-04 (3.20e-04)	Tok/s 49873 (42960)	Loss/tok 3.3600 (3.1958)	LR 1.250e-04
0: TRAIN [3][1540/7762]	Time 0.269 (0.327)	Data 1.07e-04 (3.19e-04)	Tok/s 37765 (42980)	Loss/tok 2.9941 (3.1968)	LR 1.250e-04
0: TRAIN [3][1550/7762]	Time 0.450 (0.327)	Data 1.02e-04 (3.18e-04)	Tok/s 52542 (42999)	Loss/tok 3.3681 (3.1973)	LR 1.250e-04
0: TRAIN [3][1560/7762]	Time 0.262 (0.327)	Data 1.02e-04 (3.16e-04)	Tok/s 41030 (43005)	Loss/tok 3.0422 (3.1973)	LR 1.250e-04
0: TRAIN [3][1570/7762]	Time 0.175 (0.327)	Data 1.05e-04 (3.15e-04)	Tok/s 30822 (43002)	Loss/tok 2.6677 (3.1971)	LR 1.250e-04
0: TRAIN [3][1580/7762]	Time 0.254 (0.327)	Data 9.78e-05 (3.14e-04)	Tok/s 40658 (43000)	Loss/tok 2.9931 (3.1975)	LR 1.250e-04
0: TRAIN [3][1590/7762]	Time 0.253 (0.327)	Data 9.70e-05 (3.12e-04)	Tok/s 41605 (42999)	Loss/tok 3.0017 (3.1974)	LR 1.250e-04
0: TRAIN [3][1600/7762]	Time 0.260 (0.327)	Data 1.01e-04 (3.11e-04)	Tok/s 39371 (42979)	Loss/tok 2.9627 (3.1967)	LR 1.250e-04
0: TRAIN [3][1610/7762]	Time 0.260 (0.327)	Data 1.04e-04 (3.10e-04)	Tok/s 39886 (42974)	Loss/tok 3.0035 (3.1968)	LR 1.250e-04
0: TRAIN [3][1620/7762]	Time 0.364 (0.327)	Data 1.05e-04 (3.08e-04)	Tok/s 46505 (42981)	Loss/tok 3.1359 (3.1966)	LR 1.250e-04
0: TRAIN [3][1630/7762]	Time 0.176 (0.327)	Data 1.05e-04 (3.07e-04)	Tok/s 29635 (42975)	Loss/tok 2.5518 (3.1967)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][1640/7762]	Time 0.438 (0.327)	Data 9.92e-05 (3.06e-04)	Tok/s 52437 (43000)	Loss/tok 3.3487 (3.1973)	LR 1.250e-04
0: TRAIN [3][1650/7762]	Time 0.260 (0.327)	Data 1.03e-04 (3.05e-04)	Tok/s 39755 (43018)	Loss/tok 3.1952 (3.1980)	LR 1.250e-04
0: TRAIN [3][1660/7762]	Time 0.178 (0.327)	Data 1.03e-04 (3.03e-04)	Tok/s 29713 (43019)	Loss/tok 2.5725 (3.1983)	LR 1.250e-04
0: TRAIN [3][1670/7762]	Time 0.358 (0.327)	Data 1.04e-04 (3.02e-04)	Tok/s 47068 (43012)	Loss/tok 3.1225 (3.1977)	LR 1.250e-04
0: TRAIN [3][1680/7762]	Time 0.460 (0.327)	Data 1.06e-04 (3.01e-04)	Tok/s 50875 (43023)	Loss/tok 3.3394 (3.1977)	LR 1.250e-04
0: TRAIN [3][1690/7762]	Time 0.258 (0.327)	Data 1.03e-04 (3.00e-04)	Tok/s 39979 (43033)	Loss/tok 2.9487 (3.1983)	LR 1.250e-04
0: TRAIN [3][1700/7762]	Time 0.344 (0.327)	Data 1.03e-04 (2.99e-04)	Tok/s 48692 (43040)	Loss/tok 3.1421 (3.1983)	LR 1.250e-04
0: TRAIN [3][1710/7762]	Time 0.256 (0.327)	Data 1.05e-04 (2.98e-04)	Tok/s 40640 (43040)	Loss/tok 2.9370 (3.1979)	LR 1.250e-04
0: TRAIN [3][1720/7762]	Time 0.262 (0.327)	Data 9.99e-05 (2.96e-04)	Tok/s 38842 (43031)	Loss/tok 3.0426 (3.1974)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1730/7762]	Time 0.178 (0.327)	Data 9.78e-05 (2.95e-04)	Tok/s 29696 (43022)	Loss/tok 2.6849 (3.1974)	LR 1.250e-04
0: TRAIN [3][1740/7762]	Time 0.171 (0.327)	Data 1.06e-04 (2.94e-04)	Tok/s 30675 (43023)	Loss/tok 2.6413 (3.1973)	LR 1.250e-04
0: TRAIN [3][1750/7762]	Time 0.266 (0.327)	Data 1.04e-04 (2.93e-04)	Tok/s 38165 (43017)	Loss/tok 2.9275 (3.1970)	LR 1.250e-04
0: TRAIN [3][1760/7762]	Time 0.445 (0.327)	Data 9.42e-05 (2.92e-04)	Tok/s 52981 (43000)	Loss/tok 3.2815 (3.1973)	LR 1.250e-04
0: TRAIN [3][1770/7762]	Time 0.260 (0.327)	Data 1.04e-04 (2.91e-04)	Tok/s 40494 (42995)	Loss/tok 2.9542 (3.1975)	LR 1.250e-04
0: TRAIN [3][1780/7762]	Time 0.366 (0.327)	Data 9.61e-05 (2.90e-04)	Tok/s 45838 (42992)	Loss/tok 3.0870 (3.1973)	LR 1.250e-04
0: TRAIN [3][1790/7762]	Time 0.352 (0.327)	Data 1.00e-04 (2.89e-04)	Tok/s 48368 (42997)	Loss/tok 3.3387 (3.1975)	LR 1.250e-04
0: TRAIN [3][1800/7762]	Time 0.259 (0.327)	Data 1.17e-04 (2.88e-04)	Tok/s 39946 (43013)	Loss/tok 2.9994 (3.1987)	LR 1.250e-04
0: TRAIN [3][1810/7762]	Time 0.262 (0.327)	Data 1.01e-04 (2.87e-04)	Tok/s 40281 (43007)	Loss/tok 2.9354 (3.1986)	LR 1.250e-04
0: TRAIN [3][1820/7762]	Time 0.265 (0.327)	Data 1.02e-04 (2.86e-04)	Tok/s 39091 (43001)	Loss/tok 3.0338 (3.1984)	LR 1.250e-04
0: TRAIN [3][1830/7762]	Time 0.455 (0.327)	Data 1.01e-04 (2.85e-04)	Tok/s 52114 (43015)	Loss/tok 3.2931 (3.1984)	LR 1.250e-04
0: TRAIN [3][1840/7762]	Time 0.366 (0.327)	Data 1.02e-04 (2.84e-04)	Tok/s 45576 (43040)	Loss/tok 3.1589 (3.1989)	LR 1.250e-04
0: TRAIN [3][1850/7762]	Time 0.262 (0.327)	Data 9.66e-05 (2.83e-04)	Tok/s 39329 (43040)	Loss/tok 3.0673 (3.1993)	LR 1.250e-04
0: TRAIN [3][1860/7762]	Time 0.266 (0.327)	Data 1.29e-04 (2.82e-04)	Tok/s 39341 (43042)	Loss/tok 2.9312 (3.1990)	LR 1.250e-04
0: TRAIN [3][1870/7762]	Time 0.579 (0.328)	Data 9.97e-05 (2.81e-04)	Tok/s 50636 (43051)	Loss/tok 3.5109 (3.1992)	LR 1.250e-04
0: TRAIN [3][1880/7762]	Time 0.178 (0.327)	Data 1.05e-04 (2.80e-04)	Tok/s 29971 (43038)	Loss/tok 2.6443 (3.1987)	LR 1.250e-04
0: TRAIN [3][1890/7762]	Time 0.254 (0.328)	Data 1.19e-04 (2.79e-04)	Tok/s 41570 (43054)	Loss/tok 2.8844 (3.1991)	LR 1.250e-04
0: TRAIN [3][1900/7762]	Time 0.169 (0.327)	Data 1.01e-04 (2.78e-04)	Tok/s 31286 (43044)	Loss/tok 2.6905 (3.1986)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1910/7762]	Time 0.251 (0.327)	Data 1.02e-04 (2.77e-04)	Tok/s 41292 (43022)	Loss/tok 2.9439 (3.1981)	LR 1.250e-04
0: TRAIN [3][1920/7762]	Time 0.462 (0.327)	Data 9.13e-05 (2.76e-04)	Tok/s 50346 (43004)	Loss/tok 3.3229 (3.1976)	LR 1.250e-04
0: TRAIN [3][1930/7762]	Time 0.174 (0.326)	Data 9.89e-05 (2.75e-04)	Tok/s 30045 (42979)	Loss/tok 2.6024 (3.1971)	LR 1.250e-04
0: TRAIN [3][1940/7762]	Time 0.348 (0.326)	Data 1.01e-04 (2.75e-04)	Tok/s 48035 (42987)	Loss/tok 3.2520 (3.1971)	LR 1.250e-04
0: TRAIN [3][1950/7762]	Time 0.359 (0.327)	Data 1.05e-04 (2.74e-04)	Tok/s 47745 (42998)	Loss/tok 3.1021 (3.1971)	LR 1.250e-04
0: TRAIN [3][1960/7762]	Time 0.177 (0.326)	Data 1.51e-04 (2.73e-04)	Tok/s 29724 (42977)	Loss/tok 2.6037 (3.1965)	LR 1.250e-04
0: TRAIN [3][1970/7762]	Time 0.355 (0.326)	Data 1.03e-04 (2.72e-04)	Tok/s 47595 (42986)	Loss/tok 3.1792 (3.1964)	LR 1.250e-04
0: TRAIN [3][1980/7762]	Time 0.264 (0.326)	Data 9.89e-05 (2.71e-04)	Tok/s 38845 (42984)	Loss/tok 2.9761 (3.1966)	LR 1.250e-04
0: TRAIN [3][1990/7762]	Time 0.261 (0.326)	Data 2.38e-04 (2.70e-04)	Tok/s 39587 (42982)	Loss/tok 2.9526 (3.1964)	LR 1.250e-04
0: TRAIN [3][2000/7762]	Time 0.363 (0.326)	Data 1.00e-04 (2.70e-04)	Tok/s 46992 (42971)	Loss/tok 3.1431 (3.1960)	LR 1.250e-04
0: TRAIN [3][2010/7762]	Time 0.264 (0.326)	Data 1.05e-04 (2.69e-04)	Tok/s 38580 (42963)	Loss/tok 3.0310 (3.1959)	LR 1.250e-04
0: TRAIN [3][2020/7762]	Time 0.352 (0.326)	Data 1.00e-04 (2.68e-04)	Tok/s 47471 (42953)	Loss/tok 3.2059 (3.1953)	LR 1.250e-04
0: TRAIN [3][2030/7762]	Time 0.255 (0.326)	Data 1.02e-04 (2.67e-04)	Tok/s 41767 (42952)	Loss/tok 2.9580 (3.1953)	LR 1.250e-04
0: TRAIN [3][2040/7762]	Time 0.262 (0.325)	Data 1.02e-04 (2.66e-04)	Tok/s 38879 (42940)	Loss/tok 2.9885 (3.1948)	LR 1.250e-04
0: TRAIN [3][2050/7762]	Time 0.266 (0.326)	Data 9.97e-05 (2.66e-04)	Tok/s 38689 (42955)	Loss/tok 2.9910 (3.1954)	LR 1.250e-04
0: TRAIN [3][2060/7762]	Time 0.356 (0.326)	Data 9.97e-05 (2.65e-04)	Tok/s 47052 (42959)	Loss/tok 3.1974 (3.1960)	LR 1.250e-04
0: TRAIN [3][2070/7762]	Time 0.178 (0.326)	Data 1.02e-04 (2.64e-04)	Tok/s 29046 (42962)	Loss/tok 2.6086 (3.1959)	LR 1.250e-04
0: TRAIN [3][2080/7762]	Time 0.463 (0.326)	Data 1.17e-04 (2.63e-04)	Tok/s 50774 (42964)	Loss/tok 3.3299 (3.1961)	LR 1.250e-04
0: TRAIN [3][2090/7762]	Time 0.172 (0.326)	Data 1.02e-04 (2.62e-04)	Tok/s 31815 (42969)	Loss/tok 2.5565 (3.1965)	LR 1.250e-04
0: TRAIN [3][2100/7762]	Time 0.591 (0.327)	Data 1.06e-04 (2.62e-04)	Tok/s 50988 (42990)	Loss/tok 3.4510 (3.1974)	LR 1.250e-04
0: TRAIN [3][2110/7762]	Time 0.353 (0.326)	Data 1.03e-04 (2.61e-04)	Tok/s 48056 (42978)	Loss/tok 3.0588 (3.1970)	LR 1.250e-04
0: TRAIN [3][2120/7762]	Time 0.257 (0.326)	Data 1.03e-04 (2.60e-04)	Tok/s 39479 (42975)	Loss/tok 3.0950 (3.1969)	LR 1.250e-04
0: TRAIN [3][2130/7762]	Time 0.361 (0.327)	Data 1.00e-04 (2.59e-04)	Tok/s 45983 (42982)	Loss/tok 3.3706 (3.1971)	LR 1.250e-04
0: TRAIN [3][2140/7762]	Time 0.457 (0.327)	Data 9.99e-05 (2.59e-04)	Tok/s 51136 (42988)	Loss/tok 3.4297 (3.1971)	LR 1.250e-04
0: TRAIN [3][2150/7762]	Time 0.261 (0.327)	Data 9.94e-05 (2.58e-04)	Tok/s 39957 (42980)	Loss/tok 2.9873 (3.1974)	LR 1.250e-04
0: TRAIN [3][2160/7762]	Time 0.172 (0.326)	Data 1.16e-04 (2.57e-04)	Tok/s 30818 (42965)	Loss/tok 2.5619 (3.1969)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][2170/7762]	Time 0.256 (0.326)	Data 1.54e-04 (2.57e-04)	Tok/s 40755 (42967)	Loss/tok 2.9817 (3.1969)	LR 1.250e-04
0: TRAIN [3][2180/7762]	Time 0.465 (0.327)	Data 1.00e-04 (2.56e-04)	Tok/s 50571 (42985)	Loss/tok 3.4013 (3.1976)	LR 1.250e-04
0: TRAIN [3][2190/7762]	Time 0.463 (0.327)	Data 9.87e-05 (2.55e-04)	Tok/s 50352 (42995)	Loss/tok 3.4733 (3.1981)	LR 1.250e-04
0: TRAIN [3][2200/7762]	Time 0.359 (0.327)	Data 9.99e-05 (2.55e-04)	Tok/s 46384 (43001)	Loss/tok 3.1541 (3.1982)	LR 1.250e-04
0: TRAIN [3][2210/7762]	Time 0.358 (0.327)	Data 9.70e-05 (2.54e-04)	Tok/s 47245 (42995)	Loss/tok 3.1423 (3.1980)	LR 1.250e-04
0: TRAIN [3][2220/7762]	Time 0.256 (0.327)	Data 9.82e-05 (2.53e-04)	Tok/s 40593 (42996)	Loss/tok 2.9733 (3.1978)	LR 1.250e-04
0: TRAIN [3][2230/7762]	Time 0.589 (0.327)	Data 1.21e-04 (2.53e-04)	Tok/s 50872 (43002)	Loss/tok 3.5007 (3.1982)	LR 1.250e-04
0: TRAIN [3][2240/7762]	Time 0.345 (0.327)	Data 9.94e-05 (2.52e-04)	Tok/s 49315 (43005)	Loss/tok 3.1254 (3.1978)	LR 1.250e-04
0: TRAIN [3][2250/7762]	Time 0.363 (0.327)	Data 1.01e-04 (2.51e-04)	Tok/s 46079 (43019)	Loss/tok 3.3357 (3.1980)	LR 1.250e-04
0: TRAIN [3][2260/7762]	Time 0.177 (0.327)	Data 1.02e-04 (2.51e-04)	Tok/s 29506 (43003)	Loss/tok 2.5989 (3.1976)	LR 1.250e-04
0: TRAIN [3][2270/7762]	Time 0.259 (0.327)	Data 1.13e-04 (2.50e-04)	Tok/s 40186 (42992)	Loss/tok 3.0081 (3.1970)	LR 1.250e-04
0: TRAIN [3][2280/7762]	Time 0.264 (0.327)	Data 9.99e-05 (2.49e-04)	Tok/s 39676 (43005)	Loss/tok 2.9272 (3.1974)	LR 1.250e-04
0: TRAIN [3][2290/7762]	Time 0.258 (0.327)	Data 9.94e-05 (2.49e-04)	Tok/s 41016 (43004)	Loss/tok 2.9925 (3.1974)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][2300/7762]	Time 0.261 (0.327)	Data 1.04e-04 (2.48e-04)	Tok/s 39160 (43000)	Loss/tok 3.0787 (3.1973)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2310/7762]	Time 0.349 (0.327)	Data 9.92e-05 (2.47e-04)	Tok/s 48568 (42997)	Loss/tok 3.1002 (3.1969)	LR 1.250e-04
0: TRAIN [3][2320/7762]	Time 0.256 (0.326)	Data 1.03e-04 (2.47e-04)	Tok/s 40194 (42984)	Loss/tok 3.0349 (3.1964)	LR 1.250e-04
0: TRAIN [3][2330/7762]	Time 0.259 (0.327)	Data 9.87e-05 (2.46e-04)	Tok/s 39902 (42993)	Loss/tok 3.0195 (3.1969)	LR 1.250e-04
0: TRAIN [3][2340/7762]	Time 0.362 (0.327)	Data 1.01e-04 (2.46e-04)	Tok/s 46341 (43000)	Loss/tok 3.1766 (3.1972)	LR 1.250e-04
0: TRAIN [3][2350/7762]	Time 0.462 (0.327)	Data 1.06e-04 (2.45e-04)	Tok/s 49845 (42996)	Loss/tok 3.5330 (3.1972)	LR 1.250e-04
0: TRAIN [3][2360/7762]	Time 0.359 (0.327)	Data 1.04e-04 (2.44e-04)	Tok/s 46358 (42998)	Loss/tok 3.2409 (3.1969)	LR 1.250e-04
0: TRAIN [3][2370/7762]	Time 0.266 (0.326)	Data 1.03e-04 (2.44e-04)	Tok/s 38222 (42992)	Loss/tok 2.9584 (3.1969)	LR 1.250e-04
0: TRAIN [3][2380/7762]	Time 0.175 (0.327)	Data 1.08e-04 (2.43e-04)	Tok/s 29702 (42992)	Loss/tok 2.5861 (3.1972)	LR 1.250e-04
0: TRAIN [3][2390/7762]	Time 0.364 (0.327)	Data 1.01e-04 (2.43e-04)	Tok/s 46334 (42991)	Loss/tok 3.1633 (3.1974)	LR 1.250e-04
0: TRAIN [3][2400/7762]	Time 0.456 (0.327)	Data 1.07e-04 (2.42e-04)	Tok/s 50990 (42997)	Loss/tok 3.4442 (3.1979)	LR 1.250e-04
0: TRAIN [3][2410/7762]	Time 0.368 (0.327)	Data 1.01e-04 (2.41e-04)	Tok/s 45830 (43016)	Loss/tok 3.1876 (3.1983)	LR 1.250e-04
0: TRAIN [3][2420/7762]	Time 0.363 (0.327)	Data 9.70e-05 (2.41e-04)	Tok/s 45859 (43019)	Loss/tok 3.2773 (3.1982)	LR 1.250e-04
0: TRAIN [3][2430/7762]	Time 0.357 (0.327)	Data 1.06e-04 (2.40e-04)	Tok/s 47609 (43034)	Loss/tok 3.2686 (3.1983)	LR 1.250e-04
0: TRAIN [3][2440/7762]	Time 0.367 (0.327)	Data 1.15e-04 (2.40e-04)	Tok/s 46105 (43041)	Loss/tok 3.2206 (3.1984)	LR 1.250e-04
0: TRAIN [3][2450/7762]	Time 0.364 (0.327)	Data 1.01e-04 (2.39e-04)	Tok/s 45941 (43045)	Loss/tok 3.2279 (3.1984)	LR 1.250e-04
0: TRAIN [3][2460/7762]	Time 0.266 (0.327)	Data 1.19e-04 (2.39e-04)	Tok/s 39412 (43048)	Loss/tok 2.9997 (3.1982)	LR 1.250e-04
0: TRAIN [3][2470/7762]	Time 0.262 (0.327)	Data 1.20e-04 (2.38e-04)	Tok/s 39949 (43031)	Loss/tok 3.0124 (3.1976)	LR 1.250e-04
0: TRAIN [3][2480/7762]	Time 0.452 (0.327)	Data 1.01e-04 (2.37e-04)	Tok/s 51618 (43038)	Loss/tok 3.4355 (3.1979)	LR 1.250e-04
0: TRAIN [3][2490/7762]	Time 0.270 (0.327)	Data 1.17e-04 (2.37e-04)	Tok/s 37791 (43032)	Loss/tok 3.0094 (3.1976)	LR 1.250e-04
0: TRAIN [3][2500/7762]	Time 0.264 (0.327)	Data 1.05e-04 (2.36e-04)	Tok/s 39275 (43040)	Loss/tok 3.1207 (3.1978)	LR 1.250e-04
0: TRAIN [3][2510/7762]	Time 0.343 (0.327)	Data 1.04e-04 (2.36e-04)	Tok/s 49328 (43040)	Loss/tok 3.1793 (3.1979)	LR 1.250e-04
0: TRAIN [3][2520/7762]	Time 0.265 (0.327)	Data 1.01e-04 (2.35e-04)	Tok/s 39220 (43034)	Loss/tok 3.0355 (3.1979)	LR 1.250e-04
0: TRAIN [3][2530/7762]	Time 0.593 (0.327)	Data 1.02e-04 (2.35e-04)	Tok/s 50032 (43047)	Loss/tok 3.4852 (3.1982)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2540/7762]	Time 0.461 (0.327)	Data 9.80e-05 (2.34e-04)	Tok/s 50470 (43042)	Loss/tok 3.3720 (3.1980)	LR 1.250e-04
0: TRAIN [3][2550/7762]	Time 0.348 (0.327)	Data 1.04e-04 (2.34e-04)	Tok/s 47396 (43053)	Loss/tok 3.2019 (3.1985)	LR 1.250e-04
0: TRAIN [3][2560/7762]	Time 0.363 (0.327)	Data 9.89e-05 (2.33e-04)	Tok/s 46659 (43055)	Loss/tok 3.2256 (3.1982)	LR 1.250e-04
0: TRAIN [3][2570/7762]	Time 0.356 (0.328)	Data 1.02e-04 (2.33e-04)	Tok/s 47336 (43066)	Loss/tok 3.2165 (3.1987)	LR 1.250e-04
0: TRAIN [3][2580/7762]	Time 0.259 (0.327)	Data 9.97e-05 (2.32e-04)	Tok/s 40232 (43053)	Loss/tok 3.0958 (3.1984)	LR 1.250e-04
0: TRAIN [3][2590/7762]	Time 0.264 (0.327)	Data 1.06e-04 (2.32e-04)	Tok/s 38956 (43044)	Loss/tok 2.9063 (3.1985)	LR 1.250e-04
0: TRAIN [3][2600/7762]	Time 0.571 (0.327)	Data 1.01e-04 (2.31e-04)	Tok/s 51695 (43049)	Loss/tok 3.5582 (3.1987)	LR 1.250e-04
0: TRAIN [3][2610/7762]	Time 0.269 (0.327)	Data 1.09e-04 (2.31e-04)	Tok/s 38880 (43049)	Loss/tok 2.9696 (3.1990)	LR 1.250e-04
0: TRAIN [3][2620/7762]	Time 0.363 (0.328)	Data 1.02e-04 (2.30e-04)	Tok/s 46478 (43053)	Loss/tok 3.2313 (3.1994)	LR 1.250e-04
0: TRAIN [3][2630/7762]	Time 0.260 (0.327)	Data 1.01e-04 (2.30e-04)	Tok/s 39547 (43044)	Loss/tok 3.1512 (3.1992)	LR 1.250e-04
0: TRAIN [3][2640/7762]	Time 0.363 (0.328)	Data 1.01e-04 (2.29e-04)	Tok/s 46242 (43048)	Loss/tok 3.2811 (3.1993)	LR 1.250e-04
0: TRAIN [3][2650/7762]	Time 0.260 (0.327)	Data 9.97e-05 (2.29e-04)	Tok/s 39853 (43049)	Loss/tok 3.0187 (3.1990)	LR 1.250e-04
0: TRAIN [3][2660/7762]	Time 0.356 (0.328)	Data 1.05e-04 (2.29e-04)	Tok/s 47272 (43059)	Loss/tok 3.1196 (3.1992)	LR 1.250e-04
0: TRAIN [3][2670/7762]	Time 0.175 (0.328)	Data 1.18e-04 (2.28e-04)	Tok/s 30036 (43060)	Loss/tok 2.5067 (3.1996)	LR 1.250e-04
0: TRAIN [3][2680/7762]	Time 0.261 (0.328)	Data 1.05e-04 (2.28e-04)	Tok/s 39135 (43062)	Loss/tok 3.0392 (3.1995)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2690/7762]	Time 0.353 (0.328)	Data 1.09e-04 (2.27e-04)	Tok/s 47555 (43068)	Loss/tok 3.2805 (3.1996)	LR 1.250e-04
0: TRAIN [3][2700/7762]	Time 0.177 (0.328)	Data 1.05e-04 (2.27e-04)	Tok/s 29376 (43067)	Loss/tok 2.6318 (3.1995)	LR 1.250e-04
0: TRAIN [3][2710/7762]	Time 0.267 (0.328)	Data 9.78e-05 (2.26e-04)	Tok/s 39599 (43064)	Loss/tok 2.9377 (3.1993)	LR 1.250e-04
0: TRAIN [3][2720/7762]	Time 0.352 (0.328)	Data 9.78e-05 (2.26e-04)	Tok/s 47835 (43067)	Loss/tok 3.2012 (3.1993)	LR 1.250e-04
0: TRAIN [3][2730/7762]	Time 0.264 (0.328)	Data 9.97e-05 (2.25e-04)	Tok/s 39836 (43071)	Loss/tok 2.9259 (3.1994)	LR 1.250e-04
0: TRAIN [3][2740/7762]	Time 0.365 (0.328)	Data 9.97e-05 (2.25e-04)	Tok/s 45957 (43066)	Loss/tok 3.3189 (3.1992)	LR 1.250e-04
0: TRAIN [3][2750/7762]	Time 0.269 (0.328)	Data 1.30e-04 (2.24e-04)	Tok/s 38648 (43066)	Loss/tok 3.0745 (3.1991)	LR 1.250e-04
0: TRAIN [3][2760/7762]	Time 0.345 (0.328)	Data 1.02e-04 (2.24e-04)	Tok/s 48184 (43068)	Loss/tok 3.1827 (3.1988)	LR 1.250e-04
0: TRAIN [3][2770/7762]	Time 0.352 (0.328)	Data 9.85e-05 (2.24e-04)	Tok/s 47491 (43076)	Loss/tok 3.1601 (3.1991)	LR 1.250e-04
0: TRAIN [3][2780/7762]	Time 0.343 (0.328)	Data 1.01e-04 (2.23e-04)	Tok/s 49673 (43081)	Loss/tok 3.1726 (3.1991)	LR 1.250e-04
0: TRAIN [3][2790/7762]	Time 0.266 (0.328)	Data 9.99e-05 (2.23e-04)	Tok/s 38682 (43093)	Loss/tok 3.0251 (3.1994)	LR 1.250e-04
0: TRAIN [3][2800/7762]	Time 0.270 (0.328)	Data 1.02e-04 (2.22e-04)	Tok/s 37493 (43086)	Loss/tok 3.0829 (3.1992)	LR 1.250e-04
0: TRAIN [3][2810/7762]	Time 0.355 (0.328)	Data 9.85e-05 (2.22e-04)	Tok/s 47251 (43088)	Loss/tok 3.2524 (3.1994)	LR 1.250e-04
0: TRAIN [3][2820/7762]	Time 0.268 (0.328)	Data 1.15e-04 (2.21e-04)	Tok/s 38232 (43086)	Loss/tok 2.9682 (3.1994)	LR 1.250e-04
0: TRAIN [3][2830/7762]	Time 0.263 (0.328)	Data 9.56e-05 (2.21e-04)	Tok/s 39950 (43094)	Loss/tok 2.9668 (3.1999)	LR 1.250e-04
0: TRAIN [3][2840/7762]	Time 0.266 (0.328)	Data 9.80e-05 (2.21e-04)	Tok/s 39282 (43085)	Loss/tok 3.0383 (3.1998)	LR 1.250e-04
0: TRAIN [3][2850/7762]	Time 0.355 (0.328)	Data 9.80e-05 (2.20e-04)	Tok/s 47780 (43077)	Loss/tok 3.1552 (3.1994)	LR 1.250e-04
0: TRAIN [3][2860/7762]	Time 0.261 (0.328)	Data 1.04e-04 (2.20e-04)	Tok/s 38659 (43072)	Loss/tok 2.9547 (3.1990)	LR 1.250e-04
0: TRAIN [3][2870/7762]	Time 0.176 (0.328)	Data 1.02e-04 (2.19e-04)	Tok/s 29285 (43066)	Loss/tok 2.6515 (3.1989)	LR 1.250e-04
0: TRAIN [3][2880/7762]	Time 0.266 (0.328)	Data 1.19e-04 (2.19e-04)	Tok/s 38703 (43071)	Loss/tok 2.9939 (3.1990)	LR 1.250e-04
0: TRAIN [3][2890/7762]	Time 0.252 (0.328)	Data 1.00e-04 (2.19e-04)	Tok/s 40263 (43072)	Loss/tok 2.9821 (3.1992)	LR 1.250e-04
0: TRAIN [3][2900/7762]	Time 0.263 (0.328)	Data 1.02e-04 (2.18e-04)	Tok/s 39672 (43075)	Loss/tok 3.0046 (3.1992)	LR 1.250e-04
0: TRAIN [3][2910/7762]	Time 0.175 (0.328)	Data 1.03e-04 (2.18e-04)	Tok/s 30570 (43073)	Loss/tok 2.6368 (3.1991)	LR 1.250e-04
0: TRAIN [3][2920/7762]	Time 0.270 (0.328)	Data 1.04e-04 (2.17e-04)	Tok/s 38370 (43070)	Loss/tok 2.9580 (3.1991)	LR 1.250e-04
0: TRAIN [3][2930/7762]	Time 0.365 (0.328)	Data 9.51e-05 (2.17e-04)	Tok/s 46020 (43064)	Loss/tok 3.0858 (3.1992)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2940/7762]	Time 0.460 (0.328)	Data 1.01e-04 (2.17e-04)	Tok/s 50427 (43058)	Loss/tok 3.4127 (3.1992)	LR 1.250e-04
0: TRAIN [3][2950/7762]	Time 0.362 (0.328)	Data 1.14e-04 (2.16e-04)	Tok/s 46892 (43063)	Loss/tok 3.1065 (3.1995)	LR 1.250e-04
0: TRAIN [3][2960/7762]	Time 0.588 (0.328)	Data 1.01e-04 (2.16e-04)	Tok/s 50627 (43072)	Loss/tok 3.5421 (3.2001)	LR 1.250e-04
0: TRAIN [3][2970/7762]	Time 0.269 (0.328)	Data 9.92e-05 (2.15e-04)	Tok/s 38782 (43061)	Loss/tok 3.1511 (3.1998)	LR 1.250e-04
0: TRAIN [3][2980/7762]	Time 0.262 (0.328)	Data 1.11e-04 (2.15e-04)	Tok/s 39469 (43053)	Loss/tok 2.9728 (3.1994)	LR 1.250e-04
0: TRAIN [3][2990/7762]	Time 0.253 (0.328)	Data 1.00e-04 (2.15e-04)	Tok/s 40415 (43048)	Loss/tok 3.0480 (3.1994)	LR 1.250e-04
0: TRAIN [3][3000/7762]	Time 0.261 (0.328)	Data 9.97e-05 (2.14e-04)	Tok/s 38312 (43045)	Loss/tok 3.0491 (3.1992)	LR 1.250e-04
0: TRAIN [3][3010/7762]	Time 0.366 (0.328)	Data 1.02e-04 (2.14e-04)	Tok/s 46072 (43053)	Loss/tok 3.1431 (3.1990)	LR 1.250e-04
0: TRAIN [3][3020/7762]	Time 0.176 (0.328)	Data 1.01e-04 (2.14e-04)	Tok/s 29479 (43050)	Loss/tok 2.6913 (3.1989)	LR 1.250e-04
0: TRAIN [3][3030/7762]	Time 0.359 (0.328)	Data 1.01e-04 (2.13e-04)	Tok/s 47146 (43054)	Loss/tok 3.0854 (3.1989)	LR 1.250e-04
0: TRAIN [3][3040/7762]	Time 0.557 (0.328)	Data 9.87e-05 (2.13e-04)	Tok/s 54278 (43062)	Loss/tok 3.5166 (3.1991)	LR 1.250e-04
0: TRAIN [3][3050/7762]	Time 0.254 (0.328)	Data 1.00e-04 (2.13e-04)	Tok/s 40657 (43063)	Loss/tok 3.0628 (3.1991)	LR 1.250e-04
0: TRAIN [3][3060/7762]	Time 0.267 (0.328)	Data 9.99e-05 (2.12e-04)	Tok/s 38790 (43057)	Loss/tok 3.0385 (3.1990)	LR 1.250e-04
0: TRAIN [3][3070/7762]	Time 0.458 (0.328)	Data 1.03e-04 (2.12e-04)	Tok/s 51442 (43073)	Loss/tok 3.3320 (3.1993)	LR 1.250e-04
0: TRAIN [3][3080/7762]	Time 0.358 (0.328)	Data 1.01e-04 (2.11e-04)	Tok/s 47114 (43068)	Loss/tok 3.2113 (3.1991)	LR 1.250e-04
0: TRAIN [3][3090/7762]	Time 0.359 (0.328)	Data 1.01e-04 (2.11e-04)	Tok/s 46722 (43072)	Loss/tok 3.2991 (3.1991)	LR 1.250e-04
0: TRAIN [3][3100/7762]	Time 0.177 (0.328)	Data 1.00e-04 (2.11e-04)	Tok/s 30474 (43067)	Loss/tok 2.5385 (3.1991)	LR 1.250e-04
0: TRAIN [3][3110/7762]	Time 0.259 (0.328)	Data 9.87e-05 (2.10e-04)	Tok/s 39795 (43066)	Loss/tok 3.0310 (3.1988)	LR 1.250e-04
0: TRAIN [3][3120/7762]	Time 0.364 (0.328)	Data 1.04e-04 (2.10e-04)	Tok/s 46397 (43072)	Loss/tok 3.1814 (3.1990)	LR 1.250e-04
0: TRAIN [3][3130/7762]	Time 0.263 (0.328)	Data 1.01e-04 (2.10e-04)	Tok/s 39316 (43061)	Loss/tok 2.9514 (3.1986)	LR 1.250e-04
0: TRAIN [3][3140/7762]	Time 0.174 (0.327)	Data 9.70e-05 (2.09e-04)	Tok/s 30566 (43054)	Loss/tok 2.5392 (3.1983)	LR 1.250e-04
0: TRAIN [3][3150/7762]	Time 0.264 (0.327)	Data 1.19e-04 (2.09e-04)	Tok/s 39044 (43058)	Loss/tok 2.9601 (3.1984)	LR 1.250e-04
0: TRAIN [3][3160/7762]	Time 0.461 (0.328)	Data 1.04e-04 (2.09e-04)	Tok/s 49971 (43071)	Loss/tok 3.4016 (3.1989)	LR 1.250e-04
0: TRAIN [3][3170/7762]	Time 0.585 (0.328)	Data 1.05e-04 (2.08e-04)	Tok/s 51111 (43078)	Loss/tok 3.5377 (3.1990)	LR 1.250e-04
0: TRAIN [3][3180/7762]	Time 0.366 (0.328)	Data 9.66e-05 (2.08e-04)	Tok/s 45699 (43085)	Loss/tok 3.2094 (3.1991)	LR 1.250e-04
0: TRAIN [3][3190/7762]	Time 0.460 (0.328)	Data 1.21e-04 (2.08e-04)	Tok/s 50319 (43085)	Loss/tok 3.3235 (3.1989)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][3200/7762]	Time 0.594 (0.328)	Data 1.04e-04 (2.07e-04)	Tok/s 49732 (43092)	Loss/tok 3.6045 (3.1995)	LR 1.250e-04
0: TRAIN [3][3210/7762]	Time 0.358 (0.328)	Data 1.00e-04 (2.07e-04)	Tok/s 48106 (43093)	Loss/tok 3.0898 (3.1993)	LR 1.250e-04
0: TRAIN [3][3220/7762]	Time 0.269 (0.328)	Data 1.03e-04 (2.07e-04)	Tok/s 38392 (43104)	Loss/tok 2.8768 (3.1997)	LR 1.250e-04
0: TRAIN [3][3230/7762]	Time 0.362 (0.328)	Data 1.04e-04 (2.06e-04)	Tok/s 47087 (43104)	Loss/tok 3.0529 (3.1996)	LR 1.250e-04
0: TRAIN [3][3240/7762]	Time 0.178 (0.328)	Data 1.10e-04 (2.06e-04)	Tok/s 30062 (43105)	Loss/tok 2.5598 (3.1999)	LR 1.250e-04
0: TRAIN [3][3250/7762]	Time 0.266 (0.328)	Data 9.92e-05 (2.06e-04)	Tok/s 39446 (43097)	Loss/tok 3.0773 (3.1996)	LR 1.250e-04
0: TRAIN [3][3260/7762]	Time 0.437 (0.328)	Data 9.92e-05 (2.06e-04)	Tok/s 52925 (43097)	Loss/tok 3.3351 (3.1994)	LR 1.250e-04
0: TRAIN [3][3270/7762]	Time 0.356 (0.328)	Data 1.04e-04 (2.05e-04)	Tok/s 47404 (43106)	Loss/tok 3.2264 (3.2000)	LR 1.250e-04
0: TRAIN [3][3280/7762]	Time 0.358 (0.328)	Data 1.03e-04 (2.05e-04)	Tok/s 46554 (43104)	Loss/tok 3.1507 (3.1999)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3290/7762]	Time 0.256 (0.328)	Data 1.07e-04 (2.05e-04)	Tok/s 40229 (43106)	Loss/tok 2.9963 (3.1999)	LR 1.250e-04
0: TRAIN [3][3300/7762]	Time 0.350 (0.328)	Data 1.01e-04 (2.04e-04)	Tok/s 48586 (43109)	Loss/tok 3.1668 (3.1999)	LR 1.250e-04
0: TRAIN [3][3310/7762]	Time 0.168 (0.328)	Data 1.01e-04 (2.04e-04)	Tok/s 31655 (43109)	Loss/tok 2.6755 (3.1996)	LR 1.250e-04
0: TRAIN [3][3320/7762]	Time 0.459 (0.328)	Data 1.02e-04 (2.04e-04)	Tok/s 51559 (43122)	Loss/tok 3.3785 (3.2000)	LR 1.250e-04
0: TRAIN [3][3330/7762]	Time 0.357 (0.328)	Data 1.03e-04 (2.03e-04)	Tok/s 46338 (43120)	Loss/tok 3.1600 (3.2000)	LR 1.250e-04
0: TRAIN [3][3340/7762]	Time 0.355 (0.328)	Data 1.20e-04 (2.03e-04)	Tok/s 47059 (43121)	Loss/tok 3.2035 (3.1999)	LR 1.250e-04
0: TRAIN [3][3350/7762]	Time 0.354 (0.329)	Data 1.04e-04 (2.03e-04)	Tok/s 47590 (43132)	Loss/tok 3.0992 (3.2006)	LR 1.250e-04
0: TRAIN [3][3360/7762]	Time 0.364 (0.329)	Data 9.87e-05 (2.03e-04)	Tok/s 46223 (43137)	Loss/tok 3.0569 (3.2005)	LR 1.250e-04
0: TRAIN [3][3370/7762]	Time 0.566 (0.329)	Data 1.03e-04 (2.02e-04)	Tok/s 52360 (43138)	Loss/tok 3.5123 (3.2007)	LR 1.250e-04
0: TRAIN [3][3380/7762]	Time 0.177 (0.329)	Data 9.94e-05 (2.02e-04)	Tok/s 30593 (43132)	Loss/tok 2.5998 (3.2004)	LR 1.250e-04
0: TRAIN [3][3390/7762]	Time 0.262 (0.329)	Data 1.01e-04 (2.02e-04)	Tok/s 39792 (43126)	Loss/tok 3.0025 (3.2001)	LR 1.250e-04
0: TRAIN [3][3400/7762]	Time 0.439 (0.328)	Data 9.80e-05 (2.01e-04)	Tok/s 53232 (43119)	Loss/tok 3.3143 (3.1999)	LR 1.250e-04
0: TRAIN [3][3410/7762]	Time 0.263 (0.328)	Data 1.18e-04 (2.01e-04)	Tok/s 39313 (43114)	Loss/tok 3.0641 (3.1997)	LR 1.250e-04
0: TRAIN [3][3420/7762]	Time 0.357 (0.328)	Data 1.06e-04 (2.01e-04)	Tok/s 46973 (43112)	Loss/tok 3.2785 (3.1995)	LR 1.250e-04
0: TRAIN [3][3430/7762]	Time 0.364 (0.328)	Data 1.05e-04 (2.01e-04)	Tok/s 45745 (43114)	Loss/tok 3.2928 (3.1996)	LR 1.250e-04
0: TRAIN [3][3440/7762]	Time 0.363 (0.328)	Data 9.56e-05 (2.00e-04)	Tok/s 46491 (43107)	Loss/tok 3.2709 (3.1994)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3450/7762]	Time 0.453 (0.328)	Data 9.82e-05 (2.00e-04)	Tok/s 51492 (43114)	Loss/tok 3.2935 (3.1996)	LR 1.250e-04
0: TRAIN [3][3460/7762]	Time 0.264 (0.328)	Data 9.97e-05 (2.00e-04)	Tok/s 38360 (43105)	Loss/tok 3.0196 (3.1993)	LR 1.250e-04
0: TRAIN [3][3470/7762]	Time 0.173 (0.328)	Data 1.01e-04 (1.99e-04)	Tok/s 29984 (43107)	Loss/tok 2.6035 (3.1990)	LR 1.250e-04
0: TRAIN [3][3480/7762]	Time 0.356 (0.328)	Data 1.03e-04 (1.99e-04)	Tok/s 47340 (43110)	Loss/tok 3.1943 (3.1989)	LR 1.250e-04
0: TRAIN [3][3490/7762]	Time 0.365 (0.328)	Data 9.92e-05 (1.99e-04)	Tok/s 45977 (43102)	Loss/tok 3.1518 (3.1988)	LR 1.250e-04
0: TRAIN [3][3500/7762]	Time 0.463 (0.328)	Data 1.01e-04 (1.99e-04)	Tok/s 50125 (43102)	Loss/tok 3.3284 (3.1988)	LR 1.250e-04
0: TRAIN [3][3510/7762]	Time 0.446 (0.328)	Data 1.19e-04 (1.98e-04)	Tok/s 52635 (43107)	Loss/tok 3.3107 (3.1990)	LR 1.250e-04
0: TRAIN [3][3520/7762]	Time 0.172 (0.328)	Data 1.13e-04 (1.98e-04)	Tok/s 30505 (43108)	Loss/tok 2.6974 (3.1990)	LR 1.250e-04
0: TRAIN [3][3530/7762]	Time 0.259 (0.328)	Data 9.61e-05 (1.98e-04)	Tok/s 40113 (43115)	Loss/tok 2.9978 (3.1990)	LR 1.250e-04
0: TRAIN [3][3540/7762]	Time 0.264 (0.328)	Data 1.86e-04 (1.98e-04)	Tok/s 39297 (43116)	Loss/tok 3.0140 (3.1991)	LR 1.250e-04
0: TRAIN [3][3550/7762]	Time 0.362 (0.328)	Data 9.87e-05 (1.97e-04)	Tok/s 46626 (43107)	Loss/tok 3.2518 (3.1988)	LR 1.250e-04
0: TRAIN [3][3560/7762]	Time 0.453 (0.328)	Data 1.02e-04 (1.97e-04)	Tok/s 51963 (43112)	Loss/tok 3.2976 (3.1987)	LR 1.250e-04
0: TRAIN [3][3570/7762]	Time 0.262 (0.328)	Data 1.07e-04 (1.97e-04)	Tok/s 39627 (43115)	Loss/tok 2.8782 (3.1991)	LR 1.250e-04
0: TRAIN [3][3580/7762]	Time 0.460 (0.328)	Data 9.89e-05 (1.96e-04)	Tok/s 50673 (43115)	Loss/tok 3.3967 (3.1992)	LR 1.250e-04
0: TRAIN [3][3590/7762]	Time 0.266 (0.328)	Data 9.94e-05 (1.96e-04)	Tok/s 38641 (43110)	Loss/tok 3.0797 (3.1989)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3600/7762]	Time 0.357 (0.328)	Data 9.75e-05 (1.96e-04)	Tok/s 47885 (43119)	Loss/tok 3.1817 (3.1991)	LR 1.250e-04
0: TRAIN [3][3610/7762]	Time 0.358 (0.328)	Data 1.08e-04 (1.96e-04)	Tok/s 47033 (43114)	Loss/tok 3.2205 (3.1990)	LR 1.250e-04
0: TRAIN [3][3620/7762]	Time 0.364 (0.328)	Data 1.10e-04 (1.95e-04)	Tok/s 46228 (43126)	Loss/tok 3.1193 (3.1993)	LR 1.250e-04
0: TRAIN [3][3630/7762]	Time 0.255 (0.328)	Data 1.08e-04 (1.95e-04)	Tok/s 41363 (43136)	Loss/tok 3.0523 (3.1996)	LR 1.250e-04
0: TRAIN [3][3640/7762]	Time 0.452 (0.329)	Data 1.04e-04 (1.95e-04)	Tok/s 51704 (43141)	Loss/tok 3.3810 (3.1999)	LR 1.250e-04
0: TRAIN [3][3650/7762]	Time 0.360 (0.329)	Data 1.00e-04 (1.95e-04)	Tok/s 47115 (43139)	Loss/tok 3.2007 (3.1997)	LR 1.250e-04
0: TRAIN [3][3660/7762]	Time 0.355 (0.329)	Data 1.02e-04 (1.94e-04)	Tok/s 46442 (43147)	Loss/tok 3.2228 (3.2003)	LR 1.250e-04
0: TRAIN [3][3670/7762]	Time 0.264 (0.329)	Data 1.19e-04 (1.94e-04)	Tok/s 38478 (43145)	Loss/tok 2.8848 (3.2001)	LR 1.250e-04
0: TRAIN [3][3680/7762]	Time 0.590 (0.329)	Data 1.06e-04 (1.94e-04)	Tok/s 49703 (43151)	Loss/tok 3.5167 (3.2004)	LR 1.250e-04
0: TRAIN [3][3690/7762]	Time 0.261 (0.329)	Data 1.00e-04 (1.94e-04)	Tok/s 39162 (43150)	Loss/tok 2.9579 (3.2003)	LR 1.250e-04
0: TRAIN [3][3700/7762]	Time 0.266 (0.329)	Data 1.06e-04 (1.93e-04)	Tok/s 38374 (43155)	Loss/tok 3.0017 (3.2004)	LR 1.250e-04
0: TRAIN [3][3710/7762]	Time 0.264 (0.329)	Data 9.97e-05 (1.93e-04)	Tok/s 39754 (43150)	Loss/tok 3.0447 (3.2003)	LR 1.250e-04
0: TRAIN [3][3720/7762]	Time 0.359 (0.329)	Data 1.03e-04 (1.93e-04)	Tok/s 46763 (43151)	Loss/tok 3.2961 (3.2006)	LR 1.250e-04
0: TRAIN [3][3730/7762]	Time 0.465 (0.329)	Data 9.99e-05 (1.93e-04)	Tok/s 49874 (43156)	Loss/tok 3.3366 (3.2007)	LR 1.250e-04
0: TRAIN [3][3740/7762]	Time 0.263 (0.329)	Data 1.22e-04 (1.93e-04)	Tok/s 38971 (43148)	Loss/tok 2.9510 (3.2005)	LR 1.250e-04
0: TRAIN [3][3750/7762]	Time 0.352 (0.329)	Data 1.01e-04 (1.92e-04)	Tok/s 47439 (43146)	Loss/tok 3.1292 (3.2003)	LR 1.250e-04
0: TRAIN [3][3760/7762]	Time 0.363 (0.329)	Data 1.05e-04 (1.92e-04)	Tok/s 45769 (43155)	Loss/tok 3.1806 (3.2007)	LR 1.250e-04
0: TRAIN [3][3770/7762]	Time 0.357 (0.329)	Data 1.01e-04 (1.92e-04)	Tok/s 46710 (43148)	Loss/tok 3.1436 (3.2004)	LR 1.250e-04
0: TRAIN [3][3780/7762]	Time 0.258 (0.329)	Data 9.68e-05 (1.92e-04)	Tok/s 39701 (43141)	Loss/tok 3.0320 (3.2003)	LR 1.250e-04
0: TRAIN [3][3790/7762]	Time 0.362 (0.329)	Data 1.01e-04 (1.91e-04)	Tok/s 46263 (43135)	Loss/tok 3.1508 (3.2000)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3800/7762]	Time 0.259 (0.329)	Data 1.17e-04 (1.91e-04)	Tok/s 39198 (43138)	Loss/tok 2.8504 (3.2002)	LR 1.250e-04
0: TRAIN [3][3810/7762]	Time 0.346 (0.329)	Data 1.07e-04 (1.91e-04)	Tok/s 48360 (43135)	Loss/tok 3.3055 (3.2002)	LR 1.250e-04
0: TRAIN [3][3820/7762]	Time 0.177 (0.328)	Data 1.10e-04 (1.91e-04)	Tok/s 30290 (43128)	Loss/tok 2.7179 (3.2001)	LR 1.250e-04
0: TRAIN [3][3830/7762]	Time 0.267 (0.328)	Data 1.23e-04 (1.90e-04)	Tok/s 39133 (43121)	Loss/tok 2.9218 (3.2000)	LR 1.250e-04
0: TRAIN [3][3840/7762]	Time 0.264 (0.328)	Data 1.00e-04 (1.90e-04)	Tok/s 39427 (43116)	Loss/tok 3.0140 (3.1998)	LR 1.250e-04
0: TRAIN [3][3850/7762]	Time 0.261 (0.328)	Data 9.99e-05 (1.90e-04)	Tok/s 39059 (43103)	Loss/tok 2.9900 (3.1995)	LR 1.250e-04
0: TRAIN [3][3860/7762]	Time 0.460 (0.328)	Data 1.20e-04 (1.90e-04)	Tok/s 50887 (43111)	Loss/tok 3.2078 (3.1995)	LR 1.250e-04
0: TRAIN [3][3870/7762]	Time 0.265 (0.328)	Data 1.04e-04 (1.90e-04)	Tok/s 39698 (43116)	Loss/tok 2.9696 (3.1995)	LR 1.250e-04
0: TRAIN [3][3880/7762]	Time 0.175 (0.328)	Data 1.15e-04 (1.89e-04)	Tok/s 30247 (43113)	Loss/tok 2.5243 (3.1997)	LR 1.250e-04
0: TRAIN [3][3890/7762]	Time 0.250 (0.328)	Data 1.16e-04 (1.89e-04)	Tok/s 41501 (43108)	Loss/tok 3.0643 (3.1995)	LR 1.250e-04
0: TRAIN [3][3900/7762]	Time 0.465 (0.328)	Data 9.66e-05 (1.89e-04)	Tok/s 50190 (43103)	Loss/tok 3.2666 (3.1993)	LR 1.250e-04
0: TRAIN [3][3910/7762]	Time 0.360 (0.328)	Data 9.51e-05 (1.89e-04)	Tok/s 47174 (43102)	Loss/tok 3.1183 (3.1994)	LR 1.250e-04
0: TRAIN [3][3920/7762]	Time 0.557 (0.328)	Data 1.18e-04 (1.89e-04)	Tok/s 53013 (43107)	Loss/tok 3.4915 (3.1998)	LR 1.250e-04
0: TRAIN [3][3930/7762]	Time 0.264 (0.328)	Data 9.99e-05 (1.88e-04)	Tok/s 38788 (43105)	Loss/tok 3.0074 (3.1997)	LR 1.250e-04
0: TRAIN [3][3940/7762]	Time 0.438 (0.328)	Data 9.37e-05 (1.88e-04)	Tok/s 53409 (43107)	Loss/tok 3.2982 (3.1996)	LR 1.250e-04
0: TRAIN [3][3950/7762]	Time 0.267 (0.328)	Data 1.08e-04 (1.88e-04)	Tok/s 38569 (43108)	Loss/tok 2.9096 (3.1996)	LR 1.250e-04
0: TRAIN [3][3960/7762]	Time 0.267 (0.328)	Data 9.49e-05 (1.88e-04)	Tok/s 38325 (43110)	Loss/tok 2.9930 (3.1997)	LR 1.250e-04
0: TRAIN [3][3970/7762]	Time 0.366 (0.328)	Data 1.16e-04 (1.87e-04)	Tok/s 46107 (43110)	Loss/tok 3.2040 (3.1996)	LR 1.250e-04
0: TRAIN [3][3980/7762]	Time 0.265 (0.328)	Data 1.01e-04 (1.87e-04)	Tok/s 39368 (43114)	Loss/tok 3.0086 (3.1997)	LR 1.250e-04
0: TRAIN [3][3990/7762]	Time 0.354 (0.328)	Data 9.63e-05 (1.87e-04)	Tok/s 47552 (43117)	Loss/tok 3.1889 (3.1996)	LR 1.250e-04
0: TRAIN [3][4000/7762]	Time 0.261 (0.328)	Data 9.97e-05 (1.87e-04)	Tok/s 39334 (43117)	Loss/tok 2.9668 (3.1996)	LR 1.250e-04
0: TRAIN [3][4010/7762]	Time 0.263 (0.328)	Data 9.78e-05 (1.87e-04)	Tok/s 39151 (43116)	Loss/tok 3.0469 (3.1995)	LR 1.250e-04
0: TRAIN [3][4020/7762]	Time 0.173 (0.328)	Data 9.89e-05 (1.86e-04)	Tok/s 30014 (43114)	Loss/tok 2.5815 (3.1994)	LR 1.250e-04
0: TRAIN [3][4030/7762]	Time 0.178 (0.328)	Data 1.19e-04 (1.86e-04)	Tok/s 29928 (43106)	Loss/tok 2.5278 (3.1992)	LR 1.250e-04
0: TRAIN [3][4040/7762]	Time 0.365 (0.328)	Data 1.17e-04 (1.86e-04)	Tok/s 45506 (43099)	Loss/tok 3.2545 (3.1989)	LR 1.250e-04
0: TRAIN [3][4050/7762]	Time 0.178 (0.328)	Data 9.94e-05 (1.86e-04)	Tok/s 28557 (43090)	Loss/tok 2.5950 (3.1987)	LR 1.250e-04
0: TRAIN [3][4060/7762]	Time 0.265 (0.328)	Data 1.04e-04 (1.86e-04)	Tok/s 39206 (43089)	Loss/tok 2.9526 (3.1984)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][4070/7762]	Time 0.262 (0.328)	Data 9.70e-05 (1.85e-04)	Tok/s 39387 (43089)	Loss/tok 2.9049 (3.1984)	LR 1.250e-04
0: TRAIN [3][4080/7762]	Time 0.263 (0.328)	Data 9.99e-05 (1.85e-04)	Tok/s 38953 (43083)	Loss/tok 2.9911 (3.1982)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][4090/7762]	Time 0.258 (0.328)	Data 1.03e-04 (1.85e-04)	Tok/s 39390 (43080)	Loss/tok 3.0368 (3.1983)	LR 1.250e-04
0: TRAIN [3][4100/7762]	Time 0.264 (0.327)	Data 9.92e-05 (1.85e-04)	Tok/s 38590 (43075)	Loss/tok 3.1372 (3.1981)	LR 1.250e-04
0: TRAIN [3][4110/7762]	Time 0.264 (0.328)	Data 1.02e-04 (1.85e-04)	Tok/s 38471 (43077)	Loss/tok 2.9351 (3.1981)	LR 1.250e-04
0: TRAIN [3][4120/7762]	Time 0.256 (0.328)	Data 1.00e-04 (1.84e-04)	Tok/s 41541 (43078)	Loss/tok 3.0594 (3.1981)	LR 1.250e-04
0: TRAIN [3][4130/7762]	Time 0.366 (0.328)	Data 1.04e-04 (1.84e-04)	Tok/s 45960 (43086)	Loss/tok 3.1956 (3.1982)	LR 1.250e-04
0: TRAIN [3][4140/7762]	Time 0.452 (0.328)	Data 1.01e-04 (1.84e-04)	Tok/s 50926 (43082)	Loss/tok 3.4329 (3.1983)	LR 1.250e-04
0: TRAIN [3][4150/7762]	Time 0.461 (0.328)	Data 1.00e-04 (1.84e-04)	Tok/s 50354 (43089)	Loss/tok 3.3728 (3.1985)	LR 1.250e-04
0: TRAIN [3][4160/7762]	Time 0.264 (0.328)	Data 9.75e-05 (1.84e-04)	Tok/s 39828 (43081)	Loss/tok 3.0142 (3.1982)	LR 1.250e-04
0: TRAIN [3][4170/7762]	Time 0.359 (0.327)	Data 9.97e-05 (1.83e-04)	Tok/s 46758 (43077)	Loss/tok 3.2091 (3.1981)	LR 1.250e-04
0: TRAIN [3][4180/7762]	Time 0.265 (0.327)	Data 1.15e-04 (1.83e-04)	Tok/s 40010 (43067)	Loss/tok 2.9644 (3.1978)	LR 1.250e-04
0: TRAIN [3][4190/7762]	Time 0.356 (0.327)	Data 1.04e-04 (1.83e-04)	Tok/s 46975 (43079)	Loss/tok 3.2319 (3.1982)	LR 1.250e-04
0: TRAIN [3][4200/7762]	Time 0.366 (0.327)	Data 1.02e-04 (1.83e-04)	Tok/s 46392 (43081)	Loss/tok 3.2569 (3.1981)	LR 1.250e-04
0: TRAIN [3][4210/7762]	Time 0.261 (0.327)	Data 9.58e-05 (1.83e-04)	Tok/s 39755 (43078)	Loss/tok 2.9979 (3.1980)	LR 1.250e-04
0: TRAIN [3][4220/7762]	Time 0.263 (0.327)	Data 1.01e-04 (1.82e-04)	Tok/s 39633 (43072)	Loss/tok 2.9512 (3.1978)	LR 1.250e-04
0: TRAIN [3][4230/7762]	Time 0.174 (0.327)	Data 1.03e-04 (1.82e-04)	Tok/s 30651 (43074)	Loss/tok 2.5092 (3.1978)	LR 1.250e-04
0: TRAIN [3][4240/7762]	Time 0.176 (0.327)	Data 1.11e-04 (1.82e-04)	Tok/s 29267 (43079)	Loss/tok 2.6773 (3.1981)	LR 1.250e-04
0: TRAIN [3][4250/7762]	Time 0.345 (0.327)	Data 1.04e-04 (1.82e-04)	Tok/s 49416 (43079)	Loss/tok 3.1705 (3.1980)	LR 1.250e-04
0: TRAIN [3][4260/7762]	Time 0.178 (0.327)	Data 1.04e-04 (1.82e-04)	Tok/s 29959 (43076)	Loss/tok 2.6439 (3.1979)	LR 1.250e-04
0: TRAIN [3][4270/7762]	Time 0.258 (0.327)	Data 9.66e-05 (1.82e-04)	Tok/s 40139 (43075)	Loss/tok 3.0386 (3.1978)	LR 1.250e-04
0: TRAIN [3][4280/7762]	Time 0.366 (0.327)	Data 1.02e-04 (1.81e-04)	Tok/s 46145 (43080)	Loss/tok 3.2367 (3.1978)	LR 1.250e-04
0: TRAIN [3][4290/7762]	Time 0.350 (0.327)	Data 1.04e-04 (1.81e-04)	Tok/s 47993 (43089)	Loss/tok 3.2816 (3.1980)	LR 1.250e-04
0: TRAIN [3][4300/7762]	Time 0.367 (0.327)	Data 1.01e-04 (1.81e-04)	Tok/s 45572 (43092)	Loss/tok 3.2895 (3.1980)	LR 1.250e-04
0: TRAIN [3][4310/7762]	Time 0.340 (0.327)	Data 9.63e-05 (1.81e-04)	Tok/s 48919 (43098)	Loss/tok 3.2484 (3.1981)	LR 1.250e-04
0: TRAIN [3][4320/7762]	Time 0.263 (0.327)	Data 1.02e-04 (1.81e-04)	Tok/s 38225 (43093)	Loss/tok 2.9644 (3.1979)	LR 1.250e-04
0: TRAIN [3][4330/7762]	Time 0.364 (0.327)	Data 1.05e-04 (1.80e-04)	Tok/s 46197 (43098)	Loss/tok 3.2115 (3.1981)	LR 1.250e-04
0: TRAIN [3][4340/7762]	Time 0.169 (0.327)	Data 1.18e-04 (1.80e-04)	Tok/s 31225 (43099)	Loss/tok 2.6056 (3.1981)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][4350/7762]	Time 0.461 (0.327)	Data 1.06e-04 (1.80e-04)	Tok/s 50864 (43098)	Loss/tok 3.3656 (3.1983)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][4360/7762]	Time 0.260 (0.327)	Data 9.99e-05 (1.80e-04)	Tok/s 39294 (43093)	Loss/tok 3.0469 (3.1984)	LR 1.250e-04
0: TRAIN [3][4370/7762]	Time 0.258 (0.327)	Data 1.02e-04 (1.80e-04)	Tok/s 40006 (43091)	Loss/tok 3.0696 (3.1982)	LR 1.250e-04
0: TRAIN [3][4380/7762]	Time 0.438 (0.327)	Data 1.22e-04 (1.80e-04)	Tok/s 52815 (43096)	Loss/tok 3.3665 (3.1984)	LR 1.250e-04
0: TRAIN [3][4390/7762]	Time 0.264 (0.327)	Data 1.03e-04 (1.79e-04)	Tok/s 38623 (43097)	Loss/tok 2.9893 (3.1983)	LR 1.250e-04
0: TRAIN [3][4400/7762]	Time 0.360 (0.327)	Data 9.92e-05 (1.79e-04)	Tok/s 47349 (43103)	Loss/tok 3.1628 (3.1983)	LR 1.250e-04
0: TRAIN [3][4410/7762]	Time 0.466 (0.328)	Data 1.01e-04 (1.79e-04)	Tok/s 50353 (43115)	Loss/tok 3.3463 (3.1986)	LR 1.250e-04
0: TRAIN [3][4420/7762]	Time 0.264 (0.328)	Data 9.80e-05 (1.79e-04)	Tok/s 38242 (43111)	Loss/tok 2.9800 (3.1985)	LR 1.250e-04
0: TRAIN [3][4430/7762]	Time 0.352 (0.328)	Data 9.58e-05 (1.79e-04)	Tok/s 47937 (43107)	Loss/tok 3.2686 (3.1985)	LR 1.250e-04
0: TRAIN [3][4440/7762]	Time 0.261 (0.328)	Data 1.04e-04 (1.79e-04)	Tok/s 38811 (43107)	Loss/tok 2.9352 (3.1984)	LR 1.250e-04
0: TRAIN [3][4450/7762]	Time 0.354 (0.328)	Data 1.04e-04 (1.78e-04)	Tok/s 47129 (43110)	Loss/tok 3.1739 (3.1984)	LR 1.250e-04
0: TRAIN [3][4460/7762]	Time 0.253 (0.328)	Data 1.66e-04 (1.78e-04)	Tok/s 41130 (43107)	Loss/tok 3.0383 (3.1982)	LR 1.250e-04
0: TRAIN [3][4470/7762]	Time 0.582 (0.327)	Data 9.68e-05 (1.78e-04)	Tok/s 51168 (43102)	Loss/tok 3.5329 (3.1981)	LR 1.250e-04
0: TRAIN [3][4480/7762]	Time 0.357 (0.327)	Data 1.20e-04 (1.78e-04)	Tok/s 47238 (43100)	Loss/tok 3.1314 (3.1980)	LR 1.250e-04
0: TRAIN [3][4490/7762]	Time 0.353 (0.327)	Data 1.08e-04 (1.78e-04)	Tok/s 47972 (43097)	Loss/tok 3.2425 (3.1979)	LR 1.250e-04
0: TRAIN [3][4500/7762]	Time 0.463 (0.327)	Data 1.00e-04 (1.78e-04)	Tok/s 50092 (43100)	Loss/tok 3.3323 (3.1978)	LR 1.250e-04
0: TRAIN [3][4510/7762]	Time 0.361 (0.327)	Data 1.01e-04 (1.77e-04)	Tok/s 46044 (43093)	Loss/tok 3.1722 (3.1975)	LR 1.250e-04
0: TRAIN [3][4520/7762]	Time 0.358 (0.327)	Data 9.97e-05 (1.77e-04)	Tok/s 47556 (43092)	Loss/tok 3.1978 (3.1973)	LR 1.250e-04
0: TRAIN [3][4530/7762]	Time 0.175 (0.327)	Data 1.00e-04 (1.77e-04)	Tok/s 30437 (43102)	Loss/tok 2.4904 (3.1977)	LR 1.250e-04
0: TRAIN [3][4540/7762]	Time 0.258 (0.327)	Data 1.12e-04 (1.77e-04)	Tok/s 40067 (43100)	Loss/tok 3.0326 (3.1977)	LR 1.250e-04
0: TRAIN [3][4550/7762]	Time 0.269 (0.327)	Data 1.03e-04 (1.77e-04)	Tok/s 38640 (43104)	Loss/tok 3.0296 (3.1979)	LR 1.250e-04
0: TRAIN [3][4560/7762]	Time 0.354 (0.327)	Data 1.05e-04 (1.77e-04)	Tok/s 47022 (43109)	Loss/tok 3.2177 (3.1981)	LR 1.250e-04
0: TRAIN [3][4570/7762]	Time 0.361 (0.328)	Data 1.02e-04 (1.77e-04)	Tok/s 46290 (43114)	Loss/tok 3.2053 (3.1982)	LR 1.250e-04
0: TRAIN [3][4580/7762]	Time 0.174 (0.328)	Data 1.03e-04 (1.76e-04)	Tok/s 30000 (43114)	Loss/tok 2.4950 (3.1985)	LR 1.250e-04
0: TRAIN [3][4590/7762]	Time 0.589 (0.328)	Data 1.16e-04 (1.76e-04)	Tok/s 50963 (43111)	Loss/tok 3.4710 (3.1984)	LR 1.250e-04
0: TRAIN [3][4600/7762]	Time 0.360 (0.328)	Data 1.22e-04 (1.76e-04)	Tok/s 46992 (43113)	Loss/tok 3.2482 (3.1984)	LR 1.250e-04
0: TRAIN [3][4610/7762]	Time 0.460 (0.328)	Data 1.02e-04 (1.76e-04)	Tok/s 50461 (43111)	Loss/tok 3.4298 (3.1985)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][4620/7762]	Time 0.463 (0.328)	Data 1.18e-04 (1.76e-04)	Tok/s 49991 (43118)	Loss/tok 3.2807 (3.1986)	LR 1.250e-04
0: TRAIN [3][4630/7762]	Time 0.467 (0.328)	Data 1.03e-04 (1.76e-04)	Tok/s 49915 (43121)	Loss/tok 3.3567 (3.1988)	LR 1.250e-04
0: TRAIN [3][4640/7762]	Time 0.440 (0.328)	Data 1.26e-04 (1.75e-04)	Tok/s 52873 (43123)	Loss/tok 3.4030 (3.1988)	LR 1.250e-04
0: TRAIN [3][4650/7762]	Time 0.266 (0.328)	Data 1.07e-04 (1.75e-04)	Tok/s 38420 (43119)	Loss/tok 3.0774 (3.1986)	LR 1.250e-04
0: TRAIN [3][4660/7762]	Time 0.558 (0.328)	Data 1.03e-04 (1.75e-04)	Tok/s 53869 (43121)	Loss/tok 3.6160 (3.1987)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][4670/7762]	Time 0.583 (0.328)	Data 1.02e-04 (1.75e-04)	Tok/s 51363 (43120)	Loss/tok 3.5322 (3.1987)	LR 1.250e-04
0: TRAIN [3][4680/7762]	Time 0.265 (0.328)	Data 1.02e-04 (1.75e-04)	Tok/s 39848 (43113)	Loss/tok 3.0381 (3.1985)	LR 1.250e-04
0: TRAIN [3][4690/7762]	Time 0.363 (0.328)	Data 1.02e-04 (1.75e-04)	Tok/s 46338 (43114)	Loss/tok 3.2351 (3.1985)	LR 1.250e-04
0: TRAIN [3][4700/7762]	Time 0.177 (0.328)	Data 1.03e-04 (1.75e-04)	Tok/s 29224 (43108)	Loss/tok 2.5907 (3.1982)	LR 1.250e-04
0: TRAIN [3][4710/7762]	Time 0.258 (0.328)	Data 1.01e-04 (1.74e-04)	Tok/s 40596 (43107)	Loss/tok 2.9969 (3.1981)	LR 1.250e-04
0: TRAIN [3][4720/7762]	Time 0.357 (0.327)	Data 1.02e-04 (1.74e-04)	Tok/s 47011 (43107)	Loss/tok 3.2226 (3.1980)	LR 1.250e-04
0: TRAIN [3][4730/7762]	Time 0.254 (0.328)	Data 1.14e-04 (1.74e-04)	Tok/s 40124 (43113)	Loss/tok 2.9209 (3.1981)	LR 1.250e-04
0: TRAIN [3][4740/7762]	Time 0.268 (0.328)	Data 1.01e-04 (1.74e-04)	Tok/s 38053 (43114)	Loss/tok 3.0397 (3.1982)	LR 1.250e-04
0: TRAIN [3][4750/7762]	Time 0.451 (0.328)	Data 1.01e-04 (1.74e-04)	Tok/s 52124 (43111)	Loss/tok 3.3115 (3.1980)	LR 1.250e-04
0: TRAIN [3][4760/7762]	Time 0.175 (0.328)	Data 1.03e-04 (1.74e-04)	Tok/s 29463 (43113)	Loss/tok 2.5906 (3.1982)	LR 1.250e-04
0: TRAIN [3][4770/7762]	Time 0.270 (0.328)	Data 1.02e-04 (1.74e-04)	Tok/s 38288 (43113)	Loss/tok 2.9665 (3.1982)	LR 1.250e-04
0: TRAIN [3][4780/7762]	Time 0.366 (0.328)	Data 1.12e-04 (1.73e-04)	Tok/s 46117 (43108)	Loss/tok 3.1838 (3.1981)	LR 1.250e-04
0: TRAIN [3][4790/7762]	Time 0.361 (0.328)	Data 1.04e-04 (1.73e-04)	Tok/s 46223 (43113)	Loss/tok 3.2152 (3.1984)	LR 1.250e-04
0: TRAIN [3][4800/7762]	Time 0.265 (0.328)	Data 1.01e-04 (1.73e-04)	Tok/s 39002 (43113)	Loss/tok 3.0443 (3.1982)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][4810/7762]	Time 0.450 (0.328)	Data 9.94e-05 (1.73e-04)	Tok/s 51956 (43113)	Loss/tok 3.4168 (3.1983)	LR 1.250e-04
0: TRAIN [3][4820/7762]	Time 0.462 (0.328)	Data 1.01e-04 (1.73e-04)	Tok/s 50625 (43115)	Loss/tok 3.3452 (3.1982)	LR 1.250e-04
0: TRAIN [3][4830/7762]	Time 0.357 (0.328)	Data 1.04e-04 (1.73e-04)	Tok/s 47386 (43119)	Loss/tok 3.2207 (3.1985)	LR 1.250e-04
0: TRAIN [3][4840/7762]	Time 0.179 (0.328)	Data 2.02e-04 (1.73e-04)	Tok/s 28984 (43121)	Loss/tok 2.7068 (3.1985)	LR 1.250e-04
0: TRAIN [3][4850/7762]	Time 0.264 (0.328)	Data 1.01e-04 (1.72e-04)	Tok/s 39681 (43118)	Loss/tok 3.0053 (3.1986)	LR 1.250e-04
0: TRAIN [3][4860/7762]	Time 0.267 (0.328)	Data 1.06e-04 (1.72e-04)	Tok/s 38983 (43119)	Loss/tok 3.0783 (3.1986)	LR 1.250e-04
0: TRAIN [3][4870/7762]	Time 0.363 (0.328)	Data 1.04e-04 (1.72e-04)	Tok/s 46381 (43122)	Loss/tok 3.2255 (3.1988)	LR 1.250e-04
0: TRAIN [3][4880/7762]	Time 0.463 (0.328)	Data 1.07e-04 (1.72e-04)	Tok/s 50160 (43122)	Loss/tok 3.3627 (3.1989)	LR 1.250e-04
0: TRAIN [3][4890/7762]	Time 0.581 (0.328)	Data 9.78e-05 (1.72e-04)	Tok/s 50715 (43123)	Loss/tok 3.5716 (3.1990)	LR 1.250e-04
0: TRAIN [3][4900/7762]	Time 0.264 (0.328)	Data 9.85e-05 (1.72e-04)	Tok/s 38618 (43122)	Loss/tok 2.9966 (3.1990)	LR 1.250e-04
0: TRAIN [3][4910/7762]	Time 0.459 (0.328)	Data 1.07e-04 (1.72e-04)	Tok/s 51074 (43129)	Loss/tok 3.3234 (3.1994)	LR 1.250e-04
0: TRAIN [3][4920/7762]	Time 0.174 (0.328)	Data 9.94e-05 (1.71e-04)	Tok/s 30228 (43124)	Loss/tok 2.6099 (3.1992)	LR 1.250e-04
0: TRAIN [3][4930/7762]	Time 0.262 (0.328)	Data 1.04e-04 (1.71e-04)	Tok/s 39730 (43124)	Loss/tok 2.9524 (3.1992)	LR 1.250e-04
0: TRAIN [3][4940/7762]	Time 0.264 (0.328)	Data 1.14e-04 (1.71e-04)	Tok/s 39846 (43126)	Loss/tok 3.0599 (3.1992)	LR 1.250e-04
0: TRAIN [3][4950/7762]	Time 0.255 (0.328)	Data 1.04e-04 (1.71e-04)	Tok/s 40627 (43126)	Loss/tok 2.9343 (3.1990)	LR 1.250e-04
0: TRAIN [3][4960/7762]	Time 0.364 (0.328)	Data 1.14e-04 (1.71e-04)	Tok/s 46088 (43124)	Loss/tok 3.1610 (3.1988)	LR 1.250e-04
0: TRAIN [3][4970/7762]	Time 0.364 (0.328)	Data 1.01e-04 (1.71e-04)	Tok/s 47287 (43127)	Loss/tok 3.1031 (3.1988)	LR 1.250e-04
0: TRAIN [3][4980/7762]	Time 0.456 (0.328)	Data 9.99e-05 (1.71e-04)	Tok/s 51205 (43124)	Loss/tok 3.4158 (3.1987)	LR 1.250e-04
0: TRAIN [3][4990/7762]	Time 0.365 (0.328)	Data 1.00e-04 (1.71e-04)	Tok/s 44916 (43129)	Loss/tok 3.2008 (3.1988)	LR 1.250e-04
0: TRAIN [3][5000/7762]	Time 0.362 (0.328)	Data 1.18e-04 (1.70e-04)	Tok/s 46573 (43127)	Loss/tok 3.1967 (3.1989)	LR 1.250e-04
0: TRAIN [3][5010/7762]	Time 0.260 (0.328)	Data 1.10e-04 (1.70e-04)	Tok/s 38536 (43126)	Loss/tok 2.9236 (3.1989)	LR 1.250e-04
0: TRAIN [3][5020/7762]	Time 0.263 (0.328)	Data 1.00e-04 (1.70e-04)	Tok/s 38830 (43128)	Loss/tok 2.9321 (3.1990)	LR 1.250e-04
0: TRAIN [3][5030/7762]	Time 0.358 (0.328)	Data 1.01e-04 (1.70e-04)	Tok/s 47356 (43118)	Loss/tok 3.0881 (3.1986)	LR 1.250e-04
0: TRAIN [3][5040/7762]	Time 0.269 (0.328)	Data 1.02e-04 (1.70e-04)	Tok/s 37947 (43114)	Loss/tok 3.0458 (3.1985)	LR 1.250e-04
0: TRAIN [3][5050/7762]	Time 0.363 (0.328)	Data 1.04e-04 (1.70e-04)	Tok/s 46416 (43113)	Loss/tok 3.1791 (3.1984)	LR 1.250e-04
0: TRAIN [3][5060/7762]	Time 0.360 (0.328)	Data 1.18e-04 (1.70e-04)	Tok/s 46452 (43116)	Loss/tok 3.3323 (3.1985)	LR 1.250e-04
0: TRAIN [3][5070/7762]	Time 0.265 (0.328)	Data 1.02e-04 (1.69e-04)	Tok/s 38076 (43114)	Loss/tok 3.0794 (3.1986)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][5080/7762]	Time 0.260 (0.328)	Data 1.02e-04 (1.69e-04)	Tok/s 39090 (43115)	Loss/tok 2.9546 (3.1986)	LR 1.250e-04
0: TRAIN [3][5090/7762]	Time 0.359 (0.328)	Data 1.00e-04 (1.69e-04)	Tok/s 46764 (43114)	Loss/tok 3.1989 (3.1984)	LR 1.250e-04
0: TRAIN [3][5100/7762]	Time 0.362 (0.328)	Data 1.16e-04 (1.69e-04)	Tok/s 46830 (43114)	Loss/tok 3.2848 (3.1984)	LR 1.250e-04
0: TRAIN [3][5110/7762]	Time 0.177 (0.328)	Data 1.01e-04 (1.69e-04)	Tok/s 30237 (43109)	Loss/tok 2.4789 (3.1984)	LR 1.250e-04
0: TRAIN [3][5120/7762]	Time 0.358 (0.328)	Data 9.89e-05 (1.69e-04)	Tok/s 46611 (43113)	Loss/tok 3.2820 (3.1983)	LR 1.250e-04
0: TRAIN [3][5130/7762]	Time 0.341 (0.328)	Data 9.75e-05 (1.69e-04)	Tok/s 49210 (43113)	Loss/tok 3.1300 (3.1982)	LR 1.250e-04
0: TRAIN [3][5140/7762]	Time 0.356 (0.328)	Data 9.99e-05 (1.69e-04)	Tok/s 47746 (43109)	Loss/tok 3.1618 (3.1981)	LR 1.250e-04
0: TRAIN [3][5150/7762]	Time 0.367 (0.328)	Data 9.97e-05 (1.68e-04)	Tok/s 45741 (43112)	Loss/tok 3.1748 (3.1980)	LR 1.250e-04
0: TRAIN [3][5160/7762]	Time 0.359 (0.328)	Data 9.66e-05 (1.68e-04)	Tok/s 46677 (43112)	Loss/tok 3.1302 (3.1979)	LR 1.250e-04
0: TRAIN [3][5170/7762]	Time 0.355 (0.328)	Data 1.03e-04 (1.68e-04)	Tok/s 47271 (43114)	Loss/tok 3.3337 (3.1980)	LR 1.250e-04
0: TRAIN [3][5180/7762]	Time 0.172 (0.328)	Data 1.01e-04 (1.68e-04)	Tok/s 31066 (43115)	Loss/tok 2.4841 (3.1979)	LR 1.250e-04
0: TRAIN [3][5190/7762]	Time 0.466 (0.328)	Data 1.01e-04 (1.68e-04)	Tok/s 50390 (43116)	Loss/tok 3.3117 (3.1979)	LR 1.250e-04
0: TRAIN [3][5200/7762]	Time 0.357 (0.328)	Data 1.01e-04 (1.68e-04)	Tok/s 46563 (43115)	Loss/tok 3.1145 (3.1977)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][5210/7762]	Time 0.464 (0.328)	Data 9.82e-05 (1.68e-04)	Tok/s 49894 (43113)	Loss/tok 3.4234 (3.1977)	LR 1.250e-04
0: TRAIN [3][5220/7762]	Time 0.585 (0.328)	Data 9.87e-05 (1.68e-04)	Tok/s 50696 (43112)	Loss/tok 3.6131 (3.1977)	LR 1.250e-04
0: TRAIN [3][5230/7762]	Time 0.174 (0.328)	Data 9.97e-05 (1.67e-04)	Tok/s 30242 (43110)	Loss/tok 2.6381 (3.1977)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][5240/7762]	Time 0.257 (0.328)	Data 1.06e-04 (1.67e-04)	Tok/s 40647 (43111)	Loss/tok 2.9184 (3.1977)	LR 1.250e-04
0: TRAIN [3][5250/7762]	Time 0.449 (0.328)	Data 9.47e-05 (1.67e-04)	Tok/s 52670 (43112)	Loss/tok 3.3159 (3.1977)	LR 1.250e-04
0: TRAIN [3][5260/7762]	Time 0.258 (0.328)	Data 9.78e-05 (1.67e-04)	Tok/s 40929 (43111)	Loss/tok 3.0423 (3.1976)	LR 1.250e-04
0: TRAIN [3][5270/7762]	Time 0.259 (0.327)	Data 1.37e-04 (1.67e-04)	Tok/s 40312 (43102)	Loss/tok 3.0326 (3.1974)	LR 1.250e-04
0: TRAIN [3][5280/7762]	Time 0.360 (0.327)	Data 9.87e-05 (1.67e-04)	Tok/s 46352 (43101)	Loss/tok 3.1257 (3.1971)	LR 1.250e-04
0: TRAIN [3][5290/7762]	Time 0.175 (0.327)	Data 9.75e-05 (1.67e-04)	Tok/s 29662 (43099)	Loss/tok 2.5928 (3.1972)	LR 1.250e-04
0: TRAIN [3][5300/7762]	Time 0.176 (0.327)	Data 1.01e-04 (1.67e-04)	Tok/s 30078 (43093)	Loss/tok 2.5432 (3.1970)	LR 1.250e-04
0: TRAIN [3][5310/7762]	Time 0.349 (0.327)	Data 1.00e-04 (1.66e-04)	Tok/s 47475 (43096)	Loss/tok 3.1745 (3.1971)	LR 1.250e-04
0: TRAIN [3][5320/7762]	Time 0.269 (0.327)	Data 1.11e-04 (1.66e-04)	Tok/s 38264 (43096)	Loss/tok 3.0065 (3.1969)	LR 1.250e-04
0: TRAIN [3][5330/7762]	Time 0.342 (0.327)	Data 1.04e-04 (1.66e-04)	Tok/s 49486 (43100)	Loss/tok 3.1096 (3.1971)	LR 1.250e-04
0: TRAIN [3][5340/7762]	Time 0.366 (0.327)	Data 1.02e-04 (1.66e-04)	Tok/s 45361 (43098)	Loss/tok 3.3534 (3.1971)	LR 1.250e-04
0: TRAIN [3][5350/7762]	Time 0.461 (0.327)	Data 1.39e-04 (1.66e-04)	Tok/s 50797 (43102)	Loss/tok 3.3377 (3.1972)	LR 1.250e-04
0: TRAIN [3][5360/7762]	Time 0.588 (0.328)	Data 1.02e-04 (1.66e-04)	Tok/s 50426 (43110)	Loss/tok 3.5414 (3.1977)	LR 1.250e-04
0: TRAIN [3][5370/7762]	Time 0.440 (0.328)	Data 1.04e-04 (1.66e-04)	Tok/s 52510 (43112)	Loss/tok 3.4374 (3.1979)	LR 1.250e-04
0: TRAIN [3][5380/7762]	Time 0.361 (0.328)	Data 9.99e-05 (1.66e-04)	Tok/s 46729 (43113)	Loss/tok 3.1540 (3.1979)	LR 1.250e-04
0: TRAIN [3][5390/7762]	Time 0.436 (0.328)	Data 9.99e-05 (1.66e-04)	Tok/s 53016 (43114)	Loss/tok 3.4144 (3.1979)	LR 1.250e-04
0: TRAIN [3][5400/7762]	Time 0.179 (0.328)	Data 9.78e-05 (1.65e-04)	Tok/s 29347 (43109)	Loss/tok 2.6474 (3.1979)	LR 1.250e-04
0: TRAIN [3][5410/7762]	Time 0.262 (0.327)	Data 9.97e-05 (1.65e-04)	Tok/s 39295 (43104)	Loss/tok 3.0132 (3.1976)	LR 1.250e-04
0: TRAIN [3][5420/7762]	Time 0.176 (0.327)	Data 1.18e-04 (1.65e-04)	Tok/s 29535 (43104)	Loss/tok 2.5122 (3.1978)	LR 1.250e-04
0: TRAIN [3][5430/7762]	Time 0.265 (0.327)	Data 1.03e-04 (1.65e-04)	Tok/s 39775 (43098)	Loss/tok 2.9625 (3.1976)	LR 1.250e-04
0: TRAIN [3][5440/7762]	Time 0.355 (0.327)	Data 9.82e-05 (1.65e-04)	Tok/s 47316 (43099)	Loss/tok 3.3534 (3.1975)	LR 1.250e-04
0: TRAIN [3][5450/7762]	Time 0.352 (0.327)	Data 1.06e-04 (1.65e-04)	Tok/s 47383 (43095)	Loss/tok 3.2159 (3.1974)	LR 1.250e-04
0: TRAIN [3][5460/7762]	Time 0.363 (0.327)	Data 1.03e-04 (1.65e-04)	Tok/s 46200 (43099)	Loss/tok 3.2075 (3.1975)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][5470/7762]	Time 0.263 (0.327)	Data 1.01e-04 (1.65e-04)	Tok/s 39719 (43098)	Loss/tok 3.0378 (3.1975)	LR 1.250e-04
0: TRAIN [3][5480/7762]	Time 0.457 (0.327)	Data 1.03e-04 (1.65e-04)	Tok/s 51055 (43100)	Loss/tok 3.3632 (3.1976)	LR 1.250e-04
0: TRAIN [3][5490/7762]	Time 0.584 (0.327)	Data 1.03e-04 (1.64e-04)	Tok/s 50770 (43101)	Loss/tok 3.6310 (3.1976)	LR 1.250e-04
0: TRAIN [3][5500/7762]	Time 0.259 (0.327)	Data 1.04e-04 (1.64e-04)	Tok/s 39785 (43095)	Loss/tok 3.0709 (3.1974)	LR 1.250e-04
0: TRAIN [3][5510/7762]	Time 0.263 (0.327)	Data 1.34e-04 (1.64e-04)	Tok/s 39032 (43093)	Loss/tok 2.9356 (3.1974)	LR 1.250e-04
0: TRAIN [3][5520/7762]	Time 0.258 (0.327)	Data 1.04e-04 (1.64e-04)	Tok/s 39779 (43089)	Loss/tok 2.9104 (3.1972)	LR 1.250e-04
0: TRAIN [3][5530/7762]	Time 0.262 (0.327)	Data 1.02e-04 (1.64e-04)	Tok/s 39496 (43082)	Loss/tok 3.0210 (3.1970)	LR 1.250e-04
0: TRAIN [3][5540/7762]	Time 0.361 (0.327)	Data 9.87e-05 (1.64e-04)	Tok/s 46960 (43084)	Loss/tok 3.2426 (3.1970)	LR 1.250e-04
0: TRAIN [3][5550/7762]	Time 0.355 (0.327)	Data 9.73e-05 (1.64e-04)	Tok/s 47618 (43076)	Loss/tok 3.2100 (3.1968)	LR 1.250e-04
0: TRAIN [3][5560/7762]	Time 0.357 (0.327)	Data 9.99e-05 (1.64e-04)	Tok/s 46766 (43076)	Loss/tok 3.2214 (3.1967)	LR 1.250e-04
0: TRAIN [3][5570/7762]	Time 0.252 (0.327)	Data 1.02e-04 (1.64e-04)	Tok/s 40476 (43077)	Loss/tok 3.0222 (3.1967)	LR 1.250e-04
0: TRAIN [3][5580/7762]	Time 0.364 (0.327)	Data 1.03e-04 (1.64e-04)	Tok/s 46507 (43075)	Loss/tok 3.1776 (3.1965)	LR 1.250e-04
0: TRAIN [3][5590/7762]	Time 0.342 (0.327)	Data 1.03e-04 (1.63e-04)	Tok/s 49259 (43073)	Loss/tok 3.1584 (3.1963)	LR 1.250e-04
0: TRAIN [3][5600/7762]	Time 0.365 (0.327)	Data 1.00e-04 (1.63e-04)	Tok/s 46285 (43072)	Loss/tok 3.1081 (3.1963)	LR 1.250e-04
0: TRAIN [3][5610/7762]	Time 0.463 (0.327)	Data 1.04e-04 (1.63e-04)	Tok/s 50702 (43076)	Loss/tok 3.4218 (3.1964)	LR 1.250e-04
0: TRAIN [3][5620/7762]	Time 0.259 (0.327)	Data 1.03e-04 (1.63e-04)	Tok/s 40028 (43083)	Loss/tok 2.9763 (3.1966)	LR 1.250e-04
0: TRAIN [3][5630/7762]	Time 0.259 (0.327)	Data 1.14e-04 (1.63e-04)	Tok/s 40619 (43083)	Loss/tok 2.9757 (3.1966)	LR 1.250e-04
0: TRAIN [3][5640/7762]	Time 0.350 (0.327)	Data 1.42e-04 (1.63e-04)	Tok/s 47480 (43090)	Loss/tok 3.2438 (3.1968)	LR 1.250e-04
0: TRAIN [3][5650/7762]	Time 0.364 (0.327)	Data 1.43e-04 (1.63e-04)	Tok/s 46192 (43087)	Loss/tok 3.2894 (3.1967)	LR 1.250e-04
0: TRAIN [3][5660/7762]	Time 0.175 (0.327)	Data 1.01e-04 (1.63e-04)	Tok/s 30116 (43085)	Loss/tok 2.5238 (3.1967)	LR 1.250e-04
0: TRAIN [3][5670/7762]	Time 0.588 (0.327)	Data 1.00e-04 (1.63e-04)	Tok/s 50703 (43086)	Loss/tok 3.5165 (3.1968)	LR 1.250e-04
0: TRAIN [3][5680/7762]	Time 0.251 (0.327)	Data 1.00e-04 (1.62e-04)	Tok/s 41014 (43094)	Loss/tok 2.9570 (3.1970)	LR 1.250e-04
0: TRAIN [3][5690/7762]	Time 0.266 (0.327)	Data 1.03e-04 (1.62e-04)	Tok/s 38999 (43095)	Loss/tok 2.9165 (3.1969)	LR 1.250e-04
0: TRAIN [3][5700/7762]	Time 0.267 (0.327)	Data 9.99e-05 (1.62e-04)	Tok/s 38856 (43095)	Loss/tok 3.0740 (3.1967)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][5710/7762]	Time 0.260 (0.327)	Data 1.17e-04 (1.62e-04)	Tok/s 39793 (43095)	Loss/tok 2.9502 (3.1967)	LR 1.250e-04
0: TRAIN [3][5720/7762]	Time 0.265 (0.327)	Data 1.00e-04 (1.62e-04)	Tok/s 38404 (43094)	Loss/tok 3.1127 (3.1967)	LR 1.250e-04
0: TRAIN [3][5730/7762]	Time 0.256 (0.327)	Data 9.85e-05 (1.62e-04)	Tok/s 40697 (43095)	Loss/tok 2.9125 (3.1966)	LR 1.250e-04
0: TRAIN [3][5740/7762]	Time 0.175 (0.327)	Data 9.68e-05 (1.62e-04)	Tok/s 30503 (43092)	Loss/tok 2.5786 (3.1965)	LR 1.250e-04
0: TRAIN [3][5750/7762]	Time 0.259 (0.327)	Data 9.73e-05 (1.62e-04)	Tok/s 39119 (43092)	Loss/tok 2.9705 (3.1966)	LR 1.250e-04
0: TRAIN [3][5760/7762]	Time 0.261 (0.327)	Data 9.82e-05 (1.62e-04)	Tok/s 39202 (43090)	Loss/tok 3.0375 (3.1967)	LR 1.250e-04
0: TRAIN [3][5770/7762]	Time 0.457 (0.327)	Data 1.24e-04 (1.62e-04)	Tok/s 51816 (43090)	Loss/tok 3.3238 (3.1966)	LR 1.250e-04
0: TRAIN [3][5780/7762]	Time 0.462 (0.327)	Data 9.99e-05 (1.61e-04)	Tok/s 50497 (43089)	Loss/tok 3.4165 (3.1967)	LR 1.250e-04
0: TRAIN [3][5790/7762]	Time 0.463 (0.327)	Data 9.87e-05 (1.61e-04)	Tok/s 50530 (43096)	Loss/tok 3.3227 (3.1967)	LR 1.250e-04
0: TRAIN [3][5800/7762]	Time 0.258 (0.327)	Data 1.06e-04 (1.61e-04)	Tok/s 39647 (43096)	Loss/tok 3.0201 (3.1969)	LR 1.250e-04
0: TRAIN [3][5810/7762]	Time 0.270 (0.327)	Data 1.01e-04 (1.61e-04)	Tok/s 38787 (43097)	Loss/tok 3.1085 (3.1969)	LR 1.250e-04
0: TRAIN [3][5820/7762]	Time 0.264 (0.327)	Data 1.02e-04 (1.61e-04)	Tok/s 39386 (43092)	Loss/tok 2.8618 (3.1967)	LR 1.250e-04
0: TRAIN [3][5830/7762]	Time 0.265 (0.327)	Data 1.04e-04 (1.61e-04)	Tok/s 38660 (43092)	Loss/tok 3.1597 (3.1966)	LR 1.250e-04
0: TRAIN [3][5840/7762]	Time 0.366 (0.327)	Data 1.02e-04 (1.61e-04)	Tok/s 45060 (43090)	Loss/tok 3.2512 (3.1966)	LR 1.250e-04
0: TRAIN [3][5850/7762]	Time 0.265 (0.327)	Data 1.00e-04 (1.61e-04)	Tok/s 38341 (43087)	Loss/tok 3.1212 (3.1965)	LR 1.250e-04
0: TRAIN [3][5860/7762]	Time 0.356 (0.327)	Data 1.03e-04 (1.61e-04)	Tok/s 47353 (43081)	Loss/tok 3.1827 (3.1963)	LR 1.250e-04
0: TRAIN [3][5870/7762]	Time 0.263 (0.327)	Data 1.02e-04 (1.61e-04)	Tok/s 39773 (43078)	Loss/tok 3.0228 (3.1962)	LR 1.250e-04
0: TRAIN [3][5880/7762]	Time 0.265 (0.327)	Data 1.12e-04 (1.61e-04)	Tok/s 38556 (43079)	Loss/tok 2.9106 (3.1961)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][5890/7762]	Time 0.578 (0.327)	Data 1.18e-04 (1.60e-04)	Tok/s 51541 (43074)	Loss/tok 3.5683 (3.1960)	LR 1.250e-04
0: TRAIN [3][5900/7762]	Time 0.259 (0.327)	Data 1.02e-04 (1.60e-04)	Tok/s 41363 (43076)	Loss/tok 2.9506 (3.1961)	LR 1.250e-04
0: TRAIN [3][5910/7762]	Time 0.361 (0.327)	Data 1.02e-04 (1.60e-04)	Tok/s 45241 (43079)	Loss/tok 3.2635 (3.1962)	LR 1.250e-04
0: TRAIN [3][5920/7762]	Time 0.251 (0.327)	Data 1.02e-04 (1.60e-04)	Tok/s 41253 (43082)	Loss/tok 2.9038 (3.1961)	LR 1.250e-04
0: TRAIN [3][5930/7762]	Time 0.354 (0.327)	Data 1.16e-04 (1.60e-04)	Tok/s 47327 (43088)	Loss/tok 3.0667 (3.1962)	LR 1.250e-04
0: TRAIN [3][5940/7762]	Time 0.250 (0.327)	Data 9.85e-05 (1.60e-04)	Tok/s 40809 (43083)	Loss/tok 2.9979 (3.1961)	LR 1.250e-04
0: TRAIN [3][5950/7762]	Time 0.564 (0.327)	Data 1.01e-04 (1.60e-04)	Tok/s 52206 (43085)	Loss/tok 3.6358 (3.1962)	LR 1.250e-04
0: TRAIN [3][5960/7762]	Time 0.468 (0.327)	Data 9.82e-05 (1.60e-04)	Tok/s 49510 (43081)	Loss/tok 3.4532 (3.1960)	LR 1.250e-04
0: TRAIN [3][5970/7762]	Time 0.267 (0.327)	Data 9.75e-05 (1.60e-04)	Tok/s 38966 (43074)	Loss/tok 2.9566 (3.1958)	LR 1.250e-04
0: TRAIN [3][5980/7762]	Time 0.175 (0.327)	Data 9.99e-05 (1.60e-04)	Tok/s 30247 (43071)	Loss/tok 2.5744 (3.1957)	LR 1.250e-04
0: TRAIN [3][5990/7762]	Time 0.262 (0.327)	Data 1.03e-04 (1.60e-04)	Tok/s 40338 (43070)	Loss/tok 3.0346 (3.1957)	LR 1.250e-04
0: TRAIN [3][6000/7762]	Time 0.178 (0.327)	Data 9.47e-05 (1.59e-04)	Tok/s 29649 (43066)	Loss/tok 2.5991 (3.1956)	LR 1.250e-04
0: TRAIN [3][6010/7762]	Time 0.583 (0.327)	Data 9.94e-05 (1.59e-04)	Tok/s 50827 (43069)	Loss/tok 3.5582 (3.1957)	LR 1.250e-04
0: TRAIN [3][6020/7762]	Time 0.465 (0.327)	Data 1.07e-04 (1.59e-04)	Tok/s 50450 (43069)	Loss/tok 3.3417 (3.1956)	LR 1.250e-04
0: TRAIN [3][6030/7762]	Time 0.260 (0.327)	Data 1.01e-04 (1.59e-04)	Tok/s 40224 (43067)	Loss/tok 2.9641 (3.1955)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][6040/7762]	Time 0.560 (0.327)	Data 1.04e-04 (1.59e-04)	Tok/s 52902 (43069)	Loss/tok 3.5205 (3.1954)	LR 1.250e-04
0: TRAIN [3][6050/7762]	Time 0.262 (0.327)	Data 1.16e-04 (1.59e-04)	Tok/s 39066 (43062)	Loss/tok 3.0067 (3.1952)	LR 1.250e-04
0: TRAIN [3][6060/7762]	Time 0.362 (0.327)	Data 1.00e-04 (1.59e-04)	Tok/s 46527 (43068)	Loss/tok 3.1557 (3.1953)	LR 1.250e-04
0: TRAIN [3][6070/7762]	Time 0.262 (0.327)	Data 1.03e-04 (1.59e-04)	Tok/s 40233 (43065)	Loss/tok 3.1046 (3.1953)	LR 1.250e-04
0: TRAIN [3][6080/7762]	Time 0.175 (0.327)	Data 9.94e-05 (1.59e-04)	Tok/s 30191 (43063)	Loss/tok 2.6917 (3.1952)	LR 1.250e-04
0: TRAIN [3][6090/7762]	Time 0.357 (0.327)	Data 9.87e-05 (1.59e-04)	Tok/s 46816 (43064)	Loss/tok 3.2742 (3.1952)	LR 1.250e-04
0: TRAIN [3][6100/7762]	Time 0.260 (0.327)	Data 9.87e-05 (1.58e-04)	Tok/s 39270 (43062)	Loss/tok 2.9465 (3.1952)	LR 1.250e-04
0: TRAIN [3][6110/7762]	Time 0.579 (0.327)	Data 9.89e-05 (1.58e-04)	Tok/s 50892 (43061)	Loss/tok 3.5838 (3.1953)	LR 1.250e-04
0: TRAIN [3][6120/7762]	Time 0.361 (0.327)	Data 1.25e-04 (1.58e-04)	Tok/s 46342 (43056)	Loss/tok 3.1724 (3.1951)	LR 1.250e-04
0: TRAIN [3][6130/7762]	Time 0.267 (0.327)	Data 9.94e-05 (1.58e-04)	Tok/s 38656 (43054)	Loss/tok 2.9798 (3.1952)	LR 1.250e-04
0: TRAIN [3][6140/7762]	Time 0.178 (0.327)	Data 9.97e-05 (1.58e-04)	Tok/s 29315 (43051)	Loss/tok 2.6551 (3.1951)	LR 1.250e-04
0: TRAIN [3][6150/7762]	Time 0.360 (0.326)	Data 1.24e-04 (1.58e-04)	Tok/s 46997 (43048)	Loss/tok 3.1941 (3.1950)	LR 1.250e-04
0: TRAIN [3][6160/7762]	Time 0.177 (0.326)	Data 1.03e-04 (1.58e-04)	Tok/s 29446 (43044)	Loss/tok 2.5265 (3.1949)	LR 1.250e-04
0: TRAIN [3][6170/7762]	Time 0.254 (0.326)	Data 1.02e-04 (1.58e-04)	Tok/s 41248 (43047)	Loss/tok 2.9554 (3.1948)	LR 1.250e-04
0: TRAIN [3][6180/7762]	Time 0.358 (0.326)	Data 9.87e-05 (1.58e-04)	Tok/s 47214 (43048)	Loss/tok 3.1099 (3.1949)	LR 1.250e-04
0: TRAIN [3][6190/7762]	Time 0.588 (0.326)	Data 1.02e-04 (1.58e-04)	Tok/s 50889 (43048)	Loss/tok 3.5978 (3.1949)	LR 1.250e-04
0: TRAIN [3][6200/7762]	Time 0.256 (0.327)	Data 9.94e-05 (1.58e-04)	Tok/s 39386 (43050)	Loss/tok 3.0675 (3.1950)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][6210/7762]	Time 0.452 (0.327)	Data 1.04e-04 (1.58e-04)	Tok/s 51270 (43058)	Loss/tok 3.3683 (3.1952)	LR 1.250e-04
0: TRAIN [3][6220/7762]	Time 0.264 (0.327)	Data 9.73e-05 (1.57e-04)	Tok/s 38406 (43064)	Loss/tok 3.0561 (3.1954)	LR 1.250e-04
0: TRAIN [3][6230/7762]	Time 0.265 (0.327)	Data 1.05e-04 (1.57e-04)	Tok/s 38433 (43060)	Loss/tok 2.9150 (3.1954)	LR 1.250e-04
0: TRAIN [3][6240/7762]	Time 0.462 (0.327)	Data 9.61e-05 (1.57e-04)	Tok/s 50172 (43059)	Loss/tok 3.3405 (3.1955)	LR 1.250e-04
0: TRAIN [3][6250/7762]	Time 0.463 (0.327)	Data 9.80e-05 (1.57e-04)	Tok/s 50615 (43064)	Loss/tok 3.2447 (3.1955)	LR 1.250e-04
0: TRAIN [3][6260/7762]	Time 0.348 (0.327)	Data 1.69e-04 (1.57e-04)	Tok/s 48617 (43066)	Loss/tok 3.3016 (3.1955)	LR 1.250e-04
0: TRAIN [3][6270/7762]	Time 0.265 (0.327)	Data 1.03e-04 (1.57e-04)	Tok/s 38594 (43068)	Loss/tok 3.0070 (3.1956)	LR 1.250e-04
0: TRAIN [3][6280/7762]	Time 0.361 (0.327)	Data 9.78e-05 (1.57e-04)	Tok/s 46341 (43068)	Loss/tok 3.1896 (3.1956)	LR 1.250e-04
0: TRAIN [3][6290/7762]	Time 0.464 (0.327)	Data 9.99e-05 (1.57e-04)	Tok/s 50862 (43069)	Loss/tok 3.2512 (3.1956)	LR 1.250e-04
0: TRAIN [3][6300/7762]	Time 0.265 (0.327)	Data 9.58e-05 (1.57e-04)	Tok/s 38202 (43068)	Loss/tok 2.8943 (3.1956)	LR 1.250e-04
0: TRAIN [3][6310/7762]	Time 0.267 (0.327)	Data 9.85e-05 (1.57e-04)	Tok/s 38190 (43062)	Loss/tok 3.0416 (3.1954)	LR 1.250e-04
0: TRAIN [3][6320/7762]	Time 0.177 (0.327)	Data 1.02e-04 (1.57e-04)	Tok/s 29185 (43059)	Loss/tok 2.6180 (3.1953)	LR 1.250e-04
0: TRAIN [3][6330/7762]	Time 0.451 (0.327)	Data 9.63e-05 (1.56e-04)	Tok/s 51940 (43058)	Loss/tok 3.3571 (3.1953)	LR 1.250e-04
0: TRAIN [3][6340/7762]	Time 0.265 (0.327)	Data 1.03e-04 (1.56e-04)	Tok/s 38220 (43057)	Loss/tok 3.0135 (3.1953)	LR 1.250e-04
0: TRAIN [3][6350/7762]	Time 0.463 (0.327)	Data 1.56e-04 (1.56e-04)	Tok/s 51014 (43060)	Loss/tok 3.3632 (3.1954)	LR 1.250e-04
0: TRAIN [3][6360/7762]	Time 0.259 (0.327)	Data 9.94e-05 (1.56e-04)	Tok/s 40059 (43060)	Loss/tok 3.0676 (3.1952)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][6370/7762]	Time 0.361 (0.327)	Data 1.03e-04 (1.56e-04)	Tok/s 46501 (43065)	Loss/tok 3.2188 (3.1954)	LR 1.250e-04
0: TRAIN [3][6380/7762]	Time 0.258 (0.327)	Data 9.66e-05 (1.56e-04)	Tok/s 40279 (43061)	Loss/tok 3.0143 (3.1953)	LR 1.250e-04
0: TRAIN [3][6390/7762]	Time 0.266 (0.327)	Data 1.06e-04 (1.56e-04)	Tok/s 38944 (43058)	Loss/tok 3.1408 (3.1952)	LR 1.250e-04
0: TRAIN [3][6400/7762]	Time 0.261 (0.327)	Data 1.00e-04 (1.56e-04)	Tok/s 39446 (43060)	Loss/tok 2.9597 (3.1954)	LR 1.250e-04
0: TRAIN [3][6410/7762]	Time 0.366 (0.327)	Data 9.66e-05 (1.56e-04)	Tok/s 46040 (43060)	Loss/tok 3.2692 (3.1954)	LR 1.250e-04
0: TRAIN [3][6420/7762]	Time 0.258 (0.327)	Data 1.04e-04 (1.56e-04)	Tok/s 39323 (43061)	Loss/tok 2.9969 (3.1954)	LR 1.250e-04
0: TRAIN [3][6430/7762]	Time 0.356 (0.327)	Data 9.97e-05 (1.56e-04)	Tok/s 47031 (43062)	Loss/tok 3.3038 (3.1954)	LR 1.250e-04
0: TRAIN [3][6440/7762]	Time 0.350 (0.327)	Data 9.97e-05 (1.56e-04)	Tok/s 48146 (43062)	Loss/tok 3.0936 (3.1953)	LR 1.250e-04
0: TRAIN [3][6450/7762]	Time 0.259 (0.327)	Data 9.80e-05 (1.56e-04)	Tok/s 39458 (43060)	Loss/tok 3.0558 (3.1952)	LR 1.250e-04
0: TRAIN [3][6460/7762]	Time 0.263 (0.327)	Data 1.03e-04 (1.55e-04)	Tok/s 39584 (43057)	Loss/tok 2.9625 (3.1951)	LR 1.250e-04
0: TRAIN [3][6470/7762]	Time 0.558 (0.327)	Data 1.01e-04 (1.55e-04)	Tok/s 53591 (43057)	Loss/tok 3.4704 (3.1952)	LR 1.250e-04
0: TRAIN [3][6480/7762]	Time 0.264 (0.327)	Data 9.97e-05 (1.55e-04)	Tok/s 39163 (43055)	Loss/tok 2.9373 (3.1951)	LR 1.250e-04
0: TRAIN [3][6490/7762]	Time 0.464 (0.327)	Data 1.01e-04 (1.55e-04)	Tok/s 50592 (43055)	Loss/tok 3.3692 (3.1951)	LR 1.250e-04
0: TRAIN [3][6500/7762]	Time 0.258 (0.327)	Data 1.02e-04 (1.55e-04)	Tok/s 39339 (43054)	Loss/tok 2.9126 (3.1951)	LR 1.250e-04
0: TRAIN [3][6510/7762]	Time 0.264 (0.327)	Data 1.04e-04 (1.55e-04)	Tok/s 39744 (43050)	Loss/tok 2.9656 (3.1950)	LR 1.250e-04
0: TRAIN [3][6520/7762]	Time 0.260 (0.327)	Data 9.89e-05 (1.55e-04)	Tok/s 38777 (43050)	Loss/tok 3.0406 (3.1950)	LR 1.250e-04
0: TRAIN [3][6530/7762]	Time 0.266 (0.327)	Data 9.89e-05 (1.55e-04)	Tok/s 38673 (43050)	Loss/tok 2.9861 (3.1951)	LR 1.250e-04
0: TRAIN [3][6540/7762]	Time 0.587 (0.327)	Data 1.03e-04 (1.55e-04)	Tok/s 50528 (43054)	Loss/tok 3.5919 (3.1954)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][6550/7762]	Time 0.455 (0.327)	Data 1.13e-04 (1.55e-04)	Tok/s 51971 (43055)	Loss/tok 3.4015 (3.1955)	LR 1.250e-04
0: TRAIN [3][6560/7762]	Time 0.364 (0.327)	Data 9.97e-05 (1.55e-04)	Tok/s 46374 (43054)	Loss/tok 3.1010 (3.1954)	LR 1.250e-04
0: TRAIN [3][6570/7762]	Time 0.256 (0.327)	Data 1.03e-04 (1.55e-04)	Tok/s 40333 (43053)	Loss/tok 2.9810 (3.1954)	LR 1.250e-04
0: TRAIN [3][6580/7762]	Time 0.265 (0.327)	Data 9.94e-05 (1.55e-04)	Tok/s 37793 (43049)	Loss/tok 2.9892 (3.1953)	LR 1.250e-04
0: TRAIN [3][6590/7762]	Time 0.265 (0.327)	Data 9.89e-05 (1.54e-04)	Tok/s 39376 (43051)	Loss/tok 3.0983 (3.1954)	LR 1.250e-04
0: TRAIN [3][6600/7762]	Time 0.361 (0.327)	Data 1.04e-04 (1.54e-04)	Tok/s 47043 (43053)	Loss/tok 3.1962 (3.1954)	LR 1.250e-04
0: TRAIN [3][6610/7762]	Time 0.356 (0.327)	Data 1.13e-04 (1.54e-04)	Tok/s 46838 (43053)	Loss/tok 3.1386 (3.1953)	LR 1.250e-04
0: TRAIN [3][6620/7762]	Time 0.174 (0.327)	Data 1.21e-04 (1.54e-04)	Tok/s 30589 (43053)	Loss/tok 2.6408 (3.1954)	LR 1.250e-04
0: TRAIN [3][6630/7762]	Time 0.354 (0.327)	Data 1.04e-04 (1.54e-04)	Tok/s 47282 (43056)	Loss/tok 3.2727 (3.1953)	LR 1.250e-04
0: TRAIN [3][6640/7762]	Time 0.562 (0.327)	Data 1.11e-04 (1.54e-04)	Tok/s 53043 (43056)	Loss/tok 3.5434 (3.1954)	LR 1.250e-04
0: TRAIN [3][6650/7762]	Time 0.268 (0.327)	Data 9.80e-05 (1.54e-04)	Tok/s 38976 (43053)	Loss/tok 2.9670 (3.1952)	LR 1.250e-04
0: TRAIN [3][6660/7762]	Time 0.261 (0.327)	Data 1.02e-04 (1.54e-04)	Tok/s 39796 (43055)	Loss/tok 2.9855 (3.1954)	LR 1.250e-04
0: TRAIN [3][6670/7762]	Time 0.361 (0.327)	Data 9.75e-05 (1.54e-04)	Tok/s 47497 (43058)	Loss/tok 3.2133 (3.1955)	LR 1.250e-04
0: TRAIN [3][6680/7762]	Time 0.463 (0.327)	Data 1.12e-04 (1.54e-04)	Tok/s 50005 (43059)	Loss/tok 3.3473 (3.1954)	LR 1.250e-04
0: TRAIN [3][6690/7762]	Time 0.267 (0.327)	Data 1.01e-04 (1.54e-04)	Tok/s 38614 (43058)	Loss/tok 3.0098 (3.1954)	LR 1.250e-04
0: TRAIN [3][6700/7762]	Time 0.263 (0.327)	Data 9.99e-05 (1.54e-04)	Tok/s 38796 (43057)	Loss/tok 2.9372 (3.1954)	LR 1.250e-04
0: TRAIN [3][6710/7762]	Time 0.265 (0.327)	Data 9.85e-05 (1.54e-04)	Tok/s 38963 (43052)	Loss/tok 3.1123 (3.1953)	LR 1.250e-04
0: TRAIN [3][6720/7762]	Time 0.356 (0.327)	Data 9.58e-05 (1.53e-04)	Tok/s 46999 (43052)	Loss/tok 3.1520 (3.1952)	LR 1.250e-04
0: TRAIN [3][6730/7762]	Time 0.365 (0.327)	Data 1.01e-04 (1.53e-04)	Tok/s 46370 (43053)	Loss/tok 3.1380 (3.1952)	LR 1.250e-04
0: TRAIN [3][6740/7762]	Time 0.267 (0.327)	Data 9.85e-05 (1.53e-04)	Tok/s 37815 (43051)	Loss/tok 3.0369 (3.1951)	LR 1.250e-04
0: TRAIN [3][6750/7762]	Time 0.258 (0.327)	Data 1.01e-04 (1.53e-04)	Tok/s 40353 (43047)	Loss/tok 2.9530 (3.1951)	LR 1.250e-04
0: TRAIN [3][6760/7762]	Time 0.452 (0.327)	Data 1.02e-04 (1.53e-04)	Tok/s 52271 (43051)	Loss/tok 3.2013 (3.1951)	LR 1.250e-04
0: TRAIN [3][6770/7762]	Time 0.260 (0.327)	Data 9.44e-05 (1.53e-04)	Tok/s 40689 (43051)	Loss/tok 2.9519 (3.1951)	LR 1.250e-04
0: TRAIN [3][6780/7762]	Time 0.262 (0.326)	Data 9.63e-05 (1.53e-04)	Tok/s 39416 (43044)	Loss/tok 3.0259 (3.1949)	LR 1.250e-04
0: TRAIN [3][6790/7762]	Time 0.265 (0.326)	Data 1.01e-04 (1.53e-04)	Tok/s 38817 (43045)	Loss/tok 3.0799 (3.1948)	LR 1.250e-04
0: TRAIN [3][6800/7762]	Time 0.361 (0.326)	Data 9.70e-05 (1.53e-04)	Tok/s 45804 (43043)	Loss/tok 3.1804 (3.1949)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][6810/7762]	Time 0.581 (0.326)	Data 1.02e-04 (1.53e-04)	Tok/s 51085 (43039)	Loss/tok 3.4907 (3.1948)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][6820/7762]	Time 0.462 (0.326)	Data 9.78e-05 (1.53e-04)	Tok/s 50557 (43043)	Loss/tok 3.3143 (3.1950)	LR 1.250e-04
0: TRAIN [3][6830/7762]	Time 0.458 (0.326)	Data 1.04e-04 (1.53e-04)	Tok/s 49655 (43043)	Loss/tok 3.4422 (3.1950)	LR 1.250e-04
0: TRAIN [3][6840/7762]	Time 0.262 (0.326)	Data 9.80e-05 (1.53e-04)	Tok/s 40124 (43040)	Loss/tok 2.9526 (3.1949)	LR 1.250e-04
0: TRAIN [3][6850/7762]	Time 0.460 (0.326)	Data 1.01e-04 (1.52e-04)	Tok/s 50880 (43040)	Loss/tok 3.4325 (3.1949)	LR 1.250e-04
0: TRAIN [3][6860/7762]	Time 0.359 (0.326)	Data 1.01e-04 (1.52e-04)	Tok/s 46571 (43042)	Loss/tok 3.2590 (3.1949)	LR 1.250e-04
0: TRAIN [3][6870/7762]	Time 0.366 (0.326)	Data 1.02e-04 (1.52e-04)	Tok/s 46387 (43042)	Loss/tok 3.1486 (3.1949)	LR 1.250e-04
0: TRAIN [3][6880/7762]	Time 0.580 (0.327)	Data 1.02e-04 (1.52e-04)	Tok/s 51257 (43047)	Loss/tok 3.5846 (3.1952)	LR 1.250e-04
0: TRAIN [3][6890/7762]	Time 0.363 (0.327)	Data 1.00e-04 (1.52e-04)	Tok/s 46497 (43050)	Loss/tok 3.1194 (3.1952)	LR 1.250e-04
0: TRAIN [3][6900/7762]	Time 0.260 (0.327)	Data 1.03e-04 (1.52e-04)	Tok/s 38913 (43052)	Loss/tok 2.9343 (3.1951)	LR 1.250e-04
0: TRAIN [3][6910/7762]	Time 0.177 (0.327)	Data 1.00e-04 (1.52e-04)	Tok/s 29509 (43051)	Loss/tok 2.5539 (3.1951)	LR 1.250e-04
0: TRAIN [3][6920/7762]	Time 0.260 (0.326)	Data 1.27e-04 (1.52e-04)	Tok/s 39844 (43047)	Loss/tok 3.0736 (3.1950)	LR 1.250e-04
0: TRAIN [3][6930/7762]	Time 0.342 (0.326)	Data 9.75e-05 (1.52e-04)	Tok/s 49213 (43045)	Loss/tok 3.1593 (3.1949)	LR 1.250e-04
0: TRAIN [3][6940/7762]	Time 0.362 (0.326)	Data 1.15e-04 (1.52e-04)	Tok/s 46580 (43042)	Loss/tok 3.0782 (3.1947)	LR 1.250e-04
0: TRAIN [3][6950/7762]	Time 0.258 (0.326)	Data 1.21e-04 (1.52e-04)	Tok/s 40246 (43042)	Loss/tok 2.9963 (3.1947)	LR 1.250e-04
0: TRAIN [3][6960/7762]	Time 0.260 (0.326)	Data 9.80e-05 (1.52e-04)	Tok/s 39116 (43038)	Loss/tok 2.9641 (3.1945)	LR 1.250e-04
0: TRAIN [3][6970/7762]	Time 0.363 (0.326)	Data 1.05e-04 (1.52e-04)	Tok/s 46575 (43038)	Loss/tok 3.1522 (3.1945)	LR 1.250e-04
0: TRAIN [3][6980/7762]	Time 0.464 (0.326)	Data 1.01e-04 (1.52e-04)	Tok/s 50592 (43041)	Loss/tok 3.3069 (3.1945)	LR 1.250e-04
0: TRAIN [3][6990/7762]	Time 0.259 (0.326)	Data 1.05e-04 (1.51e-04)	Tok/s 39361 (43038)	Loss/tok 2.9544 (3.1943)	LR 1.250e-04
0: TRAIN [3][7000/7762]	Time 0.263 (0.326)	Data 1.17e-04 (1.51e-04)	Tok/s 39789 (43041)	Loss/tok 3.0204 (3.1946)	LR 1.250e-04
0: TRAIN [3][7010/7762]	Time 0.363 (0.326)	Data 1.24e-04 (1.51e-04)	Tok/s 46395 (43041)	Loss/tok 3.1195 (3.1945)	LR 1.250e-04
0: TRAIN [3][7020/7762]	Time 0.362 (0.326)	Data 1.00e-04 (1.51e-04)	Tok/s 46364 (43037)	Loss/tok 3.2068 (3.1944)	LR 1.250e-04
0: TRAIN [3][7030/7762]	Time 0.170 (0.326)	Data 1.15e-04 (1.51e-04)	Tok/s 29890 (43033)	Loss/tok 2.5830 (3.1942)	LR 1.250e-04
0: TRAIN [3][7040/7762]	Time 0.363 (0.326)	Data 1.00e-04 (1.51e-04)	Tok/s 45720 (43036)	Loss/tok 3.3282 (3.1943)	LR 1.250e-04
0: TRAIN [3][7050/7762]	Time 0.436 (0.326)	Data 9.73e-05 (1.51e-04)	Tok/s 53916 (43033)	Loss/tok 3.3863 (3.1943)	LR 1.250e-04
0: TRAIN [3][7060/7762]	Time 0.264 (0.326)	Data 1.01e-04 (1.51e-04)	Tok/s 39123 (43033)	Loss/tok 2.9803 (3.1942)	LR 1.250e-04
0: TRAIN [3][7070/7762]	Time 0.265 (0.326)	Data 1.01e-04 (1.51e-04)	Tok/s 38867 (43036)	Loss/tok 2.9509 (3.1943)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][7080/7762]	Time 0.543 (0.326)	Data 1.08e-04 (1.51e-04)	Tok/s 54292 (43036)	Loss/tok 3.5563 (3.1944)	LR 1.250e-04
0: TRAIN [3][7090/7762]	Time 0.263 (0.326)	Data 1.00e-04 (1.51e-04)	Tok/s 39946 (43037)	Loss/tok 3.0931 (3.1944)	LR 1.250e-04
0: TRAIN [3][7100/7762]	Time 0.266 (0.326)	Data 1.04e-04 (1.51e-04)	Tok/s 38961 (43039)	Loss/tok 2.9663 (3.1946)	LR 1.250e-04
0: TRAIN [3][7110/7762]	Time 0.258 (0.326)	Data 9.82e-05 (1.51e-04)	Tok/s 40576 (43036)	Loss/tok 2.8111 (3.1943)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][7120/7762]	Time 0.258 (0.326)	Data 1.07e-04 (1.51e-04)	Tok/s 40288 (43037)	Loss/tok 3.0737 (3.1945)	LR 1.250e-04
0: TRAIN [3][7130/7762]	Time 0.362 (0.326)	Data 9.85e-05 (1.51e-04)	Tok/s 46872 (43036)	Loss/tok 3.1239 (3.1944)	LR 1.250e-04
0: TRAIN [3][7140/7762]	Time 0.264 (0.326)	Data 9.82e-05 (1.50e-04)	Tok/s 39771 (43037)	Loss/tok 3.0985 (3.1944)	LR 1.250e-04
0: TRAIN [3][7150/7762]	Time 0.576 (0.326)	Data 9.87e-05 (1.50e-04)	Tok/s 52646 (43040)	Loss/tok 3.4974 (3.1945)	LR 1.250e-04
0: TRAIN [3][7160/7762]	Time 0.253 (0.326)	Data 9.89e-05 (1.50e-04)	Tok/s 40785 (43038)	Loss/tok 2.9055 (3.1944)	LR 1.250e-04
0: TRAIN [3][7170/7762]	Time 0.265 (0.326)	Data 9.68e-05 (1.50e-04)	Tok/s 39453 (43034)	Loss/tok 2.8822 (3.1942)	LR 1.250e-04
0: TRAIN [3][7180/7762]	Time 0.266 (0.326)	Data 1.00e-04 (1.50e-04)	Tok/s 39870 (43034)	Loss/tok 2.9996 (3.1942)	LR 1.250e-04
0: TRAIN [3][7190/7762]	Time 0.177 (0.326)	Data 1.08e-04 (1.50e-04)	Tok/s 29873 (43033)	Loss/tok 2.5776 (3.1941)	LR 1.250e-04
0: TRAIN [3][7200/7762]	Time 0.358 (0.326)	Data 9.75e-05 (1.50e-04)	Tok/s 46312 (43030)	Loss/tok 3.1843 (3.1940)	LR 1.250e-04
0: TRAIN [3][7210/7762]	Time 0.173 (0.326)	Data 1.08e-04 (1.50e-04)	Tok/s 30796 (43033)	Loss/tok 2.6254 (3.1942)	LR 1.250e-04
0: TRAIN [3][7220/7762]	Time 0.355 (0.326)	Data 1.02e-04 (1.50e-04)	Tok/s 47714 (43032)	Loss/tok 3.1422 (3.1941)	LR 1.250e-04
0: TRAIN [3][7230/7762]	Time 0.349 (0.326)	Data 1.03e-04 (1.50e-04)	Tok/s 47951 (43034)	Loss/tok 3.2675 (3.1941)	LR 1.250e-04
0: TRAIN [3][7240/7762]	Time 0.360 (0.326)	Data 1.03e-04 (1.50e-04)	Tok/s 46454 (43036)	Loss/tok 3.1885 (3.1942)	LR 1.250e-04
0: TRAIN [3][7250/7762]	Time 0.350 (0.326)	Data 9.92e-05 (1.50e-04)	Tok/s 47527 (43036)	Loss/tok 3.1553 (3.1942)	LR 1.250e-04
0: TRAIN [3][7260/7762]	Time 0.266 (0.326)	Data 1.01e-04 (1.50e-04)	Tok/s 38685 (43037)	Loss/tok 2.9538 (3.1943)	LR 1.250e-04
0: TRAIN [3][7270/7762]	Time 0.464 (0.326)	Data 1.16e-04 (1.50e-04)	Tok/s 49585 (43036)	Loss/tok 3.5242 (3.1945)	LR 1.250e-04
0: TRAIN [3][7280/7762]	Time 0.463 (0.326)	Data 9.82e-05 (1.50e-04)	Tok/s 50533 (43039)	Loss/tok 3.4131 (3.1947)	LR 1.250e-04
0: TRAIN [3][7290/7762]	Time 0.265 (0.326)	Data 9.58e-05 (1.50e-04)	Tok/s 38647 (43040)	Loss/tok 2.9360 (3.1947)	LR 1.250e-04
0: TRAIN [3][7300/7762]	Time 0.266 (0.326)	Data 1.01e-04 (1.49e-04)	Tok/s 38524 (43037)	Loss/tok 3.0406 (3.1946)	LR 1.250e-04
0: TRAIN [3][7310/7762]	Time 0.256 (0.326)	Data 1.12e-04 (1.49e-04)	Tok/s 40124 (43037)	Loss/tok 3.0408 (3.1946)	LR 1.250e-04
0: TRAIN [3][7320/7762]	Time 0.270 (0.326)	Data 1.05e-04 (1.49e-04)	Tok/s 38040 (43037)	Loss/tok 3.0470 (3.1948)	LR 1.250e-04
0: TRAIN [3][7330/7762]	Time 0.343 (0.326)	Data 1.01e-04 (1.49e-04)	Tok/s 48613 (43036)	Loss/tok 3.1680 (3.1948)	LR 1.250e-04
0: TRAIN [3][7340/7762]	Time 0.262 (0.326)	Data 1.05e-04 (1.49e-04)	Tok/s 39227 (43036)	Loss/tok 2.9297 (3.1947)	LR 1.250e-04
0: TRAIN [3][7350/7762]	Time 0.462 (0.326)	Data 9.73e-05 (1.49e-04)	Tok/s 50227 (43036)	Loss/tok 3.3828 (3.1947)	LR 1.250e-04
0: TRAIN [3][7360/7762]	Time 0.362 (0.326)	Data 9.92e-05 (1.49e-04)	Tok/s 46568 (43033)	Loss/tok 3.1432 (3.1945)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][7370/7762]	Time 0.461 (0.326)	Data 1.00e-04 (1.49e-04)	Tok/s 50119 (43034)	Loss/tok 3.4373 (3.1945)	LR 1.250e-04
0: TRAIN [3][7380/7762]	Time 0.342 (0.326)	Data 1.03e-04 (1.49e-04)	Tok/s 48462 (43032)	Loss/tok 3.2879 (3.1944)	LR 1.250e-04
0: TRAIN [3][7390/7762]	Time 0.251 (0.326)	Data 9.97e-05 (1.49e-04)	Tok/s 40973 (43034)	Loss/tok 2.9090 (3.1944)	LR 1.250e-04
0: TRAIN [3][7400/7762]	Time 0.267 (0.326)	Data 9.87e-05 (1.49e-04)	Tok/s 39346 (43031)	Loss/tok 3.1435 (3.1944)	LR 1.250e-04
0: TRAIN [3][7410/7762]	Time 0.359 (0.326)	Data 1.01e-04 (1.49e-04)	Tok/s 46387 (43034)	Loss/tok 3.3035 (3.1946)	LR 1.250e-04
0: TRAIN [3][7420/7762]	Time 0.343 (0.326)	Data 1.07e-04 (1.49e-04)	Tok/s 49077 (43032)	Loss/tok 3.2205 (3.1946)	LR 1.250e-04
0: TRAIN [3][7430/7762]	Time 0.366 (0.326)	Data 1.20e-04 (1.49e-04)	Tok/s 45774 (43030)	Loss/tok 3.2139 (3.1945)	LR 1.250e-04
0: TRAIN [3][7440/7762]	Time 0.267 (0.326)	Data 9.78e-05 (1.49e-04)	Tok/s 38257 (43032)	Loss/tok 2.9702 (3.1945)	LR 1.250e-04
0: TRAIN [3][7450/7762]	Time 0.365 (0.326)	Data 1.57e-04 (1.49e-04)	Tok/s 45842 (43033)	Loss/tok 3.1931 (3.1945)	LR 1.250e-04
0: TRAIN [3][7460/7762]	Time 0.359 (0.326)	Data 9.82e-05 (1.48e-04)	Tok/s 46985 (43031)	Loss/tok 3.1780 (3.1945)	LR 1.250e-04
0: TRAIN [3][7470/7762]	Time 0.263 (0.326)	Data 1.02e-04 (1.48e-04)	Tok/s 39454 (43032)	Loss/tok 2.9855 (3.1944)	LR 1.250e-04
0: TRAIN [3][7480/7762]	Time 0.588 (0.326)	Data 1.10e-04 (1.48e-04)	Tok/s 50375 (43032)	Loss/tok 3.5210 (3.1945)	LR 1.250e-04
0: TRAIN [3][7490/7762]	Time 0.254 (0.326)	Data 9.82e-05 (1.48e-04)	Tok/s 41664 (43030)	Loss/tok 2.9798 (3.1944)	LR 1.250e-04
0: TRAIN [3][7500/7762]	Time 0.452 (0.326)	Data 1.03e-04 (1.48e-04)	Tok/s 51240 (43028)	Loss/tok 3.3541 (3.1943)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][7510/7762]	Time 0.357 (0.326)	Data 1.13e-04 (1.48e-04)	Tok/s 47456 (43024)	Loss/tok 3.1498 (3.1943)	LR 1.250e-04
0: TRAIN [3][7520/7762]	Time 0.364 (0.326)	Data 1.17e-04 (1.48e-04)	Tok/s 45437 (43025)	Loss/tok 3.2201 (3.1943)	LR 1.250e-04
0: TRAIN [3][7530/7762]	Time 0.262 (0.326)	Data 9.94e-05 (1.48e-04)	Tok/s 39589 (43024)	Loss/tok 3.0582 (3.1942)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][7540/7762]	Time 0.265 (0.326)	Data 9.94e-05 (1.48e-04)	Tok/s 38454 (43024)	Loss/tok 2.9473 (3.1943)	LR 1.250e-04
0: TRAIN [3][7550/7762]	Time 0.357 (0.326)	Data 1.17e-04 (1.48e-04)	Tok/s 46991 (43023)	Loss/tok 3.1046 (3.1942)	LR 1.250e-04
0: TRAIN [3][7560/7762]	Time 0.261 (0.326)	Data 9.94e-05 (1.48e-04)	Tok/s 40624 (43025)	Loss/tok 3.0024 (3.1942)	LR 1.250e-04
0: TRAIN [3][7570/7762]	Time 0.363 (0.326)	Data 1.02e-04 (1.48e-04)	Tok/s 46200 (43025)	Loss/tok 3.1452 (3.1942)	LR 1.250e-04
0: TRAIN [3][7580/7762]	Time 0.267 (0.326)	Data 2.02e-04 (1.48e-04)	Tok/s 39341 (43023)	Loss/tok 2.9703 (3.1941)	LR 1.250e-04
0: TRAIN [3][7590/7762]	Time 0.587 (0.326)	Data 9.78e-05 (1.48e-04)	Tok/s 50340 (43024)	Loss/tok 3.5669 (3.1942)	LR 1.250e-04
0: TRAIN [3][7600/7762]	Time 0.365 (0.326)	Data 1.17e-04 (1.48e-04)	Tok/s 45461 (43028)	Loss/tok 3.1657 (3.1943)	LR 1.250e-04
0: TRAIN [3][7610/7762]	Time 0.260 (0.326)	Data 9.97e-05 (1.48e-04)	Tok/s 39520 (43030)	Loss/tok 3.1005 (3.1944)	LR 1.250e-04
0: TRAIN [3][7620/7762]	Time 0.267 (0.326)	Data 1.04e-04 (1.48e-04)	Tok/s 39644 (43030)	Loss/tok 2.9318 (3.1944)	LR 1.250e-04
0: TRAIN [3][7630/7762]	Time 0.365 (0.326)	Data 1.04e-04 (1.48e-04)	Tok/s 45973 (43032)	Loss/tok 3.2611 (3.1945)	LR 1.250e-04
0: TRAIN [3][7640/7762]	Time 0.266 (0.326)	Data 1.07e-04 (1.47e-04)	Tok/s 37783 (43036)	Loss/tok 3.0780 (3.1946)	LR 1.250e-04
0: TRAIN [3][7650/7762]	Time 0.174 (0.326)	Data 9.70e-05 (1.47e-04)	Tok/s 29670 (43032)	Loss/tok 2.5461 (3.1945)	LR 1.250e-04
0: TRAIN [3][7660/7762]	Time 0.357 (0.326)	Data 1.00e-04 (1.47e-04)	Tok/s 46401 (43037)	Loss/tok 3.1331 (3.1946)	LR 1.250e-04
0: TRAIN [3][7670/7762]	Time 0.451 (0.326)	Data 1.17e-04 (1.47e-04)	Tok/s 51769 (43036)	Loss/tok 3.5347 (3.1946)	LR 1.250e-04
0: TRAIN [3][7680/7762]	Time 0.462 (0.326)	Data 1.03e-04 (1.47e-04)	Tok/s 51154 (43038)	Loss/tok 3.3168 (3.1946)	LR 1.250e-04
0: TRAIN [3][7690/7762]	Time 0.469 (0.326)	Data 1.01e-04 (1.47e-04)	Tok/s 49429 (43040)	Loss/tok 3.3683 (3.1945)	LR 1.250e-04
0: TRAIN [3][7700/7762]	Time 0.267 (0.326)	Data 9.92e-05 (1.47e-04)	Tok/s 38804 (43037)	Loss/tok 2.9399 (3.1945)	LR 1.250e-04
0: TRAIN [3][7710/7762]	Time 0.262 (0.326)	Data 1.04e-04 (1.47e-04)	Tok/s 39916 (43041)	Loss/tok 3.0305 (3.1945)	LR 1.250e-04
0: TRAIN [3][7720/7762]	Time 0.462 (0.326)	Data 1.06e-04 (1.47e-04)	Tok/s 50269 (43045)	Loss/tok 3.3936 (3.1947)	LR 1.250e-04
0: TRAIN [3][7730/7762]	Time 0.354 (0.326)	Data 1.03e-04 (1.47e-04)	Tok/s 47937 (43043)	Loss/tok 3.1452 (3.1946)	LR 1.250e-04
0: TRAIN [3][7740/7762]	Time 0.343 (0.327)	Data 1.05e-04 (1.47e-04)	Tok/s 49133 (43047)	Loss/tok 3.1125 (3.1946)	LR 1.250e-04
0: TRAIN [3][7750/7762]	Time 0.175 (0.326)	Data 1.03e-04 (1.47e-04)	Tok/s 29667 (43043)	Loss/tok 2.6437 (3.1945)	LR 1.250e-04
0: TRAIN [3][7760/7762]	Time 0.270 (0.326)	Data 1.50e-02 (1.49e-04)	Tok/s 38336 (43041)	Loss/tok 2.8897 (3.1945)	LR 1.250e-04
:::MLL 1573755998.788 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1573755998.789 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/12]	Time 0.867 (0.867)	Decoder iters 141.0 (141.0)	Tok/s 18882 (18882)
0: TEST [3][10/12]	Time 0.118 (0.294)	Decoder iters 25.0 (52.0)	Tok/s 31457 (29939)
0: Running moses detokenizer
0: BLEU(score=23.361160224875956, counts=[36494, 18019, 10139, 5925], totals=[64944, 61941, 58939, 55942], precisions=[56.1930278393693, 29.090586202999628, 17.202531430801336, 10.591326731257373], bp=1.0, sys_len=64944, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1573756004.367 eval_accuracy: {"value": 23.36, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1573756004.368 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.1940	Test BLEU: 23.36
0: Performance: Epoch: 3	Training: 86077 Tok/s
0: Finished epoch 3
:::MLL 1573756004.368 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
:::MLL 1573756004.369 block_start: {"value": null, "metadata": {"first_epoch_num": 5, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1573756004.369 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 514}}
0: Starting epoch 4
0: Executing preallocation
0: Sampler for epoch 4 uses seed 3854938788
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][0/7762]	Time 0.588 (0.588)	Data 3.08e-01 (3.08e-01)	Tok/s 17220 (17220)	Loss/tok 3.1221 (3.1221)	LR 1.250e-04
0: TRAIN [4][10/7762]	Time 0.566 (0.334)	Data 9.94e-05 (2.81e-02)	Tok/s 53205 (39899)	Loss/tok 3.4464 (3.1806)	LR 1.250e-04
0: TRAIN [4][20/7762]	Time 0.257 (0.303)	Data 9.54e-05 (1.48e-02)	Tok/s 41198 (40240)	Loss/tok 2.9464 (3.1180)	LR 1.250e-04
0: TRAIN [4][30/7762]	Time 0.254 (0.314)	Data 1.02e-04 (1.00e-02)	Tok/s 41297 (41598)	Loss/tok 2.9810 (3.1429)	LR 1.250e-04
0: TRAIN [4][40/7762]	Time 0.256 (0.315)	Data 1.01e-04 (7.61e-03)	Tok/s 40778 (42082)	Loss/tok 3.0120 (3.1538)	LR 1.250e-04
0: TRAIN [4][50/7762]	Time 0.259 (0.308)	Data 9.99e-05 (6.14e-03)	Tok/s 39181 (41803)	Loss/tok 2.8841 (3.1411)	LR 1.250e-04
0: TRAIN [4][60/7762]	Time 0.358 (0.314)	Data 9.89e-05 (5.15e-03)	Tok/s 46769 (42502)	Loss/tok 3.1810 (3.1575)	LR 1.250e-04
0: TRAIN [4][70/7762]	Time 0.449 (0.313)	Data 9.99e-05 (4.44e-03)	Tok/s 52203 (42524)	Loss/tok 3.3533 (3.1557)	LR 1.250e-04
0: TRAIN [4][80/7762]	Time 0.351 (0.314)	Data 1.03e-04 (3.90e-03)	Tok/s 47730 (42524)	Loss/tok 3.2826 (3.1604)	LR 1.250e-04
0: TRAIN [4][90/7762]	Time 0.458 (0.319)	Data 1.01e-04 (3.49e-03)	Tok/s 50407 (42887)	Loss/tok 3.3874 (3.1739)	LR 1.250e-04
0: TRAIN [4][100/7762]	Time 0.178 (0.321)	Data 9.82e-05 (3.15e-03)	Tok/s 29603 (42978)	Loss/tok 2.6284 (3.1793)	LR 1.250e-04
0: TRAIN [4][110/7762]	Time 0.260 (0.323)	Data 1.01e-04 (2.88e-03)	Tok/s 39547 (43150)	Loss/tok 2.9233 (3.1758)	LR 1.250e-04
0: TRAIN [4][120/7762]	Time 0.364 (0.323)	Data 1.18e-04 (2.65e-03)	Tok/s 47008 (43193)	Loss/tok 3.1934 (3.1699)	LR 1.250e-04
0: TRAIN [4][130/7762]	Time 0.264 (0.322)	Data 1.15e-04 (2.45e-03)	Tok/s 39911 (43183)	Loss/tok 3.0607 (3.1671)	LR 1.250e-04
0: TRAIN [4][140/7762]	Time 0.255 (0.321)	Data 1.02e-04 (2.29e-03)	Tok/s 40762 (43180)	Loss/tok 3.0497 (3.1685)	LR 1.250e-04
0: TRAIN [4][150/7762]	Time 0.461 (0.326)	Data 1.04e-04 (2.14e-03)	Tok/s 50447 (43328)	Loss/tok 3.3688 (3.1758)	LR 1.250e-04
0: TRAIN [4][160/7762]	Time 0.259 (0.324)	Data 9.92e-05 (2.02e-03)	Tok/s 40139 (43251)	Loss/tok 3.0766 (3.1732)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][170/7762]	Time 0.175 (0.324)	Data 1.01e-04 (1.90e-03)	Tok/s 30150 (43234)	Loss/tok 2.5421 (3.1717)	LR 1.250e-04
0: TRAIN [4][180/7762]	Time 0.266 (0.323)	Data 1.01e-04 (1.80e-03)	Tok/s 38426 (43156)	Loss/tok 2.9616 (3.1686)	LR 1.250e-04
0: TRAIN [4][190/7762]	Time 0.363 (0.322)	Data 1.23e-04 (1.72e-03)	Tok/s 46786 (43123)	Loss/tok 3.2324 (3.1655)	LR 1.250e-04
0: TRAIN [4][200/7762]	Time 0.267 (0.321)	Data 1.00e-04 (1.64e-03)	Tok/s 38787 (43076)	Loss/tok 2.9989 (3.1625)	LR 1.250e-04
0: TRAIN [4][210/7762]	Time 0.462 (0.320)	Data 1.05e-04 (1.56e-03)	Tok/s 50907 (43033)	Loss/tok 3.3584 (3.1610)	LR 1.250e-04
0: TRAIN [4][220/7762]	Time 0.589 (0.321)	Data 1.07e-04 (1.50e-03)	Tok/s 50099 (43024)	Loss/tok 3.5353 (3.1625)	LR 1.250e-04
0: TRAIN [4][230/7762]	Time 0.344 (0.320)	Data 1.07e-04 (1.44e-03)	Tok/s 48949 (43006)	Loss/tok 3.2515 (3.1609)	LR 1.250e-04
0: TRAIN [4][240/7762]	Time 0.363 (0.319)	Data 1.17e-04 (1.38e-03)	Tok/s 45329 (42896)	Loss/tok 3.1996 (3.1618)	LR 1.250e-04
0: TRAIN [4][250/7762]	Time 0.459 (0.319)	Data 1.02e-04 (1.33e-03)	Tok/s 50405 (42833)	Loss/tok 3.3170 (3.1601)	LR 1.250e-04
0: TRAIN [4][260/7762]	Time 0.176 (0.319)	Data 9.61e-05 (1.28e-03)	Tok/s 29872 (42778)	Loss/tok 2.5065 (3.1598)	LR 1.250e-04
0: TRAIN [4][270/7762]	Time 0.265 (0.320)	Data 1.02e-04 (1.24e-03)	Tok/s 39554 (42859)	Loss/tok 2.8667 (3.1624)	LR 1.250e-04
0: TRAIN [4][280/7762]	Time 0.259 (0.320)	Data 9.89e-05 (1.20e-03)	Tok/s 39885 (42891)	Loss/tok 3.0335 (3.1619)	LR 1.250e-04
0: TRAIN [4][290/7762]	Time 0.354 (0.321)	Data 1.02e-04 (1.16e-03)	Tok/s 46752 (42921)	Loss/tok 3.1949 (3.1612)	LR 1.250e-04
0: TRAIN [4][300/7762]	Time 0.262 (0.319)	Data 1.04e-04 (1.13e-03)	Tok/s 38643 (42820)	Loss/tok 3.0804 (3.1578)	LR 1.250e-04
0: TRAIN [4][310/7762]	Time 0.260 (0.318)	Data 9.78e-05 (1.09e-03)	Tok/s 40661 (42742)	Loss/tok 2.9461 (3.1538)	LR 1.250e-04
0: TRAIN [4][320/7762]	Time 0.588 (0.318)	Data 9.54e-05 (1.06e-03)	Tok/s 50392 (42707)	Loss/tok 3.5563 (3.1561)	LR 1.250e-04
0: TRAIN [4][330/7762]	Time 0.265 (0.319)	Data 1.03e-04 (1.03e-03)	Tok/s 39093 (42750)	Loss/tok 3.0010 (3.1596)	LR 1.250e-04
0: TRAIN [4][340/7762]	Time 0.362 (0.318)	Data 1.03e-04 (1.01e-03)	Tok/s 46459 (42697)	Loss/tok 3.1936 (3.1581)	LR 1.250e-04
0: TRAIN [4][350/7762]	Time 0.260 (0.317)	Data 9.94e-05 (9.82e-04)	Tok/s 39387 (42659)	Loss/tok 2.9988 (3.1564)	LR 1.250e-04
0: TRAIN [4][360/7762]	Time 0.264 (0.317)	Data 1.02e-04 (9.57e-04)	Tok/s 39290 (42588)	Loss/tok 2.9680 (3.1558)	LR 1.250e-04
0: TRAIN [4][370/7762]	Time 0.261 (0.316)	Data 1.02e-04 (9.34e-04)	Tok/s 40103 (42556)	Loss/tok 3.0213 (3.1544)	LR 1.250e-04
0: TRAIN [4][380/7762]	Time 0.258 (0.316)	Data 9.61e-05 (9.13e-04)	Tok/s 40230 (42567)	Loss/tok 2.9782 (3.1543)	LR 1.250e-04
0: TRAIN [4][390/7762]	Time 0.363 (0.316)	Data 1.17e-04 (8.92e-04)	Tok/s 47222 (42593)	Loss/tok 3.0326 (3.1542)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][400/7762]	Time 0.359 (0.317)	Data 1.17e-04 (8.72e-04)	Tok/s 47222 (42618)	Loss/tok 3.2272 (3.1542)	LR 1.250e-04
0: TRAIN [4][410/7762]	Time 0.264 (0.317)	Data 1.05e-04 (8.53e-04)	Tok/s 37608 (42634)	Loss/tok 3.0230 (3.1535)	LR 1.250e-04
0: TRAIN [4][420/7762]	Time 0.364 (0.316)	Data 1.00e-04 (8.36e-04)	Tok/s 46261 (42581)	Loss/tok 3.2176 (3.1514)	LR 1.250e-04
0: TRAIN [4][430/7762]	Time 0.175 (0.315)	Data 9.89e-05 (8.19e-04)	Tok/s 29560 (42489)	Loss/tok 2.4850 (3.1515)	LR 1.250e-04
0: TRAIN [4][440/7762]	Time 0.461 (0.315)	Data 1.19e-04 (8.02e-04)	Tok/s 50730 (42475)	Loss/tok 3.2720 (3.1517)	LR 1.250e-04
0: TRAIN [4][450/7762]	Time 0.175 (0.316)	Data 1.05e-04 (7.87e-04)	Tok/s 30194 (42477)	Loss/tok 2.6012 (3.1534)	LR 1.250e-04
0: TRAIN [4][460/7762]	Time 0.260 (0.317)	Data 1.05e-04 (7.72e-04)	Tok/s 39977 (42563)	Loss/tok 3.0882 (3.1561)	LR 1.250e-04
0: TRAIN [4][470/7762]	Time 0.259 (0.317)	Data 1.02e-04 (7.58e-04)	Tok/s 39402 (42587)	Loss/tok 3.0310 (3.1564)	LR 1.250e-04
0: TRAIN [4][480/7762]	Time 0.589 (0.319)	Data 1.03e-04 (7.44e-04)	Tok/s 51228 (42673)	Loss/tok 3.4578 (3.1591)	LR 1.250e-04
0: TRAIN [4][490/7762]	Time 0.368 (0.319)	Data 1.04e-04 (7.31e-04)	Tok/s 45935 (42716)	Loss/tok 3.1335 (3.1586)	LR 1.250e-04
0: TRAIN [4][500/7762]	Time 0.568 (0.319)	Data 9.66e-05 (7.19e-04)	Tok/s 52315 (42677)	Loss/tok 3.5105 (3.1607)	LR 1.250e-04
0: TRAIN [4][510/7762]	Time 0.367 (0.318)	Data 1.04e-04 (7.07e-04)	Tok/s 45661 (42600)	Loss/tok 3.0831 (3.1582)	LR 1.250e-04
0: TRAIN [4][520/7762]	Time 0.180 (0.319)	Data 1.03e-04 (6.95e-04)	Tok/s 29027 (42627)	Loss/tok 2.5823 (3.1591)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][530/7762]	Time 0.254 (0.319)	Data 1.06e-04 (6.84e-04)	Tok/s 39562 (42622)	Loss/tok 2.8324 (3.1600)	LR 1.250e-04
0: TRAIN [4][540/7762]	Time 0.465 (0.320)	Data 1.08e-04 (6.73e-04)	Tok/s 49965 (42656)	Loss/tok 3.3888 (3.1627)	LR 1.250e-04
0: TRAIN [4][550/7762]	Time 0.268 (0.320)	Data 1.02e-04 (6.63e-04)	Tok/s 38423 (42645)	Loss/tok 2.9793 (3.1627)	LR 1.250e-04
0: TRAIN [4][560/7762]	Time 0.256 (0.320)	Data 1.00e-04 (6.53e-04)	Tok/s 39903 (42647)	Loss/tok 3.0142 (3.1616)	LR 1.250e-04
0: TRAIN [4][570/7762]	Time 0.262 (0.321)	Data 1.02e-04 (6.43e-04)	Tok/s 39465 (42726)	Loss/tok 2.9581 (3.1632)	LR 1.250e-04
0: TRAIN [4][580/7762]	Time 0.176 (0.320)	Data 1.01e-04 (6.34e-04)	Tok/s 29556 (42686)	Loss/tok 2.5822 (3.1617)	LR 1.250e-04
0: TRAIN [4][590/7762]	Time 0.263 (0.319)	Data 1.03e-04 (6.25e-04)	Tok/s 39429 (42644)	Loss/tok 3.0088 (3.1596)	LR 1.250e-04
0: TRAIN [4][600/7762]	Time 0.342 (0.321)	Data 1.03e-04 (6.16e-04)	Tok/s 49122 (42729)	Loss/tok 3.2161 (3.1621)	LR 1.250e-04
0: TRAIN [4][610/7762]	Time 0.265 (0.320)	Data 1.20e-04 (6.08e-04)	Tok/s 39937 (42712)	Loss/tok 3.0216 (3.1609)	LR 1.250e-04
0: TRAIN [4][620/7762]	Time 0.263 (0.320)	Data 1.15e-04 (6.00e-04)	Tok/s 39644 (42705)	Loss/tok 2.9329 (3.1611)	LR 1.250e-04
0: TRAIN [4][630/7762]	Time 0.363 (0.321)	Data 1.04e-04 (5.92e-04)	Tok/s 46103 (42766)	Loss/tok 3.3562 (3.1640)	LR 1.250e-04
0: TRAIN [4][640/7762]	Time 0.177 (0.321)	Data 1.19e-04 (5.85e-04)	Tok/s 28666 (42746)	Loss/tok 2.5070 (3.1637)	LR 1.250e-04
0: TRAIN [4][650/7762]	Time 0.262 (0.320)	Data 9.56e-05 (5.77e-04)	Tok/s 38784 (42672)	Loss/tok 2.9624 (3.1619)	LR 1.250e-04
0: TRAIN [4][660/7762]	Time 0.262 (0.320)	Data 1.03e-04 (5.70e-04)	Tok/s 39815 (42647)	Loss/tok 2.9879 (3.1603)	LR 1.250e-04
0: TRAIN [4][670/7762]	Time 0.353 (0.319)	Data 1.05e-04 (5.64e-04)	Tok/s 47558 (42651)	Loss/tok 3.1566 (3.1594)	LR 1.250e-04
0: TRAIN [4][680/7762]	Time 0.252 (0.319)	Data 9.97e-05 (5.57e-04)	Tok/s 39669 (42635)	Loss/tok 3.0014 (3.1590)	LR 1.250e-04
0: TRAIN [4][690/7762]	Time 0.587 (0.319)	Data 1.01e-04 (5.50e-04)	Tok/s 49972 (42632)	Loss/tok 3.5829 (3.1592)	LR 1.250e-04
0: TRAIN [4][700/7762]	Time 0.451 (0.320)	Data 9.97e-05 (5.44e-04)	Tok/s 51037 (42670)	Loss/tok 3.3997 (3.1604)	LR 1.250e-04
0: TRAIN [4][710/7762]	Time 0.368 (0.321)	Data 1.18e-04 (5.38e-04)	Tok/s 45559 (42709)	Loss/tok 3.2184 (3.1617)	LR 1.250e-04
0: TRAIN [4][720/7762]	Time 0.262 (0.320)	Data 1.16e-04 (5.32e-04)	Tok/s 39198 (42684)	Loss/tok 3.0895 (3.1609)	LR 1.250e-04
0: TRAIN [4][730/7762]	Time 0.341 (0.321)	Data 1.24e-04 (5.26e-04)	Tok/s 48649 (42727)	Loss/tok 3.1443 (3.1608)	LR 1.250e-04
0: TRAIN [4][740/7762]	Time 0.573 (0.321)	Data 9.75e-05 (5.20e-04)	Tok/s 51433 (42732)	Loss/tok 3.5943 (3.1618)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][750/7762]	Time 0.361 (0.321)	Data 1.01e-04 (5.15e-04)	Tok/s 45537 (42738)	Loss/tok 3.2022 (3.1620)	LR 1.250e-04
0: TRAIN [4][760/7762]	Time 0.461 (0.321)	Data 9.61e-05 (5.09e-04)	Tok/s 50431 (42763)	Loss/tok 3.3793 (3.1623)	LR 1.250e-04
0: TRAIN [4][770/7762]	Time 0.269 (0.322)	Data 1.04e-04 (5.04e-04)	Tok/s 37814 (42787)	Loss/tok 2.9709 (3.1641)	LR 1.250e-04
0: TRAIN [4][780/7762]	Time 0.262 (0.322)	Data 1.13e-04 (4.99e-04)	Tok/s 38425 (42778)	Loss/tok 3.0278 (3.1638)	LR 1.250e-04
0: TRAIN [4][790/7762]	Time 0.267 (0.322)	Data 1.02e-04 (4.94e-04)	Tok/s 38948 (42797)	Loss/tok 2.9848 (3.1651)	LR 1.250e-04
0: TRAIN [4][800/7762]	Time 0.262 (0.322)	Data 9.78e-05 (4.89e-04)	Tok/s 39381 (42763)	Loss/tok 2.9109 (3.1639)	LR 1.250e-04
0: TRAIN [4][810/7762]	Time 0.584 (0.323)	Data 1.01e-04 (4.84e-04)	Tok/s 51421 (42800)	Loss/tok 3.5254 (3.1658)	LR 1.250e-04
0: TRAIN [4][820/7762]	Time 0.435 (0.323)	Data 1.04e-04 (4.80e-04)	Tok/s 54134 (42802)	Loss/tok 3.2822 (3.1663)	LR 1.250e-04
0: TRAIN [4][830/7762]	Time 0.366 (0.323)	Data 1.13e-04 (4.75e-04)	Tok/s 45809 (42805)	Loss/tok 3.1833 (3.1668)	LR 1.250e-04
0: TRAIN [4][840/7762]	Time 0.259 (0.322)	Data 9.82e-05 (4.71e-04)	Tok/s 40582 (42779)	Loss/tok 2.9500 (3.1658)	LR 1.250e-04
0: TRAIN [4][850/7762]	Time 0.263 (0.323)	Data 1.03e-04 (4.67e-04)	Tok/s 39171 (42786)	Loss/tok 2.9931 (3.1662)	LR 1.250e-04
0: TRAIN [4][860/7762]	Time 0.261 (0.323)	Data 1.01e-04 (4.62e-04)	Tok/s 38937 (42799)	Loss/tok 2.9471 (3.1670)	LR 1.250e-04
0: TRAIN [4][870/7762]	Time 0.261 (0.322)	Data 9.92e-05 (4.58e-04)	Tok/s 40253 (42767)	Loss/tok 3.0001 (3.1661)	LR 1.250e-04
0: TRAIN [4][880/7762]	Time 0.251 (0.322)	Data 1.18e-04 (4.54e-04)	Tok/s 41398 (42766)	Loss/tok 2.9134 (3.1662)	LR 1.250e-04
0: TRAIN [4][890/7762]	Time 0.348 (0.323)	Data 9.51e-05 (4.50e-04)	Tok/s 48523 (42777)	Loss/tok 3.1759 (3.1661)	LR 1.250e-04
0: TRAIN [4][900/7762]	Time 0.459 (0.323)	Data 9.99e-05 (4.46e-04)	Tok/s 50795 (42804)	Loss/tok 3.3605 (3.1663)	LR 1.250e-04
0: TRAIN [4][910/7762]	Time 0.254 (0.323)	Data 1.15e-04 (4.43e-04)	Tok/s 41722 (42806)	Loss/tok 3.0131 (3.1662)	LR 1.250e-04
0: TRAIN [4][920/7762]	Time 0.457 (0.323)	Data 1.00e-04 (4.39e-04)	Tok/s 51284 (42834)	Loss/tok 3.2838 (3.1663)	LR 1.250e-04
0: TRAIN [4][930/7762]	Time 0.261 (0.323)	Data 9.94e-05 (4.35e-04)	Tok/s 39105 (42827)	Loss/tok 3.0155 (3.1658)	LR 1.250e-04
0: TRAIN [4][940/7762]	Time 0.265 (0.323)	Data 9.99e-05 (4.32e-04)	Tok/s 39491 (42827)	Loss/tok 2.9618 (3.1651)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][950/7762]	Time 0.252 (0.323)	Data 1.09e-04 (4.28e-04)	Tok/s 40336 (42819)	Loss/tok 2.9617 (3.1653)	LR 1.250e-04
0: TRAIN [4][960/7762]	Time 0.264 (0.323)	Data 1.05e-04 (4.25e-04)	Tok/s 38472 (42804)	Loss/tok 2.9463 (3.1647)	LR 1.250e-04
0: TRAIN [4][970/7762]	Time 0.266 (0.323)	Data 1.03e-04 (4.22e-04)	Tok/s 37844 (42795)	Loss/tok 2.9651 (3.1649)	LR 1.250e-04
0: TRAIN [4][980/7762]	Time 0.267 (0.322)	Data 1.06e-04 (4.18e-04)	Tok/s 38696 (42769)	Loss/tok 2.8595 (3.1638)	LR 1.250e-04
0: TRAIN [4][990/7762]	Time 0.252 (0.322)	Data 1.03e-04 (4.15e-04)	Tok/s 40097 (42778)	Loss/tok 2.8552 (3.1643)	LR 1.250e-04
0: TRAIN [4][1000/7762]	Time 0.461 (0.322)	Data 1.01e-04 (4.12e-04)	Tok/s 51082 (42790)	Loss/tok 3.2907 (3.1642)	LR 1.250e-04
0: TRAIN [4][1010/7762]	Time 0.261 (0.322)	Data 1.07e-04 (4.09e-04)	Tok/s 39015 (42804)	Loss/tok 2.9120 (3.1641)	LR 1.250e-04
0: TRAIN [4][1020/7762]	Time 0.363 (0.322)	Data 1.00e-04 (4.06e-04)	Tok/s 46542 (42804)	Loss/tok 3.1643 (3.1640)	LR 1.250e-04
0: TRAIN [4][1030/7762]	Time 0.345 (0.323)	Data 1.01e-04 (4.03e-04)	Tok/s 48893 (42801)	Loss/tok 3.0736 (3.1637)	LR 1.250e-04
0: TRAIN [4][1040/7762]	Time 0.260 (0.323)	Data 1.05e-04 (4.00e-04)	Tok/s 39581 (42837)	Loss/tok 3.0120 (3.1642)	LR 1.250e-04
0: TRAIN [4][1050/7762]	Time 0.360 (0.323)	Data 1.01e-04 (3.98e-04)	Tok/s 46110 (42837)	Loss/tok 3.1607 (3.1644)	LR 1.250e-04
0: TRAIN [4][1060/7762]	Time 0.592 (0.324)	Data 1.18e-04 (3.95e-04)	Tok/s 50096 (42863)	Loss/tok 3.5864 (3.1657)	LR 1.250e-04
0: TRAIN [4][1070/7762]	Time 0.357 (0.323)	Data 9.99e-05 (3.92e-04)	Tok/s 47417 (42839)	Loss/tok 3.2588 (3.1653)	LR 1.250e-04
0: TRAIN [4][1080/7762]	Time 0.363 (0.324)	Data 9.75e-05 (3.90e-04)	Tok/s 46277 (42865)	Loss/tok 3.1943 (3.1663)	LR 1.250e-04
0: TRAIN [4][1090/7762]	Time 0.261 (0.324)	Data 1.14e-04 (3.87e-04)	Tok/s 39331 (42863)	Loss/tok 2.9383 (3.1660)	LR 1.250e-04
0: TRAIN [4][1100/7762]	Time 0.359 (0.324)	Data 1.02e-04 (3.84e-04)	Tok/s 46261 (42873)	Loss/tok 3.2510 (3.1665)	LR 1.250e-04
0: TRAIN [4][1110/7762]	Time 0.176 (0.324)	Data 1.00e-04 (3.82e-04)	Tok/s 29621 (42851)	Loss/tok 2.5717 (3.1659)	LR 1.250e-04
0: TRAIN [4][1120/7762]	Time 0.259 (0.323)	Data 1.03e-04 (3.79e-04)	Tok/s 39119 (42855)	Loss/tok 3.0153 (3.1656)	LR 1.250e-04
0: TRAIN [4][1130/7762]	Time 0.357 (0.324)	Data 1.11e-04 (3.77e-04)	Tok/s 47150 (42860)	Loss/tok 3.1485 (3.1655)	LR 1.250e-04
0: TRAIN [4][1140/7762]	Time 0.268 (0.324)	Data 1.19e-04 (3.75e-04)	Tok/s 38440 (42864)	Loss/tok 2.9782 (3.1654)	LR 1.250e-04
0: TRAIN [4][1150/7762]	Time 0.257 (0.324)	Data 1.03e-04 (3.72e-04)	Tok/s 40516 (42878)	Loss/tok 2.9574 (3.1656)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1160/7762]	Time 0.460 (0.324)	Data 1.02e-04 (3.70e-04)	Tok/s 50624 (42894)	Loss/tok 3.4235 (3.1674)	LR 1.250e-04
0: TRAIN [4][1170/7762]	Time 0.451 (0.325)	Data 1.06e-04 (3.68e-04)	Tok/s 51889 (42928)	Loss/tok 3.3424 (3.1690)	LR 1.250e-04
0: TRAIN [4][1180/7762]	Time 0.265 (0.325)	Data 1.03e-04 (3.66e-04)	Tok/s 38989 (42945)	Loss/tok 2.9577 (3.1707)	LR 1.250e-04
0: TRAIN [4][1190/7762]	Time 0.349 (0.325)	Data 1.15e-04 (3.63e-04)	Tok/s 48401 (42951)	Loss/tok 3.2140 (3.1702)	LR 1.250e-04
0: TRAIN [4][1200/7762]	Time 0.263 (0.325)	Data 1.03e-04 (3.61e-04)	Tok/s 39057 (42938)	Loss/tok 3.0067 (3.1697)	LR 1.250e-04
0: TRAIN [4][1210/7762]	Time 0.367 (0.325)	Data 1.17e-04 (3.59e-04)	Tok/s 45796 (42959)	Loss/tok 3.2809 (3.1705)	LR 1.250e-04
0: TRAIN [4][1220/7762]	Time 0.257 (0.325)	Data 1.03e-04 (3.57e-04)	Tok/s 39743 (42953)	Loss/tok 2.9914 (3.1705)	LR 1.250e-04
0: TRAIN [4][1230/7762]	Time 0.269 (0.325)	Data 1.03e-04 (3.55e-04)	Tok/s 39733 (42941)	Loss/tok 2.8644 (3.1702)	LR 1.250e-04
0: TRAIN [4][1240/7762]	Time 0.361 (0.326)	Data 1.17e-04 (3.53e-04)	Tok/s 46545 (42971)	Loss/tok 3.2451 (3.1711)	LR 1.250e-04
0: TRAIN [4][1250/7762]	Time 0.344 (0.326)	Data 1.03e-04 (3.51e-04)	Tok/s 47987 (42993)	Loss/tok 3.1895 (3.1713)	LR 1.250e-04
0: TRAIN [4][1260/7762]	Time 0.463 (0.326)	Data 1.12e-04 (3.49e-04)	Tok/s 50356 (43009)	Loss/tok 3.3893 (3.1716)	LR 1.250e-04
0: TRAIN [4][1270/7762]	Time 0.255 (0.326)	Data 1.03e-04 (3.47e-04)	Tok/s 40387 (42999)	Loss/tok 2.9206 (3.1709)	LR 1.250e-04
0: TRAIN [4][1280/7762]	Time 0.264 (0.326)	Data 1.05e-04 (3.45e-04)	Tok/s 39139 (43022)	Loss/tok 3.0181 (3.1712)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1290/7762]	Time 0.332 (0.326)	Data 1.07e-04 (3.43e-04)	Tok/s 50823 (43019)	Loss/tok 3.0683 (3.1711)	LR 1.250e-04
0: TRAIN [4][1300/7762]	Time 0.436 (0.326)	Data 1.03e-04 (3.42e-04)	Tok/s 52536 (43040)	Loss/tok 3.3297 (3.1713)	LR 1.250e-04
0: TRAIN [4][1310/7762]	Time 0.361 (0.326)	Data 1.01e-04 (3.40e-04)	Tok/s 47015 (43037)	Loss/tok 3.1659 (3.1709)	LR 1.250e-04
0: TRAIN [4][1320/7762]	Time 0.364 (0.326)	Data 1.14e-04 (3.38e-04)	Tok/s 46049 (43044)	Loss/tok 3.1902 (3.1704)	LR 1.250e-04
0: TRAIN [4][1330/7762]	Time 0.470 (0.326)	Data 9.78e-05 (3.36e-04)	Tok/s 49465 (43047)	Loss/tok 3.3293 (3.1709)	LR 1.250e-04
0: TRAIN [4][1340/7762]	Time 0.260 (0.326)	Data 1.03e-04 (3.34e-04)	Tok/s 40548 (43041)	Loss/tok 2.9887 (3.1710)	LR 1.250e-04
0: TRAIN [4][1350/7762]	Time 0.361 (0.327)	Data 9.85e-05 (3.33e-04)	Tok/s 46681 (43048)	Loss/tok 3.2235 (3.1709)	LR 1.250e-04
0: TRAIN [4][1360/7762]	Time 0.358 (0.326)	Data 1.01e-04 (3.31e-04)	Tok/s 47354 (43048)	Loss/tok 3.2025 (3.1706)	LR 1.250e-04
0: TRAIN [4][1370/7762]	Time 0.367 (0.326)	Data 1.01e-04 (3.29e-04)	Tok/s 45944 (43064)	Loss/tok 3.2730 (3.1706)	LR 1.250e-04
0: TRAIN [4][1380/7762]	Time 0.268 (0.326)	Data 9.80e-05 (3.28e-04)	Tok/s 37581 (43057)	Loss/tok 2.8301 (3.1703)	LR 1.250e-04
0: TRAIN [4][1390/7762]	Time 0.269 (0.326)	Data 9.78e-05 (3.26e-04)	Tok/s 38935 (43032)	Loss/tok 2.9895 (3.1697)	LR 1.250e-04
0: TRAIN [4][1400/7762]	Time 0.354 (0.326)	Data 1.02e-04 (3.24e-04)	Tok/s 46905 (43041)	Loss/tok 3.1061 (3.1694)	LR 1.250e-04
0: TRAIN [4][1410/7762]	Time 0.253 (0.326)	Data 9.85e-05 (3.23e-04)	Tok/s 40785 (43039)	Loss/tok 2.9299 (3.1694)	LR 1.250e-04
0: TRAIN [4][1420/7762]	Time 0.264 (0.326)	Data 1.00e-04 (3.21e-04)	Tok/s 39395 (43033)	Loss/tok 3.0269 (3.1691)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1430/7762]	Time 0.260 (0.326)	Data 9.89e-05 (3.20e-04)	Tok/s 38721 (43033)	Loss/tok 2.9830 (3.1693)	LR 1.250e-04
0: TRAIN [4][1440/7762]	Time 0.264 (0.326)	Data 1.03e-04 (3.19e-04)	Tok/s 39243 (43033)	Loss/tok 2.8998 (3.1691)	LR 1.250e-04
0: TRAIN [4][1450/7762]	Time 0.462 (0.326)	Data 1.01e-04 (3.17e-04)	Tok/s 50762 (43047)	Loss/tok 3.2625 (3.1694)	LR 1.250e-04
0: TRAIN [4][1460/7762]	Time 0.459 (0.327)	Data 1.20e-04 (3.16e-04)	Tok/s 51008 (43074)	Loss/tok 3.1840 (3.1703)	LR 1.250e-04
0: TRAIN [4][1470/7762]	Time 0.562 (0.327)	Data 9.73e-05 (3.14e-04)	Tok/s 53433 (43068)	Loss/tok 3.4060 (3.1700)	LR 1.250e-04
0: TRAIN [4][1480/7762]	Time 0.175 (0.327)	Data 1.01e-04 (3.13e-04)	Tok/s 29995 (43077)	Loss/tok 2.6313 (3.1703)	LR 1.250e-04
0: TRAIN [4][1490/7762]	Time 0.462 (0.327)	Data 1.06e-04 (3.11e-04)	Tok/s 50636 (43095)	Loss/tok 3.2929 (3.1705)	LR 1.250e-04
0: TRAIN [4][1500/7762]	Time 0.364 (0.327)	Data 1.03e-04 (3.10e-04)	Tok/s 46332 (43089)	Loss/tok 3.2108 (3.1700)	LR 1.250e-04
0: TRAIN [4][1510/7762]	Time 0.590 (0.327)	Data 9.80e-05 (3.09e-04)	Tok/s 50339 (43091)	Loss/tok 3.5794 (3.1708)	LR 1.250e-04
0: TRAIN [4][1520/7762]	Time 0.263 (0.327)	Data 9.66e-05 (3.07e-04)	Tok/s 38656 (43065)	Loss/tok 2.9289 (3.1701)	LR 1.250e-04
0: TRAIN [4][1530/7762]	Time 0.360 (0.326)	Data 9.80e-05 (3.06e-04)	Tok/s 46987 (43051)	Loss/tok 3.1355 (3.1695)	LR 1.250e-04
0: TRAIN [4][1540/7762]	Time 0.177 (0.326)	Data 1.03e-04 (3.05e-04)	Tok/s 29606 (43044)	Loss/tok 2.5338 (3.1694)	LR 1.250e-04
0: TRAIN [4][1550/7762]	Time 0.360 (0.326)	Data 1.02e-04 (3.03e-04)	Tok/s 46794 (43033)	Loss/tok 3.2181 (3.1692)	LR 1.250e-04
0: TRAIN [4][1560/7762]	Time 0.359 (0.326)	Data 1.04e-04 (3.02e-04)	Tok/s 46654 (43031)	Loss/tok 3.2467 (3.1696)	LR 1.250e-04
0: TRAIN [4][1570/7762]	Time 0.264 (0.326)	Data 1.02e-04 (3.01e-04)	Tok/s 39693 (43041)	Loss/tok 2.9522 (3.1696)	LR 1.250e-04
0: TRAIN [4][1580/7762]	Time 0.583 (0.326)	Data 9.99e-05 (3.00e-04)	Tok/s 50712 (43038)	Loss/tok 3.6141 (3.1696)	LR 1.250e-04
0: TRAIN [4][1590/7762]	Time 0.349 (0.326)	Data 9.87e-05 (2.98e-04)	Tok/s 47792 (43011)	Loss/tok 3.2423 (3.1688)	LR 1.250e-04
0: TRAIN [4][1600/7762]	Time 0.364 (0.326)	Data 9.82e-05 (2.97e-04)	Tok/s 45808 (42986)	Loss/tok 3.2404 (3.1680)	LR 1.250e-04
0: TRAIN [4][1610/7762]	Time 0.173 (0.326)	Data 9.78e-05 (2.96e-04)	Tok/s 30391 (42991)	Loss/tok 2.6008 (3.1683)	LR 1.250e-04
0: TRAIN [4][1620/7762]	Time 0.261 (0.326)	Data 1.03e-04 (2.95e-04)	Tok/s 39751 (42984)	Loss/tok 3.0109 (3.1678)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1630/7762]	Time 0.461 (0.326)	Data 1.19e-04 (2.94e-04)	Tok/s 49885 (43012)	Loss/tok 3.3642 (3.1690)	LR 1.250e-04
0: TRAIN [4][1640/7762]	Time 0.341 (0.326)	Data 9.49e-05 (2.92e-04)	Tok/s 49448 (43013)	Loss/tok 3.0705 (3.1688)	LR 1.250e-04
0: TRAIN [4][1650/7762]	Time 0.255 (0.326)	Data 1.01e-04 (2.91e-04)	Tok/s 39937 (43034)	Loss/tok 2.9588 (3.1695)	LR 1.250e-04
0: TRAIN [4][1660/7762]	Time 0.365 (0.327)	Data 1.02e-04 (2.90e-04)	Tok/s 45578 (43037)	Loss/tok 3.1455 (3.1702)	LR 1.250e-04
0: TRAIN [4][1670/7762]	Time 0.251 (0.327)	Data 1.01e-04 (2.89e-04)	Tok/s 41183 (43044)	Loss/tok 2.9767 (3.1702)	LR 1.250e-04
0: TRAIN [4][1680/7762]	Time 0.353 (0.327)	Data 1.00e-04 (2.88e-04)	Tok/s 46779 (43037)	Loss/tok 3.1323 (3.1697)	LR 1.250e-04
0: TRAIN [4][1690/7762]	Time 0.264 (0.326)	Data 1.12e-04 (2.87e-04)	Tok/s 39056 (43017)	Loss/tok 2.9784 (3.1692)	LR 1.250e-04
0: TRAIN [4][1700/7762]	Time 0.453 (0.326)	Data 1.04e-04 (2.86e-04)	Tok/s 51516 (43033)	Loss/tok 3.3848 (3.1693)	LR 1.250e-04
0: TRAIN [4][1710/7762]	Time 0.259 (0.326)	Data 1.06e-04 (2.85e-04)	Tok/s 39973 (43032)	Loss/tok 2.8888 (3.1689)	LR 1.250e-04
0: TRAIN [4][1720/7762]	Time 0.266 (0.326)	Data 1.24e-04 (2.84e-04)	Tok/s 38476 (43020)	Loss/tok 3.0070 (3.1687)	LR 1.250e-04
0: TRAIN [4][1730/7762]	Time 0.342 (0.326)	Data 1.02e-04 (2.83e-04)	Tok/s 49153 (43036)	Loss/tok 3.2340 (3.1686)	LR 1.250e-04
0: TRAIN [4][1740/7762]	Time 0.257 (0.326)	Data 1.17e-04 (2.82e-04)	Tok/s 40622 (43040)	Loss/tok 2.9318 (3.1696)	LR 1.250e-04
0: TRAIN [4][1750/7762]	Time 0.366 (0.326)	Data 9.66e-05 (2.81e-04)	Tok/s 45986 (43022)	Loss/tok 3.1688 (3.1691)	LR 1.250e-04
0: TRAIN [4][1760/7762]	Time 0.266 (0.326)	Data 1.14e-04 (2.80e-04)	Tok/s 38577 (43007)	Loss/tok 3.0030 (3.1688)	LR 1.250e-04
0: TRAIN [4][1770/7762]	Time 0.456 (0.326)	Data 1.18e-04 (2.79e-04)	Tok/s 50873 (43015)	Loss/tok 3.2852 (3.1690)	LR 1.250e-04
0: TRAIN [4][1780/7762]	Time 0.364 (0.326)	Data 9.92e-05 (2.78e-04)	Tok/s 45573 (43016)	Loss/tok 3.2051 (3.1690)	LR 1.250e-04
0: TRAIN [4][1790/7762]	Time 0.458 (0.326)	Data 1.01e-04 (2.77e-04)	Tok/s 50909 (43029)	Loss/tok 3.3494 (3.1694)	LR 1.250e-04
0: TRAIN [4][1800/7762]	Time 0.259 (0.326)	Data 1.00e-04 (2.76e-04)	Tok/s 39900 (43025)	Loss/tok 3.0409 (3.1697)	LR 1.250e-04
0: TRAIN [4][1810/7762]	Time 0.261 (0.326)	Data 1.02e-04 (2.75e-04)	Tok/s 39182 (43025)	Loss/tok 3.0200 (3.1696)	LR 1.250e-04
0: TRAIN [4][1820/7762]	Time 0.252 (0.326)	Data 9.87e-05 (2.74e-04)	Tok/s 40523 (43032)	Loss/tok 3.0224 (3.1696)	LR 1.250e-04
0: TRAIN [4][1830/7762]	Time 0.174 (0.326)	Data 1.12e-04 (2.73e-04)	Tok/s 30373 (43026)	Loss/tok 2.5950 (3.1692)	LR 1.250e-04
0: TRAIN [4][1840/7762]	Time 0.170 (0.326)	Data 1.18e-04 (2.72e-04)	Tok/s 30599 (43006)	Loss/tok 2.6660 (3.1686)	LR 1.250e-04
0: TRAIN [4][1850/7762]	Time 0.252 (0.326)	Data 1.06e-04 (2.71e-04)	Tok/s 40975 (43019)	Loss/tok 2.9033 (3.1685)	LR 1.250e-04
0: TRAIN [4][1860/7762]	Time 0.464 (0.326)	Data 9.94e-05 (2.70e-04)	Tok/s 50729 (43014)	Loss/tok 3.2446 (3.1685)	LR 1.250e-04
0: TRAIN [4][1870/7762]	Time 0.362 (0.326)	Data 1.02e-04 (2.69e-04)	Tok/s 46786 (43013)	Loss/tok 3.2265 (3.1686)	LR 1.250e-04
0: TRAIN [4][1880/7762]	Time 0.175 (0.326)	Data 1.18e-04 (2.68e-04)	Tok/s 29779 (43015)	Loss/tok 2.5024 (3.1689)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][1890/7762]	Time 0.568 (0.326)	Data 9.85e-05 (2.68e-04)	Tok/s 53058 (43019)	Loss/tok 3.5242 (3.1692)	LR 1.250e-04
0: TRAIN [4][1900/7762]	Time 0.367 (0.326)	Data 1.03e-04 (2.67e-04)	Tok/s 46115 (43014)	Loss/tok 3.1399 (3.1692)	LR 1.250e-04
0: TRAIN [4][1910/7762]	Time 0.265 (0.326)	Data 1.15e-04 (2.66e-04)	Tok/s 38944 (43017)	Loss/tok 2.9463 (3.1691)	LR 1.250e-04
0: TRAIN [4][1920/7762]	Time 0.363 (0.326)	Data 1.21e-04 (2.65e-04)	Tok/s 45736 (43024)	Loss/tok 3.1743 (3.1691)	LR 1.250e-04
0: TRAIN [4][1930/7762]	Time 0.172 (0.326)	Data 1.00e-04 (2.64e-04)	Tok/s 30758 (43017)	Loss/tok 2.5838 (3.1686)	LR 1.250e-04
0: TRAIN [4][1940/7762]	Time 0.260 (0.326)	Data 9.25e-05 (2.63e-04)	Tok/s 39414 (42989)	Loss/tok 2.9789 (3.1679)	LR 1.250e-04
0: TRAIN [4][1950/7762]	Time 0.259 (0.326)	Data 1.14e-04 (2.63e-04)	Tok/s 39818 (42988)	Loss/tok 3.0009 (3.1676)	LR 1.250e-04
0: TRAIN [4][1960/7762]	Time 0.362 (0.325)	Data 9.94e-05 (2.62e-04)	Tok/s 46694 (42978)	Loss/tok 3.1866 (3.1675)	LR 1.250e-04
0: TRAIN [4][1970/7762]	Time 0.257 (0.326)	Data 1.18e-04 (2.61e-04)	Tok/s 40473 (42992)	Loss/tok 2.9108 (3.1676)	LR 1.250e-04
0: TRAIN [4][1980/7762]	Time 0.257 (0.326)	Data 1.03e-04 (2.60e-04)	Tok/s 41131 (42994)	Loss/tok 3.0070 (3.1675)	LR 1.250e-04
0: TRAIN [4][1990/7762]	Time 0.458 (0.326)	Data 1.03e-04 (2.59e-04)	Tok/s 50776 (43000)	Loss/tok 3.4303 (3.1677)	LR 1.250e-04
0: TRAIN [4][2000/7762]	Time 0.348 (0.326)	Data 9.70e-05 (2.59e-04)	Tok/s 48201 (43006)	Loss/tok 3.2049 (3.1677)	LR 1.250e-04
0: TRAIN [4][2010/7762]	Time 0.464 (0.326)	Data 1.01e-04 (2.58e-04)	Tok/s 50379 (43008)	Loss/tok 3.1902 (3.1677)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][2020/7762]	Time 0.444 (0.326)	Data 1.05e-04 (2.57e-04)	Tok/s 52460 (43005)	Loss/tok 3.3123 (3.1676)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2030/7762]	Time 0.267 (0.325)	Data 9.87e-05 (2.56e-04)	Tok/s 38341 (42988)	Loss/tok 2.9997 (3.1674)	LR 1.250e-04
0: TRAIN [4][2040/7762]	Time 0.176 (0.325)	Data 1.00e-04 (2.56e-04)	Tok/s 29849 (42987)	Loss/tok 2.6596 (3.1676)	LR 1.250e-04
0: TRAIN [4][2050/7762]	Time 0.259 (0.325)	Data 1.02e-04 (2.55e-04)	Tok/s 40214 (42989)	Loss/tok 2.8565 (3.1675)	LR 1.250e-04
0: TRAIN [4][2060/7762]	Time 0.436 (0.326)	Data 9.66e-05 (2.54e-04)	Tok/s 53989 (42996)	Loss/tok 3.2934 (3.1676)	LR 1.250e-04
0: TRAIN [4][2070/7762]	Time 0.254 (0.326)	Data 1.01e-04 (2.53e-04)	Tok/s 40359 (43001)	Loss/tok 3.0041 (3.1675)	LR 1.250e-04
0: TRAIN [4][2080/7762]	Time 0.269 (0.325)	Data 1.18e-04 (2.53e-04)	Tok/s 38424 (42987)	Loss/tok 2.9518 (3.1669)	LR 1.250e-04
0: TRAIN [4][2090/7762]	Time 0.364 (0.325)	Data 9.87e-05 (2.52e-04)	Tok/s 46404 (42979)	Loss/tok 3.1326 (3.1663)	LR 1.250e-04
0: TRAIN [4][2100/7762]	Time 0.263 (0.325)	Data 9.92e-05 (2.51e-04)	Tok/s 39843 (42973)	Loss/tok 2.9207 (3.1659)	LR 1.250e-04
0: TRAIN [4][2110/7762]	Time 0.352 (0.325)	Data 9.80e-05 (2.51e-04)	Tok/s 47235 (42973)	Loss/tok 3.2422 (3.1657)	LR 1.250e-04
0: TRAIN [4][2120/7762]	Time 0.257 (0.325)	Data 1.01e-04 (2.50e-04)	Tok/s 40434 (42973)	Loss/tok 2.9984 (3.1655)	LR 1.250e-04
0: TRAIN [4][2130/7762]	Time 0.357 (0.325)	Data 1.03e-04 (2.49e-04)	Tok/s 47379 (42982)	Loss/tok 3.1085 (3.1659)	LR 1.250e-04
0: TRAIN [4][2140/7762]	Time 0.261 (0.325)	Data 9.73e-05 (2.49e-04)	Tok/s 39737 (42979)	Loss/tok 2.9643 (3.1655)	LR 1.250e-04
0: TRAIN [4][2150/7762]	Time 0.596 (0.325)	Data 1.05e-04 (2.48e-04)	Tok/s 49413 (42999)	Loss/tok 3.5939 (3.1660)	LR 1.250e-04
0: TRAIN [4][2160/7762]	Time 0.180 (0.325)	Data 9.54e-05 (2.47e-04)	Tok/s 29041 (42991)	Loss/tok 2.4806 (3.1658)	LR 1.250e-04
0: TRAIN [4][2170/7762]	Time 0.265 (0.325)	Data 1.03e-04 (2.47e-04)	Tok/s 39541 (42994)	Loss/tok 3.0062 (3.1658)	LR 1.250e-04
0: TRAIN [4][2180/7762]	Time 0.368 (0.325)	Data 1.02e-04 (2.46e-04)	Tok/s 45511 (43005)	Loss/tok 3.1734 (3.1658)	LR 1.250e-04
0: TRAIN [4][2190/7762]	Time 0.363 (0.325)	Data 1.13e-04 (2.45e-04)	Tok/s 46295 (42994)	Loss/tok 3.1747 (3.1658)	LR 1.250e-04
0: TRAIN [4][2200/7762]	Time 0.264 (0.325)	Data 1.00e-04 (2.45e-04)	Tok/s 38949 (42986)	Loss/tok 2.9361 (3.1653)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2210/7762]	Time 0.462 (0.325)	Data 1.04e-04 (2.44e-04)	Tok/s 50225 (42995)	Loss/tok 3.2601 (3.1660)	LR 1.250e-04
0: TRAIN [4][2220/7762]	Time 0.363 (0.325)	Data 9.78e-05 (2.43e-04)	Tok/s 46277 (43007)	Loss/tok 3.1358 (3.1664)	LR 1.250e-04
0: TRAIN [4][2230/7762]	Time 0.363 (0.325)	Data 1.07e-04 (2.43e-04)	Tok/s 46312 (43004)	Loss/tok 3.1873 (3.1665)	LR 1.250e-04
0: TRAIN [4][2240/7762]	Time 0.170 (0.326)	Data 1.15e-04 (2.42e-04)	Tok/s 30744 (43014)	Loss/tok 2.5625 (3.1670)	LR 1.250e-04
0: TRAIN [4][2250/7762]	Time 0.261 (0.326)	Data 1.01e-04 (2.41e-04)	Tok/s 39288 (43016)	Loss/tok 3.0614 (3.1668)	LR 1.250e-04
0: TRAIN [4][2260/7762]	Time 0.263 (0.326)	Data 1.03e-04 (2.41e-04)	Tok/s 39024 (43031)	Loss/tok 2.9823 (3.1673)	LR 1.250e-04
0: TRAIN [4][2270/7762]	Time 0.361 (0.326)	Data 1.02e-04 (2.40e-04)	Tok/s 46019 (43047)	Loss/tok 3.1581 (3.1677)	LR 1.250e-04
0: TRAIN [4][2280/7762]	Time 0.250 (0.326)	Data 1.15e-04 (2.40e-04)	Tok/s 41212 (43037)	Loss/tok 3.1388 (3.1673)	LR 1.250e-04
0: TRAIN [4][2290/7762]	Time 0.460 (0.326)	Data 1.10e-04 (2.39e-04)	Tok/s 51259 (43037)	Loss/tok 3.2682 (3.1676)	LR 1.250e-04
0: TRAIN [4][2300/7762]	Time 0.363 (0.326)	Data 1.05e-04 (2.39e-04)	Tok/s 46164 (43045)	Loss/tok 3.2132 (3.1677)	LR 1.250e-04
0: TRAIN [4][2310/7762]	Time 0.176 (0.326)	Data 1.03e-04 (2.38e-04)	Tok/s 29971 (43049)	Loss/tok 2.5882 (3.1681)	LR 1.250e-04
0: TRAIN [4][2320/7762]	Time 0.358 (0.326)	Data 9.99e-05 (2.37e-04)	Tok/s 47071 (43051)	Loss/tok 3.2138 (3.1680)	LR 1.250e-04
0: TRAIN [4][2330/7762]	Time 0.254 (0.326)	Data 9.94e-05 (2.37e-04)	Tok/s 40360 (43054)	Loss/tok 2.9147 (3.1681)	LR 1.250e-04
0: TRAIN [4][2340/7762]	Time 0.365 (0.326)	Data 1.06e-04 (2.36e-04)	Tok/s 46042 (43048)	Loss/tok 3.2281 (3.1681)	LR 1.250e-04
0: TRAIN [4][2350/7762]	Time 0.460 (0.326)	Data 9.97e-05 (2.36e-04)	Tok/s 50001 (43033)	Loss/tok 3.3522 (3.1677)	LR 1.250e-04
0: TRAIN [4][2360/7762]	Time 0.361 (0.326)	Data 9.78e-05 (2.35e-04)	Tok/s 46692 (43029)	Loss/tok 3.2014 (3.1675)	LR 1.250e-04
0: TRAIN [4][2370/7762]	Time 0.366 (0.326)	Data 1.21e-04 (2.35e-04)	Tok/s 46392 (43033)	Loss/tok 3.0455 (3.1681)	LR 1.250e-04
0: TRAIN [4][2380/7762]	Time 0.355 (0.326)	Data 1.04e-04 (2.34e-04)	Tok/s 47231 (43042)	Loss/tok 3.1417 (3.1680)	LR 1.250e-04
0: TRAIN [4][2390/7762]	Time 0.262 (0.326)	Data 9.92e-05 (2.34e-04)	Tok/s 39432 (43049)	Loss/tok 2.9755 (3.1681)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2400/7762]	Time 0.364 (0.326)	Data 1.05e-04 (2.33e-04)	Tok/s 46106 (43050)	Loss/tok 3.2777 (3.1679)	LR 1.250e-04
0: TRAIN [4][2410/7762]	Time 0.262 (0.326)	Data 1.03e-04 (2.32e-04)	Tok/s 39011 (43037)	Loss/tok 2.9718 (3.1674)	LR 1.250e-04
0: TRAIN [4][2420/7762]	Time 0.259 (0.326)	Data 1.05e-04 (2.32e-04)	Tok/s 39837 (43037)	Loss/tok 2.9544 (3.1677)	LR 1.250e-04
0: TRAIN [4][2430/7762]	Time 0.363 (0.326)	Data 9.94e-05 (2.31e-04)	Tok/s 46255 (43038)	Loss/tok 3.0886 (3.1674)	LR 1.250e-04
0: TRAIN [4][2440/7762]	Time 0.460 (0.326)	Data 1.01e-04 (2.31e-04)	Tok/s 50657 (43038)	Loss/tok 3.3304 (3.1673)	LR 1.250e-04
0: TRAIN [4][2450/7762]	Time 0.366 (0.326)	Data 1.06e-04 (2.30e-04)	Tok/s 46405 (43038)	Loss/tok 3.1016 (3.1675)	LR 1.250e-04
0: TRAIN [4][2460/7762]	Time 0.178 (0.326)	Data 9.89e-05 (2.30e-04)	Tok/s 29962 (43028)	Loss/tok 2.5577 (3.1673)	LR 1.250e-04
0: TRAIN [4][2470/7762]	Time 0.587 (0.326)	Data 9.89e-05 (2.29e-04)	Tok/s 50853 (43029)	Loss/tok 3.5137 (3.1677)	LR 1.250e-04
0: TRAIN [4][2480/7762]	Time 0.343 (0.326)	Data 9.70e-05 (2.29e-04)	Tok/s 48490 (43021)	Loss/tok 3.2889 (3.1674)	LR 1.250e-04
0: TRAIN [4][2490/7762]	Time 0.550 (0.326)	Data 1.06e-04 (2.28e-04)	Tok/s 54293 (43027)	Loss/tok 3.3990 (3.1672)	LR 1.250e-04
0: TRAIN [4][2500/7762]	Time 0.267 (0.326)	Data 1.05e-04 (2.28e-04)	Tok/s 38654 (43017)	Loss/tok 2.9827 (3.1669)	LR 1.250e-04
0: TRAIN [4][2510/7762]	Time 0.268 (0.326)	Data 1.08e-04 (2.27e-04)	Tok/s 38177 (43023)	Loss/tok 2.9993 (3.1673)	LR 1.250e-04
0: TRAIN [4][2520/7762]	Time 0.259 (0.326)	Data 1.03e-04 (2.27e-04)	Tok/s 39820 (43025)	Loss/tok 3.0778 (3.1670)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2530/7762]	Time 0.353 (0.326)	Data 1.08e-04 (2.26e-04)	Tok/s 46817 (43025)	Loss/tok 3.1868 (3.1670)	LR 1.250e-04
0: TRAIN [4][2540/7762]	Time 0.364 (0.326)	Data 1.05e-04 (2.26e-04)	Tok/s 45568 (43025)	Loss/tok 3.1485 (3.1672)	LR 1.250e-04
0: TRAIN [4][2550/7762]	Time 0.170 (0.326)	Data 1.02e-04 (2.25e-04)	Tok/s 30997 (43018)	Loss/tok 2.6072 (3.1669)	LR 1.250e-04
0: TRAIN [4][2560/7762]	Time 0.176 (0.325)	Data 1.23e-04 (2.25e-04)	Tok/s 29908 (43003)	Loss/tok 2.6084 (3.1666)	LR 1.250e-04
0: TRAIN [4][2570/7762]	Time 0.351 (0.325)	Data 9.63e-05 (2.24e-04)	Tok/s 48144 (42996)	Loss/tok 3.1780 (3.1665)	LR 1.250e-04
0: TRAIN [4][2580/7762]	Time 0.586 (0.325)	Data 1.15e-04 (2.24e-04)	Tok/s 51031 (42997)	Loss/tok 3.4423 (3.1668)	LR 1.250e-04
0: TRAIN [4][2590/7762]	Time 0.258 (0.325)	Data 1.03e-04 (2.24e-04)	Tok/s 40251 (43002)	Loss/tok 2.9055 (3.1669)	LR 1.250e-04
0: TRAIN [4][2600/7762]	Time 0.177 (0.325)	Data 9.92e-05 (2.23e-04)	Tok/s 29308 (42988)	Loss/tok 2.6035 (3.1667)	LR 1.250e-04
0: TRAIN [4][2610/7762]	Time 0.175 (0.325)	Data 1.21e-04 (2.23e-04)	Tok/s 29762 (42980)	Loss/tok 2.6017 (3.1666)	LR 1.250e-04
0: TRAIN [4][2620/7762]	Time 0.262 (0.325)	Data 1.03e-04 (2.22e-04)	Tok/s 39227 (42990)	Loss/tok 3.0589 (3.1670)	LR 1.250e-04
0: TRAIN [4][2630/7762]	Time 0.176 (0.325)	Data 9.99e-05 (2.22e-04)	Tok/s 30007 (42992)	Loss/tok 2.5783 (3.1671)	LR 1.250e-04
0: TRAIN [4][2640/7762]	Time 0.267 (0.325)	Data 9.87e-05 (2.21e-04)	Tok/s 38519 (42998)	Loss/tok 2.8445 (3.1672)	LR 1.250e-04
0: TRAIN [4][2650/7762]	Time 0.454 (0.326)	Data 1.05e-04 (2.21e-04)	Tok/s 51616 (42997)	Loss/tok 3.4426 (3.1674)	LR 1.250e-04
0: TRAIN [4][2660/7762]	Time 0.264 (0.326)	Data 1.06e-04 (2.21e-04)	Tok/s 38076 (43001)	Loss/tok 2.9315 (3.1674)	LR 1.250e-04
0: TRAIN [4][2670/7762]	Time 0.357 (0.326)	Data 1.01e-04 (2.20e-04)	Tok/s 47854 (43004)	Loss/tok 3.1528 (3.1674)	LR 1.250e-04
0: TRAIN [4][2680/7762]	Time 0.261 (0.326)	Data 9.92e-05 (2.20e-04)	Tok/s 39795 (42996)	Loss/tok 2.9551 (3.1673)	LR 1.250e-04
0: TRAIN [4][2690/7762]	Time 0.179 (0.325)	Data 9.70e-05 (2.19e-04)	Tok/s 29678 (42996)	Loss/tok 2.5808 (3.1672)	LR 1.250e-04
0: TRAIN [4][2700/7762]	Time 0.359 (0.326)	Data 1.00e-04 (2.19e-04)	Tok/s 46964 (43005)	Loss/tok 3.0400 (3.1672)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2710/7762]	Time 0.269 (0.326)	Data 1.15e-04 (2.18e-04)	Tok/s 38220 (43002)	Loss/tok 2.9818 (3.1674)	LR 1.250e-04
0: TRAIN [4][2720/7762]	Time 0.264 (0.325)	Data 9.58e-05 (2.18e-04)	Tok/s 39361 (42994)	Loss/tok 2.9824 (3.1670)	LR 1.250e-04
0: TRAIN [4][2730/7762]	Time 0.265 (0.325)	Data 1.05e-04 (2.18e-04)	Tok/s 38025 (43002)	Loss/tok 2.8379 (3.1671)	LR 1.250e-04
0: TRAIN [4][2740/7762]	Time 0.356 (0.325)	Data 9.70e-05 (2.17e-04)	Tok/s 47424 (42996)	Loss/tok 3.1785 (3.1668)	LR 1.250e-04
0: TRAIN [4][2750/7762]	Time 0.175 (0.325)	Data 1.00e-04 (2.17e-04)	Tok/s 30249 (42990)	Loss/tok 2.5371 (3.1666)	LR 1.250e-04
0: TRAIN [4][2760/7762]	Time 0.259 (0.325)	Data 9.82e-05 (2.16e-04)	Tok/s 39955 (42984)	Loss/tok 3.0052 (3.1663)	LR 1.250e-04
0: TRAIN [4][2770/7762]	Time 0.344 (0.325)	Data 9.73e-05 (2.16e-04)	Tok/s 49515 (42993)	Loss/tok 3.2628 (3.1662)	LR 1.250e-04
0: TRAIN [4][2780/7762]	Time 0.366 (0.325)	Data 1.16e-04 (2.15e-04)	Tok/s 45662 (42986)	Loss/tok 3.1251 (3.1660)	LR 1.250e-04
0: TRAIN [4][2790/7762]	Time 0.257 (0.325)	Data 1.02e-04 (2.15e-04)	Tok/s 40463 (42988)	Loss/tok 2.9140 (3.1660)	LR 1.250e-04
0: TRAIN [4][2800/7762]	Time 0.262 (0.325)	Data 1.02e-04 (2.15e-04)	Tok/s 39657 (42987)	Loss/tok 3.0706 (3.1660)	LR 1.250e-04
0: TRAIN [4][2810/7762]	Time 0.362 (0.325)	Data 9.92e-05 (2.14e-04)	Tok/s 47136 (42973)	Loss/tok 3.0865 (3.1657)	LR 1.250e-04
0: TRAIN [4][2820/7762]	Time 0.366 (0.325)	Data 1.02e-04 (2.14e-04)	Tok/s 46484 (42980)	Loss/tok 3.1217 (3.1658)	LR 1.250e-04
0: TRAIN [4][2830/7762]	Time 0.461 (0.325)	Data 1.01e-04 (2.13e-04)	Tok/s 50569 (42981)	Loss/tok 3.3824 (3.1658)	LR 1.250e-04
0: TRAIN [4][2840/7762]	Time 0.254 (0.325)	Data 9.75e-05 (2.13e-04)	Tok/s 41093 (42978)	Loss/tok 3.0287 (3.1656)	LR 1.250e-04
0: TRAIN [4][2850/7762]	Time 0.269 (0.325)	Data 9.70e-05 (2.13e-04)	Tok/s 37501 (42974)	Loss/tok 3.0236 (3.1655)	LR 1.250e-04
0: TRAIN [4][2860/7762]	Time 0.453 (0.325)	Data 1.06e-04 (2.12e-04)	Tok/s 51309 (42983)	Loss/tok 3.4543 (3.1657)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2870/7762]	Time 0.170 (0.325)	Data 1.03e-04 (2.12e-04)	Tok/s 30716 (42979)	Loss/tok 2.6022 (3.1659)	LR 1.250e-04
0: TRAIN [4][2880/7762]	Time 0.266 (0.325)	Data 1.08e-04 (2.12e-04)	Tok/s 38685 (42983)	Loss/tok 2.9410 (3.1662)	LR 1.250e-04
0: TRAIN [4][2890/7762]	Time 0.365 (0.325)	Data 1.03e-04 (2.11e-04)	Tok/s 46036 (42986)	Loss/tok 3.2165 (3.1662)	LR 1.250e-04
0: TRAIN [4][2900/7762]	Time 0.265 (0.325)	Data 1.01e-04 (2.11e-04)	Tok/s 38956 (42985)	Loss/tok 2.9513 (3.1662)	LR 1.250e-04
0: TRAIN [4][2910/7762]	Time 0.178 (0.325)	Data 1.01e-04 (2.11e-04)	Tok/s 29769 (42983)	Loss/tok 2.6093 (3.1660)	LR 1.250e-04
0: TRAIN [4][2920/7762]	Time 0.361 (0.325)	Data 9.61e-05 (2.10e-04)	Tok/s 45682 (42978)	Loss/tok 3.3428 (3.1658)	LR 1.250e-04
0: TRAIN [4][2930/7762]	Time 0.464 (0.325)	Data 1.01e-04 (2.10e-04)	Tok/s 49847 (42982)	Loss/tok 3.3646 (3.1660)	LR 1.250e-04
0: TRAIN [4][2940/7762]	Time 0.342 (0.325)	Data 1.04e-04 (2.10e-04)	Tok/s 49108 (42990)	Loss/tok 3.0160 (3.1664)	LR 1.250e-04
0: TRAIN [4][2950/7762]	Time 0.467 (0.325)	Data 9.82e-05 (2.10e-04)	Tok/s 50373 (42997)	Loss/tok 3.2456 (3.1664)	LR 1.250e-04
0: TRAIN [4][2960/7762]	Time 0.364 (0.325)	Data 9.89e-05 (2.09e-04)	Tok/s 46348 (42999)	Loss/tok 3.2033 (3.1663)	LR 1.250e-04
0: TRAIN [4][2970/7762]	Time 0.268 (0.325)	Data 1.02e-04 (2.09e-04)	Tok/s 37854 (42995)	Loss/tok 2.9992 (3.1661)	LR 1.250e-04
0: TRAIN [4][2980/7762]	Time 0.369 (0.325)	Data 1.02e-04 (2.09e-04)	Tok/s 45931 (42999)	Loss/tok 3.2487 (3.1661)	LR 1.250e-04
0: TRAIN [4][2990/7762]	Time 0.269 (0.325)	Data 1.07e-04 (2.08e-04)	Tok/s 38221 (42993)	Loss/tok 3.0975 (3.1661)	LR 1.250e-04
0: TRAIN [4][3000/7762]	Time 0.364 (0.325)	Data 1.03e-04 (2.08e-04)	Tok/s 46080 (42997)	Loss/tok 3.0948 (3.1661)	LR 1.250e-04
0: TRAIN [4][3010/7762]	Time 0.259 (0.325)	Data 1.01e-04 (2.08e-04)	Tok/s 39713 (43001)	Loss/tok 2.8661 (3.1661)	LR 1.250e-04
0: TRAIN [4][3020/7762]	Time 0.175 (0.325)	Data 9.87e-05 (2.07e-04)	Tok/s 30617 (42999)	Loss/tok 2.5846 (3.1660)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3030/7762]	Time 0.260 (0.325)	Data 1.00e-04 (2.07e-04)	Tok/s 40630 (42991)	Loss/tok 2.9657 (3.1657)	LR 1.250e-04
0: TRAIN [4][3040/7762]	Time 0.365 (0.325)	Data 9.80e-05 (2.07e-04)	Tok/s 46032 (43000)	Loss/tok 3.0184 (3.1659)	LR 1.250e-04
0: TRAIN [4][3050/7762]	Time 0.261 (0.325)	Data 1.04e-04 (2.06e-04)	Tok/s 40470 (42995)	Loss/tok 2.9081 (3.1659)	LR 1.250e-04
0: TRAIN [4][3060/7762]	Time 0.176 (0.325)	Data 9.82e-05 (2.06e-04)	Tok/s 29750 (42987)	Loss/tok 2.5920 (3.1659)	LR 1.250e-04
0: TRAIN [4][3070/7762]	Time 0.173 (0.325)	Data 1.17e-04 (2.06e-04)	Tok/s 30369 (42984)	Loss/tok 2.5480 (3.1657)	LR 1.250e-04
0: TRAIN [4][3080/7762]	Time 0.365 (0.325)	Data 1.01e-04 (2.05e-04)	Tok/s 45989 (42990)	Loss/tok 3.1792 (3.1657)	LR 1.250e-04
0: TRAIN [4][3090/7762]	Time 0.365 (0.325)	Data 1.00e-04 (2.05e-04)	Tok/s 45734 (42997)	Loss/tok 3.1979 (3.1659)	LR 1.250e-04
0: TRAIN [4][3100/7762]	Time 0.363 (0.326)	Data 1.02e-04 (2.05e-04)	Tok/s 45658 (43007)	Loss/tok 3.2560 (3.1664)	LR 1.250e-04
0: TRAIN [4][3110/7762]	Time 0.464 (0.326)	Data 1.05e-04 (2.04e-04)	Tok/s 50826 (43018)	Loss/tok 3.2876 (3.1667)	LR 1.250e-04
0: TRAIN [4][3120/7762]	Time 0.454 (0.326)	Data 1.02e-04 (2.04e-04)	Tok/s 51636 (43018)	Loss/tok 3.2970 (3.1667)	LR 1.250e-04
0: TRAIN [4][3130/7762]	Time 0.177 (0.326)	Data 9.70e-05 (2.04e-04)	Tok/s 29995 (43007)	Loss/tok 2.5846 (3.1665)	LR 1.250e-04
0: TRAIN [4][3140/7762]	Time 0.461 (0.326)	Data 9.94e-05 (2.03e-04)	Tok/s 51065 (43006)	Loss/tok 3.3187 (3.1665)	LR 1.250e-04
0: TRAIN [4][3150/7762]	Time 0.264 (0.326)	Data 1.14e-04 (2.03e-04)	Tok/s 39489 (43009)	Loss/tok 2.9905 (3.1664)	LR 1.250e-04
0: TRAIN [4][3160/7762]	Time 0.177 (0.326)	Data 9.97e-05 (2.03e-04)	Tok/s 29851 (43008)	Loss/tok 2.5934 (3.1664)	LR 1.250e-04
0: TRAIN [4][3170/7762]	Time 0.266 (0.326)	Data 1.05e-04 (2.02e-04)	Tok/s 39116 (43013)	Loss/tok 3.0536 (3.1666)	LR 1.250e-04
0: TRAIN [4][3180/7762]	Time 0.362 (0.326)	Data 9.89e-05 (2.02e-04)	Tok/s 45376 (43015)	Loss/tok 3.2300 (3.1668)	LR 1.250e-04
0: TRAIN [4][3190/7762]	Time 0.252 (0.326)	Data 1.01e-04 (2.02e-04)	Tok/s 40168 (43020)	Loss/tok 2.9705 (3.1669)	LR 1.250e-04
0: TRAIN [4][3200/7762]	Time 0.458 (0.326)	Data 1.18e-04 (2.02e-04)	Tok/s 49802 (43014)	Loss/tok 3.4590 (3.1668)	LR 1.250e-04
0: TRAIN [4][3210/7762]	Time 0.360 (0.325)	Data 9.73e-05 (2.01e-04)	Tok/s 47133 (43006)	Loss/tok 3.1592 (3.1665)	LR 1.250e-04
0: TRAIN [4][3220/7762]	Time 0.267 (0.325)	Data 9.78e-05 (2.01e-04)	Tok/s 39084 (43001)	Loss/tok 2.9882 (3.1666)	LR 1.250e-04
0: TRAIN [4][3230/7762]	Time 0.586 (0.326)	Data 1.00e-04 (2.01e-04)	Tok/s 50919 (43010)	Loss/tok 3.4612 (3.1670)	LR 1.250e-04
0: TRAIN [4][3240/7762]	Time 0.363 (0.325)	Data 1.03e-04 (2.00e-04)	Tok/s 45936 (43001)	Loss/tok 3.0665 (3.1667)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3250/7762]	Time 0.266 (0.326)	Data 1.04e-04 (2.00e-04)	Tok/s 39435 (43013)	Loss/tok 3.0226 (3.1672)	LR 1.250e-04
0: TRAIN [4][3260/7762]	Time 0.569 (0.326)	Data 2.33e-04 (2.00e-04)	Tok/s 51578 (43022)	Loss/tok 3.5227 (3.1675)	LR 1.250e-04
0: TRAIN [4][3270/7762]	Time 0.461 (0.326)	Data 1.01e-04 (1.99e-04)	Tok/s 50202 (43018)	Loss/tok 3.3830 (3.1674)	LR 1.250e-04
0: TRAIN [4][3280/7762]	Time 0.342 (0.326)	Data 1.16e-04 (1.99e-04)	Tok/s 49024 (43011)	Loss/tok 3.1539 (3.1671)	LR 1.250e-04
0: TRAIN [4][3290/7762]	Time 0.457 (0.326)	Data 1.02e-04 (1.99e-04)	Tok/s 51285 (43008)	Loss/tok 3.3201 (3.1669)	LR 1.250e-04
0: TRAIN [4][3300/7762]	Time 0.266 (0.326)	Data 1.18e-04 (1.99e-04)	Tok/s 39060 (43009)	Loss/tok 3.0282 (3.1672)	LR 1.250e-04
0: TRAIN [4][3310/7762]	Time 0.172 (0.326)	Data 1.02e-04 (1.98e-04)	Tok/s 30574 (43005)	Loss/tok 2.6443 (3.1671)	LR 1.250e-04
0: TRAIN [4][3320/7762]	Time 0.174 (0.326)	Data 1.01e-04 (1.98e-04)	Tok/s 31030 (43007)	Loss/tok 2.5319 (3.1672)	LR 1.250e-04
0: TRAIN [4][3330/7762]	Time 0.460 (0.326)	Data 1.15e-04 (1.98e-04)	Tok/s 50999 (43012)	Loss/tok 3.3957 (3.1673)	LR 1.250e-04
0: TRAIN [4][3340/7762]	Time 0.255 (0.326)	Data 9.68e-05 (1.98e-04)	Tok/s 41137 (43013)	Loss/tok 2.8621 (3.1672)	LR 1.250e-04
0: TRAIN [4][3350/7762]	Time 0.457 (0.326)	Data 1.03e-04 (1.97e-04)	Tok/s 51654 (43018)	Loss/tok 3.3102 (3.1672)	LR 1.250e-04
0: TRAIN [4][3360/7762]	Time 0.268 (0.326)	Data 9.51e-05 (1.97e-04)	Tok/s 37773 (43015)	Loss/tok 2.9369 (3.1672)	LR 1.250e-04
0: TRAIN [4][3370/7762]	Time 0.265 (0.326)	Data 9.80e-05 (1.97e-04)	Tok/s 39447 (43007)	Loss/tok 2.9021 (3.1672)	LR 1.250e-04
0: TRAIN [4][3380/7762]	Time 0.175 (0.326)	Data 1.13e-04 (1.96e-04)	Tok/s 29828 (43004)	Loss/tok 2.5306 (3.1670)	LR 1.250e-04
0: TRAIN [4][3390/7762]	Time 0.262 (0.325)	Data 1.01e-04 (1.96e-04)	Tok/s 39982 (42996)	Loss/tok 2.9880 (3.1668)	LR 1.250e-04
0: TRAIN [4][3400/7762]	Time 0.258 (0.325)	Data 1.02e-04 (1.96e-04)	Tok/s 39631 (42994)	Loss/tok 2.8476 (3.1666)	LR 1.250e-04
0: TRAIN [4][3410/7762]	Time 0.176 (0.325)	Data 9.97e-05 (1.96e-04)	Tok/s 30893 (42990)	Loss/tok 2.6058 (3.1665)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3420/7762]	Time 0.561 (0.325)	Data 1.00e-04 (1.95e-04)	Tok/s 53056 (42991)	Loss/tok 3.4478 (3.1667)	LR 1.250e-04
0: TRAIN [4][3430/7762]	Time 0.251 (0.325)	Data 1.01e-04 (1.95e-04)	Tok/s 41170 (42990)	Loss/tok 2.9710 (3.1665)	LR 1.250e-04
0: TRAIN [4][3440/7762]	Time 0.258 (0.325)	Data 1.18e-04 (1.95e-04)	Tok/s 40452 (42986)	Loss/tok 3.0300 (3.1664)	LR 1.250e-04
0: TRAIN [4][3450/7762]	Time 0.267 (0.325)	Data 1.06e-04 (1.95e-04)	Tok/s 37870 (42988)	Loss/tok 2.9418 (3.1663)	LR 1.250e-04
0: TRAIN [4][3460/7762]	Time 0.265 (0.325)	Data 1.02e-04 (1.94e-04)	Tok/s 38542 (42988)	Loss/tok 3.0087 (3.1662)	LR 1.250e-04
0: TRAIN [4][3470/7762]	Time 0.176 (0.325)	Data 9.87e-05 (1.94e-04)	Tok/s 30217 (42991)	Loss/tok 2.5488 (3.1665)	LR 1.250e-04
0: TRAIN [4][3480/7762]	Time 0.364 (0.325)	Data 1.04e-04 (1.94e-04)	Tok/s 46198 (42986)	Loss/tok 3.2015 (3.1662)	LR 1.250e-04
0: TRAIN [4][3490/7762]	Time 0.175 (0.325)	Data 1.05e-04 (1.94e-04)	Tok/s 30355 (42972)	Loss/tok 2.5520 (3.1658)	LR 1.250e-04
0: TRAIN [4][3500/7762]	Time 0.259 (0.325)	Data 1.03e-04 (1.93e-04)	Tok/s 39876 (42973)	Loss/tok 2.9868 (3.1660)	LR 1.250e-04
0: TRAIN [4][3510/7762]	Time 0.253 (0.325)	Data 1.38e-04 (1.93e-04)	Tok/s 40975 (42979)	Loss/tok 2.8539 (3.1660)	LR 1.250e-04
0: TRAIN [4][3520/7762]	Time 0.266 (0.325)	Data 1.01e-04 (1.93e-04)	Tok/s 38515 (42984)	Loss/tok 2.9286 (3.1660)	LR 1.250e-04
0: TRAIN [4][3530/7762]	Time 0.260 (0.325)	Data 1.11e-04 (1.93e-04)	Tok/s 39000 (42972)	Loss/tok 2.9298 (3.1657)	LR 1.250e-04
0: TRAIN [4][3540/7762]	Time 0.363 (0.325)	Data 1.19e-04 (1.92e-04)	Tok/s 45773 (42968)	Loss/tok 3.2852 (3.1656)	LR 1.250e-04
0: TRAIN [4][3550/7762]	Time 0.353 (0.325)	Data 1.03e-04 (1.92e-04)	Tok/s 47831 (42975)	Loss/tok 3.0712 (3.1658)	LR 1.250e-04
0: TRAIN [4][3560/7762]	Time 0.464 (0.325)	Data 1.22e-04 (1.92e-04)	Tok/s 50543 (42989)	Loss/tok 3.2741 (3.1660)	LR 1.250e-04
0: TRAIN [4][3570/7762]	Time 0.367 (0.325)	Data 1.04e-04 (1.92e-04)	Tok/s 45450 (42994)	Loss/tok 3.1932 (3.1663)	LR 1.250e-04
0: TRAIN [4][3580/7762]	Time 0.177 (0.325)	Data 9.92e-05 (1.91e-04)	Tok/s 29789 (42984)	Loss/tok 2.5469 (3.1659)	LR 1.250e-04
0: TRAIN [4][3590/7762]	Time 0.445 (0.325)	Data 1.02e-04 (1.91e-04)	Tok/s 51987 (42990)	Loss/tok 3.3594 (3.1660)	LR 1.250e-04
0: TRAIN [4][3600/7762]	Time 0.365 (0.325)	Data 9.56e-05 (1.91e-04)	Tok/s 45456 (42984)	Loss/tok 3.2812 (3.1659)	LR 1.250e-04
0: TRAIN [4][3610/7762]	Time 0.261 (0.325)	Data 1.15e-04 (1.91e-04)	Tok/s 40513 (42976)	Loss/tok 2.9805 (3.1658)	LR 1.250e-04
0: TRAIN [4][3620/7762]	Time 0.179 (0.325)	Data 1.04e-04 (1.90e-04)	Tok/s 29371 (42977)	Loss/tok 2.5929 (3.1660)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3630/7762]	Time 0.264 (0.325)	Data 9.97e-05 (1.90e-04)	Tok/s 38979 (42977)	Loss/tok 2.9272 (3.1660)	LR 1.250e-04
0: TRAIN [4][3640/7762]	Time 0.440 (0.325)	Data 9.80e-05 (1.90e-04)	Tok/s 53257 (42974)	Loss/tok 3.3586 (3.1660)	LR 1.250e-04
0: TRAIN [4][3650/7762]	Time 0.263 (0.325)	Data 1.06e-04 (1.90e-04)	Tok/s 39058 (42979)	Loss/tok 2.9492 (3.1664)	LR 1.250e-04
0: TRAIN [4][3660/7762]	Time 0.366 (0.325)	Data 9.49e-05 (1.89e-04)	Tok/s 45394 (42981)	Loss/tok 3.1924 (3.1666)	LR 1.250e-04
0: TRAIN [4][3670/7762]	Time 0.432 (0.325)	Data 1.03e-04 (1.89e-04)	Tok/s 54182 (42976)	Loss/tok 3.3458 (3.1663)	LR 1.250e-04
0: TRAIN [4][3680/7762]	Time 0.257 (0.325)	Data 1.04e-04 (1.89e-04)	Tok/s 39711 (42978)	Loss/tok 2.9403 (3.1664)	LR 1.250e-04
0: TRAIN [4][3690/7762]	Time 0.267 (0.325)	Data 1.03e-04 (1.89e-04)	Tok/s 38453 (42984)	Loss/tok 3.0137 (3.1666)	LR 1.250e-04
0: TRAIN [4][3700/7762]	Time 0.439 (0.325)	Data 1.00e-04 (1.88e-04)	Tok/s 53369 (42986)	Loss/tok 3.3588 (3.1667)	LR 1.250e-04
0: TRAIN [4][3710/7762]	Time 0.342 (0.325)	Data 1.03e-04 (1.88e-04)	Tok/s 48578 (42992)	Loss/tok 3.2045 (3.1667)	LR 1.250e-04
0: TRAIN [4][3720/7762]	Time 0.360 (0.325)	Data 1.02e-04 (1.88e-04)	Tok/s 46904 (42999)	Loss/tok 3.2178 (3.1668)	LR 1.250e-04
0: TRAIN [4][3730/7762]	Time 0.256 (0.325)	Data 9.61e-05 (1.88e-04)	Tok/s 39694 (42989)	Loss/tok 3.0223 (3.1665)	LR 1.250e-04
0: TRAIN [4][3740/7762]	Time 0.260 (0.326)	Data 1.01e-04 (1.88e-04)	Tok/s 40091 (43000)	Loss/tok 2.9664 (3.1671)	LR 1.250e-04
0: TRAIN [4][3750/7762]	Time 0.262 (0.326)	Data 1.02e-04 (1.87e-04)	Tok/s 39846 (43004)	Loss/tok 2.9689 (3.1672)	LR 1.250e-04
0: TRAIN [4][3760/7762]	Time 0.365 (0.326)	Data 9.99e-05 (1.87e-04)	Tok/s 46317 (43005)	Loss/tok 3.0993 (3.1671)	LR 1.250e-04
0: TRAIN [4][3770/7762]	Time 0.260 (0.326)	Data 1.02e-04 (1.87e-04)	Tok/s 39617 (43008)	Loss/tok 3.0479 (3.1671)	LR 1.250e-04
0: TRAIN [4][3780/7762]	Time 0.436 (0.326)	Data 1.04e-04 (1.87e-04)	Tok/s 53616 (43019)	Loss/tok 3.3577 (3.1674)	LR 1.250e-04
0: TRAIN [4][3790/7762]	Time 0.263 (0.326)	Data 9.87e-05 (1.86e-04)	Tok/s 39183 (43019)	Loss/tok 3.0745 (3.1675)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3800/7762]	Time 0.462 (0.326)	Data 9.87e-05 (1.86e-04)	Tok/s 50608 (43028)	Loss/tok 3.3453 (3.1677)	LR 1.250e-04
0: TRAIN [4][3810/7762]	Time 0.262 (0.326)	Data 1.01e-04 (1.86e-04)	Tok/s 40018 (43031)	Loss/tok 2.9127 (3.1682)	LR 1.250e-04
0: TRAIN [4][3820/7762]	Time 0.263 (0.326)	Data 1.01e-04 (1.86e-04)	Tok/s 39388 (43036)	Loss/tok 3.0162 (3.1685)	LR 1.250e-04
0: TRAIN [4][3830/7762]	Time 0.176 (0.326)	Data 1.01e-04 (1.86e-04)	Tok/s 31035 (43029)	Loss/tok 2.5136 (3.1682)	LR 1.250e-04
0: TRAIN [4][3840/7762]	Time 0.177 (0.326)	Data 1.15e-04 (1.85e-04)	Tok/s 29471 (43033)	Loss/tok 2.5591 (3.1685)	LR 1.250e-04
0: TRAIN [4][3850/7762]	Time 0.265 (0.326)	Data 1.00e-04 (1.85e-04)	Tok/s 38596 (43028)	Loss/tok 2.9070 (3.1686)	LR 1.250e-04
0: TRAIN [4][3860/7762]	Time 0.355 (0.326)	Data 1.04e-04 (1.85e-04)	Tok/s 47270 (43026)	Loss/tok 3.1663 (3.1686)	LR 1.250e-04
0: TRAIN [4][3870/7762]	Time 0.340 (0.326)	Data 1.19e-04 (1.85e-04)	Tok/s 48701 (43030)	Loss/tok 3.1700 (3.1685)	LR 1.250e-04
0: TRAIN [4][3880/7762]	Time 0.266 (0.326)	Data 1.17e-04 (1.85e-04)	Tok/s 38918 (43032)	Loss/tok 2.8777 (3.1685)	LR 1.250e-04
0: TRAIN [4][3890/7762]	Time 0.264 (0.326)	Data 1.02e-04 (1.84e-04)	Tok/s 39172 (43039)	Loss/tok 2.9969 (3.1685)	LR 1.250e-04
0: TRAIN [4][3900/7762]	Time 0.261 (0.326)	Data 9.92e-05 (1.84e-04)	Tok/s 40127 (43040)	Loss/tok 2.9773 (3.1684)	LR 1.250e-04
0: TRAIN [4][3910/7762]	Time 0.360 (0.326)	Data 1.00e-04 (1.84e-04)	Tok/s 46142 (43047)	Loss/tok 3.0840 (3.1685)	LR 1.250e-04
0: TRAIN [4][3920/7762]	Time 0.177 (0.326)	Data 1.00e-04 (1.84e-04)	Tok/s 29836 (43051)	Loss/tok 2.5685 (3.1684)	LR 1.250e-04
0: TRAIN [4][3930/7762]	Time 0.264 (0.326)	Data 9.58e-05 (1.83e-04)	Tok/s 40039 (43049)	Loss/tok 3.1702 (3.1684)	LR 1.250e-04
0: TRAIN [4][3940/7762]	Time 0.257 (0.326)	Data 1.18e-04 (1.83e-04)	Tok/s 40358 (43059)	Loss/tok 2.9048 (3.1687)	LR 1.250e-04
0: TRAIN [4][3950/7762]	Time 0.364 (0.326)	Data 1.01e-04 (1.83e-04)	Tok/s 46892 (43062)	Loss/tok 3.1916 (3.1686)	LR 1.250e-04
0: TRAIN [4][3960/7762]	Time 0.250 (0.326)	Data 9.85e-05 (1.83e-04)	Tok/s 40700 (43056)	Loss/tok 2.9425 (3.1685)	LR 1.250e-04
0: TRAIN [4][3970/7762]	Time 0.260 (0.326)	Data 1.16e-04 (1.83e-04)	Tok/s 39592 (43048)	Loss/tok 3.0293 (3.1682)	LR 1.250e-04
0: TRAIN [4][3980/7762]	Time 0.451 (0.326)	Data 1.16e-04 (1.83e-04)	Tok/s 51564 (43052)	Loss/tok 3.3769 (3.1684)	LR 1.250e-04
0: TRAIN [4][3990/7762]	Time 0.352 (0.326)	Data 1.04e-04 (1.82e-04)	Tok/s 47835 (43053)	Loss/tok 3.1807 (3.1682)	LR 1.250e-04
0: TRAIN [4][4000/7762]	Time 0.253 (0.326)	Data 1.21e-04 (1.82e-04)	Tok/s 40440 (43053)	Loss/tok 2.9673 (3.1681)	LR 1.250e-04
0: TRAIN [4][4010/7762]	Time 0.341 (0.326)	Data 1.02e-04 (1.82e-04)	Tok/s 49895 (43063)	Loss/tok 3.1354 (3.1684)	LR 1.250e-04
0: TRAIN [4][4020/7762]	Time 0.260 (0.326)	Data 1.03e-04 (1.82e-04)	Tok/s 39963 (43056)	Loss/tok 3.0688 (3.1683)	LR 1.250e-04
0: TRAIN [4][4030/7762]	Time 0.363 (0.326)	Data 9.82e-05 (1.82e-04)	Tok/s 46053 (43059)	Loss/tok 3.2041 (3.1683)	LR 1.250e-04
0: TRAIN [4][4040/7762]	Time 0.266 (0.326)	Data 1.01e-04 (1.81e-04)	Tok/s 39445 (43060)	Loss/tok 2.9595 (3.1682)	LR 1.250e-04
0: TRAIN [4][4050/7762]	Time 0.353 (0.326)	Data 1.04e-04 (1.81e-04)	Tok/s 47545 (43068)	Loss/tok 3.1657 (3.1682)	LR 1.250e-04
0: TRAIN [4][4060/7762]	Time 0.363 (0.326)	Data 1.00e-04 (1.81e-04)	Tok/s 45879 (43066)	Loss/tok 3.3121 (3.1682)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][4070/7762]	Time 0.254 (0.326)	Data 9.82e-05 (1.81e-04)	Tok/s 40746 (43069)	Loss/tok 2.9340 (3.1682)	LR 1.250e-04
0: TRAIN [4][4080/7762]	Time 0.260 (0.326)	Data 1.50e-04 (1.81e-04)	Tok/s 40139 (43071)	Loss/tok 2.9710 (3.1682)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][4090/7762]	Time 0.256 (0.326)	Data 1.18e-04 (1.80e-04)	Tok/s 40490 (43073)	Loss/tok 3.0885 (3.1683)	LR 1.250e-04
0: TRAIN [4][4100/7762]	Time 0.452 (0.326)	Data 1.15e-04 (1.80e-04)	Tok/s 52077 (43079)	Loss/tok 3.2132 (3.1685)	LR 1.250e-04
0: TRAIN [4][4110/7762]	Time 0.366 (0.326)	Data 1.29e-04 (1.80e-04)	Tok/s 45588 (43079)	Loss/tok 3.2141 (3.1683)	LR 1.250e-04
0: TRAIN [4][4120/7762]	Time 0.264 (0.326)	Data 1.04e-04 (1.80e-04)	Tok/s 38690 (43081)	Loss/tok 3.0605 (3.1683)	LR 1.250e-04
0: TRAIN [4][4130/7762]	Time 0.266 (0.326)	Data 1.03e-04 (1.80e-04)	Tok/s 38645 (43082)	Loss/tok 2.8521 (3.1684)	LR 1.250e-04
0: TRAIN [4][4140/7762]	Time 0.355 (0.326)	Data 1.01e-04 (1.80e-04)	Tok/s 47050 (43082)	Loss/tok 3.1281 (3.1683)	LR 1.250e-04
0: TRAIN [4][4150/7762]	Time 0.460 (0.327)	Data 1.04e-04 (1.79e-04)	Tok/s 50811 (43090)	Loss/tok 3.3405 (3.1684)	LR 1.250e-04
0: TRAIN [4][4160/7762]	Time 0.265 (0.326)	Data 1.13e-04 (1.79e-04)	Tok/s 38894 (43074)	Loss/tok 3.0587 (3.1681)	LR 1.250e-04
0: TRAIN [4][4170/7762]	Time 0.363 (0.326)	Data 1.01e-04 (1.79e-04)	Tok/s 46142 (43076)	Loss/tok 3.0727 (3.1680)	LR 1.250e-04
0: TRAIN [4][4180/7762]	Time 0.456 (0.326)	Data 1.03e-04 (1.79e-04)	Tok/s 50878 (43083)	Loss/tok 3.2818 (3.1682)	LR 1.250e-04
0: TRAIN [4][4190/7762]	Time 0.461 (0.326)	Data 1.03e-04 (1.79e-04)	Tok/s 50738 (43082)	Loss/tok 3.4086 (3.1681)	LR 1.250e-04
0: TRAIN [4][4200/7762]	Time 0.268 (0.326)	Data 1.24e-04 (1.78e-04)	Tok/s 38251 (43081)	Loss/tok 2.9790 (3.1683)	LR 1.250e-04
0: TRAIN [4][4210/7762]	Time 0.170 (0.326)	Data 1.05e-04 (1.78e-04)	Tok/s 31205 (43081)	Loss/tok 2.5327 (3.1683)	LR 1.250e-04
0: TRAIN [4][4220/7762]	Time 0.456 (0.327)	Data 9.99e-05 (1.78e-04)	Tok/s 51748 (43088)	Loss/tok 3.2757 (3.1686)	LR 1.250e-04
0: TRAIN [4][4230/7762]	Time 0.461 (0.327)	Data 1.01e-04 (1.78e-04)	Tok/s 51334 (43090)	Loss/tok 3.3630 (3.1688)	LR 1.250e-04
0: TRAIN [4][4240/7762]	Time 0.269 (0.327)	Data 1.05e-04 (1.78e-04)	Tok/s 38287 (43091)	Loss/tok 2.9230 (3.1689)	LR 1.250e-04
0: TRAIN [4][4250/7762]	Time 0.268 (0.327)	Data 1.08e-04 (1.78e-04)	Tok/s 39325 (43092)	Loss/tok 2.9343 (3.1688)	LR 1.250e-04
0: TRAIN [4][4260/7762]	Time 0.266 (0.327)	Data 1.01e-04 (1.77e-04)	Tok/s 38721 (43093)	Loss/tok 2.9907 (3.1688)	LR 1.250e-04
0: TRAIN [4][4270/7762]	Time 0.563 (0.327)	Data 1.07e-04 (1.77e-04)	Tok/s 52022 (43091)	Loss/tok 3.5720 (3.1687)	LR 1.250e-04
0: TRAIN [4][4280/7762]	Time 0.267 (0.327)	Data 1.04e-04 (1.77e-04)	Tok/s 38536 (43093)	Loss/tok 2.9464 (3.1690)	LR 1.250e-04
0: TRAIN [4][4290/7762]	Time 0.365 (0.327)	Data 1.02e-04 (1.77e-04)	Tok/s 46405 (43096)	Loss/tok 3.2184 (3.1691)	LR 1.250e-04
0: TRAIN [4][4300/7762]	Time 0.462 (0.327)	Data 1.07e-04 (1.77e-04)	Tok/s 50309 (43096)	Loss/tok 3.3522 (3.1691)	LR 1.250e-04
0: TRAIN [4][4310/7762]	Time 0.459 (0.327)	Data 1.03e-04 (1.77e-04)	Tok/s 51488 (43095)	Loss/tok 3.2570 (3.1692)	LR 1.250e-04
0: TRAIN [4][4320/7762]	Time 0.174 (0.327)	Data 1.04e-04 (1.76e-04)	Tok/s 30973 (43096)	Loss/tok 2.6766 (3.1693)	LR 1.250e-04
0: TRAIN [4][4330/7762]	Time 0.356 (0.327)	Data 1.03e-04 (1.76e-04)	Tok/s 46406 (43103)	Loss/tok 3.2801 (3.1697)	LR 1.250e-04
0: TRAIN [4][4340/7762]	Time 0.352 (0.327)	Data 1.17e-04 (1.76e-04)	Tok/s 47340 (43103)	Loss/tok 3.1764 (3.1696)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][4350/7762]	Time 0.266 (0.327)	Data 1.01e-04 (1.76e-04)	Tok/s 37991 (43103)	Loss/tok 3.1121 (3.1698)	LR 1.250e-04
0: TRAIN [4][4360/7762]	Time 0.266 (0.327)	Data 9.68e-05 (1.76e-04)	Tok/s 38353 (43102)	Loss/tok 2.9895 (3.1698)	LR 1.250e-04
0: TRAIN [4][4370/7762]	Time 0.258 (0.327)	Data 1.03e-04 (1.76e-04)	Tok/s 40530 (43097)	Loss/tok 3.0162 (3.1696)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][4380/7762]	Time 0.461 (0.327)	Data 9.89e-05 (1.75e-04)	Tok/s 50099 (43106)	Loss/tok 3.4162 (3.1699)	LR 1.250e-04
0: TRAIN [4][4390/7762]	Time 0.266 (0.327)	Data 1.02e-04 (1.75e-04)	Tok/s 38617 (43105)	Loss/tok 2.9046 (3.1697)	LR 1.250e-04
0: TRAIN [4][4400/7762]	Time 0.364 (0.327)	Data 1.12e-04 (1.75e-04)	Tok/s 45923 (43103)	Loss/tok 3.1119 (3.1695)	LR 1.250e-04
0: TRAIN [4][4410/7762]	Time 0.265 (0.327)	Data 1.02e-04 (1.75e-04)	Tok/s 38504 (43102)	Loss/tok 3.0073 (3.1695)	LR 1.250e-04
0: TRAIN [4][4420/7762]	Time 0.172 (0.327)	Data 1.17e-04 (1.75e-04)	Tok/s 30833 (43094)	Loss/tok 2.6911 (3.1695)	LR 1.250e-04
0: TRAIN [4][4430/7762]	Time 0.261 (0.327)	Data 9.47e-05 (1.75e-04)	Tok/s 40604 (43085)	Loss/tok 2.9606 (3.1692)	LR 1.250e-04
0: TRAIN [4][4440/7762]	Time 0.259 (0.327)	Data 1.04e-04 (1.75e-04)	Tok/s 40240 (43077)	Loss/tok 2.9906 (3.1692)	LR 1.250e-04
0: TRAIN [4][4450/7762]	Time 0.264 (0.327)	Data 9.87e-05 (1.74e-04)	Tok/s 38881 (43070)	Loss/tok 2.9415 (3.1691)	LR 1.250e-04
0: TRAIN [4][4460/7762]	Time 0.264 (0.327)	Data 1.06e-04 (1.74e-04)	Tok/s 39255 (43067)	Loss/tok 3.0354 (3.1690)	LR 1.250e-04
0: TRAIN [4][4470/7762]	Time 0.461 (0.326)	Data 1.03e-04 (1.74e-04)	Tok/s 50472 (43065)	Loss/tok 3.3199 (3.1690)	LR 1.250e-04
0: TRAIN [4][4480/7762]	Time 0.580 (0.327)	Data 1.04e-04 (1.74e-04)	Tok/s 51078 (43070)	Loss/tok 3.4687 (3.1692)	LR 1.250e-04
0: TRAIN [4][4490/7762]	Time 0.361 (0.327)	Data 9.73e-05 (1.74e-04)	Tok/s 46347 (43072)	Loss/tok 3.0354 (3.1692)	LR 1.250e-04
0: TRAIN [4][4500/7762]	Time 0.170 (0.327)	Data 1.07e-04 (1.74e-04)	Tok/s 30513 (43067)	Loss/tok 2.5040 (3.1692)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][4510/7762]	Time 0.266 (0.327)	Data 1.22e-04 (1.73e-04)	Tok/s 38458 (43068)	Loss/tok 2.8228 (3.1691)	LR 1.250e-04
0: TRAIN [4][4520/7762]	Time 0.344 (0.327)	Data 1.14e-04 (1.73e-04)	Tok/s 48390 (43071)	Loss/tok 3.2021 (3.1691)	LR 1.250e-04
0: TRAIN [4][4530/7762]	Time 0.453 (0.327)	Data 1.00e-04 (1.73e-04)	Tok/s 51878 (43068)	Loss/tok 3.3017 (3.1691)	LR 1.250e-04
0: TRAIN [4][4540/7762]	Time 0.264 (0.327)	Data 9.80e-05 (1.73e-04)	Tok/s 38588 (43065)	Loss/tok 2.9443 (3.1690)	LR 1.250e-04
0: TRAIN [4][4550/7762]	Time 0.267 (0.327)	Data 1.01e-04 (1.73e-04)	Tok/s 38344 (43067)	Loss/tok 3.0788 (3.1689)	LR 1.250e-04
0: TRAIN [4][4560/7762]	Time 0.263 (0.327)	Data 1.04e-04 (1.73e-04)	Tok/s 39517 (43067)	Loss/tok 2.9911 (3.1689)	LR 1.250e-04
0: TRAIN [4][4570/7762]	Time 0.454 (0.327)	Data 1.47e-04 (1.73e-04)	Tok/s 50951 (43064)	Loss/tok 3.4111 (3.1690)	LR 1.250e-04
0: TRAIN [4][4580/7762]	Time 0.363 (0.327)	Data 9.80e-05 (1.72e-04)	Tok/s 46157 (43063)	Loss/tok 3.1836 (3.1689)	LR 1.250e-04
0: TRAIN [4][4590/7762]	Time 0.264 (0.327)	Data 1.02e-04 (1.72e-04)	Tok/s 38565 (43063)	Loss/tok 2.9335 (3.1688)	LR 1.250e-04
0: TRAIN [4][4600/7762]	Time 0.265 (0.326)	Data 1.02e-04 (1.72e-04)	Tok/s 39064 (43057)	Loss/tok 2.8392 (3.1686)	LR 1.250e-04
0: TRAIN [4][4610/7762]	Time 0.356 (0.326)	Data 1.29e-04 (1.72e-04)	Tok/s 46890 (43057)	Loss/tok 3.2574 (3.1685)	LR 1.250e-04
0: TRAIN [4][4620/7762]	Time 0.269 (0.326)	Data 1.23e-04 (1.72e-04)	Tok/s 38636 (43054)	Loss/tok 2.9403 (3.1682)	LR 1.250e-04
0: TRAIN [4][4630/7762]	Time 0.264 (0.326)	Data 1.02e-04 (1.72e-04)	Tok/s 38882 (43057)	Loss/tok 3.0318 (3.1684)	LR 1.250e-04
0: TRAIN [4][4640/7762]	Time 0.256 (0.326)	Data 1.17e-04 (1.72e-04)	Tok/s 40475 (43058)	Loss/tok 2.9908 (3.1685)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][4650/7762]	Time 0.265 (0.326)	Data 1.04e-04 (1.71e-04)	Tok/s 38391 (43066)	Loss/tok 2.9150 (3.1685)	LR 1.250e-04
0: TRAIN [4][4660/7762]	Time 0.449 (0.327)	Data 1.13e-04 (1.71e-04)	Tok/s 51425 (43068)	Loss/tok 3.4364 (3.1686)	LR 1.250e-04
0: TRAIN [4][4670/7762]	Time 0.355 (0.326)	Data 9.92e-05 (1.71e-04)	Tok/s 47863 (43061)	Loss/tok 3.1144 (3.1684)	LR 1.250e-04
0: TRAIN [4][4680/7762]	Time 0.462 (0.326)	Data 1.03e-04 (1.71e-04)	Tok/s 50752 (43062)	Loss/tok 3.3337 (3.1685)	LR 1.250e-04
0: TRAIN [4][4690/7762]	Time 0.366 (0.326)	Data 1.21e-04 (1.71e-04)	Tok/s 45693 (43063)	Loss/tok 3.1829 (3.1685)	LR 1.250e-04
0: TRAIN [4][4700/7762]	Time 0.350 (0.326)	Data 9.78e-05 (1.71e-04)	Tok/s 48384 (43064)	Loss/tok 3.1557 (3.1686)	LR 1.250e-04
0: TRAIN [4][4710/7762]	Time 0.358 (0.326)	Data 1.15e-04 (1.71e-04)	Tok/s 46836 (43066)	Loss/tok 3.2118 (3.1686)	LR 1.250e-04
0: TRAIN [4][4720/7762]	Time 0.268 (0.327)	Data 1.07e-04 (1.70e-04)	Tok/s 38073 (43072)	Loss/tok 3.0779 (3.1689)	LR 1.250e-04
0: TRAIN [4][4730/7762]	Time 0.259 (0.326)	Data 1.01e-04 (1.70e-04)	Tok/s 39646 (43065)	Loss/tok 3.0350 (3.1687)	LR 1.250e-04
0: TRAIN [4][4740/7762]	Time 0.350 (0.327)	Data 1.20e-04 (1.70e-04)	Tok/s 48114 (43073)	Loss/tok 3.2003 (3.1690)	LR 1.250e-04
0: TRAIN [4][4750/7762]	Time 0.364 (0.327)	Data 1.02e-04 (1.70e-04)	Tok/s 45909 (43073)	Loss/tok 3.1437 (3.1691)	LR 1.250e-04
0: TRAIN [4][4760/7762]	Time 0.266 (0.327)	Data 9.75e-05 (1.70e-04)	Tok/s 38352 (43077)	Loss/tok 3.0663 (3.1693)	LR 1.250e-04
0: TRAIN [4][4770/7762]	Time 0.175 (0.327)	Data 1.02e-04 (1.70e-04)	Tok/s 30360 (43076)	Loss/tok 2.6034 (3.1693)	LR 1.250e-04
0: TRAIN [4][4780/7762]	Time 0.365 (0.327)	Data 9.99e-05 (1.70e-04)	Tok/s 45691 (43070)	Loss/tok 3.1810 (3.1691)	LR 1.250e-04
0: TRAIN [4][4790/7762]	Time 0.355 (0.327)	Data 1.12e-04 (1.69e-04)	Tok/s 47862 (43067)	Loss/tok 3.0671 (3.1689)	LR 1.250e-04
0: TRAIN [4][4800/7762]	Time 0.174 (0.327)	Data 1.01e-04 (1.69e-04)	Tok/s 30058 (43071)	Loss/tok 2.5183 (3.1694)	LR 1.250e-04
0: TRAIN [4][4810/7762]	Time 0.452 (0.327)	Data 1.01e-04 (1.69e-04)	Tok/s 51113 (43079)	Loss/tok 3.3445 (3.1697)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][4820/7762]	Time 0.434 (0.327)	Data 1.19e-04 (1.69e-04)	Tok/s 53081 (43081)	Loss/tok 3.4042 (3.1698)	LR 1.250e-04
0: TRAIN [4][4830/7762]	Time 0.358 (0.327)	Data 1.00e-04 (1.69e-04)	Tok/s 47108 (43080)	Loss/tok 3.1379 (3.1697)	LR 1.250e-04
0: TRAIN [4][4840/7762]	Time 0.265 (0.327)	Data 1.18e-04 (1.69e-04)	Tok/s 39223 (43083)	Loss/tok 2.9969 (3.1698)	LR 1.250e-04
0: TRAIN [4][4850/7762]	Time 0.435 (0.327)	Data 1.01e-04 (1.69e-04)	Tok/s 54233 (43086)	Loss/tok 3.3422 (3.1698)	LR 1.250e-04
0: TRAIN [4][4860/7762]	Time 0.265 (0.327)	Data 9.85e-05 (1.68e-04)	Tok/s 39313 (43090)	Loss/tok 2.9431 (3.1697)	LR 1.250e-04
0: TRAIN [4][4870/7762]	Time 0.263 (0.327)	Data 1.11e-04 (1.68e-04)	Tok/s 39898 (43084)	Loss/tok 2.9557 (3.1694)	LR 1.250e-04
0: TRAIN [4][4880/7762]	Time 0.256 (0.327)	Data 1.05e-04 (1.68e-04)	Tok/s 39929 (43087)	Loss/tok 2.9640 (3.1695)	LR 1.250e-04
0: TRAIN [4][4890/7762]	Time 0.359 (0.327)	Data 1.01e-04 (1.68e-04)	Tok/s 47337 (43085)	Loss/tok 3.2436 (3.1695)	LR 1.250e-04
0: TRAIN [4][4900/7762]	Time 0.267 (0.327)	Data 9.99e-05 (1.68e-04)	Tok/s 38588 (43087)	Loss/tok 2.9064 (3.1698)	LR 1.250e-04
0: TRAIN [4][4910/7762]	Time 0.172 (0.327)	Data 9.75e-05 (1.68e-04)	Tok/s 30515 (43080)	Loss/tok 2.5983 (3.1697)	LR 1.250e-04
0: TRAIN [4][4920/7762]	Time 0.349 (0.327)	Data 1.01e-04 (1.68e-04)	Tok/s 48407 (43085)	Loss/tok 3.0675 (3.1700)	LR 1.250e-04
0: TRAIN [4][4930/7762]	Time 0.260 (0.327)	Data 1.01e-04 (1.68e-04)	Tok/s 39192 (43081)	Loss/tok 2.9643 (3.1700)	LR 1.250e-04
0: TRAIN [4][4940/7762]	Time 0.265 (0.327)	Data 1.13e-04 (1.67e-04)	Tok/s 39138 (43074)	Loss/tok 2.8756 (3.1698)	LR 1.250e-04
0: TRAIN [4][4950/7762]	Time 0.456 (0.327)	Data 9.80e-05 (1.67e-04)	Tok/s 51351 (43075)	Loss/tok 3.3925 (3.1698)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][4960/7762]	Time 0.365 (0.327)	Data 1.00e-04 (1.67e-04)	Tok/s 46121 (43076)	Loss/tok 3.2485 (3.1698)	LR 1.250e-04
0: TRAIN [4][4970/7762]	Time 0.464 (0.327)	Data 1.00e-04 (1.67e-04)	Tok/s 49717 (43080)	Loss/tok 3.3777 (3.1699)	LR 1.250e-04
0: TRAIN [4][4980/7762]	Time 0.254 (0.327)	Data 1.18e-04 (1.67e-04)	Tok/s 40725 (43083)	Loss/tok 2.8841 (3.1701)	LR 1.250e-04
0: TRAIN [4][4990/7762]	Time 0.366 (0.327)	Data 1.03e-04 (1.67e-04)	Tok/s 45873 (43082)	Loss/tok 3.0710 (3.1699)	LR 1.250e-04
0: TRAIN [4][5000/7762]	Time 0.259 (0.327)	Data 1.05e-04 (1.67e-04)	Tok/s 41003 (43085)	Loss/tok 2.9361 (3.1700)	LR 1.250e-04
0: TRAIN [4][5010/7762]	Time 0.365 (0.327)	Data 9.94e-05 (1.67e-04)	Tok/s 45228 (43083)	Loss/tok 3.1483 (3.1700)	LR 1.250e-04
0: TRAIN [4][5020/7762]	Time 0.354 (0.327)	Data 1.03e-04 (1.66e-04)	Tok/s 47374 (43090)	Loss/tok 3.2383 (3.1700)	LR 1.250e-04
0: TRAIN [4][5030/7762]	Time 0.253 (0.327)	Data 1.02e-04 (1.66e-04)	Tok/s 41533 (43089)	Loss/tok 2.9174 (3.1701)	LR 1.250e-04
0: TRAIN [4][5040/7762]	Time 0.265 (0.327)	Data 1.05e-04 (1.66e-04)	Tok/s 38383 (43089)	Loss/tok 3.0142 (3.1701)	LR 1.250e-04
0: TRAIN [4][5050/7762]	Time 0.342 (0.327)	Data 1.03e-04 (1.66e-04)	Tok/s 48742 (43091)	Loss/tok 3.2596 (3.1700)	LR 1.250e-04
0: TRAIN [4][5060/7762]	Time 0.258 (0.327)	Data 1.03e-04 (1.66e-04)	Tok/s 40366 (43093)	Loss/tok 2.9792 (3.1700)	LR 1.250e-04
0: TRAIN [4][5070/7762]	Time 0.361 (0.327)	Data 1.01e-04 (1.66e-04)	Tok/s 45391 (43098)	Loss/tok 3.2319 (3.1701)	LR 1.250e-04
0: TRAIN [4][5080/7762]	Time 0.464 (0.327)	Data 1.03e-04 (1.66e-04)	Tok/s 50314 (43099)	Loss/tok 3.3256 (3.1701)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][5090/7762]	Time 0.369 (0.327)	Data 1.00e-04 (1.66e-04)	Tok/s 45750 (43098)	Loss/tok 3.1199 (3.1701)	LR 1.250e-04
0: TRAIN [4][5100/7762]	Time 0.365 (0.327)	Data 9.80e-05 (1.65e-04)	Tok/s 46092 (43099)	Loss/tok 3.1739 (3.1700)	LR 1.250e-04
0: TRAIN [4][5110/7762]	Time 0.261 (0.327)	Data 1.00e-04 (1.65e-04)	Tok/s 39768 (43094)	Loss/tok 2.9392 (3.1699)	LR 1.250e-04
0: TRAIN [4][5120/7762]	Time 0.268 (0.327)	Data 1.03e-04 (1.65e-04)	Tok/s 37897 (43092)	Loss/tok 3.0013 (3.1698)	LR 1.250e-04
0: TRAIN [4][5130/7762]	Time 0.175 (0.327)	Data 1.00e-04 (1.65e-04)	Tok/s 30032 (43086)	Loss/tok 2.6140 (3.1696)	LR 1.250e-04
0: TRAIN [4][5140/7762]	Time 0.462 (0.327)	Data 9.92e-05 (1.65e-04)	Tok/s 50319 (43084)	Loss/tok 3.4426 (3.1695)	LR 1.250e-04
0: TRAIN [4][5150/7762]	Time 0.463 (0.327)	Data 1.03e-04 (1.65e-04)	Tok/s 50303 (43092)	Loss/tok 3.2933 (3.1696)	LR 1.250e-04
0: TRAIN [4][5160/7762]	Time 0.462 (0.327)	Data 1.02e-04 (1.65e-04)	Tok/s 50268 (43094)	Loss/tok 3.4566 (3.1697)	LR 1.250e-04
0: TRAIN [4][5170/7762]	Time 0.265 (0.327)	Data 9.89e-05 (1.65e-04)	Tok/s 38107 (43095)	Loss/tok 3.0162 (3.1699)	LR 1.250e-04
0: TRAIN [4][5180/7762]	Time 0.259 (0.327)	Data 1.04e-04 (1.65e-04)	Tok/s 40626 (43095)	Loss/tok 3.0762 (3.1698)	LR 1.250e-04
0: TRAIN [4][5190/7762]	Time 0.345 (0.327)	Data 1.00e-04 (1.64e-04)	Tok/s 49180 (43093)	Loss/tok 3.1608 (3.1696)	LR 1.250e-04
0: TRAIN [4][5200/7762]	Time 0.362 (0.327)	Data 1.04e-04 (1.64e-04)	Tok/s 46891 (43091)	Loss/tok 3.1738 (3.1696)	LR 1.250e-04
0: TRAIN [4][5210/7762]	Time 0.255 (0.327)	Data 1.15e-04 (1.64e-04)	Tok/s 40351 (43093)	Loss/tok 2.9155 (3.1698)	LR 1.250e-04
0: TRAIN [4][5220/7762]	Time 0.360 (0.327)	Data 9.68e-05 (1.64e-04)	Tok/s 47021 (43089)	Loss/tok 3.1313 (3.1697)	LR 1.250e-04
0: TRAIN [4][5230/7762]	Time 0.443 (0.327)	Data 9.49e-05 (1.64e-04)	Tok/s 52555 (43085)	Loss/tok 3.3320 (3.1696)	LR 1.250e-04
0: TRAIN [4][5240/7762]	Time 0.263 (0.327)	Data 1.02e-04 (1.64e-04)	Tok/s 38991 (43079)	Loss/tok 3.0867 (3.1695)	LR 1.250e-04
0: TRAIN [4][5250/7762]	Time 0.265 (0.327)	Data 1.02e-04 (1.64e-04)	Tok/s 38819 (43083)	Loss/tok 2.8953 (3.1695)	LR 1.250e-04
0: TRAIN [4][5260/7762]	Time 0.258 (0.327)	Data 1.02e-04 (1.64e-04)	Tok/s 39805 (43082)	Loss/tok 3.0370 (3.1694)	LR 1.250e-04
0: TRAIN [4][5270/7762]	Time 0.254 (0.327)	Data 9.82e-05 (1.64e-04)	Tok/s 40411 (43083)	Loss/tok 2.8722 (3.1694)	LR 1.250e-04
0: TRAIN [4][5280/7762]	Time 0.344 (0.327)	Data 9.85e-05 (1.63e-04)	Tok/s 49100 (43084)	Loss/tok 3.1754 (3.1693)	LR 1.250e-04
0: TRAIN [4][5290/7762]	Time 0.264 (0.327)	Data 1.17e-04 (1.63e-04)	Tok/s 39247 (43083)	Loss/tok 2.8631 (3.1693)	LR 1.250e-04
0: TRAIN [4][5300/7762]	Time 0.264 (0.327)	Data 1.04e-04 (1.63e-04)	Tok/s 39143 (43082)	Loss/tok 2.9197 (3.1692)	LR 1.250e-04
0: TRAIN [4][5310/7762]	Time 0.262 (0.327)	Data 9.87e-05 (1.63e-04)	Tok/s 39569 (43078)	Loss/tok 2.8920 (3.1691)	LR 1.250e-04
0: TRAIN [4][5320/7762]	Time 0.270 (0.327)	Data 1.20e-04 (1.63e-04)	Tok/s 38037 (43084)	Loss/tok 2.9559 (3.1693)	LR 1.250e-04
0: TRAIN [4][5330/7762]	Time 0.265 (0.327)	Data 9.99e-05 (1.63e-04)	Tok/s 38873 (43085)	Loss/tok 3.0411 (3.1693)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][5340/7762]	Time 0.457 (0.327)	Data 1.01e-04 (1.63e-04)	Tok/s 51018 (43084)	Loss/tok 3.4465 (3.1692)	LR 1.250e-04
0: TRAIN [4][5350/7762]	Time 0.450 (0.327)	Data 9.78e-05 (1.63e-04)	Tok/s 51502 (43083)	Loss/tok 3.3550 (3.1692)	LR 1.250e-04
0: TRAIN [4][5360/7762]	Time 0.259 (0.327)	Data 1.01e-04 (1.63e-04)	Tok/s 40220 (43086)	Loss/tok 2.9682 (3.1692)	LR 1.250e-04
0: TRAIN [4][5370/7762]	Time 0.463 (0.327)	Data 9.87e-05 (1.62e-04)	Tok/s 50332 (43084)	Loss/tok 3.4118 (3.1692)	LR 1.250e-04
0: TRAIN [4][5380/7762]	Time 0.368 (0.327)	Data 1.03e-04 (1.62e-04)	Tok/s 46032 (43089)	Loss/tok 3.1547 (3.1693)	LR 1.250e-04
0: TRAIN [4][5390/7762]	Time 0.557 (0.327)	Data 9.66e-05 (1.62e-04)	Tok/s 53414 (43090)	Loss/tok 3.5109 (3.1693)	LR 1.250e-04
0: TRAIN [4][5400/7762]	Time 0.591 (0.327)	Data 1.03e-04 (1.62e-04)	Tok/s 49571 (43095)	Loss/tok 3.6009 (3.1695)	LR 1.250e-04
0: TRAIN [4][5410/7762]	Time 0.464 (0.327)	Data 1.02e-04 (1.62e-04)	Tok/s 50250 (43093)	Loss/tok 3.2991 (3.1694)	LR 1.250e-04
0: TRAIN [4][5420/7762]	Time 0.360 (0.327)	Data 1.00e-04 (1.62e-04)	Tok/s 47037 (43097)	Loss/tok 3.1357 (3.1696)	LR 1.250e-04
0: TRAIN [4][5430/7762]	Time 0.266 (0.327)	Data 9.97e-05 (1.62e-04)	Tok/s 37722 (43101)	Loss/tok 2.9920 (3.1698)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][5440/7762]	Time 0.255 (0.327)	Data 1.05e-04 (1.62e-04)	Tok/s 40047 (43098)	Loss/tok 2.9897 (3.1697)	LR 1.250e-04
0: TRAIN [4][5450/7762]	Time 0.462 (0.327)	Data 1.02e-04 (1.62e-04)	Tok/s 50597 (43102)	Loss/tok 3.3149 (3.1698)	LR 1.250e-04
0: TRAIN [4][5460/7762]	Time 0.365 (0.327)	Data 9.58e-05 (1.61e-04)	Tok/s 46107 (43099)	Loss/tok 3.1183 (3.1697)	LR 1.250e-04
0: TRAIN [4][5470/7762]	Time 0.268 (0.327)	Data 1.00e-04 (1.61e-04)	Tok/s 38797 (43096)	Loss/tok 2.9675 (3.1695)	LR 1.250e-04
0: TRAIN [4][5480/7762]	Time 0.178 (0.327)	Data 9.99e-05 (1.61e-04)	Tok/s 28865 (43098)	Loss/tok 2.5708 (3.1695)	LR 1.250e-04
0: TRAIN [4][5490/7762]	Time 0.261 (0.327)	Data 1.01e-04 (1.61e-04)	Tok/s 39180 (43094)	Loss/tok 2.9510 (3.1694)	LR 1.250e-04
0: TRAIN [4][5500/7762]	Time 0.369 (0.327)	Data 1.14e-04 (1.61e-04)	Tok/s 45237 (43095)	Loss/tok 3.1966 (3.1694)	LR 1.250e-04
0: TRAIN [4][5510/7762]	Time 0.457 (0.327)	Data 1.05e-04 (1.61e-04)	Tok/s 51230 (43097)	Loss/tok 3.2664 (3.1694)	LR 1.250e-04
0: TRAIN [4][5520/7762]	Time 0.260 (0.327)	Data 9.89e-05 (1.61e-04)	Tok/s 40403 (43096)	Loss/tok 3.0088 (3.1693)	LR 1.250e-04
0: TRAIN [4][5530/7762]	Time 0.259 (0.327)	Data 1.13e-04 (1.61e-04)	Tok/s 40244 (43096)	Loss/tok 2.9953 (3.1695)	LR 1.250e-04
0: TRAIN [4][5540/7762]	Time 0.457 (0.327)	Data 9.63e-05 (1.61e-04)	Tok/s 50556 (43091)	Loss/tok 3.4281 (3.1693)	LR 1.250e-04
0: TRAIN [4][5550/7762]	Time 0.264 (0.327)	Data 1.03e-04 (1.60e-04)	Tok/s 39232 (43090)	Loss/tok 2.9174 (3.1693)	LR 1.250e-04
0: TRAIN [4][5560/7762]	Time 0.259 (0.327)	Data 1.15e-04 (1.60e-04)	Tok/s 39304 (43087)	Loss/tok 2.8889 (3.1692)	LR 1.250e-04
0: TRAIN [4][5570/7762]	Time 0.359 (0.327)	Data 1.02e-04 (1.60e-04)	Tok/s 46500 (43087)	Loss/tok 3.0784 (3.1690)	LR 1.250e-04
0: TRAIN [4][5580/7762]	Time 0.176 (0.327)	Data 1.04e-04 (1.60e-04)	Tok/s 29536 (43083)	Loss/tok 2.5901 (3.1690)	LR 1.250e-04
0: TRAIN [4][5590/7762]	Time 0.266 (0.327)	Data 1.01e-04 (1.60e-04)	Tok/s 38345 (43088)	Loss/tok 2.9998 (3.1690)	LR 1.250e-04
0: TRAIN [4][5600/7762]	Time 0.263 (0.327)	Data 1.02e-04 (1.60e-04)	Tok/s 39519 (43087)	Loss/tok 2.8816 (3.1690)	LR 1.250e-04
0: TRAIN [4][5610/7762]	Time 0.358 (0.327)	Data 9.70e-05 (1.60e-04)	Tok/s 47358 (43087)	Loss/tok 3.1967 (3.1689)	LR 1.250e-04
0: TRAIN [4][5620/7762]	Time 0.176 (0.327)	Data 9.75e-05 (1.60e-04)	Tok/s 30511 (43087)	Loss/tok 2.6147 (3.1689)	LR 1.250e-04
0: TRAIN [4][5630/7762]	Time 0.458 (0.327)	Data 1.31e-04 (1.60e-04)	Tok/s 51635 (43083)	Loss/tok 3.2945 (3.1688)	LR 1.250e-04
0: TRAIN [4][5640/7762]	Time 0.357 (0.327)	Data 9.70e-05 (1.60e-04)	Tok/s 47315 (43079)	Loss/tok 3.1885 (3.1687)	LR 1.250e-04
0: TRAIN [4][5650/7762]	Time 0.566 (0.327)	Data 9.78e-05 (1.59e-04)	Tok/s 52705 (43080)	Loss/tok 3.5630 (3.1689)	LR 1.250e-04
0: TRAIN [4][5660/7762]	Time 0.177 (0.327)	Data 1.26e-04 (1.59e-04)	Tok/s 29522 (43076)	Loss/tok 2.5188 (3.1688)	LR 1.250e-04
0: TRAIN [4][5670/7762]	Time 0.452 (0.327)	Data 1.09e-04 (1.59e-04)	Tok/s 51825 (43077)	Loss/tok 3.2372 (3.1688)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][5680/7762]	Time 0.354 (0.327)	Data 1.07e-04 (1.59e-04)	Tok/s 47210 (43080)	Loss/tok 3.0489 (3.1689)	LR 1.250e-04
0: TRAIN [4][5690/7762]	Time 0.456 (0.327)	Data 1.17e-04 (1.59e-04)	Tok/s 50527 (43077)	Loss/tok 3.4107 (3.1688)	LR 1.250e-04
0: TRAIN [4][5700/7762]	Time 0.464 (0.327)	Data 1.12e-04 (1.59e-04)	Tok/s 50381 (43077)	Loss/tok 3.3101 (3.1687)	LR 1.250e-04
0: TRAIN [4][5710/7762]	Time 0.362 (0.327)	Data 1.07e-04 (1.59e-04)	Tok/s 46438 (43080)	Loss/tok 3.1848 (3.1687)	LR 1.250e-04
0: TRAIN [4][5720/7762]	Time 0.262 (0.327)	Data 1.17e-04 (1.59e-04)	Tok/s 39194 (43083)	Loss/tok 2.8779 (3.1688)	LR 1.250e-04
0: TRAIN [4][5730/7762]	Time 0.264 (0.327)	Data 1.19e-04 (1.59e-04)	Tok/s 37880 (43077)	Loss/tok 3.0036 (3.1688)	LR 1.250e-04
0: TRAIN [4][5740/7762]	Time 0.261 (0.327)	Data 1.01e-04 (1.59e-04)	Tok/s 38872 (43072)	Loss/tok 3.0055 (3.1686)	LR 1.250e-04
0: TRAIN [4][5750/7762]	Time 0.261 (0.327)	Data 9.82e-05 (1.59e-04)	Tok/s 39850 (43070)	Loss/tok 2.8824 (3.1685)	LR 1.250e-04
0: TRAIN [4][5760/7762]	Time 0.584 (0.327)	Data 1.17e-04 (1.59e-04)	Tok/s 51506 (43067)	Loss/tok 3.4572 (3.1684)	LR 1.250e-04
0: TRAIN [4][5770/7762]	Time 0.362 (0.327)	Data 1.10e-04 (1.58e-04)	Tok/s 47497 (43070)	Loss/tok 3.0527 (3.1686)	LR 1.250e-04
0: TRAIN [4][5780/7762]	Time 0.351 (0.327)	Data 1.01e-04 (1.58e-04)	Tok/s 47687 (43073)	Loss/tok 3.2285 (3.1685)	LR 1.250e-04
0: TRAIN [4][5790/7762]	Time 0.451 (0.327)	Data 1.18e-04 (1.58e-04)	Tok/s 51559 (43075)	Loss/tok 3.2448 (3.1686)	LR 1.250e-04
0: TRAIN [4][5800/7762]	Time 0.364 (0.327)	Data 1.03e-04 (1.58e-04)	Tok/s 46431 (43081)	Loss/tok 3.2242 (3.1688)	LR 1.250e-04
0: TRAIN [4][5810/7762]	Time 0.346 (0.327)	Data 9.61e-05 (1.58e-04)	Tok/s 48903 (43078)	Loss/tok 3.3061 (3.1687)	LR 1.250e-04
0: TRAIN [4][5820/7762]	Time 0.266 (0.327)	Data 1.03e-04 (1.58e-04)	Tok/s 39039 (43081)	Loss/tok 3.0307 (3.1688)	LR 1.250e-04
0: TRAIN [4][5830/7762]	Time 0.266 (0.327)	Data 9.85e-05 (1.58e-04)	Tok/s 38767 (43076)	Loss/tok 3.0310 (3.1686)	LR 1.250e-04
0: TRAIN [4][5840/7762]	Time 0.263 (0.327)	Data 1.01e-04 (1.58e-04)	Tok/s 38602 (43076)	Loss/tok 2.8960 (3.1686)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][5850/7762]	Time 0.180 (0.327)	Data 9.99e-05 (1.58e-04)	Tok/s 29468 (43074)	Loss/tok 2.6053 (3.1687)	LR 1.250e-04
0: TRAIN [4][5860/7762]	Time 0.357 (0.327)	Data 9.82e-05 (1.58e-04)	Tok/s 46956 (43077)	Loss/tok 3.1701 (3.1688)	LR 1.250e-04
0: TRAIN [4][5870/7762]	Time 0.585 (0.327)	Data 1.03e-04 (1.58e-04)	Tok/s 50884 (43081)	Loss/tok 3.5485 (3.1691)	LR 1.250e-04
0: TRAIN [4][5880/7762]	Time 0.259 (0.327)	Data 1.02e-04 (1.57e-04)	Tok/s 39625 (43079)	Loss/tok 2.9759 (3.1691)	LR 1.250e-04
0: TRAIN [4][5890/7762]	Time 0.362 (0.327)	Data 2.53e-04 (1.57e-04)	Tok/s 46262 (43082)	Loss/tok 3.1381 (3.1690)	LR 1.250e-04
0: TRAIN [4][5900/7762]	Time 0.260 (0.327)	Data 1.04e-04 (1.57e-04)	Tok/s 39860 (43080)	Loss/tok 2.9912 (3.1690)	LR 1.250e-04
0: TRAIN [4][5910/7762]	Time 0.588 (0.327)	Data 9.97e-05 (1.57e-04)	Tok/s 50166 (43078)	Loss/tok 3.4540 (3.1690)	LR 1.250e-04
0: TRAIN [4][5920/7762]	Time 0.367 (0.327)	Data 9.73e-05 (1.57e-04)	Tok/s 45747 (43073)	Loss/tok 3.1543 (3.1688)	LR 1.250e-04
0: TRAIN [4][5930/7762]	Time 0.263 (0.327)	Data 1.01e-04 (1.57e-04)	Tok/s 39176 (43080)	Loss/tok 2.9600 (3.1690)	LR 1.250e-04
0: TRAIN [4][5940/7762]	Time 0.360 (0.327)	Data 9.89e-05 (1.57e-04)	Tok/s 46283 (43084)	Loss/tok 3.1702 (3.1692)	LR 1.250e-04
0: TRAIN [4][5950/7762]	Time 0.357 (0.327)	Data 9.99e-05 (1.57e-04)	Tok/s 46967 (43084)	Loss/tok 3.1846 (3.1691)	LR 1.250e-04
0: TRAIN [4][5960/7762]	Time 0.351 (0.327)	Data 1.01e-04 (1.57e-04)	Tok/s 47834 (43081)	Loss/tok 3.1859 (3.1690)	LR 1.250e-04
0: TRAIN [4][5970/7762]	Time 0.172 (0.327)	Data 1.18e-04 (1.57e-04)	Tok/s 30371 (43082)	Loss/tok 2.6902 (3.1692)	LR 1.250e-04
0: TRAIN [4][5980/7762]	Time 0.264 (0.327)	Data 9.51e-05 (1.57e-04)	Tok/s 38912 (43078)	Loss/tok 3.0252 (3.1692)	LR 1.250e-04
0: TRAIN [4][5990/7762]	Time 0.264 (0.327)	Data 9.94e-05 (1.56e-04)	Tok/s 39316 (43083)	Loss/tok 2.9935 (3.1692)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][6000/7762]	Time 0.355 (0.327)	Data 1.08e-04 (1.56e-04)	Tok/s 47113 (43082)	Loss/tok 3.1937 (3.1692)	LR 1.250e-04
0: TRAIN [4][6010/7762]	Time 0.361 (0.327)	Data 9.80e-05 (1.56e-04)	Tok/s 46349 (43076)	Loss/tok 3.0973 (3.1690)	LR 1.250e-04
0: TRAIN [4][6020/7762]	Time 0.263 (0.327)	Data 1.13e-04 (1.56e-04)	Tok/s 39210 (43076)	Loss/tok 2.8402 (3.1690)	LR 1.250e-04
0: TRAIN [4][6030/7762]	Time 0.358 (0.327)	Data 1.18e-04 (1.56e-04)	Tok/s 46853 (43071)	Loss/tok 3.2189 (3.1689)	LR 1.250e-04
0: TRAIN [4][6040/7762]	Time 0.367 (0.327)	Data 1.10e-04 (1.56e-04)	Tok/s 45804 (43077)	Loss/tok 3.2625 (3.1689)	LR 1.250e-04
0: TRAIN [4][6050/7762]	Time 0.263 (0.327)	Data 9.70e-05 (1.56e-04)	Tok/s 38657 (43071)	Loss/tok 2.9719 (3.1688)	LR 1.250e-04
0: TRAIN [4][6060/7762]	Time 0.368 (0.327)	Data 1.03e-04 (1.56e-04)	Tok/s 45816 (43080)	Loss/tok 3.1569 (3.1690)	LR 1.250e-04
0: TRAIN [4][6070/7762]	Time 0.267 (0.327)	Data 1.18e-04 (1.56e-04)	Tok/s 38696 (43078)	Loss/tok 2.9033 (3.1690)	LR 1.250e-04
0: TRAIN [4][6080/7762]	Time 0.260 (0.327)	Data 1.02e-04 (1.56e-04)	Tok/s 39031 (43077)	Loss/tok 2.9363 (3.1690)	LR 1.250e-04
0: TRAIN [4][6090/7762]	Time 0.460 (0.327)	Data 1.05e-04 (1.56e-04)	Tok/s 50223 (43079)	Loss/tok 3.4632 (3.1690)	LR 1.250e-04
0: TRAIN [4][6100/7762]	Time 0.256 (0.327)	Data 1.02e-04 (1.56e-04)	Tok/s 39607 (43078)	Loss/tok 2.9280 (3.1690)	LR 1.250e-04
0: TRAIN [4][6110/7762]	Time 0.364 (0.327)	Data 9.89e-05 (1.55e-04)	Tok/s 45762 (43081)	Loss/tok 3.2189 (3.1690)	LR 1.250e-04
0: TRAIN [4][6120/7762]	Time 0.454 (0.327)	Data 1.09e-04 (1.55e-04)	Tok/s 51877 (43081)	Loss/tok 3.2068 (3.1689)	LR 1.250e-04
0: TRAIN [4][6130/7762]	Time 0.260 (0.327)	Data 1.05e-04 (1.55e-04)	Tok/s 38807 (43080)	Loss/tok 2.9617 (3.1688)	LR 1.250e-04
0: TRAIN [4][6140/7762]	Time 0.366 (0.327)	Data 1.24e-04 (1.55e-04)	Tok/s 46629 (43077)	Loss/tok 3.1423 (3.1686)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][6150/7762]	Time 0.578 (0.327)	Data 9.89e-05 (1.55e-04)	Tok/s 51357 (43075)	Loss/tok 3.5542 (3.1686)	LR 1.250e-04
0: TRAIN [4][6160/7762]	Time 0.366 (0.327)	Data 9.61e-05 (1.55e-04)	Tok/s 45025 (43080)	Loss/tok 3.2019 (3.1689)	LR 1.250e-04
0: TRAIN [4][6170/7762]	Time 0.361 (0.327)	Data 9.87e-05 (1.55e-04)	Tok/s 46471 (43079)	Loss/tok 3.1623 (3.1688)	LR 1.250e-04
0: TRAIN [4][6180/7762]	Time 0.265 (0.327)	Data 9.73e-05 (1.55e-04)	Tok/s 38892 (43074)	Loss/tok 2.9232 (3.1686)	LR 1.250e-04
0: TRAIN [4][6190/7762]	Time 0.462 (0.327)	Data 1.20e-04 (1.55e-04)	Tok/s 49518 (43075)	Loss/tok 3.4067 (3.1687)	LR 1.250e-04
0: TRAIN [4][6200/7762]	Time 0.264 (0.327)	Data 1.18e-04 (1.55e-04)	Tok/s 39545 (43072)	Loss/tok 2.8934 (3.1686)	LR 1.250e-04
0: TRAIN [4][6210/7762]	Time 0.258 (0.327)	Data 1.02e-04 (1.55e-04)	Tok/s 39384 (43076)	Loss/tok 3.0715 (3.1686)	LR 1.250e-04
0: TRAIN [4][6220/7762]	Time 0.452 (0.327)	Data 1.02e-04 (1.55e-04)	Tok/s 51710 (43078)	Loss/tok 3.3297 (3.1687)	LR 1.250e-04
0: TRAIN [4][6230/7762]	Time 0.266 (0.327)	Data 1.02e-04 (1.55e-04)	Tok/s 39214 (43077)	Loss/tok 2.9635 (3.1685)	LR 1.250e-04
0: TRAIN [4][6240/7762]	Time 0.353 (0.327)	Data 1.02e-04 (1.54e-04)	Tok/s 47892 (43079)	Loss/tok 3.1651 (3.1685)	LR 1.250e-04
0: TRAIN [4][6250/7762]	Time 0.265 (0.327)	Data 9.97e-05 (1.54e-04)	Tok/s 38623 (43077)	Loss/tok 3.0755 (3.1685)	LR 1.250e-04
0: TRAIN [4][6260/7762]	Time 0.458 (0.327)	Data 1.02e-04 (1.54e-04)	Tok/s 51312 (43077)	Loss/tok 3.2701 (3.1685)	LR 1.250e-04
0: TRAIN [4][6270/7762]	Time 0.265 (0.327)	Data 9.73e-05 (1.54e-04)	Tok/s 38935 (43076)	Loss/tok 2.9128 (3.1684)	LR 1.250e-04
0: TRAIN [4][6280/7762]	Time 0.366 (0.327)	Data 9.80e-05 (1.54e-04)	Tok/s 46057 (43071)	Loss/tok 3.2084 (3.1682)	LR 1.250e-04
0: TRAIN [4][6290/7762]	Time 0.265 (0.327)	Data 1.03e-04 (1.54e-04)	Tok/s 39535 (43074)	Loss/tok 2.9063 (3.1683)	LR 1.250e-04
0: TRAIN [4][6300/7762]	Time 0.461 (0.327)	Data 1.03e-04 (1.54e-04)	Tok/s 50019 (43078)	Loss/tok 3.3367 (3.1684)	LR 1.250e-04
0: TRAIN [4][6310/7762]	Time 0.268 (0.327)	Data 1.17e-04 (1.54e-04)	Tok/s 38901 (43079)	Loss/tok 2.9895 (3.1683)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][6320/7762]	Time 0.587 (0.327)	Data 1.01e-04 (1.54e-04)	Tok/s 51313 (43079)	Loss/tok 3.5122 (3.1684)	LR 1.250e-04
0: TRAIN [4][6330/7762]	Time 0.262 (0.327)	Data 1.15e-04 (1.54e-04)	Tok/s 38916 (43077)	Loss/tok 3.0227 (3.1684)	LR 1.250e-04
0: TRAIN [4][6340/7762]	Time 0.260 (0.327)	Data 1.06e-04 (1.54e-04)	Tok/s 38689 (43074)	Loss/tok 3.0389 (3.1685)	LR 1.250e-04
0: TRAIN [4][6350/7762]	Time 0.266 (0.327)	Data 1.00e-04 (1.54e-04)	Tok/s 38836 (43077)	Loss/tok 3.0223 (3.1686)	LR 1.250e-04
0: TRAIN [4][6360/7762]	Time 0.270 (0.327)	Data 1.16e-04 (1.54e-04)	Tok/s 37632 (43078)	Loss/tok 3.0350 (3.1686)	LR 1.250e-04
0: TRAIN [4][6370/7762]	Time 0.266 (0.327)	Data 1.02e-04 (1.54e-04)	Tok/s 38355 (43076)	Loss/tok 2.8996 (3.1686)	LR 1.250e-04
0: TRAIN [4][6380/7762]	Time 0.265 (0.327)	Data 1.17e-04 (1.53e-04)	Tok/s 39009 (43078)	Loss/tok 2.9845 (3.1686)	LR 1.250e-04
0: TRAIN [4][6390/7762]	Time 0.264 (0.327)	Data 1.04e-04 (1.53e-04)	Tok/s 39488 (43073)	Loss/tok 3.0074 (3.1684)	LR 1.250e-04
0: TRAIN [4][6400/7762]	Time 0.260 (0.327)	Data 1.08e-04 (1.53e-04)	Tok/s 39665 (43071)	Loss/tok 2.9615 (3.1684)	LR 1.250e-04
0: TRAIN [4][6410/7762]	Time 0.363 (0.327)	Data 9.99e-05 (1.53e-04)	Tok/s 46582 (43066)	Loss/tok 3.1791 (3.1682)	LR 1.250e-04
0: TRAIN [4][6420/7762]	Time 0.268 (0.327)	Data 9.68e-05 (1.53e-04)	Tok/s 39648 (43064)	Loss/tok 3.0102 (3.1683)	LR 1.250e-04
0: TRAIN [4][6430/7762]	Time 0.252 (0.327)	Data 1.63e-04 (1.53e-04)	Tok/s 41026 (43062)	Loss/tok 3.0418 (3.1683)	LR 1.250e-04
0: TRAIN [4][6440/7762]	Time 0.355 (0.327)	Data 9.89e-05 (1.53e-04)	Tok/s 47025 (43066)	Loss/tok 3.1704 (3.1683)	LR 1.250e-04
0: TRAIN [4][6450/7762]	Time 0.363 (0.327)	Data 1.22e-04 (1.53e-04)	Tok/s 46079 (43068)	Loss/tok 3.2084 (3.1682)	LR 1.250e-04
0: TRAIN [4][6460/7762]	Time 0.574 (0.327)	Data 1.00e-04 (1.53e-04)	Tok/s 51006 (43071)	Loss/tok 3.5406 (3.1683)	LR 1.250e-04
0: TRAIN [4][6470/7762]	Time 0.265 (0.327)	Data 1.20e-04 (1.53e-04)	Tok/s 38502 (43069)	Loss/tok 2.8911 (3.1684)	LR 1.250e-04
0: TRAIN [4][6480/7762]	Time 0.259 (0.327)	Data 1.15e-04 (1.53e-04)	Tok/s 39517 (43070)	Loss/tok 2.9437 (3.1683)	LR 1.250e-04
0: TRAIN [4][6490/7762]	Time 0.467 (0.327)	Data 9.94e-05 (1.53e-04)	Tok/s 50229 (43070)	Loss/tok 3.3331 (3.1684)	LR 1.250e-04
0: TRAIN [4][6500/7762]	Time 0.467 (0.327)	Data 1.02e-04 (1.53e-04)	Tok/s 50164 (43070)	Loss/tok 3.4029 (3.1685)	LR 1.250e-04
0: TRAIN [4][6510/7762]	Time 0.355 (0.327)	Data 1.02e-04 (1.52e-04)	Tok/s 47666 (43073)	Loss/tok 3.0752 (3.1685)	LR 1.250e-04
0: TRAIN [4][6520/7762]	Time 0.267 (0.327)	Data 1.03e-04 (1.52e-04)	Tok/s 38440 (43073)	Loss/tok 3.0239 (3.1687)	LR 1.250e-04
0: TRAIN [4][6530/7762]	Time 0.261 (0.327)	Data 1.05e-04 (1.52e-04)	Tok/s 39068 (43073)	Loss/tok 3.0392 (3.1687)	LR 1.250e-04
0: TRAIN [4][6540/7762]	Time 0.174 (0.327)	Data 1.18e-04 (1.52e-04)	Tok/s 29668 (43070)	Loss/tok 2.5213 (3.1686)	LR 1.250e-04
0: TRAIN [4][6550/7762]	Time 0.354 (0.327)	Data 9.94e-05 (1.52e-04)	Tok/s 47835 (43072)	Loss/tok 3.2209 (3.1686)	LR 1.250e-04
0: TRAIN [4][6560/7762]	Time 0.266 (0.327)	Data 1.01e-04 (1.52e-04)	Tok/s 38899 (43070)	Loss/tok 3.0842 (3.1685)	LR 1.250e-04
0: TRAIN [4][6570/7762]	Time 0.268 (0.327)	Data 1.01e-04 (1.52e-04)	Tok/s 38285 (43066)	Loss/tok 3.0030 (3.1685)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][6580/7762]	Time 0.465 (0.327)	Data 9.99e-05 (1.52e-04)	Tok/s 50709 (43069)	Loss/tok 3.2661 (3.1685)	LR 1.250e-04
0: TRAIN [4][6590/7762]	Time 0.435 (0.327)	Data 1.21e-04 (1.52e-04)	Tok/s 53376 (43069)	Loss/tok 3.3798 (3.1686)	LR 1.250e-04
0: TRAIN [4][6600/7762]	Time 0.464 (0.327)	Data 1.32e-04 (1.52e-04)	Tok/s 49877 (43070)	Loss/tok 3.2208 (3.1686)	LR 1.250e-04
0: TRAIN [4][6610/7762]	Time 0.262 (0.327)	Data 9.85e-05 (1.52e-04)	Tok/s 39489 (43069)	Loss/tok 2.9535 (3.1685)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][6620/7762]	Time 0.359 (0.327)	Data 1.10e-04 (1.52e-04)	Tok/s 46521 (43070)	Loss/tok 3.1667 (3.1685)	LR 1.250e-04
0: TRAIN [4][6630/7762]	Time 0.264 (0.327)	Data 1.16e-04 (1.52e-04)	Tok/s 38536 (43061)	Loss/tok 3.1375 (3.1683)	LR 1.250e-04
0: TRAIN [4][6640/7762]	Time 0.260 (0.327)	Data 1.01e-04 (1.52e-04)	Tok/s 38863 (43058)	Loss/tok 3.0777 (3.1683)	LR 1.250e-04
0: TRAIN [4][6650/7762]	Time 0.263 (0.327)	Data 1.06e-04 (1.52e-04)	Tok/s 38672 (43062)	Loss/tok 2.9835 (3.1683)	LR 1.250e-04
0: TRAIN [4][6660/7762]	Time 0.268 (0.327)	Data 1.01e-04 (1.51e-04)	Tok/s 38367 (43065)	Loss/tok 3.0261 (3.1683)	LR 1.250e-04
0: TRAIN [4][6670/7762]	Time 0.352 (0.327)	Data 9.80e-05 (1.51e-04)	Tok/s 47456 (43065)	Loss/tok 3.1386 (3.1683)	LR 1.250e-04
0: TRAIN [4][6680/7762]	Time 0.265 (0.327)	Data 9.85e-05 (1.51e-04)	Tok/s 39244 (43064)	Loss/tok 3.0614 (3.1684)	LR 1.250e-04
0: TRAIN [4][6690/7762]	Time 0.364 (0.327)	Data 1.18e-04 (1.51e-04)	Tok/s 45820 (43065)	Loss/tok 3.2587 (3.1684)	LR 1.250e-04
0: TRAIN [4][6700/7762]	Time 0.354 (0.327)	Data 9.80e-05 (1.51e-04)	Tok/s 46936 (43065)	Loss/tok 3.1796 (3.1684)	LR 1.250e-04
0: TRAIN [4][6710/7762]	Time 0.353 (0.327)	Data 1.03e-04 (1.51e-04)	Tok/s 47198 (43068)	Loss/tok 3.1841 (3.1685)	LR 1.250e-04
0: TRAIN [4][6720/7762]	Time 0.174 (0.327)	Data 1.03e-04 (1.51e-04)	Tok/s 30719 (43066)	Loss/tok 2.6298 (3.1685)	LR 1.250e-04
0: TRAIN [4][6730/7762]	Time 0.584 (0.327)	Data 1.13e-04 (1.51e-04)	Tok/s 50616 (43061)	Loss/tok 3.4361 (3.1684)	LR 1.250e-04
0: TRAIN [4][6740/7762]	Time 0.462 (0.327)	Data 9.97e-05 (1.51e-04)	Tok/s 50632 (43059)	Loss/tok 3.2103 (3.1683)	LR 1.250e-04
0: TRAIN [4][6750/7762]	Time 0.455 (0.327)	Data 1.02e-04 (1.51e-04)	Tok/s 51439 (43057)	Loss/tok 3.4469 (3.1685)	LR 1.250e-04
0: TRAIN [4][6760/7762]	Time 0.362 (0.327)	Data 9.94e-05 (1.51e-04)	Tok/s 46884 (43052)	Loss/tok 3.2632 (3.1684)	LR 1.250e-04
0: TRAIN [4][6770/7762]	Time 0.175 (0.326)	Data 1.12e-04 (1.51e-04)	Tok/s 29950 (43046)	Loss/tok 2.5366 (3.1683)	LR 1.250e-04
0: TRAIN [4][6780/7762]	Time 0.355 (0.327)	Data 1.18e-04 (1.51e-04)	Tok/s 47221 (43051)	Loss/tok 3.0590 (3.1684)	LR 1.250e-04
0: TRAIN [4][6790/7762]	Time 0.586 (0.327)	Data 1.03e-04 (1.51e-04)	Tok/s 50716 (43051)	Loss/tok 3.6083 (3.1685)	LR 1.250e-04
0: TRAIN [4][6800/7762]	Time 0.463 (0.327)	Data 1.04e-04 (1.50e-04)	Tok/s 50195 (43050)	Loss/tok 3.2738 (3.1685)	LR 1.250e-04
0: TRAIN [4][6810/7762]	Time 0.260 (0.327)	Data 9.51e-05 (1.50e-04)	Tok/s 40782 (43048)	Loss/tok 2.9017 (3.1683)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][6820/7762]	Time 0.256 (0.326)	Data 1.04e-04 (1.50e-04)	Tok/s 41396 (43045)	Loss/tok 2.9528 (3.1684)	LR 1.250e-04
0: TRAIN [4][6830/7762]	Time 0.463 (0.327)	Data 1.02e-04 (1.50e-04)	Tok/s 49937 (43051)	Loss/tok 3.2344 (3.1684)	LR 1.250e-04
0: TRAIN [4][6840/7762]	Time 0.267 (0.327)	Data 1.08e-04 (1.50e-04)	Tok/s 39636 (43052)	Loss/tok 2.8862 (3.1685)	LR 1.250e-04
0: TRAIN [4][6850/7762]	Time 0.345 (0.327)	Data 1.19e-04 (1.50e-04)	Tok/s 48289 (43058)	Loss/tok 3.1313 (3.1687)	LR 1.250e-04
0: TRAIN [4][6860/7762]	Time 0.453 (0.327)	Data 1.05e-04 (1.50e-04)	Tok/s 52076 (43055)	Loss/tok 3.3727 (3.1687)	LR 1.250e-04
0: TRAIN [4][6870/7762]	Time 0.362 (0.327)	Data 9.85e-05 (1.50e-04)	Tok/s 46191 (43056)	Loss/tok 3.2000 (3.1687)	LR 1.250e-04
0: TRAIN [4][6880/7762]	Time 0.353 (0.327)	Data 1.04e-04 (1.50e-04)	Tok/s 47077 (43057)	Loss/tok 3.2861 (3.1687)	LR 1.250e-04
0: TRAIN [4][6890/7762]	Time 0.456 (0.327)	Data 1.00e-04 (1.50e-04)	Tok/s 50343 (43057)	Loss/tok 3.4053 (3.1687)	LR 1.250e-04
0: TRAIN [4][6900/7762]	Time 0.255 (0.327)	Data 9.75e-05 (1.50e-04)	Tok/s 40423 (43057)	Loss/tok 3.0009 (3.1687)	LR 1.250e-04
0: TRAIN [4][6910/7762]	Time 0.353 (0.327)	Data 1.02e-04 (1.50e-04)	Tok/s 47531 (43056)	Loss/tok 3.1752 (3.1687)	LR 1.250e-04
0: TRAIN [4][6920/7762]	Time 0.265 (0.327)	Data 9.97e-05 (1.50e-04)	Tok/s 38423 (43050)	Loss/tok 2.9070 (3.1685)	LR 1.250e-04
0: TRAIN [4][6930/7762]	Time 0.255 (0.327)	Data 9.63e-05 (1.50e-04)	Tok/s 40719 (43046)	Loss/tok 2.9606 (3.1683)	LR 1.250e-04
0: TRAIN [4][6940/7762]	Time 0.351 (0.326)	Data 9.61e-05 (1.50e-04)	Tok/s 47162 (43044)	Loss/tok 3.2047 (3.1683)	LR 1.250e-04
0: TRAIN [4][6950/7762]	Time 0.265 (0.326)	Data 9.63e-05 (1.49e-04)	Tok/s 39226 (43040)	Loss/tok 2.9647 (3.1683)	LR 1.250e-04
0: TRAIN [4][6960/7762]	Time 0.366 (0.326)	Data 1.03e-04 (1.49e-04)	Tok/s 46510 (43042)	Loss/tok 3.1661 (3.1682)	LR 1.250e-04
0: TRAIN [4][6970/7762]	Time 0.268 (0.326)	Data 1.06e-04 (1.49e-04)	Tok/s 38581 (43039)	Loss/tok 2.9814 (3.1682)	LR 1.250e-04
0: TRAIN [4][6980/7762]	Time 0.367 (0.326)	Data 1.02e-04 (1.49e-04)	Tok/s 46320 (43042)	Loss/tok 3.2315 (3.1683)	LR 1.250e-04
0: TRAIN [4][6990/7762]	Time 0.257 (0.326)	Data 1.01e-04 (1.49e-04)	Tok/s 39778 (43043)	Loss/tok 2.9856 (3.1683)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][7000/7762]	Time 0.584 (0.327)	Data 1.04e-04 (1.49e-04)	Tok/s 50880 (43045)	Loss/tok 3.5563 (3.1684)	LR 1.250e-04
0: TRAIN [4][7010/7762]	Time 0.266 (0.326)	Data 9.87e-05 (1.49e-04)	Tok/s 38497 (43042)	Loss/tok 2.9861 (3.1683)	LR 1.250e-04
0: TRAIN [4][7020/7762]	Time 0.263 (0.327)	Data 9.94e-05 (1.49e-04)	Tok/s 39250 (43042)	Loss/tok 2.9431 (3.1684)	LR 1.250e-04
0: TRAIN [4][7030/7762]	Time 0.265 (0.326)	Data 1.15e-04 (1.49e-04)	Tok/s 39570 (43041)	Loss/tok 2.9221 (3.1684)	LR 1.250e-04
0: TRAIN [4][7040/7762]	Time 0.360 (0.327)	Data 1.02e-04 (1.49e-04)	Tok/s 46598 (43042)	Loss/tok 3.1629 (3.1684)	LR 1.250e-04
0: TRAIN [4][7050/7762]	Time 0.260 (0.326)	Data 1.02e-04 (1.49e-04)	Tok/s 40756 (43042)	Loss/tok 2.9971 (3.1683)	LR 1.250e-04
0: TRAIN [4][7060/7762]	Time 0.461 (0.326)	Data 1.03e-04 (1.49e-04)	Tok/s 50522 (43043)	Loss/tok 3.2727 (3.1683)	LR 1.250e-04
0: TRAIN [4][7070/7762]	Time 0.369 (0.326)	Data 1.18e-04 (1.49e-04)	Tok/s 45800 (43044)	Loss/tok 3.1422 (3.1683)	LR 1.250e-04
0: TRAIN [4][7080/7762]	Time 0.266 (0.326)	Data 1.05e-04 (1.49e-04)	Tok/s 39759 (43045)	Loss/tok 3.0778 (3.1683)	LR 1.250e-04
0: TRAIN [4][7090/7762]	Time 0.357 (0.327)	Data 1.13e-04 (1.49e-04)	Tok/s 46756 (43050)	Loss/tok 3.1160 (3.1684)	LR 1.250e-04
0: TRAIN [4][7100/7762]	Time 0.265 (0.327)	Data 9.92e-05 (1.49e-04)	Tok/s 39177 (43051)	Loss/tok 2.9164 (3.1684)	LR 1.250e-04
0: TRAIN [4][7110/7762]	Time 0.267 (0.327)	Data 9.80e-05 (1.48e-04)	Tok/s 38455 (43048)	Loss/tok 3.0194 (3.1682)	LR 1.250e-04
0: TRAIN [4][7120/7762]	Time 0.365 (0.326)	Data 9.94e-05 (1.48e-04)	Tok/s 46076 (43048)	Loss/tok 3.2545 (3.1681)	LR 1.250e-04
0: TRAIN [4][7130/7762]	Time 0.256 (0.327)	Data 1.08e-04 (1.48e-04)	Tok/s 39633 (43049)	Loss/tok 2.9638 (3.1682)	LR 1.250e-04
0: TRAIN [4][7140/7762]	Time 0.263 (0.327)	Data 9.89e-05 (1.48e-04)	Tok/s 38703 (43051)	Loss/tok 2.9808 (3.1682)	LR 1.250e-04
0: TRAIN [4][7150/7762]	Time 0.460 (0.327)	Data 9.70e-05 (1.48e-04)	Tok/s 50237 (43052)	Loss/tok 3.3744 (3.1683)	LR 1.250e-04
0: TRAIN [4][7160/7762]	Time 0.267 (0.327)	Data 1.17e-04 (1.48e-04)	Tok/s 38282 (43051)	Loss/tok 2.9473 (3.1682)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][7170/7762]	Time 0.365 (0.327)	Data 9.85e-05 (1.48e-04)	Tok/s 46180 (43055)	Loss/tok 3.1164 (3.1685)	LR 1.250e-04
0: TRAIN [4][7180/7762]	Time 0.343 (0.327)	Data 1.01e-04 (1.48e-04)	Tok/s 48714 (43054)	Loss/tok 3.2542 (3.1684)	LR 1.250e-04
0: TRAIN [4][7190/7762]	Time 0.339 (0.327)	Data 2.01e-04 (1.48e-04)	Tok/s 49557 (43054)	Loss/tok 3.1412 (3.1684)	LR 1.250e-04
0: TRAIN [4][7200/7762]	Time 0.266 (0.327)	Data 1.00e-04 (1.48e-04)	Tok/s 39154 (43052)	Loss/tok 2.9373 (3.1684)	LR 1.250e-04
0: TRAIN [4][7210/7762]	Time 0.356 (0.327)	Data 1.03e-04 (1.48e-04)	Tok/s 47313 (43052)	Loss/tok 3.1970 (3.1683)	LR 1.250e-04
0: TRAIN [4][7220/7762]	Time 0.267 (0.327)	Data 9.82e-05 (1.48e-04)	Tok/s 38741 (43053)	Loss/tok 3.0600 (3.1683)	LR 1.250e-04
0: TRAIN [4][7230/7762]	Time 0.258 (0.327)	Data 1.02e-04 (1.48e-04)	Tok/s 40620 (43055)	Loss/tok 2.9190 (3.1683)	LR 1.250e-04
0: TRAIN [4][7240/7762]	Time 0.258 (0.327)	Data 1.04e-04 (1.48e-04)	Tok/s 39702 (43060)	Loss/tok 2.9732 (3.1684)	LR 1.250e-04
0: TRAIN [4][7250/7762]	Time 0.362 (0.327)	Data 1.08e-04 (1.48e-04)	Tok/s 46501 (43061)	Loss/tok 3.0747 (3.1684)	LR 1.250e-04
0: TRAIN [4][7260/7762]	Time 0.178 (0.327)	Data 9.94e-05 (1.48e-04)	Tok/s 29377 (43055)	Loss/tok 2.6917 (3.1683)	LR 1.250e-04
0: TRAIN [4][7270/7762]	Time 0.264 (0.327)	Data 9.58e-05 (1.48e-04)	Tok/s 39723 (43054)	Loss/tok 2.9870 (3.1682)	LR 1.250e-04
0: TRAIN [4][7280/7762]	Time 0.589 (0.327)	Data 1.13e-04 (1.47e-04)	Tok/s 50426 (43049)	Loss/tok 3.4410 (3.1681)	LR 1.250e-04
0: TRAIN [4][7290/7762]	Time 0.447 (0.327)	Data 1.01e-04 (1.47e-04)	Tok/s 52163 (43051)	Loss/tok 3.2175 (3.1681)	LR 1.250e-04
0: TRAIN [4][7300/7762]	Time 0.172 (0.327)	Data 1.02e-04 (1.47e-04)	Tok/s 30593 (43053)	Loss/tok 2.6167 (3.1681)	LR 1.250e-04
0: TRAIN [4][7310/7762]	Time 0.358 (0.327)	Data 9.99e-05 (1.47e-04)	Tok/s 47234 (43053)	Loss/tok 3.1675 (3.1682)	LR 1.250e-04
0: TRAIN [4][7320/7762]	Time 0.344 (0.327)	Data 9.66e-05 (1.47e-04)	Tok/s 48738 (43050)	Loss/tok 3.2396 (3.1681)	LR 1.250e-04
0: TRAIN [4][7330/7762]	Time 0.362 (0.327)	Data 9.92e-05 (1.47e-04)	Tok/s 46630 (43049)	Loss/tok 3.1643 (3.1679)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][7340/7762]	Time 0.588 (0.327)	Data 1.02e-04 (1.47e-04)	Tok/s 50767 (43052)	Loss/tok 3.5338 (3.1681)	LR 1.250e-04
0: TRAIN [4][7350/7762]	Time 0.268 (0.327)	Data 9.89e-05 (1.47e-04)	Tok/s 38100 (43050)	Loss/tok 3.0005 (3.1680)	LR 1.250e-04
0: TRAIN [4][7360/7762]	Time 0.178 (0.326)	Data 1.14e-04 (1.47e-04)	Tok/s 30309 (43046)	Loss/tok 2.7106 (3.1678)	LR 1.250e-04
0: TRAIN [4][7370/7762]	Time 0.344 (0.326)	Data 1.05e-04 (1.47e-04)	Tok/s 48724 (43048)	Loss/tok 3.3090 (3.1678)	LR 1.250e-04
0: TRAIN [4][7380/7762]	Time 0.467 (0.327)	Data 1.03e-04 (1.47e-04)	Tok/s 50257 (43050)	Loss/tok 3.3668 (3.1679)	LR 1.250e-04
0: TRAIN [4][7390/7762]	Time 0.579 (0.327)	Data 9.70e-05 (1.47e-04)	Tok/s 51056 (43051)	Loss/tok 3.4776 (3.1680)	LR 1.250e-04
0: TRAIN [4][7400/7762]	Time 0.558 (0.327)	Data 1.03e-04 (1.47e-04)	Tok/s 52542 (43054)	Loss/tok 3.6520 (3.1680)	LR 1.250e-04
0: TRAIN [4][7410/7762]	Time 0.264 (0.327)	Data 1.00e-04 (1.47e-04)	Tok/s 39324 (43053)	Loss/tok 2.9240 (3.1679)	LR 1.250e-04
0: TRAIN [4][7420/7762]	Time 0.365 (0.327)	Data 1.13e-04 (1.47e-04)	Tok/s 46445 (43052)	Loss/tok 3.1739 (3.1679)	LR 1.250e-04
0: TRAIN [4][7430/7762]	Time 0.177 (0.327)	Data 1.02e-04 (1.47e-04)	Tok/s 30145 (43051)	Loss/tok 2.5333 (3.1679)	LR 1.250e-04
0: TRAIN [4][7440/7762]	Time 0.256 (0.326)	Data 1.16e-04 (1.47e-04)	Tok/s 40433 (43048)	Loss/tok 3.0409 (3.1677)	LR 1.250e-04
0: TRAIN [4][7450/7762]	Time 0.263 (0.326)	Data 1.00e-04 (1.47e-04)	Tok/s 39244 (43049)	Loss/tok 2.9813 (3.1678)	LR 1.250e-04
0: TRAIN [4][7460/7762]	Time 0.358 (0.326)	Data 9.58e-05 (1.46e-04)	Tok/s 46573 (43047)	Loss/tok 3.1045 (3.1676)	LR 1.250e-04
0: TRAIN [4][7470/7762]	Time 0.268 (0.326)	Data 1.03e-04 (1.46e-04)	Tok/s 38541 (43049)	Loss/tok 2.9885 (3.1677)	LR 1.250e-04
0: TRAIN [4][7480/7762]	Time 0.178 (0.326)	Data 9.92e-05 (1.46e-04)	Tok/s 29266 (43048)	Loss/tok 2.6458 (3.1677)	LR 1.250e-04
0: TRAIN [4][7490/7762]	Time 0.264 (0.326)	Data 9.85e-05 (1.46e-04)	Tok/s 39243 (43045)	Loss/tok 2.9626 (3.1676)	LR 1.250e-04
0: TRAIN [4][7500/7762]	Time 0.455 (0.326)	Data 1.02e-04 (1.46e-04)	Tok/s 50854 (43049)	Loss/tok 3.4091 (3.1677)	LR 1.250e-04
0: TRAIN [4][7510/7762]	Time 0.179 (0.327)	Data 1.03e-04 (1.46e-04)	Tok/s 29189 (43050)	Loss/tok 2.4596 (3.1678)	LR 1.250e-04
0: TRAIN [4][7520/7762]	Time 0.171 (0.327)	Data 1.07e-04 (1.46e-04)	Tok/s 31545 (43051)	Loss/tok 2.5689 (3.1678)	LR 1.250e-04
0: TRAIN [4][7530/7762]	Time 0.256 (0.326)	Data 1.04e-04 (1.46e-04)	Tok/s 39816 (43049)	Loss/tok 2.9492 (3.1678)	LR 1.250e-04
0: TRAIN [4][7540/7762]	Time 0.255 (0.326)	Data 9.99e-05 (1.46e-04)	Tok/s 40456 (43046)	Loss/tok 3.0490 (3.1677)	LR 1.250e-04
0: TRAIN [4][7550/7762]	Time 0.267 (0.326)	Data 1.02e-04 (1.46e-04)	Tok/s 38183 (43044)	Loss/tok 2.9123 (3.1677)	LR 1.250e-04
0: TRAIN [4][7560/7762]	Time 0.455 (0.326)	Data 1.01e-04 (1.46e-04)	Tok/s 51153 (43045)	Loss/tok 3.3156 (3.1676)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][7570/7762]	Time 0.333 (0.326)	Data 1.10e-04 (1.46e-04)	Tok/s 50736 (43049)	Loss/tok 3.1807 (3.1677)	LR 1.250e-04
0: TRAIN [4][7580/7762]	Time 0.354 (0.327)	Data 1.01e-04 (1.46e-04)	Tok/s 46914 (43053)	Loss/tok 3.1563 (3.1678)	LR 1.250e-04
0: TRAIN [4][7590/7762]	Time 0.356 (0.326)	Data 9.51e-05 (1.46e-04)	Tok/s 47355 (43051)	Loss/tok 3.1756 (3.1677)	LR 1.250e-04
0: TRAIN [4][7600/7762]	Time 0.260 (0.326)	Data 1.00e-04 (1.46e-04)	Tok/s 38911 (43048)	Loss/tok 2.9802 (3.1676)	LR 1.250e-04
0: TRAIN [4][7610/7762]	Time 0.269 (0.326)	Data 9.75e-05 (1.46e-04)	Tok/s 37705 (43045)	Loss/tok 3.1200 (3.1675)	LR 1.250e-04
0: TRAIN [4][7620/7762]	Time 0.265 (0.326)	Data 1.08e-04 (1.46e-04)	Tok/s 39086 (43044)	Loss/tok 2.9189 (3.1677)	LR 1.250e-04
0: TRAIN [4][7630/7762]	Time 0.464 (0.326)	Data 1.01e-04 (1.45e-04)	Tok/s 49372 (43044)	Loss/tok 3.3281 (3.1677)	LR 1.250e-04
0: TRAIN [4][7640/7762]	Time 0.359 (0.326)	Data 1.00e-04 (1.45e-04)	Tok/s 46281 (43047)	Loss/tok 3.2012 (3.1678)	LR 1.250e-04
0: TRAIN [4][7650/7762]	Time 0.363 (0.326)	Data 1.02e-04 (1.45e-04)	Tok/s 46470 (43046)	Loss/tok 3.1219 (3.1677)	LR 1.250e-04
0: TRAIN [4][7660/7762]	Time 0.262 (0.326)	Data 1.05e-04 (1.45e-04)	Tok/s 39787 (43044)	Loss/tok 2.8586 (3.1676)	LR 1.250e-04
0: TRAIN [4][7670/7762]	Time 0.260 (0.326)	Data 1.01e-04 (1.45e-04)	Tok/s 39810 (43044)	Loss/tok 2.8720 (3.1677)	LR 1.250e-04
0: TRAIN [4][7680/7762]	Time 0.263 (0.326)	Data 9.80e-05 (1.45e-04)	Tok/s 38822 (43041)	Loss/tok 2.9132 (3.1675)	LR 1.250e-04
0: TRAIN [4][7690/7762]	Time 0.177 (0.326)	Data 1.02e-04 (1.45e-04)	Tok/s 29310 (43040)	Loss/tok 2.6398 (3.1674)	LR 1.250e-04
0: TRAIN [4][7700/7762]	Time 0.267 (0.326)	Data 1.02e-04 (1.45e-04)	Tok/s 38545 (43043)	Loss/tok 2.9177 (3.1674)	LR 1.250e-04
0: TRAIN [4][7710/7762]	Time 0.565 (0.326)	Data 1.21e-04 (1.45e-04)	Tok/s 53138 (43045)	Loss/tok 3.5048 (3.1676)	LR 1.250e-04
0: TRAIN [4][7720/7762]	Time 0.353 (0.326)	Data 1.04e-04 (1.45e-04)	Tok/s 47406 (43043)	Loss/tok 3.1711 (3.1674)	LR 1.250e-04
0: TRAIN [4][7730/7762]	Time 0.462 (0.326)	Data 1.03e-04 (1.45e-04)	Tok/s 49655 (43044)	Loss/tok 3.4763 (3.1675)	LR 1.250e-04
0: TRAIN [4][7740/7762]	Time 0.258 (0.326)	Data 1.02e-04 (1.45e-04)	Tok/s 40093 (43041)	Loss/tok 3.1103 (3.1675)	LR 1.250e-04
0: TRAIN [4][7750/7762]	Time 0.364 (0.326)	Data 1.14e-04 (1.45e-04)	Tok/s 46083 (43040)	Loss/tok 3.1782 (3.1674)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][7760/7762]	Time 0.579 (0.326)	Data 1.50e-02 (1.47e-04)	Tok/s 52058 (43041)	Loss/tok 3.4598 (3.1674)	LR 1.250e-04
:::MLL 1573758538.845 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 524}}
:::MLL 1573758538.846 eval_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [4][0/12]	Time 0.806 (0.806)	Decoder iters 101.0 (101.0)	Tok/s 20361 (20361)
0: TEST [4][10/12]	Time 0.127 (0.289)	Decoder iters 29.0 (48.6)	Tok/s 29283 (29580)
0: Running moses detokenizer
0: BLEU(score=23.247605219625484, counts=[36669, 18064, 10117, 5906], totals=[65267, 62264, 59262, 56265], precisions=[56.183063416427906, 29.011949119876654, 17.071647936282947, 10.496756420510087], bp=1.0, sys_len=65267, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1573758544.399 eval_accuracy: {"value": 23.25, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 535}}
:::MLL 1573758544.400 eval_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 4	Training Loss: 3.1670	Test BLEU: 23.25
0: Performance: Epoch: 4	Training: 86089 Tok/s
0: Finished epoch 4
:::MLL 1573758544.400 block_stop: {"value": null, "metadata": {"first_epoch_num": 5, "file": "train.py", "lineno": 557}}
:::MLL 1573758544.401 block_start: {"value": null, "metadata": {"first_epoch_num": 6, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1573758544.401 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 514}}
0: Starting epoch 5
0: Executing preallocation
0: Sampler for epoch 5 uses seed 503297421
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [5][0/7762]	Time 0.640 (0.640)	Data 3.11e-01 (3.11e-01)	Tok/s 26140 (26140)	Loss/tok 3.1301 (3.1301)	LR 1.250e-04
0: TRAIN [5][10/7762]	Time 0.174 (0.405)	Data 1.01e-04 (2.84e-02)	Tok/s 30358 (45512)	Loss/tok 2.5539 (3.2612)	LR 1.250e-04
0: TRAIN [5][20/7762]	Time 0.261 (0.376)	Data 1.00e-04 (1.49e-02)	Tok/s 39388 (45780)	Loss/tok 2.9109 (3.2166)	LR 1.250e-04
0: TRAIN [5][30/7762]	Time 0.260 (0.348)	Data 1.03e-04 (1.01e-02)	Tok/s 40192 (44369)	Loss/tok 2.9757 (3.1876)	LR 1.250e-04
0: TRAIN [5][40/7762]	Time 0.265 (0.329)	Data 1.20e-04 (7.69e-03)	Tok/s 38816 (43255)	Loss/tok 2.9046 (3.1614)	LR 1.250e-04
0: TRAIN [5][50/7762]	Time 0.442 (0.332)	Data 9.89e-05 (6.20e-03)	Tok/s 53017 (43894)	Loss/tok 3.4459 (3.1549)	LR 1.250e-04
0: TRAIN [5][60/7762]	Time 0.265 (0.340)	Data 1.01e-04 (5.20e-03)	Tok/s 38534 (44266)	Loss/tok 2.9726 (3.1768)	LR 1.250e-04
0: TRAIN [5][70/7762]	Time 0.581 (0.335)	Data 1.00e-04 (4.49e-03)	Tok/s 50740 (43847)	Loss/tok 3.5445 (3.1704)	LR 1.250e-04
0: TRAIN [5][80/7762]	Time 0.349 (0.337)	Data 1.17e-04 (3.94e-03)	Tok/s 48725 (44099)	Loss/tok 3.0850 (3.1733)	LR 1.250e-04
0: TRAIN [5][90/7762]	Time 0.260 (0.338)	Data 1.02e-04 (3.52e-03)	Tok/s 39069 (44041)	Loss/tok 2.9703 (3.1726)	LR 1.250e-04
0: TRAIN [5][100/7762]	Time 0.461 (0.339)	Data 9.73e-05 (3.18e-03)	Tok/s 50387 (44097)	Loss/tok 3.3307 (3.1769)	LR 1.250e-04
0: TRAIN [5][110/7762]	Time 0.253 (0.335)	Data 1.62e-04 (2.91e-03)	Tok/s 39854 (43862)	Loss/tok 3.0537 (3.1664)	LR 1.250e-04
0: TRAIN [5][120/7762]	Time 0.343 (0.332)	Data 1.17e-04 (2.67e-03)	Tok/s 49742 (43765)	Loss/tok 3.0379 (3.1582)	LR 1.250e-04
0: TRAIN [5][130/7762]	Time 0.267 (0.332)	Data 1.02e-04 (2.48e-03)	Tok/s 39278 (43778)	Loss/tok 2.8934 (3.1541)	LR 1.250e-04
0: TRAIN [5][140/7762]	Time 0.454 (0.330)	Data 9.68e-05 (2.31e-03)	Tok/s 51738 (43622)	Loss/tok 3.2278 (3.1495)	LR 1.250e-04
0: TRAIN [5][150/7762]	Time 0.342 (0.330)	Data 1.04e-04 (2.16e-03)	Tok/s 48578 (43711)	Loss/tok 3.2030 (3.1470)	LR 1.250e-04
0: TRAIN [5][160/7762]	Time 0.173 (0.326)	Data 9.35e-05 (2.04e-03)	Tok/s 30524 (43414)	Loss/tok 2.6554 (3.1394)	LR 1.250e-04
0: TRAIN [5][170/7762]	Time 0.446 (0.327)	Data 1.20e-04 (1.92e-03)	Tok/s 51975 (43467)	Loss/tok 3.3272 (3.1405)	LR 1.250e-04
0: TRAIN [5][180/7762]	Time 0.461 (0.329)	Data 1.16e-04 (1.82e-03)	Tok/s 50388 (43526)	Loss/tok 3.2879 (3.1469)	LR 1.250e-04
0: TRAIN [5][190/7762]	Time 0.262 (0.327)	Data 1.06e-04 (1.73e-03)	Tok/s 39170 (43389)	Loss/tok 2.9664 (3.1452)	LR 1.250e-04
0: TRAIN [5][200/7762]	Time 0.175 (0.327)	Data 9.89e-05 (1.65e-03)	Tok/s 30516 (43356)	Loss/tok 2.5032 (3.1413)	LR 1.250e-04
0: TRAIN [5][210/7762]	Time 0.261 (0.326)	Data 1.01e-04 (1.58e-03)	Tok/s 39259 (43343)	Loss/tok 2.9273 (3.1394)	LR 1.250e-04
0: TRAIN [5][220/7762]	Time 0.260 (0.327)	Data 1.02e-04 (1.51e-03)	Tok/s 39876 (43483)	Loss/tok 2.9495 (3.1401)	LR 1.250e-04
0: TRAIN [5][230/7762]	Time 0.261 (0.327)	Data 1.00e-04 (1.45e-03)	Tok/s 39458 (43423)	Loss/tok 3.0040 (3.1370)	LR 1.250e-04
0: TRAIN [5][240/7762]	Time 0.461 (0.326)	Data 9.66e-05 (1.39e-03)	Tok/s 50230 (43400)	Loss/tok 3.2375 (3.1347)	LR 1.250e-04
0: TRAIN [5][250/7762]	Time 0.454 (0.326)	Data 1.02e-04 (1.34e-03)	Tok/s 50884 (43446)	Loss/tok 3.3093 (3.1344)	LR 1.250e-04
0: TRAIN [5][260/7762]	Time 0.261 (0.326)	Data 1.02e-04 (1.29e-03)	Tok/s 39413 (43363)	Loss/tok 2.9359 (3.1325)	LR 1.250e-04
0: TRAIN [5][270/7762]	Time 0.263 (0.325)	Data 1.02e-04 (1.25e-03)	Tok/s 39891 (43323)	Loss/tok 2.9273 (3.1292)	LR 1.250e-04
0: TRAIN [5][280/7762]	Time 0.265 (0.325)	Data 1.05e-04 (1.21e-03)	Tok/s 37518 (43357)	Loss/tok 2.9738 (3.1308)	LR 1.250e-04
0: TRAIN [5][290/7762]	Time 0.354 (0.325)	Data 9.49e-05 (1.17e-03)	Tok/s 47786 (43318)	Loss/tok 3.1784 (3.1318)	LR 1.250e-04
0: TRAIN [5][300/7762]	Time 0.355 (0.325)	Data 9.85e-05 (1.14e-03)	Tok/s 47781 (43363)	Loss/tok 3.0227 (3.1302)	LR 1.250e-04
0: TRAIN [5][310/7762]	Time 0.461 (0.326)	Data 9.56e-05 (1.10e-03)	Tok/s 50522 (43410)	Loss/tok 3.2778 (3.1329)	LR 1.250e-04
0: TRAIN [5][320/7762]	Time 0.263 (0.327)	Data 1.03e-04 (1.07e-03)	Tok/s 39889 (43459)	Loss/tok 2.9179 (3.1354)	LR 1.250e-04
0: TRAIN [5][330/7762]	Time 0.264 (0.328)	Data 1.03e-04 (1.04e-03)	Tok/s 39610 (43489)	Loss/tok 2.8176 (3.1356)	LR 1.250e-04
0: TRAIN [5][340/7762]	Time 0.456 (0.328)	Data 1.01e-04 (1.01e-03)	Tok/s 50636 (43493)	Loss/tok 3.2935 (3.1376)	LR 1.250e-04
0: TRAIN [5][350/7762]	Time 0.265 (0.328)	Data 9.97e-05 (9.89e-04)	Tok/s 38453 (43430)	Loss/tok 2.9114 (3.1363)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][360/7762]	Time 0.260 (0.328)	Data 1.03e-04 (9.64e-04)	Tok/s 40033 (43427)	Loss/tok 2.9615 (3.1374)	LR 1.250e-04
0: TRAIN [5][370/7762]	Time 0.440 (0.328)	Data 1.03e-04 (9.41e-04)	Tok/s 53151 (43423)	Loss/tok 3.3173 (3.1377)	LR 1.250e-04
0: TRAIN [5][380/7762]	Time 0.346 (0.329)	Data 1.03e-04 (9.19e-04)	Tok/s 48943 (43522)	Loss/tok 2.9990 (3.1394)	LR 1.250e-04
0: TRAIN [5][390/7762]	Time 0.255 (0.328)	Data 1.03e-04 (8.98e-04)	Tok/s 41152 (43440)	Loss/tok 3.0156 (3.1378)	LR 1.250e-04
0: TRAIN [5][400/7762]	Time 0.455 (0.329)	Data 1.02e-04 (8.78e-04)	Tok/s 51348 (43498)	Loss/tok 3.3429 (3.1428)	LR 1.250e-04
0: TRAIN [5][410/7762]	Time 0.176 (0.328)	Data 1.01e-04 (8.59e-04)	Tok/s 29225 (43411)	Loss/tok 2.6243 (3.1403)	LR 1.250e-04
0: TRAIN [5][420/7762]	Time 0.265 (0.329)	Data 1.07e-04 (8.41e-04)	Tok/s 39314 (43454)	Loss/tok 2.9599 (3.1437)	LR 1.250e-04
0: TRAIN [5][430/7762]	Time 0.366 (0.329)	Data 1.04e-04 (8.24e-04)	Tok/s 46368 (43456)	Loss/tok 3.0974 (3.1433)	LR 1.250e-04
0: TRAIN [5][440/7762]	Time 0.589 (0.329)	Data 1.05e-04 (8.08e-04)	Tok/s 50236 (43470)	Loss/tok 3.5542 (3.1442)	LR 1.250e-04
0: TRAIN [5][450/7762]	Time 0.364 (0.330)	Data 1.23e-04 (7.92e-04)	Tok/s 45593 (43531)	Loss/tok 3.1686 (3.1461)	LR 1.250e-04
0: TRAIN [5][460/7762]	Time 0.260 (0.330)	Data 1.18e-04 (7.78e-04)	Tok/s 40194 (43487)	Loss/tok 2.8992 (3.1441)	LR 1.250e-04
0: TRAIN [5][470/7762]	Time 0.265 (0.331)	Data 9.92e-05 (7.63e-04)	Tok/s 39526 (43539)	Loss/tok 2.8895 (3.1475)	LR 1.250e-04
0: TRAIN [5][480/7762]	Time 0.262 (0.332)	Data 1.19e-04 (7.50e-04)	Tok/s 39168 (43547)	Loss/tok 2.8942 (3.1512)	LR 1.250e-04
0: TRAIN [5][490/7762]	Time 0.253 (0.331)	Data 1.02e-04 (7.36e-04)	Tok/s 40609 (43489)	Loss/tok 2.8737 (3.1489)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][500/7762]	Time 0.263 (0.331)	Data 1.05e-04 (7.24e-04)	Tok/s 39238 (43478)	Loss/tok 2.9460 (3.1487)	LR 1.250e-04
0: TRAIN [5][510/7762]	Time 0.363 (0.330)	Data 9.85e-05 (7.12e-04)	Tok/s 46477 (43432)	Loss/tok 3.1157 (3.1462)	LR 1.250e-04
0: TRAIN [5][520/7762]	Time 0.266 (0.330)	Data 1.04e-04 (7.00e-04)	Tok/s 38592 (43431)	Loss/tok 3.0324 (3.1446)	LR 1.250e-04
0: TRAIN [5][530/7762]	Time 0.263 (0.329)	Data 9.56e-05 (6.89e-04)	Tok/s 38612 (43403)	Loss/tok 3.0461 (3.1426)	LR 1.250e-04
0: TRAIN [5][540/7762]	Time 0.266 (0.330)	Data 1.03e-04 (6.78e-04)	Tok/s 38846 (43449)	Loss/tok 2.8530 (3.1438)	LR 1.250e-04
0: TRAIN [5][550/7762]	Time 0.590 (0.330)	Data 1.01e-04 (6.67e-04)	Tok/s 50491 (43488)	Loss/tok 3.3956 (3.1456)	LR 1.250e-04
0: TRAIN [5][560/7762]	Time 0.263 (0.330)	Data 1.03e-04 (6.57e-04)	Tok/s 39322 (43496)	Loss/tok 3.0857 (3.1464)	LR 1.250e-04
0: TRAIN [5][570/7762]	Time 0.362 (0.330)	Data 1.03e-04 (6.48e-04)	Tok/s 45753 (43476)	Loss/tok 3.2100 (3.1463)	LR 1.250e-04
0: TRAIN [5][580/7762]	Time 0.360 (0.331)	Data 1.05e-04 (6.38e-04)	Tok/s 47564 (43471)	Loss/tok 3.0433 (3.1474)	LR 1.250e-04
0: TRAIN [5][590/7762]	Time 0.269 (0.331)	Data 1.02e-04 (6.29e-04)	Tok/s 37440 (43466)	Loss/tok 3.0997 (3.1487)	LR 1.250e-04
0: TRAIN [5][600/7762]	Time 0.175 (0.332)	Data 1.01e-04 (6.21e-04)	Tok/s 30044 (43426)	Loss/tok 2.5344 (3.1505)	LR 1.250e-04
0: TRAIN [5][610/7762]	Time 0.463 (0.332)	Data 9.61e-05 (6.12e-04)	Tok/s 50772 (43448)	Loss/tok 3.3903 (3.1510)	LR 1.250e-04
0: TRAIN [5][620/7762]	Time 0.464 (0.332)	Data 9.94e-05 (6.04e-04)	Tok/s 50387 (43434)	Loss/tok 3.3691 (3.1503)	LR 1.250e-04
0: TRAIN [5][630/7762]	Time 0.259 (0.331)	Data 1.03e-04 (5.96e-04)	Tok/s 40084 (43419)	Loss/tok 3.0557 (3.1501)	LR 1.250e-04
0: TRAIN [5][640/7762]	Time 0.361 (0.331)	Data 1.04e-04 (5.88e-04)	Tok/s 46349 (43431)	Loss/tok 3.1331 (3.1492)	LR 1.250e-04
0: TRAIN [5][650/7762]	Time 0.448 (0.332)	Data 1.02e-04 (5.81e-04)	Tok/s 51472 (43471)	Loss/tok 3.3304 (3.1514)	LR 1.250e-04
0: TRAIN [5][660/7762]	Time 0.271 (0.331)	Data 9.68e-05 (5.74e-04)	Tok/s 37544 (43417)	Loss/tok 3.0397 (3.1506)	LR 1.250e-04
0: TRAIN [5][670/7762]	Time 0.264 (0.331)	Data 1.06e-04 (5.66e-04)	Tok/s 39194 (43380)	Loss/tok 2.9524 (3.1495)	LR 1.250e-04
0: TRAIN [5][680/7762]	Time 0.366 (0.330)	Data 9.54e-05 (5.60e-04)	Tok/s 46068 (43353)	Loss/tok 3.2220 (3.1489)	LR 1.250e-04
0: TRAIN [5][690/7762]	Time 0.179 (0.330)	Data 9.66e-05 (5.53e-04)	Tok/s 29328 (43346)	Loss/tok 2.6160 (3.1489)	LR 1.250e-04
0: TRAIN [5][700/7762]	Time 0.177 (0.330)	Data 9.70e-05 (5.47e-04)	Tok/s 30190 (43351)	Loss/tok 2.5316 (3.1484)	LR 1.250e-04
0: TRAIN [5][710/7762]	Time 0.265 (0.330)	Data 1.22e-04 (5.40e-04)	Tok/s 39248 (43333)	Loss/tok 2.9389 (3.1476)	LR 1.250e-04
0: TRAIN [5][720/7762]	Time 0.365 (0.330)	Data 1.04e-04 (5.34e-04)	Tok/s 46200 (43328)	Loss/tok 3.1685 (3.1480)	LR 1.250e-04
0: TRAIN [5][730/7762]	Time 0.361 (0.330)	Data 1.03e-04 (5.28e-04)	Tok/s 46996 (43285)	Loss/tok 3.0872 (3.1473)	LR 1.250e-04
0: TRAIN [5][740/7762]	Time 0.584 (0.330)	Data 1.01e-04 (5.23e-04)	Tok/s 50614 (43303)	Loss/tok 3.5756 (3.1493)	LR 1.250e-04
0: TRAIN [5][750/7762]	Time 0.363 (0.330)	Data 9.89e-05 (5.17e-04)	Tok/s 46045 (43293)	Loss/tok 3.1988 (3.1485)	LR 1.250e-04
0: TRAIN [5][760/7762]	Time 0.265 (0.330)	Data 9.73e-05 (5.12e-04)	Tok/s 39010 (43275)	Loss/tok 2.9522 (3.1481)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [5][770/7762]	Time 0.260 (0.329)	Data 1.05e-04 (5.06e-04)	Tok/s 39468 (43254)	Loss/tok 2.9937 (3.1466)	LR 1.250e-04
0: TRAIN [5][780/7762]	Time 0.268 (0.328)	Data 9.85e-05 (5.01e-04)	Tok/s 39024 (43171)	Loss/tok 2.8304 (3.1452)	LR 1.250e-04
0: TRAIN [5][790/7762]	Time 0.364 (0.328)	Data 1.04e-04 (4.96e-04)	Tok/s 45939 (43176)	Loss/tok 3.1580 (3.1448)	LR 1.250e-04
0: TRAIN [5][800/7762]	Time 0.268 (0.328)	Data 1.17e-04 (4.91e-04)	Tok/s 38454 (43133)	Loss/tok 3.0457 (3.1437)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][810/7762]	Time 0.586 (0.328)	Data 9.87e-05 (4.86e-04)	Tok/s 50818 (43145)	Loss/tok 3.5192 (3.1443)	LR 1.250e-04
0: TRAIN [5][820/7762]	Time 0.268 (0.328)	Data 2.38e-04 (4.82e-04)	Tok/s 38887 (43180)	Loss/tok 2.9018 (3.1438)	LR 1.250e-04
0: TRAIN [5][830/7762]	Time 0.362 (0.328)	Data 1.17e-04 (4.77e-04)	Tok/s 46125 (43167)	Loss/tok 3.0548 (3.1434)	LR 1.250e-04
0: TRAIN [5][840/7762]	Time 0.254 (0.327)	Data 9.80e-05 (4.73e-04)	Tok/s 39961 (43106)	Loss/tok 2.9319 (3.1424)	LR 1.250e-04
0: TRAIN [5][850/7762]	Time 0.369 (0.328)	Data 1.05e-04 (4.69e-04)	Tok/s 45309 (43145)	Loss/tok 3.2095 (3.1427)	LR 1.250e-04
0: TRAIN [5][860/7762]	Time 0.262 (0.327)	Data 9.97e-05 (4.64e-04)	Tok/s 38645 (43118)	Loss/tok 2.9816 (3.1423)	LR 1.250e-04
0: TRAIN [5][870/7762]	Time 0.264 (0.327)	Data 9.70e-05 (4.60e-04)	Tok/s 38730 (43114)	Loss/tok 2.9468 (3.1421)	LR 1.250e-04
0: TRAIN [5][880/7762]	Time 0.353 (0.327)	Data 9.32e-05 (4.56e-04)	Tok/s 47547 (43092)	Loss/tok 3.0641 (3.1421)	LR 1.250e-04
0: TRAIN [5][890/7762]	Time 0.176 (0.327)	Data 9.92e-05 (4.52e-04)	Tok/s 30152 (43079)	Loss/tok 2.5376 (3.1407)	LR 1.250e-04
0: TRAIN [5][900/7762]	Time 0.265 (0.327)	Data 1.19e-04 (4.48e-04)	Tok/s 39401 (43075)	Loss/tok 3.0056 (3.1418)	LR 1.250e-04
0: TRAIN [5][910/7762]	Time 0.353 (0.327)	Data 1.17e-04 (4.45e-04)	Tok/s 47035 (43114)	Loss/tok 2.9874 (3.1429)	LR 1.250e-04
0: TRAIN [5][920/7762]	Time 0.350 (0.327)	Data 9.80e-05 (4.41e-04)	Tok/s 47788 (43114)	Loss/tok 3.1623 (3.1430)	LR 1.250e-04
0: TRAIN [5][930/7762]	Time 0.466 (0.328)	Data 1.07e-04 (4.37e-04)	Tok/s 49865 (43137)	Loss/tok 3.3228 (3.1444)	LR 1.250e-04
0: TRAIN [5][940/7762]	Time 0.265 (0.327)	Data 9.68e-05 (4.34e-04)	Tok/s 39640 (43120)	Loss/tok 2.8398 (3.1434)	LR 1.250e-04
0: TRAIN [5][950/7762]	Time 0.441 (0.328)	Data 1.06e-04 (4.30e-04)	Tok/s 53061 (43142)	Loss/tok 3.2701 (3.1436)	LR 1.250e-04
0: TRAIN [5][960/7762]	Time 0.464 (0.328)	Data 1.01e-04 (4.27e-04)	Tok/s 49070 (43175)	Loss/tok 3.4641 (3.1448)	LR 1.250e-04
0: TRAIN [5][970/7762]	Time 0.579 (0.329)	Data 1.17e-04 (4.24e-04)	Tok/s 51545 (43214)	Loss/tok 3.5165 (3.1468)	LR 1.250e-04
0: TRAIN [5][980/7762]	Time 0.589 (0.329)	Data 1.03e-04 (4.21e-04)	Tok/s 51255 (43204)	Loss/tok 3.3397 (3.1465)	LR 1.250e-04
0: TRAIN [5][990/7762]	Time 0.265 (0.329)	Data 9.94e-05 (4.17e-04)	Tok/s 39295 (43224)	Loss/tok 2.9214 (3.1467)	LR 1.250e-04
0: TRAIN [5][1000/7762]	Time 0.365 (0.329)	Data 1.20e-04 (4.14e-04)	Tok/s 45552 (43237)	Loss/tok 3.1128 (3.1475)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][1010/7762]	Time 0.559 (0.330)	Data 1.01e-04 (4.11e-04)	Tok/s 53315 (43264)	Loss/tok 3.4804 (3.1484)	LR 1.250e-04
0: TRAIN [5][1020/7762]	Time 0.447 (0.330)	Data 9.61e-05 (4.08e-04)	Tok/s 52571 (43297)	Loss/tok 3.3534 (3.1494)	LR 1.250e-04
0: TRAIN [5][1030/7762]	Time 0.452 (0.331)	Data 9.99e-05 (4.05e-04)	Tok/s 51704 (43321)	Loss/tok 3.3608 (3.1502)	LR 1.250e-04
0: TRAIN [5][1040/7762]	Time 0.266 (0.330)	Data 1.01e-04 (4.02e-04)	Tok/s 38349 (43295)	Loss/tok 2.9132 (3.1494)	LR 1.250e-04
0: TRAIN [5][1050/7762]	Time 0.257 (0.330)	Data 9.73e-05 (3.99e-04)	Tok/s 40752 (43306)	Loss/tok 3.0679 (3.1496)	LR 1.250e-04
0: TRAIN [5][1060/7762]	Time 0.264 (0.330)	Data 9.61e-05 (3.97e-04)	Tok/s 39313 (43277)	Loss/tok 2.9213 (3.1494)	LR 1.250e-04
0: TRAIN [5][1070/7762]	Time 0.359 (0.330)	Data 9.73e-05 (3.94e-04)	Tok/s 46080 (43292)	Loss/tok 3.2165 (3.1498)	LR 1.250e-04
0: TRAIN [5][1080/7762]	Time 0.266 (0.330)	Data 9.80e-05 (3.91e-04)	Tok/s 38167 (43270)	Loss/tok 3.0886 (3.1493)	LR 1.250e-04
0: TRAIN [5][1090/7762]	Time 0.261 (0.330)	Data 1.01e-04 (3.89e-04)	Tok/s 38837 (43258)	Loss/tok 2.9415 (3.1485)	LR 1.250e-04
0: TRAIN [5][1100/7762]	Time 0.251 (0.329)	Data 1.04e-04 (3.86e-04)	Tok/s 40625 (43232)	Loss/tok 2.9972 (3.1476)	LR 1.250e-04
0: TRAIN [5][1110/7762]	Time 0.266 (0.329)	Data 9.39e-05 (3.83e-04)	Tok/s 38764 (43211)	Loss/tok 2.9293 (3.1467)	LR 1.250e-04
0: TRAIN [5][1120/7762]	Time 0.264 (0.329)	Data 9.51e-05 (3.81e-04)	Tok/s 39235 (43194)	Loss/tok 2.9539 (3.1464)	LR 1.250e-04
0: TRAIN [5][1130/7762]	Time 0.254 (0.328)	Data 1.03e-04 (3.78e-04)	Tok/s 40532 (43165)	Loss/tok 2.8650 (3.1460)	LR 1.250e-04
0: TRAIN [5][1140/7762]	Time 0.454 (0.328)	Data 9.89e-05 (3.76e-04)	Tok/s 51584 (43132)	Loss/tok 3.3418 (3.1452)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][1150/7762]	Time 0.462 (0.328)	Data 1.04e-04 (3.74e-04)	Tok/s 50560 (43164)	Loss/tok 3.3000 (3.1459)	LR 1.250e-04
0: TRAIN [5][1160/7762]	Time 0.265 (0.328)	Data 9.87e-05 (3.71e-04)	Tok/s 39201 (43163)	Loss/tok 2.9717 (3.1456)	LR 1.250e-04
0: TRAIN [5][1170/7762]	Time 0.255 (0.328)	Data 9.92e-05 (3.69e-04)	Tok/s 39718 (43171)	Loss/tok 2.9418 (3.1455)	LR 1.250e-04
0: TRAIN [5][1180/7762]	Time 0.265 (0.328)	Data 9.99e-05 (3.67e-04)	Tok/s 37965 (43167)	Loss/tok 3.0012 (3.1452)	LR 1.250e-04
0: TRAIN [5][1190/7762]	Time 0.344 (0.328)	Data 1.21e-04 (3.65e-04)	Tok/s 49486 (43150)	Loss/tok 3.0504 (3.1444)	LR 1.250e-04
0: TRAIN [5][1200/7762]	Time 0.266 (0.328)	Data 1.13e-04 (3.62e-04)	Tok/s 38582 (43154)	Loss/tok 2.9005 (3.1443)	LR 1.250e-04
0: TRAIN [5][1210/7762]	Time 0.356 (0.328)	Data 1.02e-04 (3.60e-04)	Tok/s 46757 (43176)	Loss/tok 3.2151 (3.1450)	LR 1.250e-04
0: TRAIN [5][1220/7762]	Time 0.261 (0.328)	Data 1.02e-04 (3.58e-04)	Tok/s 39329 (43159)	Loss/tok 2.9835 (3.1444)	LR 1.250e-04
0: TRAIN [5][1230/7762]	Time 0.177 (0.328)	Data 1.01e-04 (3.56e-04)	Tok/s 29557 (43145)	Loss/tok 2.5586 (3.1439)	LR 1.250e-04
0: TRAIN [5][1240/7762]	Time 0.259 (0.327)	Data 9.78e-05 (3.54e-04)	Tok/s 40208 (43144)	Loss/tok 2.9511 (3.1434)	LR 1.250e-04
0: TRAIN [5][1250/7762]	Time 0.261 (0.327)	Data 1.01e-04 (3.52e-04)	Tok/s 39147 (43128)	Loss/tok 2.9129 (3.1434)	LR 1.250e-04
0: TRAIN [5][1260/7762]	Time 0.264 (0.327)	Data 1.03e-04 (3.50e-04)	Tok/s 38376 (43117)	Loss/tok 2.9622 (3.1430)	LR 1.250e-04
0: TRAIN [5][1270/7762]	Time 0.176 (0.327)	Data 9.97e-05 (3.48e-04)	Tok/s 29625 (43103)	Loss/tok 2.5539 (3.1429)	LR 1.250e-04
0: TRAIN [5][1280/7762]	Time 0.264 (0.327)	Data 9.78e-05 (3.46e-04)	Tok/s 39610 (43100)	Loss/tok 2.9913 (3.1425)	LR 1.250e-04
0: TRAIN [5][1290/7762]	Time 0.353 (0.327)	Data 9.97e-05 (3.44e-04)	Tok/s 48160 (43110)	Loss/tok 3.1150 (3.1420)	LR 1.250e-04
0: TRAIN [5][1300/7762]	Time 0.263 (0.327)	Data 1.06e-04 (3.43e-04)	Tok/s 40319 (43112)	Loss/tok 2.9394 (3.1416)	LR 1.250e-04
0: TRAIN [5][1310/7762]	Time 0.357 (0.327)	Data 1.16e-04 (3.41e-04)	Tok/s 47804 (43119)	Loss/tok 3.1536 (3.1417)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][1320/7762]	Time 0.265 (0.327)	Data 1.00e-04 (3.39e-04)	Tok/s 38921 (43118)	Loss/tok 3.0234 (3.1421)	LR 1.250e-04
0: TRAIN [5][1330/7762]	Time 0.365 (0.327)	Data 9.54e-05 (3.37e-04)	Tok/s 46398 (43130)	Loss/tok 3.0648 (3.1418)	LR 1.250e-04
0: TRAIN [5][1340/7762]	Time 0.178 (0.326)	Data 9.73e-05 (3.35e-04)	Tok/s 29724 (43100)	Loss/tok 2.6452 (3.1409)	LR 1.250e-04
0: TRAIN [5][1350/7762]	Time 0.258 (0.326)	Data 1.02e-04 (3.34e-04)	Tok/s 40121 (43097)	Loss/tok 2.9433 (3.1410)	LR 1.250e-04
0: TRAIN [5][1360/7762]	Time 0.343 (0.327)	Data 9.68e-05 (3.32e-04)	Tok/s 48390 (43112)	Loss/tok 3.2098 (3.1411)	LR 1.250e-04
0: TRAIN [5][1370/7762]	Time 0.450 (0.327)	Data 9.85e-05 (3.30e-04)	Tok/s 52491 (43113)	Loss/tok 3.2162 (3.1409)	LR 1.250e-04
0: TRAIN [5][1380/7762]	Time 0.262 (0.327)	Data 9.97e-05 (3.29e-04)	Tok/s 38950 (43120)	Loss/tok 2.9824 (3.1408)	LR 1.250e-04
0: TRAIN [5][1390/7762]	Time 0.468 (0.327)	Data 1.05e-04 (3.27e-04)	Tok/s 49674 (43127)	Loss/tok 3.4692 (3.1418)	LR 1.250e-04
0: TRAIN [5][1400/7762]	Time 0.261 (0.326)	Data 9.80e-05 (3.25e-04)	Tok/s 39264 (43110)	Loss/tok 2.9050 (3.1412)	LR 1.250e-04
0: TRAIN [5][1410/7762]	Time 0.269 (0.326)	Data 9.97e-05 (3.24e-04)	Tok/s 38333 (43085)	Loss/tok 2.8467 (3.1403)	LR 1.250e-04
0: TRAIN [5][1420/7762]	Time 0.581 (0.326)	Data 9.92e-05 (3.22e-04)	Tok/s 51444 (43081)	Loss/tok 3.4581 (3.1402)	LR 1.250e-04
0: TRAIN [5][1430/7762]	Time 0.453 (0.326)	Data 9.87e-05 (3.21e-04)	Tok/s 51316 (43101)	Loss/tok 3.2605 (3.1406)	LR 1.250e-04
0: TRAIN [5][1440/7762]	Time 0.363 (0.326)	Data 1.12e-04 (3.19e-04)	Tok/s 45297 (43098)	Loss/tok 3.1438 (3.1403)	LR 1.250e-04
0: TRAIN [5][1450/7762]	Time 0.260 (0.326)	Data 1.00e-04 (3.18e-04)	Tok/s 39752 (43091)	Loss/tok 2.8412 (3.1398)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][1460/7762]	Time 0.361 (0.326)	Data 9.99e-05 (3.16e-04)	Tok/s 46647 (43095)	Loss/tok 3.1375 (3.1400)	LR 1.250e-04
0: TRAIN [5][1470/7762]	Time 0.356 (0.326)	Data 1.02e-04 (3.15e-04)	Tok/s 47044 (43099)	Loss/tok 3.1868 (3.1399)	LR 1.250e-04
0: TRAIN [5][1480/7762]	Time 0.174 (0.326)	Data 9.94e-05 (3.13e-04)	Tok/s 30097 (43105)	Loss/tok 2.4729 (3.1404)	LR 1.250e-04
0: TRAIN [5][1490/7762]	Time 0.358 (0.326)	Data 9.85e-05 (3.12e-04)	Tok/s 46451 (43115)	Loss/tok 3.1235 (3.1406)	LR 1.250e-04
0: TRAIN [5][1500/7762]	Time 0.355 (0.326)	Data 9.75e-05 (3.11e-04)	Tok/s 47858 (43108)	Loss/tok 3.1248 (3.1402)	LR 1.250e-04
0: TRAIN [5][1510/7762]	Time 0.172 (0.326)	Data 1.03e-04 (3.09e-04)	Tok/s 30482 (43101)	Loss/tok 2.6277 (3.1398)	LR 1.250e-04
0: TRAIN [5][1520/7762]	Time 0.261 (0.326)	Data 1.09e-04 (3.08e-04)	Tok/s 39180 (43109)	Loss/tok 2.9054 (3.1399)	LR 1.250e-04
0: TRAIN [5][1530/7762]	Time 0.259 (0.326)	Data 1.02e-04 (3.07e-04)	Tok/s 39828 (43103)	Loss/tok 2.9315 (3.1399)	LR 1.250e-04
0: TRAIN [5][1540/7762]	Time 0.263 (0.326)	Data 1.66e-04 (3.05e-04)	Tok/s 39243 (43111)	Loss/tok 2.9302 (3.1403)	LR 1.250e-04
0: TRAIN [5][1550/7762]	Time 0.265 (0.326)	Data 1.00e-04 (3.04e-04)	Tok/s 39028 (43113)	Loss/tok 3.0403 (3.1405)	LR 1.250e-04
0: TRAIN [5][1560/7762]	Time 0.365 (0.326)	Data 9.99e-05 (3.03e-04)	Tok/s 46054 (43121)	Loss/tok 3.0553 (3.1406)	LR 1.250e-04
0: TRAIN [5][1570/7762]	Time 0.462 (0.326)	Data 9.66e-05 (3.01e-04)	Tok/s 50847 (43101)	Loss/tok 3.3480 (3.1401)	LR 1.250e-04
0: TRAIN [5][1580/7762]	Time 0.174 (0.326)	Data 1.02e-04 (3.00e-04)	Tok/s 30880 (43094)	Loss/tok 2.5523 (3.1400)	LR 1.250e-04
0: TRAIN [5][1590/7762]	Time 0.260 (0.326)	Data 9.97e-05 (2.99e-04)	Tok/s 39531 (43105)	Loss/tok 2.8849 (3.1402)	LR 1.250e-04
0: TRAIN [5][1600/7762]	Time 0.265 (0.326)	Data 1.26e-04 (2.98e-04)	Tok/s 39445 (43110)	Loss/tok 2.9885 (3.1401)	LR 1.250e-04
0: TRAIN [5][1610/7762]	Time 0.265 (0.326)	Data 1.04e-04 (2.96e-04)	Tok/s 39446 (43106)	Loss/tok 2.9666 (3.1397)	LR 1.250e-04
0: TRAIN [5][1620/7762]	Time 0.368 (0.325)	Data 1.13e-04 (2.95e-04)	Tok/s 45610 (43081)	Loss/tok 3.0807 (3.1390)	LR 1.250e-04
0: TRAIN [5][1630/7762]	Time 0.365 (0.326)	Data 1.04e-04 (2.94e-04)	Tok/s 45194 (43099)	Loss/tok 3.1569 (3.1392)	LR 1.250e-04
0: TRAIN [5][1640/7762]	Time 0.264 (0.326)	Data 1.01e-04 (2.93e-04)	Tok/s 38465 (43097)	Loss/tok 2.9257 (3.1391)	LR 1.250e-04
0: TRAIN [5][1650/7762]	Time 0.356 (0.326)	Data 1.21e-04 (2.92e-04)	Tok/s 47497 (43115)	Loss/tok 3.2444 (3.1396)	LR 1.250e-04
0: TRAIN [5][1660/7762]	Time 0.265 (0.326)	Data 9.99e-05 (2.91e-04)	Tok/s 39351 (43113)	Loss/tok 2.9409 (3.1393)	LR 1.250e-04
0: TRAIN [5][1670/7762]	Time 0.174 (0.326)	Data 1.03e-04 (2.90e-04)	Tok/s 30255 (43111)	Loss/tok 2.5566 (3.1392)	LR 1.250e-04
0: TRAIN [5][1680/7762]	Time 0.175 (0.326)	Data 1.04e-04 (2.89e-04)	Tok/s 30531 (43115)	Loss/tok 2.5627 (3.1392)	LR 1.250e-04
0: TRAIN [5][1690/7762]	Time 0.362 (0.326)	Data 1.02e-04 (2.87e-04)	Tok/s 47145 (43102)	Loss/tok 3.1026 (3.1386)	LR 1.250e-04
0: TRAIN [5][1700/7762]	Time 0.259 (0.326)	Data 1.18e-04 (2.86e-04)	Tok/s 40512 (43110)	Loss/tok 2.8634 (3.1393)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [5][1710/7762]	Time 0.331 (0.326)	Data 1.09e-04 (2.85e-04)	Tok/s 50292 (43116)	Loss/tok 3.0415 (3.1394)	LR 1.250e-04
0: TRAIN [5][1720/7762]	Time 0.363 (0.326)	Data 1.02e-04 (2.84e-04)	Tok/s 47019 (43120)	Loss/tok 3.0913 (3.1395)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][1730/7762]	Time 0.265 (0.326)	Data 1.12e-04 (2.83e-04)	Tok/s 38898 (43125)	Loss/tok 2.9624 (3.1397)	LR 1.250e-04
0: TRAIN [5][1740/7762]	Time 0.261 (0.326)	Data 1.00e-04 (2.82e-04)	Tok/s 39888 (43122)	Loss/tok 3.0013 (3.1395)	LR 1.250e-04
0: TRAIN [5][1750/7762]	Time 0.361 (0.326)	Data 1.05e-04 (2.81e-04)	Tok/s 46345 (43132)	Loss/tok 3.1262 (3.1401)	LR 1.250e-04
0: TRAIN [5][1760/7762]	Time 0.459 (0.326)	Data 1.02e-04 (2.80e-04)	Tok/s 51004 (43137)	Loss/tok 3.2827 (3.1398)	LR 1.250e-04
0: TRAIN [5][1770/7762]	Time 0.362 (0.326)	Data 9.87e-05 (2.79e-04)	Tok/s 46086 (43137)	Loss/tok 3.1556 (3.1404)	LR 1.250e-04
0: TRAIN [5][1780/7762]	Time 0.455 (0.326)	Data 1.03e-04 (2.78e-04)	Tok/s 51181 (43146)	Loss/tok 3.1687 (3.1405)	LR 1.250e-04
0: TRAIN [5][1790/7762]	Time 0.263 (0.326)	Data 9.92e-05 (2.77e-04)	Tok/s 39348 (43136)	Loss/tok 2.9225 (3.1407)	LR 1.250e-04
0: TRAIN [5][1800/7762]	Time 0.363 (0.327)	Data 1.04e-04 (2.76e-04)	Tok/s 46235 (43147)	Loss/tok 3.2055 (3.1411)	LR 1.250e-04
0: TRAIN [5][1810/7762]	Time 0.262 (0.326)	Data 1.03e-04 (2.75e-04)	Tok/s 39806 (43130)	Loss/tok 2.9160 (3.1410)	LR 1.250e-04
0: TRAIN [5][1820/7762]	Time 0.363 (0.326)	Data 1.01e-04 (2.74e-04)	Tok/s 46289 (43117)	Loss/tok 3.1800 (3.1409)	LR 1.250e-04
0: TRAIN [5][1830/7762]	Time 0.264 (0.326)	Data 9.99e-05 (2.74e-04)	Tok/s 38907 (43117)	Loss/tok 2.8233 (3.1411)	LR 1.250e-04
0: TRAIN [5][1840/7762]	Time 0.461 (0.326)	Data 1.02e-04 (2.73e-04)	Tok/s 50873 (43123)	Loss/tok 3.2039 (3.1407)	LR 1.250e-04
0: TRAIN [5][1850/7762]	Time 0.362 (0.326)	Data 1.03e-04 (2.72e-04)	Tok/s 46471 (43123)	Loss/tok 3.2110 (3.1409)	LR 1.250e-04
0: TRAIN [5][1860/7762]	Time 0.588 (0.326)	Data 1.03e-04 (2.71e-04)	Tok/s 50730 (43129)	Loss/tok 3.4905 (3.1413)	LR 1.250e-04
0: TRAIN [5][1870/7762]	Time 0.265 (0.327)	Data 9.87e-05 (2.70e-04)	Tok/s 39126 (43135)	Loss/tok 2.9136 (3.1416)	LR 1.250e-04
0: TRAIN [5][1880/7762]	Time 0.262 (0.327)	Data 1.04e-04 (2.69e-04)	Tok/s 39520 (43129)	Loss/tok 2.8982 (3.1417)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][1890/7762]	Time 0.362 (0.327)	Data 1.02e-04 (2.68e-04)	Tok/s 46383 (43136)	Loss/tok 3.1259 (3.1420)	LR 1.250e-04
0: TRAIN [5][1900/7762]	Time 0.259 (0.327)	Data 9.68e-05 (2.67e-04)	Tok/s 39644 (43146)	Loss/tok 2.8922 (3.1424)	LR 1.250e-04
0: TRAIN [5][1910/7762]	Time 0.256 (0.327)	Data 9.97e-05 (2.66e-04)	Tok/s 40437 (43118)	Loss/tok 2.9075 (3.1417)	LR 1.250e-04
0: TRAIN [5][1920/7762]	Time 0.263 (0.326)	Data 1.19e-04 (2.66e-04)	Tok/s 39909 (43112)	Loss/tok 2.8955 (3.1415)	LR 1.250e-04
0: TRAIN [5][1930/7762]	Time 0.261 (0.327)	Data 1.05e-04 (2.65e-04)	Tok/s 39209 (43128)	Loss/tok 2.9854 (3.1425)	LR 1.250e-04
0: TRAIN [5][1940/7762]	Time 0.468 (0.327)	Data 1.11e-04 (2.64e-04)	Tok/s 50158 (43139)	Loss/tok 3.2696 (3.1433)	LR 1.250e-04
0: TRAIN [5][1950/7762]	Time 0.179 (0.327)	Data 1.02e-04 (2.63e-04)	Tok/s 29549 (43133)	Loss/tok 2.5431 (3.1429)	LR 1.250e-04
0: TRAIN [5][1960/7762]	Time 0.460 (0.327)	Data 1.17e-04 (2.62e-04)	Tok/s 51245 (43150)	Loss/tok 3.4156 (3.1434)	LR 1.250e-04
0: TRAIN [5][1970/7762]	Time 0.263 (0.327)	Data 1.19e-04 (2.62e-04)	Tok/s 39111 (43148)	Loss/tok 2.9225 (3.1432)	LR 1.250e-04
0: TRAIN [5][1980/7762]	Time 0.268 (0.327)	Data 9.94e-05 (2.61e-04)	Tok/s 38599 (43147)	Loss/tok 2.9359 (3.1431)	LR 1.250e-04
0: TRAIN [5][1990/7762]	Time 0.451 (0.327)	Data 1.02e-04 (2.60e-04)	Tok/s 52379 (43165)	Loss/tok 3.2168 (3.1433)	LR 1.250e-04
0: TRAIN [5][2000/7762]	Time 0.576 (0.327)	Data 1.03e-04 (2.59e-04)	Tok/s 51139 (43176)	Loss/tok 3.6064 (3.1439)	LR 1.250e-04
0: TRAIN [5][2010/7762]	Time 0.259 (0.327)	Data 9.92e-05 (2.58e-04)	Tok/s 39783 (43164)	Loss/tok 2.9491 (3.1441)	LR 1.250e-04
0: TRAIN [5][2020/7762]	Time 0.463 (0.327)	Data 9.39e-05 (2.58e-04)	Tok/s 50413 (43166)	Loss/tok 3.3380 (3.1439)	LR 1.250e-04
0: TRAIN [5][2030/7762]	Time 0.260 (0.327)	Data 9.99e-05 (2.57e-04)	Tok/s 40166 (43135)	Loss/tok 3.0515 (3.1434)	LR 1.250e-04
0: TRAIN [5][2040/7762]	Time 0.364 (0.327)	Data 1.19e-04 (2.56e-04)	Tok/s 46258 (43132)	Loss/tok 3.1196 (3.1434)	LR 1.250e-04
0: TRAIN [5][2050/7762]	Time 0.268 (0.327)	Data 9.89e-05 (2.55e-04)	Tok/s 38418 (43124)	Loss/tok 2.8934 (3.1427)	LR 1.250e-04
0: TRAIN [5][2060/7762]	Time 0.250 (0.327)	Data 1.01e-04 (2.55e-04)	Tok/s 41216 (43115)	Loss/tok 3.0168 (3.1423)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][2070/7762]	Time 0.360 (0.327)	Data 9.89e-05 (2.54e-04)	Tok/s 47132 (43124)	Loss/tok 3.0737 (3.1425)	LR 1.250e-04
0: TRAIN [5][2080/7762]	Time 0.263 (0.327)	Data 1.00e-04 (2.53e-04)	Tok/s 38782 (43111)	Loss/tok 3.0215 (3.1421)	LR 1.250e-04
0: TRAIN [5][2090/7762]	Time 0.466 (0.327)	Data 9.99e-05 (2.53e-04)	Tok/s 50647 (43118)	Loss/tok 3.1938 (3.1420)	LR 1.250e-04
0: TRAIN [5][2100/7762]	Time 0.261 (0.327)	Data 1.06e-04 (2.52e-04)	Tok/s 39592 (43111)	Loss/tok 2.9668 (3.1418)	LR 1.250e-04
0: TRAIN [5][2110/7762]	Time 0.258 (0.327)	Data 9.75e-05 (2.51e-04)	Tok/s 39558 (43106)	Loss/tok 2.9099 (3.1415)	LR 1.250e-04
0: TRAIN [5][2120/7762]	Time 0.259 (0.327)	Data 1.03e-04 (2.51e-04)	Tok/s 39534 (43113)	Loss/tok 2.9346 (3.1419)	LR 1.250e-04
0: TRAIN [5][2130/7762]	Time 0.262 (0.326)	Data 1.04e-04 (2.50e-04)	Tok/s 39886 (43101)	Loss/tok 2.9233 (3.1415)	LR 1.250e-04
0: TRAIN [5][2140/7762]	Time 0.364 (0.326)	Data 9.97e-05 (2.49e-04)	Tok/s 45858 (43107)	Loss/tok 3.1995 (3.1414)	LR 1.250e-04
0: TRAIN [5][2150/7762]	Time 0.352 (0.326)	Data 1.04e-04 (2.49e-04)	Tok/s 47882 (43106)	Loss/tok 3.2183 (3.1414)	LR 1.250e-04
0: TRAIN [5][2160/7762]	Time 0.583 (0.327)	Data 1.26e-04 (2.48e-04)	Tok/s 51444 (43118)	Loss/tok 3.3560 (3.1418)	LR 1.250e-04
0: TRAIN [5][2170/7762]	Time 0.365 (0.327)	Data 1.03e-04 (2.47e-04)	Tok/s 45920 (43113)	Loss/tok 3.1573 (3.1415)	LR 1.250e-04
0: TRAIN [5][2180/7762]	Time 0.586 (0.327)	Data 1.00e-04 (2.47e-04)	Tok/s 50420 (43121)	Loss/tok 3.4880 (3.1421)	LR 1.250e-04
0: TRAIN [5][2190/7762]	Time 0.273 (0.327)	Data 1.20e-04 (2.46e-04)	Tok/s 37388 (43123)	Loss/tok 2.9368 (3.1422)	LR 1.250e-04
0: TRAIN [5][2200/7762]	Time 0.262 (0.327)	Data 9.66e-05 (2.45e-04)	Tok/s 39334 (43109)	Loss/tok 2.9101 (3.1416)	LR 1.250e-04
0: TRAIN [5][2210/7762]	Time 0.266 (0.326)	Data 1.05e-04 (2.45e-04)	Tok/s 38785 (43105)	Loss/tok 2.9239 (3.1414)	LR 1.250e-04
0: TRAIN [5][2220/7762]	Time 0.364 (0.326)	Data 9.82e-05 (2.44e-04)	Tok/s 46296 (43103)	Loss/tok 3.0221 (3.1410)	LR 1.250e-04
0: TRAIN [5][2230/7762]	Time 0.351 (0.326)	Data 1.01e-04 (2.43e-04)	Tok/s 48135 (43093)	Loss/tok 3.1112 (3.1408)	LR 1.250e-04
0: TRAIN [5][2240/7762]	Time 0.251 (0.326)	Data 1.03e-04 (2.43e-04)	Tok/s 40661 (43089)	Loss/tok 2.8892 (3.1405)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][2250/7762]	Time 0.259 (0.326)	Data 1.02e-04 (2.42e-04)	Tok/s 40487 (43088)	Loss/tok 2.9322 (3.1408)	LR 1.250e-04
0: TRAIN [5][2260/7762]	Time 0.364 (0.326)	Data 1.01e-04 (2.41e-04)	Tok/s 46811 (43088)	Loss/tok 3.1224 (3.1410)	LR 1.250e-04
0: TRAIN [5][2270/7762]	Time 0.365 (0.326)	Data 9.94e-05 (2.41e-04)	Tok/s 45815 (43087)	Loss/tok 3.2200 (3.1409)	LR 1.250e-04
0: TRAIN [5][2280/7762]	Time 0.263 (0.326)	Data 9.80e-05 (2.40e-04)	Tok/s 38470 (43080)	Loss/tok 3.0058 (3.1408)	LR 1.250e-04
0: TRAIN [5][2290/7762]	Time 0.179 (0.326)	Data 1.03e-04 (2.40e-04)	Tok/s 30133 (43079)	Loss/tok 2.5840 (3.1408)	LR 1.250e-04
0: TRAIN [5][2300/7762]	Time 0.461 (0.326)	Data 1.03e-04 (2.39e-04)	Tok/s 50649 (43081)	Loss/tok 3.4511 (3.1408)	LR 1.250e-04
0: TRAIN [5][2310/7762]	Time 0.585 (0.326)	Data 1.04e-04 (2.38e-04)	Tok/s 50643 (43102)	Loss/tok 3.5527 (3.1416)	LR 1.250e-04
0: TRAIN [5][2320/7762]	Time 0.255 (0.326)	Data 1.04e-04 (2.38e-04)	Tok/s 41417 (43102)	Loss/tok 2.8748 (3.1415)	LR 1.250e-04
0: TRAIN [5][2330/7762]	Time 0.454 (0.327)	Data 9.99e-05 (2.37e-04)	Tok/s 51547 (43116)	Loss/tok 3.3256 (3.1417)	LR 1.250e-04
0: TRAIN [5][2340/7762]	Time 0.344 (0.327)	Data 9.92e-05 (2.37e-04)	Tok/s 49366 (43110)	Loss/tok 2.9852 (3.1418)	LR 1.250e-04
0: TRAIN [5][2350/7762]	Time 0.175 (0.327)	Data 1.03e-04 (2.36e-04)	Tok/s 31072 (43110)	Loss/tok 2.5008 (3.1415)	LR 1.250e-04
0: TRAIN [5][2360/7762]	Time 0.254 (0.326)	Data 9.44e-05 (2.36e-04)	Tok/s 40192 (43100)	Loss/tok 2.9079 (3.1411)	LR 1.250e-04
0: TRAIN [5][2370/7762]	Time 0.367 (0.326)	Data 9.92e-05 (2.35e-04)	Tok/s 45873 (43095)	Loss/tok 3.1980 (3.1411)	LR 1.250e-04
0: TRAIN [5][2380/7762]	Time 0.585 (0.326)	Data 1.01e-04 (2.35e-04)	Tok/s 51279 (43086)	Loss/tok 3.4229 (3.1411)	LR 1.250e-04
0: TRAIN [5][2390/7762]	Time 0.460 (0.326)	Data 1.15e-04 (2.34e-04)	Tok/s 51070 (43092)	Loss/tok 3.2426 (3.1413)	LR 1.250e-04
0: TRAIN [5][2400/7762]	Time 0.259 (0.326)	Data 1.01e-04 (2.33e-04)	Tok/s 39646 (43086)	Loss/tok 2.8288 (3.1411)	LR 1.250e-04
0: TRAIN [5][2410/7762]	Time 0.435 (0.326)	Data 1.01e-04 (2.33e-04)	Tok/s 53129 (43082)	Loss/tok 3.3958 (3.1410)	LR 1.250e-04
0: TRAIN [5][2420/7762]	Time 0.266 (0.326)	Data 1.18e-04 (2.32e-04)	Tok/s 39000 (43077)	Loss/tok 2.9402 (3.1410)	LR 1.250e-04
0: TRAIN [5][2430/7762]	Time 0.358 (0.326)	Data 1.23e-04 (2.32e-04)	Tok/s 46649 (43080)	Loss/tok 3.2621 (3.1412)	LR 1.250e-04
0: TRAIN [5][2440/7762]	Time 0.362 (0.326)	Data 1.00e-04 (2.31e-04)	Tok/s 46440 (43087)	Loss/tok 3.1525 (3.1413)	LR 1.250e-04
0: TRAIN [5][2450/7762]	Time 0.266 (0.326)	Data 1.00e-04 (2.31e-04)	Tok/s 38769 (43090)	Loss/tok 2.8623 (3.1413)	LR 1.250e-04
0: TRAIN [5][2460/7762]	Time 0.262 (0.326)	Data 9.97e-05 (2.30e-04)	Tok/s 40201 (43092)	Loss/tok 2.9153 (3.1411)	LR 1.250e-04
0: TRAIN [5][2470/7762]	Time 0.367 (0.326)	Data 1.06e-04 (2.30e-04)	Tok/s 45745 (43100)	Loss/tok 3.1607 (3.1415)	LR 1.250e-04
0: TRAIN [5][2480/7762]	Time 0.350 (0.326)	Data 9.99e-05 (2.29e-04)	Tok/s 47673 (43096)	Loss/tok 3.1412 (3.1412)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][2490/7762]	Time 0.260 (0.326)	Data 1.01e-04 (2.29e-04)	Tok/s 39355 (43082)	Loss/tok 2.8644 (3.1410)	LR 1.250e-04
0: TRAIN [5][2500/7762]	Time 0.265 (0.326)	Data 2.03e-04 (2.28e-04)	Tok/s 38887 (43076)	Loss/tok 3.0099 (3.1411)	LR 1.250e-04
0: TRAIN [5][2510/7762]	Time 0.355 (0.326)	Data 9.89e-05 (2.28e-04)	Tok/s 47203 (43077)	Loss/tok 3.0861 (3.1412)	LR 1.250e-04
0: TRAIN [5][2520/7762]	Time 0.254 (0.326)	Data 1.00e-04 (2.27e-04)	Tok/s 40749 (43083)	Loss/tok 2.9819 (3.1414)	LR 1.250e-04
0: TRAIN [5][2530/7762]	Time 0.260 (0.326)	Data 1.03e-04 (2.27e-04)	Tok/s 39512 (43089)	Loss/tok 2.9836 (3.1417)	LR 1.250e-04
0: TRAIN [5][2540/7762]	Time 0.266 (0.326)	Data 9.94e-05 (2.27e-04)	Tok/s 38566 (43091)	Loss/tok 2.8584 (3.1419)	LR 1.250e-04
0: TRAIN [5][2550/7762]	Time 0.262 (0.326)	Data 1.00e-04 (2.27e-04)	Tok/s 39988 (43076)	Loss/tok 2.9167 (3.1417)	LR 1.250e-04
0: TRAIN [5][2560/7762]	Time 0.349 (0.326)	Data 1.04e-04 (2.26e-04)	Tok/s 48651 (43088)	Loss/tok 3.0529 (3.1417)	LR 1.250e-04
0: TRAIN [5][2570/7762]	Time 0.174 (0.326)	Data 1.04e-04 (2.26e-04)	Tok/s 30671 (43085)	Loss/tok 2.6654 (3.1417)	LR 1.250e-04
0: TRAIN [5][2580/7762]	Time 0.363 (0.326)	Data 9.70e-05 (2.25e-04)	Tok/s 45886 (43079)	Loss/tok 3.2862 (3.1418)	LR 1.250e-04
0: TRAIN [5][2590/7762]	Time 0.172 (0.326)	Data 1.01e-04 (2.25e-04)	Tok/s 31317 (43072)	Loss/tok 2.6487 (3.1414)	LR 1.250e-04
0: TRAIN [5][2600/7762]	Time 0.258 (0.326)	Data 1.01e-04 (2.24e-04)	Tok/s 39877 (43071)	Loss/tok 2.8853 (3.1412)	LR 1.250e-04
0: TRAIN [5][2610/7762]	Time 0.267 (0.326)	Data 9.92e-05 (2.24e-04)	Tok/s 38537 (43059)	Loss/tok 2.8617 (3.1407)	LR 1.250e-04
0: TRAIN [5][2620/7762]	Time 0.265 (0.326)	Data 9.75e-05 (2.23e-04)	Tok/s 38915 (43063)	Loss/tok 2.9418 (3.1406)	LR 1.250e-04
0: TRAIN [5][2630/7762]	Time 0.178 (0.326)	Data 1.05e-04 (2.23e-04)	Tok/s 29796 (43056)	Loss/tok 2.5367 (3.1406)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][2640/7762]	Time 0.265 (0.326)	Data 1.02e-04 (2.23e-04)	Tok/s 39607 (43063)	Loss/tok 2.9087 (3.1405)	LR 1.250e-04
0: TRAIN [5][2650/7762]	Time 0.359 (0.326)	Data 1.02e-04 (2.22e-04)	Tok/s 46649 (43053)	Loss/tok 3.1525 (3.1404)	LR 1.250e-04
0: TRAIN [5][2660/7762]	Time 0.458 (0.326)	Data 1.02e-04 (2.22e-04)	Tok/s 51131 (43061)	Loss/tok 3.3463 (3.1406)	LR 1.250e-04
0: TRAIN [5][2670/7762]	Time 0.179 (0.326)	Data 1.04e-04 (2.21e-04)	Tok/s 29381 (43052)	Loss/tok 2.6305 (3.1403)	LR 1.250e-04
0: TRAIN [5][2680/7762]	Time 0.354 (0.326)	Data 9.82e-05 (2.21e-04)	Tok/s 47004 (43055)	Loss/tok 3.1915 (3.1402)	LR 1.250e-04
0: TRAIN [5][2690/7762]	Time 0.452 (0.326)	Data 1.03e-04 (2.20e-04)	Tok/s 51804 (43053)	Loss/tok 3.2595 (3.1403)	LR 1.250e-04
0: TRAIN [5][2700/7762]	Time 0.361 (0.326)	Data 9.89e-05 (2.20e-04)	Tok/s 46850 (43059)	Loss/tok 3.1079 (3.1407)	LR 1.250e-04
0: TRAIN [5][2710/7762]	Time 0.265 (0.326)	Data 1.02e-04 (2.19e-04)	Tok/s 39037 (43060)	Loss/tok 2.9563 (3.1411)	LR 1.250e-04
0: TRAIN [5][2720/7762]	Time 0.260 (0.326)	Data 1.03e-04 (2.19e-04)	Tok/s 39438 (43053)	Loss/tok 3.0344 (3.1407)	LR 1.250e-04
0: TRAIN [5][2730/7762]	Time 0.358 (0.326)	Data 1.06e-04 (2.19e-04)	Tok/s 46230 (43050)	Loss/tok 3.2964 (3.1411)	LR 1.250e-04
0: TRAIN [5][2740/7762]	Time 0.263 (0.326)	Data 9.80e-05 (2.18e-04)	Tok/s 40320 (43055)	Loss/tok 3.1217 (3.1415)	LR 1.250e-04
0: TRAIN [5][2750/7762]	Time 0.264 (0.326)	Data 1.01e-04 (2.18e-04)	Tok/s 39302 (43056)	Loss/tok 3.1595 (3.1415)	LR 1.250e-04
0: TRAIN [5][2760/7762]	Time 0.258 (0.326)	Data 9.89e-05 (2.17e-04)	Tok/s 40672 (43059)	Loss/tok 2.9798 (3.1416)	LR 1.250e-04
0: TRAIN [5][2770/7762]	Time 0.360 (0.326)	Data 1.15e-04 (2.17e-04)	Tok/s 47123 (43058)	Loss/tok 3.1483 (3.1415)	LR 1.250e-04
0: TRAIN [5][2780/7762]	Time 0.365 (0.326)	Data 1.19e-04 (2.17e-04)	Tok/s 46079 (43069)	Loss/tok 3.1838 (3.1418)	LR 1.250e-04
0: TRAIN [5][2790/7762]	Time 0.177 (0.326)	Data 1.01e-04 (2.16e-04)	Tok/s 30518 (43069)	Loss/tok 2.5974 (3.1421)	LR 1.250e-04
0: TRAIN [5][2800/7762]	Time 0.268 (0.326)	Data 1.15e-04 (2.16e-04)	Tok/s 39211 (43071)	Loss/tok 3.0053 (3.1423)	LR 1.250e-04
0: TRAIN [5][2810/7762]	Time 0.362 (0.326)	Data 1.00e-04 (2.15e-04)	Tok/s 47186 (43075)	Loss/tok 3.1965 (3.1423)	LR 1.250e-04
0: TRAIN [5][2820/7762]	Time 0.262 (0.326)	Data 2.47e-04 (2.15e-04)	Tok/s 39611 (43071)	Loss/tok 2.9223 (3.1422)	LR 1.250e-04
0: TRAIN [5][2830/7762]	Time 0.451 (0.326)	Data 1.01e-04 (2.15e-04)	Tok/s 51352 (43079)	Loss/tok 3.3350 (3.1424)	LR 1.250e-04
0: TRAIN [5][2840/7762]	Time 0.432 (0.326)	Data 1.04e-04 (2.14e-04)	Tok/s 53762 (43086)	Loss/tok 3.2744 (3.1425)	LR 1.250e-04
0: TRAIN [5][2850/7762]	Time 0.263 (0.326)	Data 1.03e-04 (2.14e-04)	Tok/s 38138 (43081)	Loss/tok 2.9254 (3.1425)	LR 1.250e-04
0: TRAIN [5][2860/7762]	Time 0.588 (0.326)	Data 1.00e-04 (2.14e-04)	Tok/s 50473 (43086)	Loss/tok 3.5832 (3.1428)	LR 1.250e-04
0: TRAIN [5][2870/7762]	Time 0.583 (0.326)	Data 1.13e-04 (2.13e-04)	Tok/s 50724 (43088)	Loss/tok 3.5367 (3.1432)	LR 1.250e-04
0: TRAIN [5][2880/7762]	Time 0.268 (0.326)	Data 1.05e-04 (2.13e-04)	Tok/s 39019 (43084)	Loss/tok 2.9862 (3.1429)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [5][2890/7762]	Time 0.258 (0.327)	Data 1.05e-04 (2.13e-04)	Tok/s 40309 (43096)	Loss/tok 2.9836 (3.1436)	LR 1.250e-04
0: TRAIN [5][2900/7762]	Time 0.173 (0.327)	Data 1.01e-04 (2.12e-04)	Tok/s 30007 (43097)	Loss/tok 2.5317 (3.1437)	LR 1.250e-04
0: TRAIN [5][2910/7762]	Time 0.364 (0.327)	Data 9.94e-05 (2.12e-04)	Tok/s 46827 (43108)	Loss/tok 3.1263 (3.1440)	LR 1.250e-04
0: TRAIN [5][2920/7762]	Time 0.266 (0.327)	Data 1.03e-04 (2.11e-04)	Tok/s 38052 (43106)	Loss/tok 2.9019 (3.1437)	LR 1.250e-04
0: TRAIN [5][2930/7762]	Time 0.351 (0.327)	Data 1.03e-04 (2.11e-04)	Tok/s 47802 (43107)	Loss/tok 3.2082 (3.1438)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][2940/7762]	Time 0.264 (0.327)	Data 1.03e-04 (2.11e-04)	Tok/s 38631 (43109)	Loss/tok 2.8896 (3.1441)	LR 1.250e-04
0: TRAIN [5][2950/7762]	Time 0.347 (0.327)	Data 1.19e-04 (2.10e-04)	Tok/s 47939 (43107)	Loss/tok 3.2315 (3.1442)	LR 1.250e-04
0: TRAIN [5][2960/7762]	Time 0.265 (0.327)	Data 9.97e-05 (2.10e-04)	Tok/s 38943 (43090)	Loss/tok 3.0690 (3.1438)	LR 1.250e-04
0: TRAIN [5][2970/7762]	Time 0.254 (0.326)	Data 1.04e-04 (2.10e-04)	Tok/s 40555 (43079)	Loss/tok 2.9833 (3.1434)	LR 1.250e-04
0: TRAIN [5][2980/7762]	Time 0.262 (0.326)	Data 1.01e-04 (2.09e-04)	Tok/s 39023 (43088)	Loss/tok 2.8661 (3.1434)	LR 1.250e-04
0: TRAIN [5][2990/7762]	Time 0.259 (0.326)	Data 9.99e-05 (2.09e-04)	Tok/s 40097 (43074)	Loss/tok 2.9624 (3.1430)	LR 1.250e-04
0: TRAIN [5][3000/7762]	Time 0.174 (0.326)	Data 1.04e-04 (2.09e-04)	Tok/s 30595 (43070)	Loss/tok 2.6557 (3.1429)	LR 1.250e-04
0: TRAIN [5][3010/7762]	Time 0.175 (0.326)	Data 1.07e-04 (2.08e-04)	Tok/s 30221 (43070)	Loss/tok 2.5848 (3.1428)	LR 1.250e-04
0: TRAIN [5][3020/7762]	Time 0.260 (0.326)	Data 9.85e-05 (2.08e-04)	Tok/s 39610 (43065)	Loss/tok 2.9297 (3.1426)	LR 1.250e-04
0: TRAIN [5][3030/7762]	Time 0.448 (0.326)	Data 9.80e-05 (2.08e-04)	Tok/s 52029 (43072)	Loss/tok 3.2712 (3.1429)	LR 1.250e-04
0: TRAIN [5][3040/7762]	Time 0.452 (0.326)	Data 1.03e-04 (2.07e-04)	Tok/s 50821 (43080)	Loss/tok 3.3738 (3.1431)	LR 1.250e-04
0: TRAIN [5][3050/7762]	Time 0.174 (0.326)	Data 1.04e-04 (2.07e-04)	Tok/s 30853 (43066)	Loss/tok 2.5990 (3.1427)	LR 1.250e-04
0: TRAIN [5][3060/7762]	Time 0.349 (0.326)	Data 1.05e-04 (2.07e-04)	Tok/s 47601 (43056)	Loss/tok 3.1006 (3.1422)	LR 1.250e-04
0: TRAIN [5][3070/7762]	Time 0.261 (0.326)	Data 1.03e-04 (2.06e-04)	Tok/s 39586 (43046)	Loss/tok 2.9921 (3.1420)	LR 1.250e-04
0: TRAIN [5][3080/7762]	Time 0.355 (0.326)	Data 1.04e-04 (2.06e-04)	Tok/s 47108 (43052)	Loss/tok 3.0986 (3.1422)	LR 1.250e-04
0: TRAIN [5][3090/7762]	Time 0.260 (0.326)	Data 1.03e-04 (2.06e-04)	Tok/s 40673 (43050)	Loss/tok 3.0463 (3.1423)	LR 1.250e-04
0: TRAIN [5][3100/7762]	Time 0.176 (0.326)	Data 1.05e-04 (2.05e-04)	Tok/s 29766 (43049)	Loss/tok 2.5079 (3.1423)	LR 1.250e-04
0: TRAIN [5][3110/7762]	Time 0.265 (0.326)	Data 9.94e-05 (2.05e-04)	Tok/s 38306 (43041)	Loss/tok 2.9964 (3.1419)	LR 1.250e-04
0: TRAIN [5][3120/7762]	Time 0.266 (0.326)	Data 1.02e-04 (2.05e-04)	Tok/s 39664 (43043)	Loss/tok 2.8699 (3.1418)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][3130/7762]	Time 0.365 (0.326)	Data 1.05e-04 (2.04e-04)	Tok/s 45906 (43038)	Loss/tok 3.1053 (3.1416)	LR 1.250e-04
0: TRAIN [5][3140/7762]	Time 0.455 (0.326)	Data 1.35e-04 (2.04e-04)	Tok/s 50760 (43045)	Loss/tok 3.2801 (3.1419)	LR 1.250e-04
0: TRAIN [5][3150/7762]	Time 0.261 (0.326)	Data 1.04e-04 (2.04e-04)	Tok/s 40043 (43038)	Loss/tok 3.0415 (3.1417)	LR 1.250e-04
0: TRAIN [5][3160/7762]	Time 0.461 (0.326)	Data 1.04e-04 (2.03e-04)	Tok/s 49904 (43034)	Loss/tok 3.3486 (3.1417)	LR 1.250e-04
0: TRAIN [5][3170/7762]	Time 0.460 (0.326)	Data 1.00e-04 (2.03e-04)	Tok/s 50463 (43034)	Loss/tok 3.3923 (3.1415)	LR 1.250e-04
0: TRAIN [5][3180/7762]	Time 0.363 (0.326)	Data 1.05e-04 (2.03e-04)	Tok/s 46112 (43037)	Loss/tok 3.1651 (3.1417)	LR 1.250e-04
0: TRAIN [5][3190/7762]	Time 0.364 (0.326)	Data 1.02e-04 (2.03e-04)	Tok/s 46358 (43045)	Loss/tok 3.0992 (3.1420)	LR 1.250e-04
0: TRAIN [5][3200/7762]	Time 0.365 (0.326)	Data 1.18e-04 (2.02e-04)	Tok/s 46410 (43047)	Loss/tok 3.1096 (3.1419)	LR 1.250e-04
0: TRAIN [5][3210/7762]	Time 0.460 (0.326)	Data 9.92e-05 (2.02e-04)	Tok/s 50522 (43032)	Loss/tok 3.4585 (3.1418)	LR 1.250e-04
0: TRAIN [5][3220/7762]	Time 0.174 (0.326)	Data 1.02e-04 (2.02e-04)	Tok/s 30102 (43024)	Loss/tok 2.5149 (3.1416)	LR 1.250e-04
0: TRAIN [5][3230/7762]	Time 0.451 (0.325)	Data 1.00e-04 (2.01e-04)	Tok/s 51691 (43028)	Loss/tok 3.3583 (3.1416)	LR 1.250e-04
0: TRAIN [5][3240/7762]	Time 0.365 (0.326)	Data 1.01e-04 (2.01e-04)	Tok/s 46529 (43041)	Loss/tok 3.1177 (3.1424)	LR 1.250e-04
0: TRAIN [5][3250/7762]	Time 0.266 (0.326)	Data 9.54e-05 (2.01e-04)	Tok/s 38687 (43040)	Loss/tok 2.9260 (3.1423)	LR 1.250e-04
0: TRAIN [5][3260/7762]	Time 0.465 (0.326)	Data 1.00e-04 (2.00e-04)	Tok/s 50458 (43045)	Loss/tok 3.3288 (3.1424)	LR 1.250e-04
0: TRAIN [5][3270/7762]	Time 0.267 (0.326)	Data 1.07e-04 (2.00e-04)	Tok/s 38930 (43043)	Loss/tok 2.8223 (3.1422)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][3280/7762]	Time 0.262 (0.326)	Data 1.06e-04 (2.00e-04)	Tok/s 40514 (43051)	Loss/tok 2.9989 (3.1424)	LR 1.250e-04
0: TRAIN [5][3290/7762]	Time 0.174 (0.326)	Data 1.17e-04 (2.00e-04)	Tok/s 30983 (43047)	Loss/tok 2.5728 (3.1424)	LR 1.250e-04
0: TRAIN [5][3300/7762]	Time 0.352 (0.326)	Data 1.01e-04 (1.99e-04)	Tok/s 48355 (43050)	Loss/tok 3.1362 (3.1423)	LR 1.250e-04
0: TRAIN [5][3310/7762]	Time 0.344 (0.326)	Data 9.97e-05 (1.99e-04)	Tok/s 48010 (43047)	Loss/tok 3.1953 (3.1422)	LR 1.250e-04
0: TRAIN [5][3320/7762]	Time 0.168 (0.326)	Data 9.80e-05 (1.99e-04)	Tok/s 31149 (43028)	Loss/tok 2.5953 (3.1418)	LR 1.250e-04
0: TRAIN [5][3330/7762]	Time 0.448 (0.326)	Data 1.06e-04 (1.98e-04)	Tok/s 52146 (43027)	Loss/tok 3.2707 (3.1416)	LR 1.250e-04
0: TRAIN [5][3340/7762]	Time 0.459 (0.326)	Data 1.07e-04 (1.98e-04)	Tok/s 50855 (43040)	Loss/tok 3.3187 (3.1420)	LR 1.250e-04
0: TRAIN [5][3350/7762]	Time 0.260 (0.326)	Data 1.17e-04 (1.98e-04)	Tok/s 39921 (43030)	Loss/tok 2.9625 (3.1416)	LR 1.250e-04
0: TRAIN [5][3360/7762]	Time 0.263 (0.326)	Data 9.68e-05 (1.98e-04)	Tok/s 38094 (43035)	Loss/tok 2.9866 (3.1418)	LR 1.250e-04
0: TRAIN [5][3370/7762]	Time 0.265 (0.326)	Data 1.00e-04 (1.97e-04)	Tok/s 39681 (43034)	Loss/tok 2.7370 (3.1416)	LR 1.250e-04
0: TRAIN [5][3380/7762]	Time 0.259 (0.326)	Data 1.02e-04 (1.97e-04)	Tok/s 39864 (43036)	Loss/tok 2.9143 (3.1415)	LR 1.250e-04
0: TRAIN [5][3390/7762]	Time 0.354 (0.326)	Data 1.02e-04 (1.97e-04)	Tok/s 47733 (43041)	Loss/tok 3.1231 (3.1421)	LR 1.250e-04
0: TRAIN [5][3400/7762]	Time 0.465 (0.326)	Data 1.06e-04 (1.97e-04)	Tok/s 50128 (43043)	Loss/tok 3.2322 (3.1421)	LR 1.250e-04
0: TRAIN [5][3410/7762]	Time 0.356 (0.326)	Data 1.05e-04 (1.96e-04)	Tok/s 46699 (43047)	Loss/tok 3.1207 (3.1420)	LR 1.250e-04
0: TRAIN [5][3420/7762]	Time 0.261 (0.326)	Data 1.11e-04 (1.96e-04)	Tok/s 39691 (43053)	Loss/tok 2.9884 (3.1420)	LR 1.250e-04
0: TRAIN [5][3430/7762]	Time 0.261 (0.326)	Data 1.02e-04 (1.96e-04)	Tok/s 40165 (43053)	Loss/tok 2.9924 (3.1421)	LR 1.250e-04
0: TRAIN [5][3440/7762]	Time 0.346 (0.326)	Data 1.04e-04 (1.95e-04)	Tok/s 48723 (43054)	Loss/tok 3.2121 (3.1421)	LR 1.250e-04
0: TRAIN [5][3450/7762]	Time 0.356 (0.326)	Data 1.00e-04 (1.95e-04)	Tok/s 46785 (43055)	Loss/tok 3.1575 (3.1421)	LR 1.250e-04
0: TRAIN [5][3460/7762]	Time 0.177 (0.326)	Data 1.09e-04 (1.95e-04)	Tok/s 30182 (43054)	Loss/tok 2.4904 (3.1420)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][3470/7762]	Time 0.459 (0.326)	Data 1.03e-04 (1.95e-04)	Tok/s 51102 (43056)	Loss/tok 3.3054 (3.1422)	LR 1.250e-04
0: TRAIN [5][3480/7762]	Time 0.462 (0.326)	Data 1.03e-04 (1.94e-04)	Tok/s 50881 (43056)	Loss/tok 3.3517 (3.1422)	LR 1.250e-04
0: TRAIN [5][3490/7762]	Time 0.263 (0.326)	Data 9.97e-05 (1.94e-04)	Tok/s 38294 (43068)	Loss/tok 3.0097 (3.1425)	LR 1.250e-04
0: TRAIN [5][3500/7762]	Time 0.356 (0.326)	Data 9.75e-05 (1.94e-04)	Tok/s 47997 (43069)	Loss/tok 3.1552 (3.1424)	LR 1.250e-04
0: TRAIN [5][3510/7762]	Time 0.176 (0.326)	Data 1.02e-04 (1.94e-04)	Tok/s 29715 (43068)	Loss/tok 2.6981 (3.1424)	LR 1.250e-04
0: TRAIN [5][3520/7762]	Time 0.366 (0.326)	Data 1.03e-04 (1.93e-04)	Tok/s 45180 (43072)	Loss/tok 3.2204 (3.1425)	LR 1.250e-04
0: TRAIN [5][3530/7762]	Time 0.363 (0.326)	Data 1.20e-04 (1.93e-04)	Tok/s 45956 (43073)	Loss/tok 3.0803 (3.1427)	LR 1.250e-04
0: TRAIN [5][3540/7762]	Time 0.349 (0.326)	Data 9.39e-05 (1.93e-04)	Tok/s 47796 (43067)	Loss/tok 3.2330 (3.1424)	LR 1.250e-04
0: TRAIN [5][3550/7762]	Time 0.359 (0.326)	Data 1.01e-04 (1.93e-04)	Tok/s 46635 (43070)	Loss/tok 3.1488 (3.1427)	LR 1.250e-04
0: TRAIN [5][3560/7762]	Time 0.461 (0.326)	Data 9.87e-05 (1.92e-04)	Tok/s 50195 (43070)	Loss/tok 3.2809 (3.1428)	LR 1.250e-04
0: TRAIN [5][3570/7762]	Time 0.461 (0.326)	Data 9.82e-05 (1.92e-04)	Tok/s 50304 (43061)	Loss/tok 3.3003 (3.1425)	LR 1.250e-04
0: TRAIN [5][3580/7762]	Time 0.266 (0.326)	Data 1.16e-04 (1.92e-04)	Tok/s 38285 (43064)	Loss/tok 2.9867 (3.1426)	LR 1.250e-04
0: TRAIN [5][3590/7762]	Time 0.356 (0.326)	Data 9.58e-05 (1.92e-04)	Tok/s 46864 (43064)	Loss/tok 3.0647 (3.1424)	LR 1.250e-04
0: TRAIN [5][3600/7762]	Time 0.263 (0.326)	Data 1.04e-04 (1.91e-04)	Tok/s 38657 (43067)	Loss/tok 2.9327 (3.1425)	LR 1.250e-04
0: TRAIN [5][3610/7762]	Time 0.363 (0.326)	Data 1.07e-04 (1.91e-04)	Tok/s 45693 (43067)	Loss/tok 3.1467 (3.1423)	LR 1.250e-04
0: TRAIN [5][3620/7762]	Time 0.266 (0.326)	Data 1.07e-04 (1.91e-04)	Tok/s 38789 (43066)	Loss/tok 2.9028 (3.1424)	LR 1.250e-04
0: TRAIN [5][3630/7762]	Time 0.360 (0.326)	Data 9.94e-05 (1.91e-04)	Tok/s 46481 (43063)	Loss/tok 3.2332 (3.1423)	LR 1.250e-04
0: TRAIN [5][3640/7762]	Time 0.457 (0.326)	Data 1.01e-04 (1.90e-04)	Tok/s 51484 (43057)	Loss/tok 3.2649 (3.1421)	LR 1.250e-04
0: TRAIN [5][3650/7762]	Time 0.364 (0.326)	Data 9.82e-05 (1.90e-04)	Tok/s 45885 (43059)	Loss/tok 3.0800 (3.1422)	LR 1.250e-04
0: TRAIN [5][3660/7762]	Time 0.575 (0.326)	Data 1.05e-04 (1.90e-04)	Tok/s 52440 (43069)	Loss/tok 3.3346 (3.1425)	LR 1.250e-04
0: TRAIN [5][3670/7762]	Time 0.264 (0.326)	Data 1.03e-04 (1.90e-04)	Tok/s 39955 (43070)	Loss/tok 2.9020 (3.1424)	LR 1.250e-04
0: TRAIN [5][3680/7762]	Time 0.363 (0.326)	Data 1.00e-04 (1.89e-04)	Tok/s 46463 (43070)	Loss/tok 3.1360 (3.1427)	LR 1.250e-04
0: TRAIN [5][3690/7762]	Time 0.263 (0.326)	Data 1.08e-04 (1.89e-04)	Tok/s 39584 (43078)	Loss/tok 2.9664 (3.1429)	LR 1.250e-04
0: TRAIN [5][3700/7762]	Time 0.363 (0.326)	Data 1.24e-04 (1.89e-04)	Tok/s 46486 (43082)	Loss/tok 3.1716 (3.1428)	LR 1.250e-04
0: TRAIN [5][3710/7762]	Time 0.267 (0.326)	Data 1.01e-04 (1.89e-04)	Tok/s 39654 (43083)	Loss/tok 2.9082 (3.1428)	LR 1.250e-04
0: TRAIN [5][3720/7762]	Time 0.367 (0.326)	Data 1.06e-04 (1.89e-04)	Tok/s 45717 (43089)	Loss/tok 3.1531 (3.1430)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [5][3730/7762]	Time 0.269 (0.326)	Data 9.75e-05 (1.88e-04)	Tok/s 38481 (43092)	Loss/tok 3.0027 (3.1432)	LR 1.250e-04
0: TRAIN [5][3740/7762]	Time 0.366 (0.327)	Data 9.73e-05 (1.88e-04)	Tok/s 45483 (43091)	Loss/tok 3.1679 (3.1433)	LR 1.250e-04
0: TRAIN [5][3750/7762]	Time 0.364 (0.326)	Data 1.00e-04 (1.88e-04)	Tok/s 45785 (43081)	Loss/tok 3.1057 (3.1432)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][3760/7762]	Time 0.464 (0.326)	Data 9.47e-05 (1.88e-04)	Tok/s 50335 (43081)	Loss/tok 3.2764 (3.1432)	LR 1.250e-04
0: TRAIN [5][3770/7762]	Time 0.366 (0.326)	Data 1.04e-04 (1.87e-04)	Tok/s 46071 (43081)	Loss/tok 3.1771 (3.1432)	LR 1.250e-04
0: TRAIN [5][3780/7762]	Time 0.175 (0.326)	Data 1.13e-04 (1.87e-04)	Tok/s 29751 (43083)	Loss/tok 2.5390 (3.1434)	LR 1.250e-04
0: TRAIN [5][3790/7762]	Time 0.454 (0.326)	Data 1.01e-04 (1.87e-04)	Tok/s 51321 (43077)	Loss/tok 3.2718 (3.1432)	LR 1.250e-04
0: TRAIN [5][3800/7762]	Time 0.349 (0.326)	Data 1.04e-04 (1.87e-04)	Tok/s 47519 (43082)	Loss/tok 3.1276 (3.1432)	LR 1.250e-04
0: TRAIN [5][3810/7762]	Time 0.266 (0.326)	Data 1.10e-04 (1.87e-04)	Tok/s 38969 (43083)	Loss/tok 2.9955 (3.1433)	LR 1.250e-04
0: TRAIN [5][3820/7762]	Time 0.465 (0.326)	Data 1.16e-04 (1.86e-04)	Tok/s 49714 (43077)	Loss/tok 3.2489 (3.1432)	LR 1.250e-04
0: TRAIN [5][3830/7762]	Time 0.457 (0.327)	Data 1.06e-04 (1.86e-04)	Tok/s 50310 (43088)	Loss/tok 3.4151 (3.1436)	LR 1.250e-04
0: TRAIN [5][3840/7762]	Time 0.343 (0.327)	Data 1.19e-04 (1.86e-04)	Tok/s 49608 (43089)	Loss/tok 3.1381 (3.1435)	LR 1.250e-04
0: TRAIN [5][3850/7762]	Time 0.266 (0.327)	Data 1.21e-04 (1.86e-04)	Tok/s 38447 (43092)	Loss/tok 2.9624 (3.1437)	LR 1.250e-04
0: TRAIN [5][3860/7762]	Time 0.464 (0.327)	Data 1.06e-04 (1.86e-04)	Tok/s 50254 (43101)	Loss/tok 3.2819 (3.1439)	LR 1.250e-04
0: TRAIN [5][3870/7762]	Time 0.436 (0.327)	Data 1.07e-04 (1.85e-04)	Tok/s 53915 (43101)	Loss/tok 3.3536 (3.1439)	LR 1.250e-04
0: TRAIN [5][3880/7762]	Time 0.179 (0.327)	Data 1.01e-04 (1.85e-04)	Tok/s 29395 (43098)	Loss/tok 2.5835 (3.1440)	LR 1.250e-04
0: TRAIN [5][3890/7762]	Time 0.466 (0.327)	Data 1.17e-04 (1.85e-04)	Tok/s 49714 (43100)	Loss/tok 3.3027 (3.1441)	LR 1.250e-04
0: TRAIN [5][3900/7762]	Time 0.459 (0.327)	Data 1.01e-04 (1.85e-04)	Tok/s 51206 (43100)	Loss/tok 3.2679 (3.1440)	LR 1.250e-04
0: TRAIN [5][3910/7762]	Time 0.359 (0.327)	Data 1.04e-04 (1.85e-04)	Tok/s 47232 (43098)	Loss/tok 3.1442 (3.1438)	LR 1.250e-04
0: TRAIN [5][3920/7762]	Time 0.263 (0.327)	Data 1.62e-04 (1.84e-04)	Tok/s 39528 (43098)	Loss/tok 2.9685 (3.1438)	LR 1.250e-04
0: TRAIN [5][3930/7762]	Time 0.364 (0.327)	Data 1.03e-04 (1.84e-04)	Tok/s 46248 (43097)	Loss/tok 3.1242 (3.1437)	LR 1.250e-04
0: TRAIN [5][3940/7762]	Time 0.263 (0.327)	Data 1.00e-04 (1.84e-04)	Tok/s 39134 (43098)	Loss/tok 2.9546 (3.1435)	LR 1.250e-04
0: TRAIN [5][3950/7762]	Time 0.269 (0.327)	Data 9.73e-05 (1.84e-04)	Tok/s 38210 (43096)	Loss/tok 3.0071 (3.1433)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][3960/7762]	Time 0.264 (0.327)	Data 1.04e-04 (1.84e-04)	Tok/s 38933 (43087)	Loss/tok 2.9898 (3.1434)	LR 1.250e-04
0: TRAIN [5][3970/7762]	Time 0.264 (0.327)	Data 1.21e-04 (1.83e-04)	Tok/s 39087 (43086)	Loss/tok 3.0954 (3.1433)	LR 1.250e-04
0: TRAIN [5][3980/7762]	Time 0.268 (0.327)	Data 1.13e-04 (1.83e-04)	Tok/s 38305 (43087)	Loss/tok 2.8923 (3.1432)	LR 1.250e-04
0: TRAIN [5][3990/7762]	Time 0.356 (0.326)	Data 9.87e-05 (1.83e-04)	Tok/s 47314 (43079)	Loss/tok 3.1363 (3.1430)	LR 1.250e-04
0: TRAIN [5][4000/7762]	Time 0.362 (0.326)	Data 9.87e-05 (1.83e-04)	Tok/s 46025 (43082)	Loss/tok 3.2755 (3.1430)	LR 1.250e-04
0: TRAIN [5][4010/7762]	Time 0.355 (0.326)	Data 9.89e-05 (1.83e-04)	Tok/s 47275 (43077)	Loss/tok 3.0757 (3.1430)	LR 1.250e-04
0: TRAIN [5][4020/7762]	Time 0.263 (0.326)	Data 9.32e-05 (1.82e-04)	Tok/s 39202 (43068)	Loss/tok 2.9151 (3.1427)	LR 1.250e-04
0: TRAIN [5][4030/7762]	Time 0.266 (0.326)	Data 1.01e-04 (1.82e-04)	Tok/s 39184 (43063)	Loss/tok 2.8917 (3.1425)	LR 1.250e-04
0: TRAIN [5][4040/7762]	Time 0.266 (0.326)	Data 9.89e-05 (1.82e-04)	Tok/s 38961 (43061)	Loss/tok 3.0241 (3.1425)	LR 1.250e-04
0: TRAIN [5][4050/7762]	Time 0.263 (0.326)	Data 1.17e-04 (1.82e-04)	Tok/s 39416 (43060)	Loss/tok 2.9220 (3.1424)	LR 1.250e-04
0: TRAIN [5][4060/7762]	Time 0.174 (0.326)	Data 1.16e-04 (1.82e-04)	Tok/s 30383 (43050)	Loss/tok 2.5998 (3.1422)	LR 1.250e-04
0: TRAIN [5][4070/7762]	Time 0.362 (0.326)	Data 1.08e-04 (1.82e-04)	Tok/s 46619 (43049)	Loss/tok 3.1654 (3.1420)	LR 1.250e-04
0: TRAIN [5][4080/7762]	Time 0.265 (0.326)	Data 1.04e-04 (1.81e-04)	Tok/s 39634 (43048)	Loss/tok 3.0280 (3.1420)	LR 1.250e-04
0: TRAIN [5][4090/7762]	Time 0.448 (0.326)	Data 9.66e-05 (1.81e-04)	Tok/s 51488 (43043)	Loss/tok 3.2952 (3.1420)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][4100/7762]	Time 0.461 (0.326)	Data 1.16e-04 (1.81e-04)	Tok/s 50825 (43051)	Loss/tok 3.3230 (3.1423)	LR 1.250e-04
0: TRAIN [5][4110/7762]	Time 0.342 (0.326)	Data 1.15e-04 (1.81e-04)	Tok/s 49427 (43051)	Loss/tok 3.0466 (3.1423)	LR 1.250e-04
0: TRAIN [5][4120/7762]	Time 0.261 (0.326)	Data 1.02e-04 (1.81e-04)	Tok/s 39216 (43044)	Loss/tok 2.8342 (3.1420)	LR 1.250e-04
0: TRAIN [5][4130/7762]	Time 0.259 (0.326)	Data 1.05e-04 (1.80e-04)	Tok/s 39478 (43042)	Loss/tok 2.9333 (3.1420)	LR 1.250e-04
0: TRAIN [5][4140/7762]	Time 0.264 (0.326)	Data 1.17e-04 (1.80e-04)	Tok/s 38938 (43041)	Loss/tok 2.9443 (3.1419)	LR 1.250e-04
0: TRAIN [5][4150/7762]	Time 0.259 (0.326)	Data 1.04e-04 (1.80e-04)	Tok/s 39696 (43041)	Loss/tok 2.9251 (3.1419)	LR 1.250e-04
0: TRAIN [5][4160/7762]	Time 0.355 (0.326)	Data 1.03e-04 (1.80e-04)	Tok/s 47220 (43038)	Loss/tok 3.1478 (3.1418)	LR 1.250e-04
0: TRAIN [5][4170/7762]	Time 0.260 (0.326)	Data 1.17e-04 (1.80e-04)	Tok/s 39553 (43041)	Loss/tok 2.9190 (3.1418)	LR 1.250e-04
0: TRAIN [5][4180/7762]	Time 0.351 (0.326)	Data 1.03e-04 (1.80e-04)	Tok/s 47940 (43050)	Loss/tok 3.1227 (3.1418)	LR 1.250e-04
0: TRAIN [5][4190/7762]	Time 0.265 (0.326)	Data 1.00e-04 (1.79e-04)	Tok/s 39388 (43053)	Loss/tok 2.9397 (3.1418)	LR 1.250e-04
0: TRAIN [5][4200/7762]	Time 0.170 (0.326)	Data 1.02e-04 (1.79e-04)	Tok/s 29766 (43055)	Loss/tok 2.5379 (3.1418)	LR 1.250e-04
0: TRAIN [5][4210/7762]	Time 0.266 (0.326)	Data 1.01e-04 (1.79e-04)	Tok/s 39805 (43053)	Loss/tok 2.8864 (3.1419)	LR 1.250e-04
0: TRAIN [5][4220/7762]	Time 0.447 (0.326)	Data 9.85e-05 (1.79e-04)	Tok/s 51969 (43059)	Loss/tok 3.3859 (3.1422)	LR 1.250e-04
0: TRAIN [5][4230/7762]	Time 0.462 (0.326)	Data 9.99e-05 (1.79e-04)	Tok/s 50295 (43058)	Loss/tok 3.4431 (3.1422)	LR 1.250e-04
0: TRAIN [5][4240/7762]	Time 0.262 (0.326)	Data 9.92e-05 (1.78e-04)	Tok/s 39826 (43060)	Loss/tok 2.9662 (3.1421)	LR 1.250e-04
0: TRAIN [5][4250/7762]	Time 0.264 (0.326)	Data 9.54e-05 (1.78e-04)	Tok/s 38320 (43057)	Loss/tok 2.9804 (3.1420)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][4260/7762]	Time 0.177 (0.326)	Data 9.58e-05 (1.78e-04)	Tok/s 29844 (43054)	Loss/tok 2.6371 (3.1419)	LR 1.250e-04
0: TRAIN [5][4270/7762]	Time 0.265 (0.326)	Data 1.10e-04 (1.78e-04)	Tok/s 39246 (43053)	Loss/tok 2.9664 (3.1419)	LR 1.250e-04
0: TRAIN [5][4280/7762]	Time 0.264 (0.326)	Data 1.01e-04 (1.78e-04)	Tok/s 38983 (43054)	Loss/tok 2.9994 (3.1417)	LR 1.250e-04
0: TRAIN [5][4290/7762]	Time 0.171 (0.326)	Data 1.00e-04 (1.78e-04)	Tok/s 30528 (43047)	Loss/tok 2.5291 (3.1415)	LR 1.250e-04
0: TRAIN [5][4300/7762]	Time 0.455 (0.326)	Data 1.01e-04 (1.77e-04)	Tok/s 50718 (43054)	Loss/tok 3.3355 (3.1416)	LR 1.250e-04
0: TRAIN [5][4310/7762]	Time 0.264 (0.326)	Data 1.01e-04 (1.77e-04)	Tok/s 38578 (43053)	Loss/tok 2.9010 (3.1415)	LR 1.250e-04
0: TRAIN [5][4320/7762]	Time 0.264 (0.326)	Data 1.07e-04 (1.77e-04)	Tok/s 39802 (43056)	Loss/tok 2.8729 (3.1416)	LR 1.250e-04
0: TRAIN [5][4330/7762]	Time 0.367 (0.326)	Data 1.03e-04 (1.77e-04)	Tok/s 45520 (43060)	Loss/tok 3.1746 (3.1416)	LR 1.250e-04
0: TRAIN [5][4340/7762]	Time 0.557 (0.326)	Data 9.70e-05 (1.77e-04)	Tok/s 53616 (43055)	Loss/tok 3.4500 (3.1414)	LR 1.250e-04
0: TRAIN [5][4350/7762]	Time 0.261 (0.326)	Data 9.61e-05 (1.77e-04)	Tok/s 40082 (43052)	Loss/tok 2.9151 (3.1413)	LR 1.250e-04
0: TRAIN [5][4360/7762]	Time 0.347 (0.326)	Data 1.17e-04 (1.76e-04)	Tok/s 48630 (43044)	Loss/tok 3.2308 (3.1413)	LR 1.250e-04
0: TRAIN [5][4370/7762]	Time 0.261 (0.325)	Data 1.00e-04 (1.76e-04)	Tok/s 40458 (43036)	Loss/tok 2.9183 (3.1410)	LR 1.250e-04
0: TRAIN [5][4380/7762]	Time 0.174 (0.325)	Data 1.12e-04 (1.76e-04)	Tok/s 30831 (43025)	Loss/tok 2.5362 (3.1409)	LR 1.250e-04
0: TRAIN [5][4390/7762]	Time 0.362 (0.325)	Data 1.02e-04 (1.76e-04)	Tok/s 47401 (43026)	Loss/tok 3.0825 (3.1408)	LR 1.250e-04
0: TRAIN [5][4400/7762]	Time 0.172 (0.325)	Data 9.49e-05 (1.76e-04)	Tok/s 30557 (43021)	Loss/tok 2.6199 (3.1407)	LR 1.250e-04
0: TRAIN [5][4410/7762]	Time 0.260 (0.325)	Data 1.05e-04 (1.76e-04)	Tok/s 40055 (43016)	Loss/tok 2.8879 (3.1408)	LR 1.250e-04
0: TRAIN [5][4420/7762]	Time 0.553 (0.325)	Data 9.85e-05 (1.75e-04)	Tok/s 54165 (43016)	Loss/tok 3.4819 (3.1408)	LR 1.250e-04
0: TRAIN [5][4430/7762]	Time 0.361 (0.325)	Data 9.97e-05 (1.75e-04)	Tok/s 46431 (43019)	Loss/tok 3.2923 (3.1408)	LR 1.250e-04
0: TRAIN [5][4440/7762]	Time 0.262 (0.325)	Data 1.03e-04 (1.75e-04)	Tok/s 39570 (43017)	Loss/tok 2.9329 (3.1408)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][4450/7762]	Time 0.341 (0.325)	Data 9.94e-05 (1.75e-04)	Tok/s 49878 (43025)	Loss/tok 3.1681 (3.1412)	LR 1.250e-04
0: TRAIN [5][4460/7762]	Time 0.268 (0.325)	Data 1.05e-04 (1.75e-04)	Tok/s 39415 (43021)	Loss/tok 2.9192 (3.1413)	LR 1.250e-04
0: TRAIN [5][4470/7762]	Time 0.460 (0.325)	Data 9.99e-05 (1.75e-04)	Tok/s 50741 (43020)	Loss/tok 3.3445 (3.1413)	LR 1.250e-04
0: TRAIN [5][4480/7762]	Time 0.265 (0.325)	Data 9.78e-05 (1.74e-04)	Tok/s 38657 (43018)	Loss/tok 2.9773 (3.1413)	LR 1.250e-04
0: TRAIN [5][4490/7762]	Time 0.469 (0.325)	Data 9.51e-05 (1.74e-04)	Tok/s 49543 (43018)	Loss/tok 3.4346 (3.1415)	LR 1.250e-04
0: TRAIN [5][4500/7762]	Time 0.444 (0.325)	Data 9.75e-05 (1.74e-04)	Tok/s 52307 (43021)	Loss/tok 3.3020 (3.1414)	LR 1.250e-04
0: TRAIN [5][4510/7762]	Time 0.454 (0.325)	Data 1.16e-04 (1.74e-04)	Tok/s 51435 (43017)	Loss/tok 3.2709 (3.1413)	LR 1.250e-04
0: TRAIN [5][4520/7762]	Time 0.464 (0.325)	Data 1.01e-04 (1.74e-04)	Tok/s 50126 (43016)	Loss/tok 3.2538 (3.1412)	LR 1.250e-04
0: TRAIN [5][4530/7762]	Time 0.436 (0.325)	Data 1.07e-04 (1.74e-04)	Tok/s 52776 (43021)	Loss/tok 3.3146 (3.1415)	LR 1.250e-04
0: TRAIN [5][4540/7762]	Time 0.364 (0.326)	Data 1.05e-04 (1.74e-04)	Tok/s 46276 (43026)	Loss/tok 3.1209 (3.1417)	LR 1.250e-04
0: TRAIN [5][4550/7762]	Time 0.358 (0.326)	Data 1.04e-04 (1.73e-04)	Tok/s 47199 (43030)	Loss/tok 3.1609 (3.1421)	LR 1.250e-04
0: TRAIN [5][4560/7762]	Time 0.253 (0.326)	Data 9.44e-05 (1.73e-04)	Tok/s 40843 (43021)	Loss/tok 3.0310 (3.1418)	LR 1.250e-04
0: TRAIN [5][4570/7762]	Time 0.260 (0.326)	Data 1.01e-04 (1.73e-04)	Tok/s 40286 (43027)	Loss/tok 2.9162 (3.1420)	LR 1.250e-04
0: TRAIN [5][4580/7762]	Time 0.366 (0.326)	Data 9.68e-05 (1.73e-04)	Tok/s 46504 (43020)	Loss/tok 3.0545 (3.1418)	LR 1.250e-04
0: TRAIN [5][4590/7762]	Time 0.364 (0.326)	Data 1.20e-04 (1.73e-04)	Tok/s 46002 (43014)	Loss/tok 3.1314 (3.1418)	LR 1.250e-04
0: TRAIN [5][4600/7762]	Time 0.251 (0.326)	Data 1.22e-04 (1.73e-04)	Tok/s 40613 (43016)	Loss/tok 2.8872 (3.1422)	LR 1.250e-04
0: TRAIN [5][4610/7762]	Time 0.463 (0.326)	Data 9.92e-05 (1.72e-04)	Tok/s 50584 (43021)	Loss/tok 3.2553 (3.1426)	LR 1.250e-04
0: TRAIN [5][4620/7762]	Time 0.253 (0.326)	Data 1.01e-04 (1.72e-04)	Tok/s 40746 (43029)	Loss/tok 2.9737 (3.1428)	LR 1.250e-04
0: TRAIN [5][4630/7762]	Time 0.355 (0.326)	Data 1.03e-04 (1.72e-04)	Tok/s 47146 (43033)	Loss/tok 3.1860 (3.1428)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][4640/7762]	Time 0.270 (0.326)	Data 1.03e-04 (1.72e-04)	Tok/s 38728 (43034)	Loss/tok 3.1294 (3.1431)	LR 1.250e-04
0: TRAIN [5][4650/7762]	Time 0.589 (0.326)	Data 1.00e-04 (1.72e-04)	Tok/s 50852 (43038)	Loss/tok 3.5510 (3.1432)	LR 1.250e-04
0: TRAIN [5][4660/7762]	Time 0.253 (0.326)	Data 9.82e-05 (1.72e-04)	Tok/s 41752 (43030)	Loss/tok 2.9493 (3.1429)	LR 1.250e-04
0: TRAIN [5][4670/7762]	Time 0.576 (0.326)	Data 1.03e-04 (1.72e-04)	Tok/s 52701 (43031)	Loss/tok 3.3438 (3.1429)	LR 1.250e-04
0: TRAIN [5][4680/7762]	Time 0.267 (0.326)	Data 1.21e-04 (1.72e-04)	Tok/s 37991 (43028)	Loss/tok 2.8869 (3.1430)	LR 1.250e-04
0: TRAIN [5][4690/7762]	Time 0.176 (0.326)	Data 9.63e-05 (1.71e-04)	Tok/s 29131 (43025)	Loss/tok 2.4581 (3.1428)	LR 1.250e-04
0: TRAIN [5][4700/7762]	Time 0.264 (0.326)	Data 1.00e-04 (1.71e-04)	Tok/s 40071 (43022)	Loss/tok 2.9432 (3.1426)	LR 1.250e-04
0: TRAIN [5][4710/7762]	Time 0.356 (0.326)	Data 9.78e-05 (1.71e-04)	Tok/s 47718 (43022)	Loss/tok 3.0423 (3.1425)	LR 1.250e-04
0: TRAIN [5][4720/7762]	Time 0.257 (0.326)	Data 9.35e-05 (1.71e-04)	Tok/s 40180 (43013)	Loss/tok 2.9761 (3.1423)	LR 1.250e-04
0: TRAIN [5][4730/7762]	Time 0.366 (0.326)	Data 9.97e-05 (1.71e-04)	Tok/s 45901 (43018)	Loss/tok 3.2071 (3.1425)	LR 1.250e-04
0: TRAIN [5][4740/7762]	Time 0.262 (0.326)	Data 1.02e-04 (1.71e-04)	Tok/s 39031 (43015)	Loss/tok 2.9859 (3.1423)	LR 1.250e-04
0: TRAIN [5][4750/7762]	Time 0.361 (0.326)	Data 1.01e-04 (1.71e-04)	Tok/s 46043 (43015)	Loss/tok 3.0900 (3.1422)	LR 1.250e-04
0: TRAIN [5][4760/7762]	Time 0.259 (0.326)	Data 9.75e-05 (1.70e-04)	Tok/s 40602 (43017)	Loss/tok 3.0243 (3.1422)	LR 1.250e-04
0: TRAIN [5][4770/7762]	Time 0.264 (0.326)	Data 1.28e-04 (1.70e-04)	Tok/s 38673 (43013)	Loss/tok 2.9679 (3.1423)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][4780/7762]	Time 0.174 (0.326)	Data 9.51e-05 (1.70e-04)	Tok/s 30273 (43007)	Loss/tok 2.6320 (3.1422)	LR 1.250e-04
0: TRAIN [5][4790/7762]	Time 0.261 (0.326)	Data 9.94e-05 (1.70e-04)	Tok/s 39022 (43009)	Loss/tok 2.9011 (3.1423)	LR 1.250e-04
0: TRAIN [5][4800/7762]	Time 0.365 (0.326)	Data 1.01e-04 (1.70e-04)	Tok/s 46481 (43011)	Loss/tok 3.1554 (3.1425)	LR 1.250e-04
0: TRAIN [5][4810/7762]	Time 0.258 (0.326)	Data 1.01e-04 (1.70e-04)	Tok/s 40086 (43014)	Loss/tok 3.0757 (3.1427)	LR 1.250e-04
0: TRAIN [5][4820/7762]	Time 0.365 (0.326)	Data 9.80e-05 (1.70e-04)	Tok/s 45834 (43007)	Loss/tok 3.1938 (3.1425)	LR 1.250e-04
0: TRAIN [5][4830/7762]	Time 0.452 (0.326)	Data 9.51e-05 (1.69e-04)	Tok/s 52200 (43012)	Loss/tok 3.3157 (3.1427)	LR 1.250e-04
0: TRAIN [5][4840/7762]	Time 0.265 (0.326)	Data 1.19e-04 (1.69e-04)	Tok/s 38198 (43009)	Loss/tok 2.8568 (3.1425)	LR 1.250e-04
0: TRAIN [5][4850/7762]	Time 0.363 (0.325)	Data 9.87e-05 (1.69e-04)	Tok/s 45969 (42999)	Loss/tok 3.0560 (3.1423)	LR 1.250e-04
0: TRAIN [5][4860/7762]	Time 0.460 (0.325)	Data 1.01e-04 (1.69e-04)	Tok/s 50450 (42998)	Loss/tok 3.3223 (3.1423)	LR 1.250e-04
0: TRAIN [5][4870/7762]	Time 0.264 (0.325)	Data 9.89e-05 (1.69e-04)	Tok/s 40265 (42992)	Loss/tok 2.8433 (3.1420)	LR 1.250e-04
0: TRAIN [5][4880/7762]	Time 0.265 (0.325)	Data 9.78e-05 (1.69e-04)	Tok/s 39043 (42990)	Loss/tok 3.0363 (3.1420)	LR 1.250e-04
0: TRAIN [5][4890/7762]	Time 0.266 (0.325)	Data 1.18e-04 (1.69e-04)	Tok/s 39451 (42991)	Loss/tok 3.0221 (3.1419)	LR 1.250e-04
0: TRAIN [5][4900/7762]	Time 0.362 (0.325)	Data 1.01e-04 (1.68e-04)	Tok/s 45932 (42990)	Loss/tok 3.2780 (3.1418)	LR 1.250e-04
0: TRAIN [5][4910/7762]	Time 0.256 (0.325)	Data 1.00e-04 (1.68e-04)	Tok/s 41638 (42988)	Loss/tok 2.8745 (3.1416)	LR 1.250e-04
0: TRAIN [5][4920/7762]	Time 0.176 (0.325)	Data 9.66e-05 (1.68e-04)	Tok/s 29900 (42984)	Loss/tok 2.5281 (3.1415)	LR 1.250e-04
0: TRAIN [5][4930/7762]	Time 0.354 (0.325)	Data 1.06e-04 (1.68e-04)	Tok/s 47433 (42989)	Loss/tok 3.1686 (3.1416)	LR 1.250e-04
0: TRAIN [5][4940/7762]	Time 0.263 (0.325)	Data 9.80e-05 (1.68e-04)	Tok/s 38812 (42990)	Loss/tok 2.8660 (3.1416)	LR 1.250e-04
0: TRAIN [5][4950/7762]	Time 0.176 (0.325)	Data 2.33e-04 (1.68e-04)	Tok/s 29413 (42987)	Loss/tok 2.5944 (3.1415)	LR 1.250e-04
0: TRAIN [5][4960/7762]	Time 0.257 (0.325)	Data 1.03e-04 (1.68e-04)	Tok/s 40605 (42986)	Loss/tok 2.9671 (3.1414)	LR 1.250e-04
0: TRAIN [5][4970/7762]	Time 0.260 (0.325)	Data 1.02e-04 (1.68e-04)	Tok/s 39458 (42987)	Loss/tok 2.8299 (3.1414)	LR 1.250e-04
0: TRAIN [5][4980/7762]	Time 0.578 (0.325)	Data 9.92e-05 (1.67e-04)	Tok/s 51338 (42984)	Loss/tok 3.6533 (3.1414)	LR 1.250e-04
0: TRAIN [5][4990/7762]	Time 0.265 (0.325)	Data 1.13e-04 (1.67e-04)	Tok/s 38513 (42983)	Loss/tok 2.9798 (3.1413)	LR 1.250e-04
0: TRAIN [5][5000/7762]	Time 0.362 (0.325)	Data 1.06e-04 (1.67e-04)	Tok/s 46465 (42977)	Loss/tok 3.1278 (3.1413)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][5010/7762]	Time 0.262 (0.325)	Data 1.23e-04 (1.67e-04)	Tok/s 39855 (42980)	Loss/tok 2.9546 (3.1413)	LR 1.250e-04
0: TRAIN [5][5020/7762]	Time 0.585 (0.325)	Data 1.01e-04 (1.67e-04)	Tok/s 51068 (42980)	Loss/tok 3.4596 (3.1412)	LR 1.250e-04
0: TRAIN [5][5030/7762]	Time 0.357 (0.325)	Data 1.01e-04 (1.67e-04)	Tok/s 46650 (42983)	Loss/tok 3.2454 (3.1412)	LR 1.250e-04
0: TRAIN [5][5040/7762]	Time 0.267 (0.325)	Data 1.03e-04 (1.67e-04)	Tok/s 38043 (42985)	Loss/tok 2.9901 (3.1413)	LR 1.250e-04
0: TRAIN [5][5050/7762]	Time 0.264 (0.325)	Data 9.97e-05 (1.67e-04)	Tok/s 38712 (42986)	Loss/tok 2.9321 (3.1412)	LR 1.250e-04
0: TRAIN [5][5060/7762]	Time 0.349 (0.325)	Data 9.63e-05 (1.66e-04)	Tok/s 48752 (42986)	Loss/tok 3.1260 (3.1411)	LR 1.250e-04
0: TRAIN [5][5070/7762]	Time 0.355 (0.325)	Data 1.01e-04 (1.66e-04)	Tok/s 46619 (42986)	Loss/tok 3.1060 (3.1412)	LR 1.250e-04
0: TRAIN [5][5080/7762]	Time 0.585 (0.325)	Data 9.94e-05 (1.66e-04)	Tok/s 50687 (42985)	Loss/tok 3.4849 (3.1413)	LR 1.250e-04
0: TRAIN [5][5090/7762]	Time 0.352 (0.325)	Data 1.01e-04 (1.66e-04)	Tok/s 47731 (42988)	Loss/tok 3.2720 (3.1412)	LR 1.250e-04
0: TRAIN [5][5100/7762]	Time 0.466 (0.325)	Data 1.17e-04 (1.66e-04)	Tok/s 50535 (42990)	Loss/tok 3.3225 (3.1414)	LR 1.250e-04
0: TRAIN [5][5110/7762]	Time 0.370 (0.325)	Data 1.09e-04 (1.66e-04)	Tok/s 45763 (42988)	Loss/tok 3.2684 (3.1414)	LR 1.250e-04
0: TRAIN [5][5120/7762]	Time 0.265 (0.325)	Data 9.66e-05 (1.66e-04)	Tok/s 39057 (42988)	Loss/tok 2.9175 (3.1413)	LR 1.250e-04
0: TRAIN [5][5130/7762]	Time 0.365 (0.325)	Data 1.05e-04 (1.66e-04)	Tok/s 45783 (42985)	Loss/tok 3.1489 (3.1413)	LR 1.250e-04
0: TRAIN [5][5140/7762]	Time 0.468 (0.325)	Data 1.31e-04 (1.66e-04)	Tok/s 49123 (42993)	Loss/tok 3.2717 (3.1414)	LR 1.250e-04
0: TRAIN [5][5150/7762]	Time 0.263 (0.325)	Data 1.00e-04 (1.65e-04)	Tok/s 39667 (42988)	Loss/tok 2.8713 (3.1413)	LR 1.250e-04
0: TRAIN [5][5160/7762]	Time 0.263 (0.325)	Data 9.82e-05 (1.65e-04)	Tok/s 38361 (42987)	Loss/tok 2.9821 (3.1413)	LR 1.250e-04
0: TRAIN [5][5170/7762]	Time 0.360 (0.325)	Data 9.51e-05 (1.65e-04)	Tok/s 46898 (42985)	Loss/tok 3.1160 (3.1412)	LR 1.250e-04
0: TRAIN [5][5180/7762]	Time 0.591 (0.325)	Data 9.92e-05 (1.65e-04)	Tok/s 49707 (42986)	Loss/tok 3.5336 (3.1414)	LR 1.250e-04
0: TRAIN [5][5190/7762]	Time 0.264 (0.325)	Data 1.01e-04 (1.65e-04)	Tok/s 38804 (42985)	Loss/tok 2.9589 (3.1414)	LR 1.250e-04
0: TRAIN [5][5200/7762]	Time 0.260 (0.325)	Data 9.70e-05 (1.65e-04)	Tok/s 39931 (42982)	Loss/tok 3.0448 (3.1413)	LR 1.250e-04
0: TRAIN [5][5210/7762]	Time 0.454 (0.325)	Data 1.04e-04 (1.65e-04)	Tok/s 51761 (42989)	Loss/tok 3.2487 (3.1415)	LR 1.250e-04
0: TRAIN [5][5220/7762]	Time 0.269 (0.325)	Data 1.00e-04 (1.65e-04)	Tok/s 39041 (42987)	Loss/tok 2.9230 (3.1416)	LR 1.250e-04
0: TRAIN [5][5230/7762]	Time 0.177 (0.325)	Data 1.02e-04 (1.64e-04)	Tok/s 29255 (42987)	Loss/tok 2.6331 (3.1415)	LR 1.250e-04
0: TRAIN [5][5240/7762]	Time 0.169 (0.325)	Data 1.05e-04 (1.64e-04)	Tok/s 31695 (42983)	Loss/tok 2.6176 (3.1415)	LR 1.250e-04
0: TRAIN [5][5250/7762]	Time 0.263 (0.325)	Data 1.04e-04 (1.64e-04)	Tok/s 39354 (42983)	Loss/tok 2.9842 (3.1414)	LR 1.250e-04
0: TRAIN [5][5260/7762]	Time 0.368 (0.325)	Data 9.97e-05 (1.64e-04)	Tok/s 45449 (42983)	Loss/tok 3.2358 (3.1413)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [5][5270/7762]	Time 0.451 (0.325)	Data 9.87e-05 (1.64e-04)	Tok/s 51579 (42982)	Loss/tok 3.3161 (3.1413)	LR 1.250e-04
0: TRAIN [5][5280/7762]	Time 0.358 (0.325)	Data 1.22e-04 (1.64e-04)	Tok/s 47401 (42979)	Loss/tok 3.0675 (3.1412)	LR 1.250e-04
0: TRAIN [5][5290/7762]	Time 0.259 (0.325)	Data 1.18e-04 (1.64e-04)	Tok/s 40440 (42981)	Loss/tok 2.9682 (3.1411)	LR 1.250e-04
0: TRAIN [5][5300/7762]	Time 0.263 (0.325)	Data 1.03e-04 (1.64e-04)	Tok/s 39562 (42980)	Loss/tok 2.8923 (3.1410)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][5310/7762]	Time 0.257 (0.325)	Data 1.04e-04 (1.63e-04)	Tok/s 41455 (42983)	Loss/tok 2.9840 (3.1412)	LR 1.250e-04
0: TRAIN [5][5320/7762]	Time 0.258 (0.325)	Data 1.18e-04 (1.63e-04)	Tok/s 40486 (42982)	Loss/tok 2.9242 (3.1411)	LR 1.250e-04
0: TRAIN [5][5330/7762]	Time 0.261 (0.325)	Data 1.42e-04 (1.63e-04)	Tok/s 39034 (42979)	Loss/tok 2.9092 (3.1410)	LR 1.250e-04
0: TRAIN [5][5340/7762]	Time 0.359 (0.325)	Data 9.99e-05 (1.63e-04)	Tok/s 46606 (42976)	Loss/tok 3.1708 (3.1408)	LR 1.250e-04
0: TRAIN [5][5350/7762]	Time 0.177 (0.325)	Data 1.03e-04 (1.63e-04)	Tok/s 29619 (42977)	Loss/tok 2.5021 (3.1407)	LR 1.250e-04
0: TRAIN [5][5360/7762]	Time 0.259 (0.325)	Data 9.80e-05 (1.63e-04)	Tok/s 39730 (42969)	Loss/tok 3.0059 (3.1405)	LR 1.250e-04
0: TRAIN [5][5370/7762]	Time 0.263 (0.325)	Data 1.01e-04 (1.63e-04)	Tok/s 39896 (42966)	Loss/tok 2.9857 (3.1404)	LR 1.250e-04
0: TRAIN [5][5380/7762]	Time 0.449 (0.325)	Data 1.01e-04 (1.63e-04)	Tok/s 51946 (42971)	Loss/tok 3.2984 (3.1404)	LR 1.250e-04
0: TRAIN [5][5390/7762]	Time 0.459 (0.325)	Data 9.92e-05 (1.63e-04)	Tok/s 51577 (42972)	Loss/tok 3.1215 (3.1405)	LR 1.250e-04
0: TRAIN [5][5400/7762]	Time 0.267 (0.325)	Data 9.80e-05 (1.63e-04)	Tok/s 37740 (42967)	Loss/tok 3.0029 (3.1405)	LR 1.250e-04
0: TRAIN [5][5410/7762]	Time 0.260 (0.325)	Data 1.01e-04 (1.62e-04)	Tok/s 39623 (42973)	Loss/tok 3.0034 (3.1407)	LR 1.250e-04
0: TRAIN [5][5420/7762]	Time 0.259 (0.325)	Data 9.92e-05 (1.62e-04)	Tok/s 40751 (42974)	Loss/tok 2.9086 (3.1407)	LR 1.250e-04
0: TRAIN [5][5430/7762]	Time 0.363 (0.325)	Data 9.85e-05 (1.62e-04)	Tok/s 46363 (42978)	Loss/tok 3.1034 (3.1407)	LR 1.250e-04
0: TRAIN [5][5440/7762]	Time 0.176 (0.325)	Data 1.18e-04 (1.62e-04)	Tok/s 30109 (42974)	Loss/tok 2.6070 (3.1407)	LR 1.250e-04
0: TRAIN [5][5450/7762]	Time 0.459 (0.325)	Data 1.12e-04 (1.62e-04)	Tok/s 50920 (42982)	Loss/tok 3.3072 (3.1411)	LR 1.250e-04
0: TRAIN [5][5460/7762]	Time 0.254 (0.325)	Data 1.04e-04 (1.62e-04)	Tok/s 40514 (42979)	Loss/tok 2.9619 (3.1409)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][5470/7762]	Time 0.584 (0.325)	Data 9.66e-05 (1.62e-04)	Tok/s 51090 (42989)	Loss/tok 3.4173 (3.1415)	LR 1.250e-04
0: TRAIN [5][5480/7762]	Time 0.264 (0.325)	Data 1.16e-04 (1.62e-04)	Tok/s 38866 (42985)	Loss/tok 2.9566 (3.1414)	LR 1.250e-04
0: TRAIN [5][5490/7762]	Time 0.366 (0.325)	Data 9.80e-05 (1.62e-04)	Tok/s 46207 (42986)	Loss/tok 3.1405 (3.1414)	LR 1.250e-04
0: TRAIN [5][5500/7762]	Time 0.364 (0.325)	Data 9.80e-05 (1.61e-04)	Tok/s 46048 (42986)	Loss/tok 3.0970 (3.1414)	LR 1.250e-04
0: TRAIN [5][5510/7762]	Time 0.366 (0.325)	Data 9.87e-05 (1.61e-04)	Tok/s 46072 (42985)	Loss/tok 3.1866 (3.1414)	LR 1.250e-04
0: TRAIN [5][5520/7762]	Time 0.588 (0.325)	Data 1.02e-04 (1.61e-04)	Tok/s 50809 (42989)	Loss/tok 3.5324 (3.1415)	LR 1.250e-04
0: TRAIN [5][5530/7762]	Time 0.177 (0.325)	Data 1.02e-04 (1.61e-04)	Tok/s 29276 (42986)	Loss/tok 2.4323 (3.1416)	LR 1.250e-04
0: TRAIN [5][5540/7762]	Time 0.351 (0.325)	Data 9.78e-05 (1.61e-04)	Tok/s 47809 (42981)	Loss/tok 3.2433 (3.1415)	LR 1.250e-04
0: TRAIN [5][5550/7762]	Time 0.449 (0.325)	Data 1.03e-04 (1.61e-04)	Tok/s 51864 (42983)	Loss/tok 3.4292 (3.1416)	LR 1.250e-04
0: TRAIN [5][5560/7762]	Time 0.179 (0.325)	Data 9.89e-05 (1.61e-04)	Tok/s 29712 (42978)	Loss/tok 2.6222 (3.1416)	LR 1.250e-04
0: TRAIN [5][5570/7762]	Time 0.268 (0.325)	Data 1.06e-04 (1.61e-04)	Tok/s 38771 (42978)	Loss/tok 2.9757 (3.1416)	LR 1.250e-04
0: TRAIN [5][5580/7762]	Time 0.175 (0.325)	Data 1.07e-04 (1.61e-04)	Tok/s 29175 (42973)	Loss/tok 2.5985 (3.1415)	LR 1.250e-04
0: TRAIN [5][5590/7762]	Time 0.259 (0.325)	Data 1.07e-04 (1.61e-04)	Tok/s 39284 (42976)	Loss/tok 2.9601 (3.1416)	LR 1.250e-04
0: TRAIN [5][5600/7762]	Time 0.178 (0.325)	Data 9.73e-05 (1.60e-04)	Tok/s 29480 (42976)	Loss/tok 2.4711 (3.1416)	LR 1.250e-04
0: TRAIN [5][5610/7762]	Time 0.257 (0.325)	Data 1.02e-04 (1.60e-04)	Tok/s 40787 (42974)	Loss/tok 2.8814 (3.1414)	LR 1.250e-04
0: TRAIN [5][5620/7762]	Time 0.352 (0.325)	Data 1.04e-04 (1.60e-04)	Tok/s 46725 (42979)	Loss/tok 3.1860 (3.1415)	LR 1.250e-04
0: TRAIN [5][5630/7762]	Time 0.263 (0.325)	Data 1.02e-04 (1.60e-04)	Tok/s 38672 (42978)	Loss/tok 2.9534 (3.1414)	LR 1.250e-04
0: TRAIN [5][5640/7762]	Time 0.351 (0.325)	Data 9.73e-05 (1.60e-04)	Tok/s 47996 (42978)	Loss/tok 3.0220 (3.1413)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][5650/7762]	Time 0.588 (0.325)	Data 1.22e-04 (1.60e-04)	Tok/s 50929 (42984)	Loss/tok 3.4367 (3.1415)	LR 1.250e-04
0: TRAIN [5][5660/7762]	Time 0.358 (0.325)	Data 1.02e-04 (1.60e-04)	Tok/s 47041 (42990)	Loss/tok 3.2231 (3.1416)	LR 1.250e-04
0: TRAIN [5][5670/7762]	Time 0.265 (0.325)	Data 1.08e-04 (1.60e-04)	Tok/s 38457 (42991)	Loss/tok 2.9561 (3.1417)	LR 1.250e-04
0: TRAIN [5][5680/7762]	Time 0.177 (0.325)	Data 1.03e-04 (1.60e-04)	Tok/s 29507 (42987)	Loss/tok 2.4943 (3.1416)	LR 1.250e-04
0: TRAIN [5][5690/7762]	Time 0.259 (0.325)	Data 1.01e-04 (1.60e-04)	Tok/s 39780 (42985)	Loss/tok 2.9198 (3.1415)	LR 1.250e-04
0: TRAIN [5][5700/7762]	Time 0.346 (0.325)	Data 9.87e-05 (1.59e-04)	Tok/s 48114 (42983)	Loss/tok 3.2896 (3.1416)	LR 1.250e-04
0: TRAIN [5][5710/7762]	Time 0.365 (0.325)	Data 1.20e-04 (1.59e-04)	Tok/s 46112 (42983)	Loss/tok 3.0623 (3.1416)	LR 1.250e-04
0: TRAIN [5][5720/7762]	Time 0.360 (0.325)	Data 1.01e-04 (1.59e-04)	Tok/s 47205 (42989)	Loss/tok 3.1507 (3.1417)	LR 1.250e-04
0: TRAIN [5][5730/7762]	Time 0.366 (0.325)	Data 1.02e-04 (1.59e-04)	Tok/s 45673 (42993)	Loss/tok 3.1432 (3.1418)	LR 1.250e-04
0: TRAIN [5][5740/7762]	Time 0.585 (0.325)	Data 9.92e-05 (1.59e-04)	Tok/s 50766 (42994)	Loss/tok 3.4791 (3.1420)	LR 1.250e-04
0: TRAIN [5][5750/7762]	Time 0.360 (0.325)	Data 9.89e-05 (1.59e-04)	Tok/s 46547 (42994)	Loss/tok 3.0745 (3.1423)	LR 1.250e-04
0: TRAIN [5][5760/7762]	Time 0.174 (0.325)	Data 9.92e-05 (1.59e-04)	Tok/s 30677 (42991)	Loss/tok 2.6284 (3.1422)	LR 1.250e-04
0: TRAIN [5][5770/7762]	Time 0.462 (0.325)	Data 1.02e-04 (1.59e-04)	Tok/s 50836 (42993)	Loss/tok 3.3077 (3.1422)	LR 1.250e-04
0: TRAIN [5][5780/7762]	Time 0.362 (0.325)	Data 9.70e-05 (1.59e-04)	Tok/s 47240 (42993)	Loss/tok 3.0249 (3.1421)	LR 1.250e-04
0: TRAIN [5][5790/7762]	Time 0.173 (0.325)	Data 9.89e-05 (1.59e-04)	Tok/s 30369 (42991)	Loss/tok 2.5534 (3.1420)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][5800/7762]	Time 0.178 (0.325)	Data 1.02e-04 (1.59e-04)	Tok/s 28902 (42990)	Loss/tok 2.5571 (3.1421)	LR 1.250e-04
0: TRAIN [5][5810/7762]	Time 0.584 (0.325)	Data 1.01e-04 (1.58e-04)	Tok/s 51083 (42989)	Loss/tok 3.4734 (3.1421)	LR 1.250e-04
0: TRAIN [5][5820/7762]	Time 0.342 (0.325)	Data 1.01e-04 (1.58e-04)	Tok/s 49174 (42987)	Loss/tok 3.0850 (3.1419)	LR 1.250e-04
0: TRAIN [5][5830/7762]	Time 0.255 (0.325)	Data 1.16e-04 (1.58e-04)	Tok/s 40686 (42995)	Loss/tok 2.9178 (3.1422)	LR 1.250e-04
0: TRAIN [5][5840/7762]	Time 0.172 (0.325)	Data 9.99e-05 (1.58e-04)	Tok/s 30529 (42994)	Loss/tok 2.5291 (3.1421)	LR 1.250e-04
0: TRAIN [5][5850/7762]	Time 0.365 (0.325)	Data 1.01e-04 (1.58e-04)	Tok/s 46277 (42998)	Loss/tok 3.1584 (3.1422)	LR 1.250e-04
0: TRAIN [5][5860/7762]	Time 0.463 (0.325)	Data 1.01e-04 (1.58e-04)	Tok/s 50109 (42999)	Loss/tok 3.3232 (3.1423)	LR 1.250e-04
0: TRAIN [5][5870/7762]	Time 0.363 (0.325)	Data 9.85e-05 (1.58e-04)	Tok/s 46033 (42999)	Loss/tok 3.0205 (3.1422)	LR 1.250e-04
0: TRAIN [5][5880/7762]	Time 0.254 (0.325)	Data 1.01e-04 (1.58e-04)	Tok/s 40507 (42999)	Loss/tok 3.0355 (3.1421)	LR 1.250e-04
0: TRAIN [5][5890/7762]	Time 0.464 (0.325)	Data 1.18e-04 (1.58e-04)	Tok/s 50113 (42999)	Loss/tok 3.3934 (3.1423)	LR 1.250e-04
0: TRAIN [5][5900/7762]	Time 0.361 (0.326)	Data 1.05e-04 (1.58e-04)	Tok/s 46554 (43002)	Loss/tok 3.2328 (3.1424)	LR 1.250e-04
0: TRAIN [5][5910/7762]	Time 0.461 (0.325)	Data 1.04e-04 (1.58e-04)	Tok/s 50416 (42998)	Loss/tok 3.2549 (3.1423)	LR 1.250e-04
0: TRAIN [5][5920/7762]	Time 0.251 (0.325)	Data 1.15e-04 (1.57e-04)	Tok/s 41398 (42999)	Loss/tok 2.9664 (3.1422)	LR 1.250e-04
0: TRAIN [5][5930/7762]	Time 0.355 (0.325)	Data 9.92e-05 (1.57e-04)	Tok/s 47270 (42998)	Loss/tok 3.1336 (3.1424)	LR 1.250e-04
0: TRAIN [5][5940/7762]	Time 0.350 (0.326)	Data 1.02e-04 (1.57e-04)	Tok/s 47868 (43004)	Loss/tok 3.1429 (3.1425)	LR 1.250e-04
0: TRAIN [5][5950/7762]	Time 0.459 (0.325)	Data 9.54e-05 (1.57e-04)	Tok/s 51091 (42998)	Loss/tok 3.3473 (3.1423)	LR 1.250e-04
0: TRAIN [5][5960/7762]	Time 0.345 (0.325)	Data 9.99e-05 (1.57e-04)	Tok/s 48110 (42999)	Loss/tok 3.1710 (3.1423)	LR 1.250e-04
0: TRAIN [5][5970/7762]	Time 0.587 (0.325)	Data 1.00e-04 (1.57e-04)	Tok/s 50893 (43000)	Loss/tok 3.4641 (3.1424)	LR 1.250e-04
0: TRAIN [5][5980/7762]	Time 0.260 (0.325)	Data 1.02e-04 (1.57e-04)	Tok/s 38889 (42998)	Loss/tok 2.9578 (3.1422)	LR 1.250e-04
0: TRAIN [5][5990/7762]	Time 0.578 (0.325)	Data 1.22e-04 (1.57e-04)	Tok/s 51771 (42998)	Loss/tok 3.5497 (3.1424)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][6000/7762]	Time 0.453 (0.325)	Data 9.75e-05 (1.57e-04)	Tok/s 50771 (42989)	Loss/tok 3.4911 (3.1423)	LR 1.250e-04
0: TRAIN [5][6010/7762]	Time 0.460 (0.325)	Data 1.05e-04 (1.57e-04)	Tok/s 50170 (42989)	Loss/tok 3.3831 (3.1423)	LR 1.250e-04
0: TRAIN [5][6020/7762]	Time 0.351 (0.325)	Data 9.94e-05 (1.57e-04)	Tok/s 48048 (42985)	Loss/tok 3.0793 (3.1422)	LR 1.250e-04
0: TRAIN [5][6030/7762]	Time 0.178 (0.325)	Data 1.23e-04 (1.56e-04)	Tok/s 30344 (42985)	Loss/tok 2.5227 (3.1422)	LR 1.250e-04
0: TRAIN [5][6040/7762]	Time 0.362 (0.325)	Data 1.00e-04 (1.56e-04)	Tok/s 45969 (42987)	Loss/tok 3.1225 (3.1423)	LR 1.250e-04
0: TRAIN [5][6050/7762]	Time 0.451 (0.325)	Data 1.18e-04 (1.56e-04)	Tok/s 51617 (42991)	Loss/tok 3.3343 (3.1424)	LR 1.250e-04
0: TRAIN [5][6060/7762]	Time 0.268 (0.325)	Data 1.04e-04 (1.56e-04)	Tok/s 37611 (42989)	Loss/tok 2.9530 (3.1423)	LR 1.250e-04
0: TRAIN [5][6070/7762]	Time 0.349 (0.325)	Data 9.94e-05 (1.56e-04)	Tok/s 47686 (42990)	Loss/tok 3.2212 (3.1424)	LR 1.250e-04
0: TRAIN [5][6080/7762]	Time 0.441 (0.325)	Data 9.89e-05 (1.56e-04)	Tok/s 52798 (42994)	Loss/tok 3.3316 (3.1424)	LR 1.250e-04
0: TRAIN [5][6090/7762]	Time 0.365 (0.325)	Data 1.19e-04 (1.56e-04)	Tok/s 45671 (42994)	Loss/tok 3.1457 (3.1424)	LR 1.250e-04
0: TRAIN [5][6100/7762]	Time 0.258 (0.325)	Data 1.03e-04 (1.56e-04)	Tok/s 40457 (42995)	Loss/tok 2.8974 (3.1424)	LR 1.250e-04
0: TRAIN [5][6110/7762]	Time 0.462 (0.325)	Data 1.15e-04 (1.56e-04)	Tok/s 50820 (42994)	Loss/tok 3.3443 (3.1423)	LR 1.250e-04
0: TRAIN [5][6120/7762]	Time 0.355 (0.325)	Data 9.61e-05 (1.56e-04)	Tok/s 47304 (42993)	Loss/tok 3.1403 (3.1423)	LR 1.250e-04
0: TRAIN [5][6130/7762]	Time 0.261 (0.325)	Data 9.82e-05 (1.56e-04)	Tok/s 39147 (42995)	Loss/tok 2.9623 (3.1422)	LR 1.250e-04
0: TRAIN [5][6140/7762]	Time 0.362 (0.325)	Data 1.00e-04 (1.55e-04)	Tok/s 46257 (42991)	Loss/tok 3.2168 (3.1420)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][6150/7762]	Time 0.333 (0.325)	Data 1.03e-04 (1.55e-04)	Tok/s 51535 (42991)	Loss/tok 2.9994 (3.1420)	LR 1.250e-04
0: TRAIN [5][6160/7762]	Time 0.450 (0.325)	Data 9.78e-05 (1.55e-04)	Tok/s 51198 (42991)	Loss/tok 3.3736 (3.1422)	LR 1.250e-04
0: TRAIN [5][6170/7762]	Time 0.456 (0.325)	Data 1.16e-04 (1.55e-04)	Tok/s 51719 (42989)	Loss/tok 3.2205 (3.1421)	LR 1.250e-04
0: TRAIN [5][6180/7762]	Time 0.351 (0.325)	Data 9.80e-05 (1.55e-04)	Tok/s 47570 (42993)	Loss/tok 3.0800 (3.1421)	LR 1.250e-04
0: TRAIN [5][6190/7762]	Time 0.452 (0.325)	Data 9.80e-05 (1.55e-04)	Tok/s 52115 (42992)	Loss/tok 3.2833 (3.1420)	LR 1.250e-04
0: TRAIN [5][6200/7762]	Time 0.365 (0.325)	Data 1.17e-04 (1.55e-04)	Tok/s 46289 (42996)	Loss/tok 3.0969 (3.1420)	LR 1.250e-04
0: TRAIN [5][6210/7762]	Time 0.354 (0.325)	Data 1.19e-04 (1.55e-04)	Tok/s 48044 (42996)	Loss/tok 3.2033 (3.1419)	LR 1.250e-04
0: TRAIN [5][6220/7762]	Time 0.178 (0.325)	Data 1.06e-04 (1.55e-04)	Tok/s 30260 (42995)	Loss/tok 2.5938 (3.1420)	LR 1.250e-04
0: TRAIN [5][6230/7762]	Time 0.364 (0.325)	Data 1.14e-04 (1.55e-04)	Tok/s 46081 (42995)	Loss/tok 3.2488 (3.1420)	LR 1.250e-04
0: TRAIN [5][6240/7762]	Time 0.265 (0.325)	Data 1.13e-04 (1.55e-04)	Tok/s 39189 (42990)	Loss/tok 2.8905 (3.1418)	LR 1.250e-04
0: TRAIN [5][6250/7762]	Time 0.250 (0.325)	Data 9.92e-05 (1.55e-04)	Tok/s 41285 (42987)	Loss/tok 2.9834 (3.1418)	LR 1.250e-04
0: TRAIN [5][6260/7762]	Time 0.263 (0.325)	Data 1.01e-04 (1.54e-04)	Tok/s 39271 (42987)	Loss/tok 2.9883 (3.1417)	LR 1.250e-04
0: TRAIN [5][6270/7762]	Time 0.365 (0.325)	Data 1.01e-04 (1.54e-04)	Tok/s 46082 (42992)	Loss/tok 3.0470 (3.1418)	LR 1.250e-04
0: TRAIN [5][6280/7762]	Time 0.365 (0.325)	Data 1.17e-04 (1.54e-04)	Tok/s 45983 (42993)	Loss/tok 3.1724 (3.1417)	LR 1.250e-04
0: TRAIN [5][6290/7762]	Time 0.464 (0.325)	Data 1.00e-04 (1.54e-04)	Tok/s 49887 (42994)	Loss/tok 3.3434 (3.1417)	LR 1.250e-04
0: TRAIN [5][6300/7762]	Time 0.363 (0.325)	Data 1.19e-04 (1.54e-04)	Tok/s 46404 (42999)	Loss/tok 3.1356 (3.1419)	LR 1.250e-04
0: TRAIN [5][6310/7762]	Time 0.264 (0.325)	Data 1.00e-04 (1.54e-04)	Tok/s 39159 (42995)	Loss/tok 2.8823 (3.1419)	LR 1.250e-04
0: TRAIN [5][6320/7762]	Time 0.464 (0.325)	Data 1.19e-04 (1.54e-04)	Tok/s 50432 (42991)	Loss/tok 3.1804 (3.1417)	LR 1.250e-04
0: TRAIN [5][6330/7762]	Time 0.176 (0.325)	Data 1.00e-04 (1.54e-04)	Tok/s 30199 (42993)	Loss/tok 2.5848 (3.1419)	LR 1.250e-04
0: TRAIN [5][6340/7762]	Time 0.365 (0.325)	Data 1.01e-04 (1.54e-04)	Tok/s 46246 (42987)	Loss/tok 3.2353 (3.1419)	LR 1.250e-04
0: TRAIN [5][6350/7762]	Time 0.261 (0.325)	Data 1.01e-04 (1.54e-04)	Tok/s 39868 (42992)	Loss/tok 2.9623 (3.1420)	LR 1.250e-04
0: TRAIN [5][6360/7762]	Time 0.366 (0.325)	Data 1.03e-04 (1.54e-04)	Tok/s 46065 (42995)	Loss/tok 3.1726 (3.1421)	LR 1.250e-04
0: TRAIN [5][6370/7762]	Time 0.262 (0.325)	Data 1.02e-04 (1.54e-04)	Tok/s 38772 (42996)	Loss/tok 2.9289 (3.1421)	LR 1.250e-04
0: TRAIN [5][6380/7762]	Time 0.439 (0.325)	Data 1.03e-04 (1.54e-04)	Tok/s 52817 (42999)	Loss/tok 3.3674 (3.1421)	LR 1.250e-04
0: TRAIN [5][6390/7762]	Time 0.460 (0.325)	Data 9.73e-05 (1.53e-04)	Tok/s 51095 (42998)	Loss/tok 3.2852 (3.1421)	LR 1.250e-04
0: TRAIN [5][6400/7762]	Time 0.169 (0.325)	Data 1.08e-04 (1.53e-04)	Tok/s 31227 (42996)	Loss/tok 2.6789 (3.1421)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [5][6410/7762]	Time 0.465 (0.325)	Data 1.05e-04 (1.53e-04)	Tok/s 50222 (42995)	Loss/tok 3.3560 (3.1423)	LR 1.250e-04
0: TRAIN [5][6420/7762]	Time 0.261 (0.325)	Data 1.04e-04 (1.53e-04)	Tok/s 39399 (43002)	Loss/tok 2.8859 (3.1425)	LR 1.250e-04
0: TRAIN [5][6430/7762]	Time 0.251 (0.325)	Data 1.21e-04 (1.53e-04)	Tok/s 41940 (43006)	Loss/tok 2.8899 (3.1425)	LR 1.250e-04
0: TRAIN [5][6440/7762]	Time 0.176 (0.325)	Data 1.03e-04 (1.53e-04)	Tok/s 30024 (43000)	Loss/tok 2.5916 (3.1423)	LR 1.250e-04
0: TRAIN [5][6450/7762]	Time 0.359 (0.325)	Data 1.00e-04 (1.53e-04)	Tok/s 46413 (42999)	Loss/tok 3.1529 (3.1423)	LR 1.250e-04
0: TRAIN [5][6460/7762]	Time 0.266 (0.325)	Data 1.14e-04 (1.53e-04)	Tok/s 38087 (42996)	Loss/tok 2.9743 (3.1422)	LR 1.250e-04
0: TRAIN [5][6470/7762]	Time 0.268 (0.325)	Data 9.80e-05 (1.53e-04)	Tok/s 38785 (42991)	Loss/tok 2.9995 (3.1421)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][6480/7762]	Time 0.263 (0.325)	Data 9.82e-05 (1.53e-04)	Tok/s 38631 (42990)	Loss/tok 2.9460 (3.1422)	LR 1.250e-04
0: TRAIN [5][6490/7762]	Time 0.349 (0.325)	Data 9.92e-05 (1.53e-04)	Tok/s 47851 (42987)	Loss/tok 3.0872 (3.1421)	LR 1.250e-04
0: TRAIN [5][6500/7762]	Time 0.360 (0.325)	Data 1.06e-04 (1.53e-04)	Tok/s 46851 (42992)	Loss/tok 3.2296 (3.1423)	LR 1.250e-04
0: TRAIN [5][6510/7762]	Time 0.257 (0.325)	Data 1.60e-04 (1.53e-04)	Tok/s 40155 (42994)	Loss/tok 2.9581 (3.1424)	LR 1.250e-04
0: TRAIN [5][6520/7762]	Time 0.264 (0.325)	Data 1.07e-04 (1.53e-04)	Tok/s 39461 (42991)	Loss/tok 2.9321 (3.1423)	LR 1.250e-04
0: TRAIN [5][6530/7762]	Time 0.364 (0.325)	Data 1.00e-04 (1.52e-04)	Tok/s 45749 (42989)	Loss/tok 3.1321 (3.1422)	LR 1.250e-04
0: TRAIN [5][6540/7762]	Time 0.359 (0.325)	Data 1.08e-04 (1.52e-04)	Tok/s 46993 (42993)	Loss/tok 3.1910 (3.1423)	LR 1.250e-04
0: TRAIN [5][6550/7762]	Time 0.365 (0.325)	Data 1.00e-04 (1.52e-04)	Tok/s 45029 (42994)	Loss/tok 3.1422 (3.1424)	LR 1.250e-04
0: TRAIN [5][6560/7762]	Time 0.253 (0.325)	Data 1.16e-04 (1.52e-04)	Tok/s 40510 (42996)	Loss/tok 2.9661 (3.1425)	LR 1.250e-04
0: TRAIN [5][6570/7762]	Time 0.456 (0.325)	Data 9.80e-05 (1.52e-04)	Tok/s 51759 (42995)	Loss/tok 3.3237 (3.1425)	LR 1.250e-04
0: TRAIN [5][6580/7762]	Time 0.555 (0.325)	Data 1.23e-04 (1.52e-04)	Tok/s 53600 (43001)	Loss/tok 3.5160 (3.1428)	LR 1.250e-04
0: TRAIN [5][6590/7762]	Time 0.359 (0.326)	Data 1.00e-04 (1.52e-04)	Tok/s 48147 (43000)	Loss/tok 3.0129 (3.1429)	LR 1.250e-04
0: TRAIN [5][6600/7762]	Time 0.264 (0.325)	Data 9.85e-05 (1.52e-04)	Tok/s 38380 (42996)	Loss/tok 2.8882 (3.1427)	LR 1.250e-04
0: TRAIN [5][6610/7762]	Time 0.170 (0.325)	Data 1.01e-04 (1.52e-04)	Tok/s 29852 (42999)	Loss/tok 2.5494 (3.1429)	LR 1.250e-04
0: TRAIN [5][6620/7762]	Time 0.265 (0.325)	Data 9.39e-05 (1.52e-04)	Tok/s 39223 (42996)	Loss/tok 2.9656 (3.1429)	LR 1.250e-04
0: TRAIN [5][6630/7762]	Time 0.345 (0.325)	Data 1.16e-04 (1.52e-04)	Tok/s 48449 (42997)	Loss/tok 3.1173 (3.1430)	LR 1.250e-04
0: TRAIN [5][6640/7762]	Time 0.262 (0.326)	Data 9.92e-05 (1.52e-04)	Tok/s 38701 (42999)	Loss/tok 2.9750 (3.1431)	LR 1.250e-04
0: TRAIN [5][6650/7762]	Time 0.266 (0.325)	Data 1.14e-04 (1.52e-04)	Tok/s 37739 (42993)	Loss/tok 2.9797 (3.1432)	LR 1.250e-04
0: TRAIN [5][6660/7762]	Time 0.172 (0.325)	Data 1.14e-04 (1.52e-04)	Tok/s 30223 (42991)	Loss/tok 2.4961 (3.1430)	LR 1.250e-04
0: TRAIN [5][6670/7762]	Time 0.358 (0.325)	Data 1.00e-04 (1.51e-04)	Tok/s 46857 (42988)	Loss/tok 3.1361 (3.1428)	LR 1.250e-04
0: TRAIN [5][6680/7762]	Time 0.364 (0.325)	Data 9.73e-05 (1.51e-04)	Tok/s 46133 (42988)	Loss/tok 3.1988 (3.1429)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][6690/7762]	Time 0.174 (0.325)	Data 1.01e-04 (1.51e-04)	Tok/s 30308 (42986)	Loss/tok 2.5575 (3.1428)	LR 1.250e-04
0: TRAIN [5][6700/7762]	Time 0.462 (0.325)	Data 9.94e-05 (1.51e-04)	Tok/s 49702 (42986)	Loss/tok 3.4133 (3.1428)	LR 1.250e-04
0: TRAIN [5][6710/7762]	Time 0.264 (0.325)	Data 1.03e-04 (1.51e-04)	Tok/s 39121 (42987)	Loss/tok 2.9429 (3.1429)	LR 1.250e-04
0: TRAIN [5][6720/7762]	Time 0.365 (0.325)	Data 1.03e-04 (1.51e-04)	Tok/s 46719 (42994)	Loss/tok 3.0756 (3.1431)	LR 1.250e-04
0: TRAIN [5][6730/7762]	Time 0.260 (0.326)	Data 1.03e-04 (1.51e-04)	Tok/s 39502 (42997)	Loss/tok 2.8878 (3.1431)	LR 1.250e-04
0: TRAIN [5][6740/7762]	Time 0.267 (0.325)	Data 1.00e-04 (1.51e-04)	Tok/s 38423 (42995)	Loss/tok 2.9757 (3.1431)	LR 1.250e-04
0: TRAIN [5][6750/7762]	Time 0.265 (0.325)	Data 9.68e-05 (1.51e-04)	Tok/s 39651 (42989)	Loss/tok 2.9159 (3.1429)	LR 1.250e-04
0: TRAIN [5][6760/7762]	Time 0.363 (0.325)	Data 9.44e-05 (1.51e-04)	Tok/s 46049 (42991)	Loss/tok 3.1844 (3.1429)	LR 1.250e-04
0: TRAIN [5][6770/7762]	Time 0.264 (0.325)	Data 1.01e-04 (1.51e-04)	Tok/s 39026 (42986)	Loss/tok 2.9515 (3.1429)	LR 1.250e-04
0: TRAIN [5][6780/7762]	Time 0.266 (0.325)	Data 1.04e-04 (1.51e-04)	Tok/s 39359 (42985)	Loss/tok 3.0608 (3.1430)	LR 1.250e-04
0: TRAIN [5][6790/7762]	Time 0.175 (0.325)	Data 9.97e-05 (1.51e-04)	Tok/s 30050 (42983)	Loss/tok 2.5848 (3.1430)	LR 1.250e-04
0: TRAIN [5][6800/7762]	Time 0.590 (0.325)	Data 1.03e-04 (1.51e-04)	Tok/s 50328 (42986)	Loss/tok 3.5447 (3.1432)	LR 1.250e-04
0: TRAIN [5][6810/7762]	Time 0.174 (0.325)	Data 9.56e-05 (1.50e-04)	Tok/s 30133 (42987)	Loss/tok 2.6288 (3.1431)	LR 1.250e-04
0: TRAIN [5][6820/7762]	Time 0.262 (0.325)	Data 9.78e-05 (1.50e-04)	Tok/s 38762 (42987)	Loss/tok 2.8368 (3.1430)	LR 1.250e-04
0: TRAIN [5][6830/7762]	Time 0.363 (0.325)	Data 9.58e-05 (1.50e-04)	Tok/s 46692 (42986)	Loss/tok 3.1936 (3.1430)	LR 1.250e-04
0: TRAIN [5][6840/7762]	Time 0.259 (0.325)	Data 1.04e-04 (1.50e-04)	Tok/s 40348 (42984)	Loss/tok 2.9525 (3.1429)	LR 1.250e-04
0: TRAIN [5][6850/7762]	Time 0.458 (0.325)	Data 1.03e-04 (1.50e-04)	Tok/s 49889 (42985)	Loss/tok 3.3613 (3.1429)	LR 1.250e-04
0: TRAIN [5][6860/7762]	Time 0.175 (0.325)	Data 9.99e-05 (1.50e-04)	Tok/s 30212 (42985)	Loss/tok 2.6211 (3.1429)	LR 1.250e-04
0: TRAIN [5][6870/7762]	Time 0.359 (0.325)	Data 1.15e-04 (1.50e-04)	Tok/s 46593 (42984)	Loss/tok 3.1096 (3.1428)	LR 1.250e-04
0: TRAIN [5][6880/7762]	Time 0.463 (0.325)	Data 9.80e-05 (1.50e-04)	Tok/s 50683 (42988)	Loss/tok 3.2969 (3.1430)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][6890/7762]	Time 0.463 (0.325)	Data 1.03e-04 (1.50e-04)	Tok/s 50164 (42983)	Loss/tok 3.2767 (3.1431)	LR 1.250e-04
0: TRAIN [5][6900/7762]	Time 0.268 (0.325)	Data 9.75e-05 (1.50e-04)	Tok/s 37691 (42986)	Loss/tok 3.0175 (3.1431)	LR 1.250e-04
0: TRAIN [5][6910/7762]	Time 0.363 (0.325)	Data 1.17e-04 (1.50e-04)	Tok/s 46368 (42985)	Loss/tok 3.0246 (3.1431)	LR 1.250e-04
0: TRAIN [5][6920/7762]	Time 0.594 (0.325)	Data 1.03e-04 (1.50e-04)	Tok/s 50217 (42989)	Loss/tok 3.4786 (3.1432)	LR 1.250e-04
0: TRAIN [5][6930/7762]	Time 0.354 (0.325)	Data 1.02e-04 (1.50e-04)	Tok/s 47731 (42991)	Loss/tok 3.1212 (3.1433)	LR 1.250e-04
0: TRAIN [5][6940/7762]	Time 0.365 (0.326)	Data 9.92e-05 (1.50e-04)	Tok/s 46562 (42993)	Loss/tok 3.1223 (3.1434)	LR 1.250e-04
0: TRAIN [5][6950/7762]	Time 0.266 (0.325)	Data 1.04e-04 (1.50e-04)	Tok/s 39216 (42990)	Loss/tok 2.9845 (3.1433)	LR 1.250e-04
0: TRAIN [5][6960/7762]	Time 0.363 (0.326)	Data 1.10e-04 (1.50e-04)	Tok/s 46527 (42996)	Loss/tok 3.2042 (3.1434)	LR 1.250e-04
0: TRAIN [5][6970/7762]	Time 0.365 (0.326)	Data 9.75e-05 (1.49e-04)	Tok/s 46017 (42998)	Loss/tok 3.1191 (3.1435)	LR 1.250e-04
0: TRAIN [5][6980/7762]	Time 0.366 (0.326)	Data 1.06e-04 (1.49e-04)	Tok/s 46070 (42997)	Loss/tok 3.1442 (3.1434)	LR 1.250e-04
0: TRAIN [5][6990/7762]	Time 0.257 (0.326)	Data 1.01e-04 (1.49e-04)	Tok/s 39666 (42998)	Loss/tok 2.9697 (3.1434)	LR 1.250e-04
0: TRAIN [5][7000/7762]	Time 0.254 (0.326)	Data 1.03e-04 (1.49e-04)	Tok/s 40491 (42995)	Loss/tok 2.9344 (3.1433)	LR 1.250e-04
0: TRAIN [5][7010/7762]	Time 0.342 (0.325)	Data 9.87e-05 (1.49e-04)	Tok/s 48324 (42993)	Loss/tok 3.1407 (3.1432)	LR 1.250e-04
0: TRAIN [5][7020/7762]	Time 0.356 (0.326)	Data 9.80e-05 (1.49e-04)	Tok/s 46739 (42999)	Loss/tok 3.1330 (3.1434)	LR 1.250e-04
0: TRAIN [5][7030/7762]	Time 0.174 (0.326)	Data 9.73e-05 (1.49e-04)	Tok/s 30604 (43000)	Loss/tok 2.6604 (3.1436)	LR 1.250e-04
0: TRAIN [5][7040/7762]	Time 0.366 (0.326)	Data 9.75e-05 (1.49e-04)	Tok/s 45818 (43001)	Loss/tok 3.1516 (3.1435)	LR 1.250e-04
0: TRAIN [5][7050/7762]	Time 0.592 (0.326)	Data 1.02e-04 (1.49e-04)	Tok/s 50162 (43004)	Loss/tok 3.4522 (3.1438)	LR 1.250e-04
0: TRAIN [5][7060/7762]	Time 0.359 (0.326)	Data 1.04e-04 (1.49e-04)	Tok/s 46907 (43006)	Loss/tok 3.1008 (3.1437)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][7070/7762]	Time 0.577 (0.326)	Data 9.68e-05 (1.49e-04)	Tok/s 51219 (43006)	Loss/tok 3.5845 (3.1439)	LR 1.250e-04
0: TRAIN [5][7080/7762]	Time 0.349 (0.326)	Data 1.25e-04 (1.49e-04)	Tok/s 48662 (43005)	Loss/tok 3.0570 (3.1438)	LR 1.250e-04
0: TRAIN [5][7090/7762]	Time 0.259 (0.326)	Data 9.70e-05 (1.49e-04)	Tok/s 40550 (42999)	Loss/tok 2.8586 (3.1437)	LR 1.250e-04
0: TRAIN [5][7100/7762]	Time 0.578 (0.326)	Data 9.99e-05 (1.49e-04)	Tok/s 51527 (43002)	Loss/tok 3.4757 (3.1439)	LR 1.250e-04
0: TRAIN [5][7110/7762]	Time 0.260 (0.326)	Data 1.07e-04 (1.49e-04)	Tok/s 40322 (43003)	Loss/tok 2.9261 (3.1438)	LR 1.250e-04
0: TRAIN [5][7120/7762]	Time 0.178 (0.326)	Data 9.61e-05 (1.49e-04)	Tok/s 29605 (42998)	Loss/tok 2.6248 (3.1438)	LR 1.250e-04
0: TRAIN [5][7130/7762]	Time 0.265 (0.326)	Data 9.87e-05 (1.49e-04)	Tok/s 39118 (42993)	Loss/tok 3.0643 (3.1436)	LR 1.250e-04
0: TRAIN [5][7140/7762]	Time 0.176 (0.326)	Data 9.99e-05 (1.49e-04)	Tok/s 30159 (42989)	Loss/tok 2.5293 (3.1435)	LR 1.250e-04
0: TRAIN [5][7150/7762]	Time 0.459 (0.326)	Data 1.15e-04 (1.49e-04)	Tok/s 51425 (42993)	Loss/tok 3.3116 (3.1435)	LR 1.250e-04
0: TRAIN [5][7160/7762]	Time 0.582 (0.326)	Data 9.78e-05 (1.49e-04)	Tok/s 51624 (42991)	Loss/tok 3.4688 (3.1435)	LR 1.250e-04
0: TRAIN [5][7170/7762]	Time 0.260 (0.326)	Data 1.13e-04 (1.49e-04)	Tok/s 38816 (42991)	Loss/tok 3.1080 (3.1435)	LR 1.250e-04
0: TRAIN [5][7180/7762]	Time 0.362 (0.326)	Data 1.06e-04 (1.49e-04)	Tok/s 45786 (42992)	Loss/tok 3.1537 (3.1434)	LR 1.250e-04
0: TRAIN [5][7190/7762]	Time 0.173 (0.326)	Data 1.19e-04 (1.49e-04)	Tok/s 30123 (42995)	Loss/tok 2.6814 (3.1435)	LR 1.250e-04
0: TRAIN [5][7200/7762]	Time 0.260 (0.326)	Data 1.04e-04 (1.48e-04)	Tok/s 39804 (42992)	Loss/tok 2.8844 (3.1434)	LR 1.250e-04
0: TRAIN [5][7210/7762]	Time 0.365 (0.326)	Data 1.19e-04 (1.48e-04)	Tok/s 45274 (42991)	Loss/tok 3.1163 (3.1433)	LR 1.250e-04
0: TRAIN [5][7220/7762]	Time 0.261 (0.325)	Data 1.02e-04 (1.48e-04)	Tok/s 39940 (42988)	Loss/tok 2.9843 (3.1432)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][7230/7762]	Time 0.355 (0.326)	Data 1.12e-04 (1.48e-04)	Tok/s 47439 (42993)	Loss/tok 3.1621 (3.1433)	LR 1.250e-04
0: TRAIN [5][7240/7762]	Time 0.463 (0.326)	Data 9.87e-05 (1.48e-04)	Tok/s 50424 (42996)	Loss/tok 3.2939 (3.1435)	LR 1.250e-04
0: TRAIN [5][7250/7762]	Time 0.268 (0.326)	Data 1.21e-04 (1.48e-04)	Tok/s 38407 (42998)	Loss/tok 2.9676 (3.1436)	LR 1.250e-04
0: TRAIN [5][7260/7762]	Time 0.264 (0.326)	Data 1.03e-04 (1.48e-04)	Tok/s 38943 (43000)	Loss/tok 2.9879 (3.1436)	LR 1.250e-04
0: TRAIN [5][7270/7762]	Time 0.352 (0.326)	Data 9.63e-05 (1.48e-04)	Tok/s 47561 (43001)	Loss/tok 3.2448 (3.1436)	LR 1.250e-04
0: TRAIN [5][7280/7762]	Time 0.180 (0.326)	Data 1.13e-04 (1.48e-04)	Tok/s 29620 (42998)	Loss/tok 2.4945 (3.1435)	LR 1.250e-04
0: TRAIN [5][7290/7762]	Time 0.258 (0.326)	Data 1.02e-04 (1.48e-04)	Tok/s 40624 (43001)	Loss/tok 2.9345 (3.1435)	LR 1.250e-04
0: TRAIN [5][7300/7762]	Time 0.361 (0.326)	Data 9.78e-05 (1.48e-04)	Tok/s 46517 (43002)	Loss/tok 3.0631 (3.1435)	LR 1.250e-04
0: TRAIN [5][7310/7762]	Time 0.351 (0.326)	Data 1.02e-04 (1.48e-04)	Tok/s 47499 (43004)	Loss/tok 3.2017 (3.1435)	LR 1.250e-04
0: TRAIN [5][7320/7762]	Time 0.364 (0.326)	Data 1.05e-04 (1.48e-04)	Tok/s 46034 (43008)	Loss/tok 3.1638 (3.1438)	LR 1.250e-04
0: TRAIN [5][7330/7762]	Time 0.461 (0.326)	Data 9.73e-05 (1.48e-04)	Tok/s 50325 (43009)	Loss/tok 3.3327 (3.1438)	LR 1.250e-04
0: TRAIN [5][7340/7762]	Time 0.264 (0.326)	Data 1.00e-04 (1.48e-04)	Tok/s 38432 (43007)	Loss/tok 2.8919 (3.1437)	LR 1.250e-04
0: TRAIN [5][7350/7762]	Time 0.259 (0.326)	Data 1.04e-04 (1.48e-04)	Tok/s 40086 (43004)	Loss/tok 2.9698 (3.1437)	LR 1.250e-04
0: TRAIN [5][7360/7762]	Time 0.558 (0.326)	Data 1.03e-04 (1.48e-04)	Tok/s 53698 (43005)	Loss/tok 3.4349 (3.1437)	LR 1.250e-04
0: TRAIN [5][7370/7762]	Time 0.364 (0.326)	Data 1.07e-04 (1.47e-04)	Tok/s 46884 (43006)	Loss/tok 3.0994 (3.1438)	LR 1.250e-04
0: TRAIN [5][7380/7762]	Time 0.460 (0.326)	Data 1.03e-04 (1.47e-04)	Tok/s 50548 (43010)	Loss/tok 3.3697 (3.1440)	LR 1.250e-04
0: TRAIN [5][7390/7762]	Time 0.457 (0.326)	Data 1.00e-04 (1.47e-04)	Tok/s 50763 (43016)	Loss/tok 3.2958 (3.1443)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][7400/7762]	Time 0.262 (0.326)	Data 1.02e-04 (1.47e-04)	Tok/s 40113 (43015)	Loss/tok 2.9043 (3.1442)	LR 1.250e-04
0: TRAIN [5][7410/7762]	Time 0.265 (0.326)	Data 1.01e-04 (1.47e-04)	Tok/s 39063 (43018)	Loss/tok 2.8918 (3.1443)	LR 1.250e-04
0: TRAIN [5][7420/7762]	Time 0.251 (0.326)	Data 9.89e-05 (1.47e-04)	Tok/s 41720 (43019)	Loss/tok 2.9610 (3.1443)	LR 1.250e-04
0: TRAIN [5][7430/7762]	Time 0.358 (0.326)	Data 1.01e-04 (1.47e-04)	Tok/s 46506 (43022)	Loss/tok 3.3013 (3.1444)	LR 1.250e-04
0: TRAIN [5][7440/7762]	Time 0.360 (0.326)	Data 1.03e-04 (1.47e-04)	Tok/s 46667 (43023)	Loss/tok 3.1015 (3.1444)	LR 1.250e-04
0: TRAIN [5][7450/7762]	Time 0.261 (0.326)	Data 1.22e-04 (1.47e-04)	Tok/s 38816 (43020)	Loss/tok 3.0232 (3.1444)	LR 1.250e-04
0: TRAIN [5][7460/7762]	Time 0.367 (0.326)	Data 1.20e-04 (1.47e-04)	Tok/s 45498 (43020)	Loss/tok 3.1970 (3.1444)	LR 1.250e-04
0: TRAIN [5][7470/7762]	Time 0.363 (0.326)	Data 1.10e-04 (1.47e-04)	Tok/s 46635 (43026)	Loss/tok 3.0695 (3.1447)	LR 1.250e-04
0: TRAIN [5][7480/7762]	Time 0.267 (0.326)	Data 9.82e-05 (1.47e-04)	Tok/s 38995 (43024)	Loss/tok 3.0267 (3.1446)	LR 1.250e-04
0: TRAIN [5][7490/7762]	Time 0.264 (0.326)	Data 1.20e-04 (1.47e-04)	Tok/s 38150 (43026)	Loss/tok 2.9641 (3.1448)	LR 1.250e-04
0: TRAIN [5][7500/7762]	Time 0.174 (0.326)	Data 1.03e-04 (1.47e-04)	Tok/s 30394 (43026)	Loss/tok 2.5652 (3.1448)	LR 1.250e-04
0: TRAIN [5][7510/7762]	Time 0.265 (0.326)	Data 1.03e-04 (1.47e-04)	Tok/s 38926 (43024)	Loss/tok 2.9707 (3.1447)	LR 1.250e-04
0: TRAIN [5][7520/7762]	Time 0.253 (0.326)	Data 1.01e-04 (1.47e-04)	Tok/s 40268 (43025)	Loss/tok 2.8501 (3.1447)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][7530/7762]	Time 0.254 (0.326)	Data 1.03e-04 (1.47e-04)	Tok/s 40719 (43024)	Loss/tok 3.0269 (3.1447)	LR 1.250e-04
0: TRAIN [5][7540/7762]	Time 0.361 (0.326)	Data 1.03e-04 (1.47e-04)	Tok/s 46332 (43028)	Loss/tok 2.9827 (3.1447)	LR 1.250e-04
0: TRAIN [5][7550/7762]	Time 0.571 (0.326)	Data 1.08e-04 (1.46e-04)	Tok/s 52063 (43028)	Loss/tok 3.4756 (3.1448)	LR 1.250e-04
0: TRAIN [5][7560/7762]	Time 0.363 (0.326)	Data 1.27e-04 (1.46e-04)	Tok/s 46965 (43027)	Loss/tok 3.0467 (3.1448)	LR 1.250e-04
0: TRAIN [5][7570/7762]	Time 0.464 (0.326)	Data 1.03e-04 (1.46e-04)	Tok/s 50327 (43026)	Loss/tok 3.3244 (3.1448)	LR 1.250e-04
0: TRAIN [5][7580/7762]	Time 0.361 (0.326)	Data 1.04e-04 (1.46e-04)	Tok/s 45921 (43026)	Loss/tok 3.1961 (3.1449)	LR 1.250e-04
0: TRAIN [5][7590/7762]	Time 0.453 (0.326)	Data 1.08e-04 (1.46e-04)	Tok/s 51495 (43026)	Loss/tok 3.3528 (3.1449)	LR 1.250e-04
0: TRAIN [5][7600/7762]	Time 0.259 (0.326)	Data 1.08e-04 (1.46e-04)	Tok/s 39772 (43031)	Loss/tok 2.9153 (3.1450)	LR 1.250e-04
0: TRAIN [5][7610/7762]	Time 0.586 (0.326)	Data 1.03e-04 (1.46e-04)	Tok/s 51159 (43033)	Loss/tok 3.4961 (3.1451)	LR 1.250e-04
0: TRAIN [5][7620/7762]	Time 0.266 (0.326)	Data 1.04e-04 (1.46e-04)	Tok/s 38452 (43032)	Loss/tok 2.9405 (3.1452)	LR 1.250e-04
0: TRAIN [5][7630/7762]	Time 0.358 (0.326)	Data 1.20e-04 (1.46e-04)	Tok/s 46905 (43034)	Loss/tok 3.1367 (3.1453)	LR 1.250e-04
0: TRAIN [5][7640/7762]	Time 0.365 (0.326)	Data 1.00e-04 (1.46e-04)	Tok/s 46140 (43035)	Loss/tok 3.0938 (3.1452)	LR 1.250e-04
0: TRAIN [5][7650/7762]	Time 0.267 (0.326)	Data 1.06e-04 (1.46e-04)	Tok/s 39166 (43034)	Loss/tok 2.8986 (3.1451)	LR 1.250e-04
0: TRAIN [5][7660/7762]	Time 0.264 (0.326)	Data 1.03e-04 (1.46e-04)	Tok/s 40012 (43035)	Loss/tok 2.9997 (3.1451)	LR 1.250e-04
0: TRAIN [5][7670/7762]	Time 0.344 (0.326)	Data 1.06e-04 (1.46e-04)	Tok/s 49364 (43032)	Loss/tok 3.0472 (3.1449)	LR 1.250e-04
0: TRAIN [5][7680/7762]	Time 0.262 (0.326)	Data 9.89e-05 (1.46e-04)	Tok/s 39731 (43033)	Loss/tok 2.8201 (3.1448)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][7690/7762]	Time 0.175 (0.326)	Data 1.15e-04 (1.46e-04)	Tok/s 30740 (43030)	Loss/tok 2.5700 (3.1448)	LR 1.250e-04
0: TRAIN [5][7700/7762]	Time 0.463 (0.326)	Data 1.01e-04 (1.46e-04)	Tok/s 51215 (43032)	Loss/tok 3.3797 (3.1449)	LR 1.250e-04
0: TRAIN [5][7710/7762]	Time 0.358 (0.326)	Data 1.06e-04 (1.46e-04)	Tok/s 47186 (43034)	Loss/tok 3.0906 (3.1449)	LR 1.250e-04
0: TRAIN [5][7720/7762]	Time 0.354 (0.326)	Data 1.02e-04 (1.46e-04)	Tok/s 46950 (43036)	Loss/tok 3.1260 (3.1448)	LR 1.250e-04
0: TRAIN [5][7730/7762]	Time 0.262 (0.326)	Data 1.22e-04 (1.46e-04)	Tok/s 38554 (43038)	Loss/tok 2.9027 (3.1448)	LR 1.250e-04
0: TRAIN [5][7740/7762]	Time 0.270 (0.326)	Data 1.07e-04 (1.45e-04)	Tok/s 39288 (43043)	Loss/tok 2.9858 (3.1449)	LR 1.250e-04
0: TRAIN [5][7750/7762]	Time 0.264 (0.326)	Data 1.19e-04 (1.45e-04)	Tok/s 38908 (43045)	Loss/tok 3.0197 (3.1450)	LR 1.250e-04
0: TRAIN [5][7760/7762]	Time 0.596 (0.326)	Data 1.49e-02 (1.47e-04)	Tok/s 49571 (43044)	Loss/tok 3.4691 (3.1451)	LR 1.250e-04
:::MLL 1573761078.854 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 524}}
:::MLL 1573761078.855 eval_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [5][0/12]	Time 0.897 (0.897)	Decoder iters 149.0 (149.0)	Tok/s 18473 (18473)
0: TEST [5][10/12]	Time 0.118 (0.301)	Decoder iters 25.0 (54.5)	Tok/s 31397 (29339)
0: Running moses detokenizer
0: BLEU(score=23.207126579879606, counts=[36798, 18185, 10197, 5949], totals=[65758, 62755, 59753, 56756], precisions=[55.959731135375165, 28.977770695562107, 17.065251953876793, 10.481711184720558], bp=1.0, sys_len=65758, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1573761084.505 eval_accuracy: {"value": 23.21, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 535}}
:::MLL 1573761084.506 eval_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 5	Training Loss: 3.1440	Test BLEU: 23.21
0: Performance: Epoch: 5	Training: 86089 Tok/s
0: Finished epoch 5
:::MLL 1573761084.506 block_stop: {"value": null, "metadata": {"first_epoch_num": 6, "file": "train.py", "lineno": 557}}
:::MLL 1573761084.507 block_start: {"value": null, "metadata": {"first_epoch_num": 7, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1573761084.507 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 514}}
0: Starting epoch 6
0: Executing preallocation
0: Sampler for epoch 6 uses seed 3172770834
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [6][0/7762]	Time 0.549 (0.549)	Data 3.08e-01 (3.08e-01)	Tok/s 18775 (18775)	Loss/tok 3.0213 (3.0213)	LR 1.250e-04
0: TRAIN [6][10/7762]	Time 0.255 (0.315)	Data 1.03e-04 (2.81e-02)	Tok/s 40552 (41178)	Loss/tok 2.8870 (3.0281)	LR 1.250e-04
0: TRAIN [6][20/7762]	Time 0.258 (0.311)	Data 2.29e-04 (1.48e-02)	Tok/s 39242 (42029)	Loss/tok 2.9490 (3.0491)	LR 1.250e-04
0: TRAIN [6][30/7762]	Time 0.254 (0.303)	Data 9.92e-05 (1.01e-02)	Tok/s 40898 (41884)	Loss/tok 2.8405 (3.0526)	LR 1.250e-04
0: TRAIN [6][40/7762]	Time 0.174 (0.302)	Data 1.03e-04 (7.63e-03)	Tok/s 30721 (41840)	Loss/tok 2.5307 (3.0582)	LR 1.250e-04
0: TRAIN [6][50/7762]	Time 0.262 (0.299)	Data 9.44e-05 (6.15e-03)	Tok/s 40559 (41927)	Loss/tok 2.9008 (3.0533)	LR 1.250e-04
0: TRAIN [6][60/7762]	Time 0.171 (0.292)	Data 9.89e-05 (5.16e-03)	Tok/s 30450 (41379)	Loss/tok 2.5455 (3.0414)	LR 1.250e-04
0: TRAIN [6][70/7762]	Time 0.563 (0.307)	Data 1.03e-04 (4.45e-03)	Tok/s 52498 (42119)	Loss/tok 3.4845 (3.0937)	LR 1.250e-04
0: TRAIN [6][80/7762]	Time 0.268 (0.303)	Data 9.56e-05 (3.91e-03)	Tok/s 39365 (41865)	Loss/tok 2.9562 (3.0854)	LR 1.250e-04
0: TRAIN [6][90/7762]	Time 0.268 (0.307)	Data 1.02e-04 (3.49e-03)	Tok/s 39023 (42131)	Loss/tok 2.8654 (3.0923)	LR 1.250e-04
0: TRAIN [6][100/7762]	Time 0.351 (0.309)	Data 1.03e-04 (3.16e-03)	Tok/s 47529 (42314)	Loss/tok 3.1482 (3.0953)	LR 1.250e-04
0: TRAIN [6][110/7762]	Time 0.262 (0.312)	Data 1.16e-04 (2.88e-03)	Tok/s 39314 (42445)	Loss/tok 3.0437 (3.0998)	LR 1.250e-04
0: TRAIN [6][120/7762]	Time 0.351 (0.314)	Data 1.01e-04 (2.65e-03)	Tok/s 48411 (42554)	Loss/tok 3.0862 (3.1011)	LR 1.250e-04
0: TRAIN [6][130/7762]	Time 0.559 (0.313)	Data 9.99e-05 (2.46e-03)	Tok/s 53940 (42455)	Loss/tok 3.4722 (3.1031)	LR 1.250e-04
0: TRAIN [6][140/7762]	Time 0.260 (0.312)	Data 1.14e-04 (2.29e-03)	Tok/s 39816 (42442)	Loss/tok 2.9405 (3.1002)	LR 1.250e-04
0: TRAIN [6][150/7762]	Time 0.266 (0.314)	Data 1.02e-04 (2.15e-03)	Tok/s 38630 (42580)	Loss/tok 3.0112 (3.1025)	LR 1.250e-04
0: TRAIN [6][160/7762]	Time 0.358 (0.317)	Data 1.19e-04 (2.02e-03)	Tok/s 46654 (42691)	Loss/tok 3.1467 (3.1108)	LR 1.250e-04
0: TRAIN [6][170/7762]	Time 0.579 (0.320)	Data 1.02e-04 (1.91e-03)	Tok/s 50850 (42899)	Loss/tok 3.4461 (3.1146)	LR 1.250e-04
0: TRAIN [6][180/7762]	Time 0.361 (0.321)	Data 1.02e-04 (1.81e-03)	Tok/s 46484 (42907)	Loss/tok 3.1272 (3.1140)	LR 1.250e-04
0: TRAIN [6][190/7762]	Time 0.266 (0.324)	Data 1.24e-04 (1.72e-03)	Tok/s 38803 (43096)	Loss/tok 2.8801 (3.1189)	LR 1.250e-04
0: TRAIN [6][200/7762]	Time 0.457 (0.324)	Data 9.80e-05 (1.64e-03)	Tok/s 50652 (43008)	Loss/tok 3.4102 (3.1206)	LR 1.250e-04
0: TRAIN [6][210/7762]	Time 0.251 (0.324)	Data 9.61e-05 (1.57e-03)	Tok/s 42164 (43004)	Loss/tok 2.8991 (3.1211)	LR 1.250e-04
0: TRAIN [6][220/7762]	Time 0.584 (0.328)	Data 1.00e-04 (1.50e-03)	Tok/s 50736 (43181)	Loss/tok 3.5231 (3.1319)	LR 1.250e-04
0: TRAIN [6][230/7762]	Time 0.360 (0.329)	Data 1.07e-04 (1.44e-03)	Tok/s 47239 (43189)	Loss/tok 3.1584 (3.1332)	LR 1.250e-04
0: TRAIN [6][240/7762]	Time 0.359 (0.329)	Data 1.23e-04 (1.38e-03)	Tok/s 46531 (43251)	Loss/tok 3.1736 (3.1308)	LR 1.250e-04
0: TRAIN [6][250/7762]	Time 0.261 (0.328)	Data 1.17e-04 (1.33e-03)	Tok/s 39526 (43136)	Loss/tok 2.8019 (3.1268)	LR 1.250e-04
0: TRAIN [6][260/7762]	Time 0.264 (0.327)	Data 9.87e-05 (1.29e-03)	Tok/s 39016 (43104)	Loss/tok 2.9009 (3.1237)	LR 1.250e-04
0: TRAIN [6][270/7762]	Time 0.356 (0.328)	Data 1.00e-04 (1.24e-03)	Tok/s 46514 (43164)	Loss/tok 3.1563 (3.1251)	LR 1.250e-04
0: TRAIN [6][280/7762]	Time 0.263 (0.327)	Data 1.14e-04 (1.20e-03)	Tok/s 40165 (43156)	Loss/tok 2.8151 (3.1248)	LR 1.250e-04
0: TRAIN [6][290/7762]	Time 0.461 (0.327)	Data 1.04e-04 (1.16e-03)	Tok/s 50894 (43142)	Loss/tok 3.2240 (3.1231)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][300/7762]	Time 0.365 (0.329)	Data 1.19e-04 (1.13e-03)	Tok/s 46242 (43262)	Loss/tok 3.0804 (3.1277)	LR 1.250e-04
0: TRAIN [6][310/7762]	Time 0.265 (0.330)	Data 9.94e-05 (1.10e-03)	Tok/s 38708 (43269)	Loss/tok 2.8800 (3.1305)	LR 1.250e-04
0: TRAIN [6][320/7762]	Time 0.266 (0.330)	Data 1.06e-04 (1.07e-03)	Tok/s 38371 (43249)	Loss/tok 3.1403 (3.1331)	LR 1.250e-04
0: TRAIN [6][330/7762]	Time 0.260 (0.329)	Data 9.73e-05 (1.04e-03)	Tok/s 40364 (43168)	Loss/tok 2.9309 (3.1286)	LR 1.250e-04
0: TRAIN [6][340/7762]	Time 0.359 (0.329)	Data 1.06e-04 (1.01e-03)	Tok/s 46474 (43208)	Loss/tok 3.2403 (3.1289)	LR 1.250e-04
0: TRAIN [6][350/7762]	Time 0.346 (0.329)	Data 1.23e-04 (9.83e-04)	Tok/s 49037 (43204)	Loss/tok 3.1037 (3.1294)	LR 1.250e-04
0: TRAIN [6][360/7762]	Time 0.451 (0.330)	Data 1.04e-04 (9.59e-04)	Tok/s 51882 (43275)	Loss/tok 3.4033 (3.1330)	LR 1.250e-04
0: TRAIN [6][370/7762]	Time 0.264 (0.332)	Data 1.98e-04 (9.36e-04)	Tok/s 39831 (43336)	Loss/tok 2.9833 (3.1355)	LR 1.250e-04
0: TRAIN [6][380/7762]	Time 0.367 (0.331)	Data 1.01e-04 (9.14e-04)	Tok/s 46707 (43329)	Loss/tok 3.0251 (3.1330)	LR 1.250e-04
0: TRAIN [6][390/7762]	Time 0.364 (0.333)	Data 9.94e-05 (8.94e-04)	Tok/s 45149 (43406)	Loss/tok 3.1985 (3.1364)	LR 1.250e-04
0: TRAIN [6][400/7762]	Time 0.262 (0.333)	Data 1.00e-04 (8.74e-04)	Tok/s 39129 (43411)	Loss/tok 3.0108 (3.1370)	LR 1.250e-04
0: TRAIN [6][410/7762]	Time 0.264 (0.334)	Data 1.03e-04 (8.55e-04)	Tok/s 38120 (43407)	Loss/tok 2.8197 (3.1371)	LR 1.250e-04
0: TRAIN [6][420/7762]	Time 0.264 (0.334)	Data 9.94e-05 (8.37e-04)	Tok/s 39106 (43440)	Loss/tok 3.0708 (3.1386)	LR 1.250e-04
0: TRAIN [6][430/7762]	Time 0.256 (0.333)	Data 1.02e-04 (8.21e-04)	Tok/s 39560 (43397)	Loss/tok 2.8855 (3.1370)	LR 1.250e-04
0: TRAIN [6][440/7762]	Time 0.468 (0.334)	Data 1.02e-04 (8.05e-04)	Tok/s 49680 (43436)	Loss/tok 3.3069 (3.1395)	LR 1.250e-04
0: TRAIN [6][450/7762]	Time 0.354 (0.335)	Data 1.05e-04 (7.89e-04)	Tok/s 47408 (43493)	Loss/tok 3.1248 (3.1402)	LR 1.250e-04
0: TRAIN [6][460/7762]	Time 0.262 (0.335)	Data 1.02e-04 (7.74e-04)	Tok/s 39110 (43498)	Loss/tok 2.9391 (3.1414)	LR 1.250e-04
0: TRAIN [6][470/7762]	Time 0.358 (0.335)	Data 1.01e-04 (7.60e-04)	Tok/s 46729 (43512)	Loss/tok 3.0490 (3.1400)	LR 1.250e-04
0: TRAIN [6][480/7762]	Time 0.266 (0.334)	Data 9.56e-05 (7.46e-04)	Tok/s 38958 (43428)	Loss/tok 2.9250 (3.1386)	LR 1.250e-04
0: TRAIN [6][490/7762]	Time 0.264 (0.335)	Data 1.04e-04 (7.33e-04)	Tok/s 39599 (43469)	Loss/tok 2.9550 (3.1380)	LR 1.250e-04
0: TRAIN [6][500/7762]	Time 0.264 (0.335)	Data 1.00e-04 (7.20e-04)	Tok/s 38907 (43467)	Loss/tok 2.9754 (3.1373)	LR 1.250e-04
0: TRAIN [6][510/7762]	Time 0.169 (0.334)	Data 1.03e-04 (7.08e-04)	Tok/s 31289 (43438)	Loss/tok 2.5961 (3.1356)	LR 1.250e-04
0: TRAIN [6][520/7762]	Time 0.357 (0.334)	Data 1.14e-04 (6.97e-04)	Tok/s 47441 (43442)	Loss/tok 3.1649 (3.1354)	LR 1.250e-04
0: TRAIN [6][530/7762]	Time 0.452 (0.334)	Data 1.08e-04 (6.85e-04)	Tok/s 52100 (43468)	Loss/tok 3.3283 (3.1368)	LR 1.250e-04
0: TRAIN [6][540/7762]	Time 0.174 (0.333)	Data 1.04e-04 (6.75e-04)	Tok/s 30546 (43366)	Loss/tok 2.5530 (3.1355)	LR 1.250e-04
0: TRAIN [6][550/7762]	Time 0.264 (0.333)	Data 1.15e-04 (6.65e-04)	Tok/s 39940 (43353)	Loss/tok 2.8724 (3.1346)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [6][560/7762]	Time 0.263 (0.332)	Data 9.97e-05 (6.55e-04)	Tok/s 38923 (43337)	Loss/tok 2.9451 (3.1341)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][570/7762]	Time 0.173 (0.332)	Data 9.97e-05 (6.45e-04)	Tok/s 31023 (43314)	Loss/tok 2.6887 (3.1334)	LR 1.250e-04
0: TRAIN [6][580/7762]	Time 0.262 (0.332)	Data 1.03e-04 (6.36e-04)	Tok/s 39361 (43315)	Loss/tok 2.8788 (3.1333)	LR 1.250e-04
0: TRAIN [6][590/7762]	Time 0.260 (0.333)	Data 1.03e-04 (6.27e-04)	Tok/s 39906 (43381)	Loss/tok 2.9894 (3.1340)	LR 1.250e-04
0: TRAIN [6][600/7762]	Time 0.268 (0.332)	Data 9.78e-05 (6.18e-04)	Tok/s 38225 (43295)	Loss/tok 2.9414 (3.1328)	LR 1.250e-04
0: TRAIN [6][610/7762]	Time 0.363 (0.332)	Data 9.85e-05 (6.10e-04)	Tok/s 46656 (43330)	Loss/tok 3.0345 (3.1324)	LR 1.250e-04
0: TRAIN [6][620/7762]	Time 0.583 (0.333)	Data 9.89e-05 (6.01e-04)	Tok/s 50955 (43333)	Loss/tok 3.4954 (3.1344)	LR 1.250e-04
0: TRAIN [6][630/7762]	Time 0.261 (0.333)	Data 1.17e-04 (5.94e-04)	Tok/s 39475 (43338)	Loss/tok 2.8844 (3.1349)	LR 1.250e-04
0: TRAIN [6][640/7762]	Time 0.262 (0.333)	Data 9.80e-05 (5.86e-04)	Tok/s 39043 (43317)	Loss/tok 2.9105 (3.1342)	LR 1.250e-04
0: TRAIN [6][650/7762]	Time 0.589 (0.333)	Data 1.01e-04 (5.79e-04)	Tok/s 50209 (43350)	Loss/tok 3.4645 (3.1346)	LR 1.250e-04
0: TRAIN [6][660/7762]	Time 0.255 (0.332)	Data 1.15e-04 (5.71e-04)	Tok/s 41245 (43309)	Loss/tok 2.9165 (3.1332)	LR 1.250e-04
0: TRAIN [6][670/7762]	Time 0.461 (0.333)	Data 9.94e-05 (5.64e-04)	Tok/s 51250 (43337)	Loss/tok 3.2494 (3.1337)	LR 1.250e-04
0: TRAIN [6][680/7762]	Time 0.175 (0.332)	Data 1.17e-04 (5.58e-04)	Tok/s 29612 (43310)	Loss/tok 2.4752 (3.1330)	LR 1.250e-04
0: TRAIN [6][690/7762]	Time 0.265 (0.331)	Data 1.02e-04 (5.51e-04)	Tok/s 39395 (43245)	Loss/tok 2.9263 (3.1323)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][700/7762]	Time 0.265 (0.331)	Data 1.49e-04 (5.45e-04)	Tok/s 39447 (43248)	Loss/tok 2.8995 (3.1314)	LR 1.250e-04
0: TRAIN [6][710/7762]	Time 0.461 (0.332)	Data 1.04e-04 (5.39e-04)	Tok/s 50694 (43267)	Loss/tok 3.2717 (3.1323)	LR 1.250e-04
0: TRAIN [6][720/7762]	Time 0.352 (0.331)	Data 1.16e-04 (5.33e-04)	Tok/s 47952 (43241)	Loss/tok 3.0851 (3.1312)	LR 1.250e-04
0: TRAIN [6][730/7762]	Time 0.261 (0.331)	Data 1.05e-04 (5.27e-04)	Tok/s 39635 (43240)	Loss/tok 2.9316 (3.1302)	LR 1.250e-04
0: TRAIN [6][740/7762]	Time 0.265 (0.331)	Data 1.00e-04 (5.21e-04)	Tok/s 39496 (43232)	Loss/tok 2.8873 (3.1299)	LR 1.250e-04
0: TRAIN [6][750/7762]	Time 0.258 (0.330)	Data 1.02e-04 (5.16e-04)	Tok/s 39935 (43205)	Loss/tok 2.9571 (3.1293)	LR 1.250e-04
0: TRAIN [6][760/7762]	Time 0.258 (0.331)	Data 1.18e-04 (5.10e-04)	Tok/s 41304 (43219)	Loss/tok 2.9780 (3.1294)	LR 1.250e-04
0: TRAIN [6][770/7762]	Time 0.450 (0.331)	Data 9.82e-05 (5.05e-04)	Tok/s 51413 (43215)	Loss/tok 3.2682 (3.1302)	LR 1.250e-04
0: TRAIN [6][780/7762]	Time 0.366 (0.331)	Data 1.02e-04 (5.00e-04)	Tok/s 46446 (43255)	Loss/tok 3.1107 (3.1300)	LR 1.250e-04
0: TRAIN [6][790/7762]	Time 0.570 (0.332)	Data 1.02e-04 (4.95e-04)	Tok/s 51964 (43289)	Loss/tok 3.5060 (3.1309)	LR 1.250e-04
0: TRAIN [6][800/7762]	Time 0.364 (0.332)	Data 1.05e-04 (4.90e-04)	Tok/s 46427 (43295)	Loss/tok 3.1870 (3.1307)	LR 1.250e-04
0: TRAIN [6][810/7762]	Time 0.577 (0.332)	Data 9.82e-05 (4.85e-04)	Tok/s 50364 (43313)	Loss/tok 3.5453 (3.1316)	LR 1.250e-04
0: TRAIN [6][820/7762]	Time 0.347 (0.332)	Data 1.20e-04 (4.80e-04)	Tok/s 49015 (43318)	Loss/tok 3.0349 (3.1313)	LR 1.250e-04
0: TRAIN [6][830/7762]	Time 0.586 (0.332)	Data 9.80e-05 (4.76e-04)	Tok/s 50400 (43318)	Loss/tok 3.4177 (3.1315)	LR 1.250e-04
0: TRAIN [6][840/7762]	Time 0.364 (0.332)	Data 1.04e-04 (4.71e-04)	Tok/s 46100 (43306)	Loss/tok 2.9823 (3.1312)	LR 1.250e-04
0: TRAIN [6][850/7762]	Time 0.365 (0.333)	Data 1.03e-04 (4.67e-04)	Tok/s 45900 (43309)	Loss/tok 3.1445 (3.1326)	LR 1.250e-04
0: TRAIN [6][860/7762]	Time 0.258 (0.332)	Data 1.02e-04 (4.63e-04)	Tok/s 40410 (43304)	Loss/tok 2.9318 (3.1316)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][870/7762]	Time 0.355 (0.332)	Data 1.06e-04 (4.59e-04)	Tok/s 47249 (43301)	Loss/tok 3.0634 (3.1315)	LR 1.250e-04
0: TRAIN [6][880/7762]	Time 0.264 (0.333)	Data 1.07e-04 (4.55e-04)	Tok/s 38818 (43305)	Loss/tok 2.9067 (3.1319)	LR 1.250e-04
0: TRAIN [6][890/7762]	Time 0.461 (0.332)	Data 1.06e-04 (4.51e-04)	Tok/s 50754 (43300)	Loss/tok 3.2484 (3.1316)	LR 1.250e-04
0: TRAIN [6][900/7762]	Time 0.463 (0.332)	Data 9.87e-05 (4.47e-04)	Tok/s 51071 (43309)	Loss/tok 3.0781 (3.1312)	LR 1.250e-04
0: TRAIN [6][910/7762]	Time 0.363 (0.332)	Data 9.82e-05 (4.43e-04)	Tok/s 45549 (43285)	Loss/tok 3.1231 (3.1307)	LR 1.250e-04
0: TRAIN [6][920/7762]	Time 0.262 (0.332)	Data 1.00e-04 (4.40e-04)	Tok/s 40212 (43271)	Loss/tok 2.9788 (3.1301)	LR 1.250e-04
0: TRAIN [6][930/7762]	Time 0.342 (0.331)	Data 1.06e-04 (4.36e-04)	Tok/s 49916 (43272)	Loss/tok 3.0069 (3.1294)	LR 1.250e-04
0: TRAIN [6][940/7762]	Time 0.259 (0.331)	Data 1.04e-04 (4.32e-04)	Tok/s 39230 (43283)	Loss/tok 2.9228 (3.1292)	LR 1.250e-04
0: TRAIN [6][950/7762]	Time 0.172 (0.330)	Data 1.00e-04 (4.29e-04)	Tok/s 31264 (43213)	Loss/tok 2.5386 (3.1274)	LR 1.250e-04
0: TRAIN [6][960/7762]	Time 0.582 (0.331)	Data 9.82e-05 (4.26e-04)	Tok/s 51650 (43225)	Loss/tok 3.4351 (3.1283)	LR 1.250e-04
0: TRAIN [6][970/7762]	Time 0.264 (0.331)	Data 1.03e-04 (4.22e-04)	Tok/s 39196 (43224)	Loss/tok 2.9123 (3.1282)	LR 1.250e-04
0: TRAIN [6][980/7762]	Time 0.177 (0.330)	Data 1.01e-04 (4.19e-04)	Tok/s 29211 (43192)	Loss/tok 2.5958 (3.1272)	LR 1.250e-04
0: TRAIN [6][990/7762]	Time 0.461 (0.330)	Data 9.85e-05 (4.16e-04)	Tok/s 51260 (43214)	Loss/tok 3.3145 (3.1270)	LR 1.250e-04
0: TRAIN [6][1000/7762]	Time 0.261 (0.330)	Data 1.01e-04 (4.13e-04)	Tok/s 40399 (43213)	Loss/tok 2.8853 (3.1267)	LR 1.250e-04
0: TRAIN [6][1010/7762]	Time 0.263 (0.330)	Data 9.78e-05 (4.10e-04)	Tok/s 39839 (43204)	Loss/tok 2.9332 (3.1260)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][1020/7762]	Time 0.266 (0.330)	Data 1.03e-04 (4.07e-04)	Tok/s 37975 (43206)	Loss/tok 2.9439 (3.1260)	LR 1.250e-04
0: TRAIN [6][1030/7762]	Time 0.262 (0.330)	Data 1.05e-04 (4.04e-04)	Tok/s 39367 (43221)	Loss/tok 2.8481 (3.1259)	LR 1.250e-04
0: TRAIN [6][1040/7762]	Time 0.461 (0.330)	Data 1.17e-04 (4.01e-04)	Tok/s 50423 (43223)	Loss/tok 3.2902 (3.1261)	LR 1.250e-04
0: TRAIN [6][1050/7762]	Time 0.364 (0.331)	Data 1.17e-04 (3.98e-04)	Tok/s 45484 (43259)	Loss/tok 3.2409 (3.1270)	LR 1.250e-04
0: TRAIN [6][1060/7762]	Time 0.360 (0.331)	Data 1.16e-04 (3.95e-04)	Tok/s 46876 (43273)	Loss/tok 2.9889 (3.1276)	LR 1.250e-04
0: TRAIN [6][1070/7762]	Time 0.360 (0.331)	Data 1.02e-04 (3.93e-04)	Tok/s 46735 (43276)	Loss/tok 3.2800 (3.1276)	LR 1.250e-04
0: TRAIN [6][1080/7762]	Time 0.360 (0.331)	Data 9.94e-05 (3.90e-04)	Tok/s 46646 (43279)	Loss/tok 3.2266 (3.1270)	LR 1.250e-04
0: TRAIN [6][1090/7762]	Time 0.265 (0.330)	Data 1.01e-04 (3.87e-04)	Tok/s 38674 (43254)	Loss/tok 2.9630 (3.1260)	LR 1.250e-04
0: TRAIN [6][1100/7762]	Time 0.351 (0.330)	Data 9.63e-05 (3.85e-04)	Tok/s 48237 (43260)	Loss/tok 3.0317 (3.1258)	LR 1.250e-04
0: TRAIN [6][1110/7762]	Time 0.259 (0.330)	Data 9.73e-05 (3.82e-04)	Tok/s 40719 (43245)	Loss/tok 2.9441 (3.1257)	LR 1.250e-04
0: TRAIN [6][1120/7762]	Time 0.359 (0.330)	Data 1.01e-04 (3.80e-04)	Tok/s 46235 (43268)	Loss/tok 3.1717 (3.1265)	LR 1.250e-04
0: TRAIN [6][1130/7762]	Time 0.354 (0.330)	Data 1.02e-04 (3.77e-04)	Tok/s 47675 (43269)	Loss/tok 3.0956 (3.1265)	LR 1.250e-04
0: TRAIN [6][1140/7762]	Time 0.178 (0.330)	Data 1.02e-04 (3.75e-04)	Tok/s 30086 (43264)	Loss/tok 2.5571 (3.1266)	LR 1.250e-04
0: TRAIN [6][1150/7762]	Time 0.349 (0.330)	Data 1.26e-04 (3.73e-04)	Tok/s 48255 (43265)	Loss/tok 3.0702 (3.1262)	LR 1.250e-04
0: TRAIN [6][1160/7762]	Time 0.445 (0.330)	Data 1.03e-04 (3.70e-04)	Tok/s 51834 (43279)	Loss/tok 3.2764 (3.1265)	LR 1.250e-04
0: TRAIN [6][1170/7762]	Time 0.179 (0.330)	Data 9.58e-05 (3.68e-04)	Tok/s 29418 (43260)	Loss/tok 2.5800 (3.1263)	LR 1.250e-04
0: TRAIN [6][1180/7762]	Time 0.269 (0.331)	Data 1.18e-04 (3.66e-04)	Tok/s 38496 (43277)	Loss/tok 2.9775 (3.1269)	LR 1.250e-04
0: TRAIN [6][1190/7762]	Time 0.352 (0.330)	Data 1.03e-04 (3.64e-04)	Tok/s 47487 (43271)	Loss/tok 3.1280 (3.1266)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][1200/7762]	Time 0.461 (0.331)	Data 9.49e-05 (3.61e-04)	Tok/s 51068 (43266)	Loss/tok 3.3455 (3.1270)	LR 1.250e-04
0: TRAIN [6][1210/7762]	Time 0.254 (0.331)	Data 1.01e-04 (3.59e-04)	Tok/s 40210 (43285)	Loss/tok 3.0252 (3.1281)	LR 1.250e-04
0: TRAIN [6][1220/7762]	Time 0.462 (0.331)	Data 9.89e-05 (3.57e-04)	Tok/s 50187 (43276)	Loss/tok 3.4649 (3.1287)	LR 1.250e-04
0: TRAIN [6][1230/7762]	Time 0.462 (0.331)	Data 1.02e-04 (3.55e-04)	Tok/s 50113 (43280)	Loss/tok 3.2632 (3.1283)	LR 1.250e-04
0: TRAIN [6][1240/7762]	Time 0.367 (0.331)	Data 1.18e-04 (3.53e-04)	Tok/s 46041 (43301)	Loss/tok 3.0625 (3.1285)	LR 1.250e-04
0: TRAIN [6][1250/7762]	Time 0.362 (0.331)	Data 1.04e-04 (3.51e-04)	Tok/s 46383 (43313)	Loss/tok 3.0690 (3.1281)	LR 1.250e-04
0: TRAIN [6][1260/7762]	Time 0.569 (0.331)	Data 1.07e-04 (3.49e-04)	Tok/s 52762 (43316)	Loss/tok 3.4622 (3.1282)	LR 1.250e-04
0: TRAIN [6][1270/7762]	Time 0.259 (0.331)	Data 9.87e-05 (3.47e-04)	Tok/s 39292 (43296)	Loss/tok 2.8498 (3.1282)	LR 1.250e-04
0: TRAIN [6][1280/7762]	Time 0.258 (0.331)	Data 9.80e-05 (3.45e-04)	Tok/s 39883 (43277)	Loss/tok 2.9343 (3.1283)	LR 1.250e-04
0: TRAIN [6][1290/7762]	Time 0.352 (0.331)	Data 9.56e-05 (3.44e-04)	Tok/s 47077 (43287)	Loss/tok 3.1956 (3.1281)	LR 1.250e-04
0: TRAIN [6][1300/7762]	Time 0.263 (0.331)	Data 1.14e-04 (3.42e-04)	Tok/s 39591 (43280)	Loss/tok 2.9343 (3.1283)	LR 1.250e-04
0: TRAIN [6][1310/7762]	Time 0.364 (0.331)	Data 9.94e-05 (3.40e-04)	Tok/s 45833 (43276)	Loss/tok 3.1031 (3.1281)	LR 1.250e-04
0: TRAIN [6][1320/7762]	Time 0.254 (0.331)	Data 1.01e-04 (3.38e-04)	Tok/s 40626 (43266)	Loss/tok 2.9189 (3.1274)	LR 1.250e-04
0: TRAIN [6][1330/7762]	Time 0.265 (0.330)	Data 1.00e-04 (3.36e-04)	Tok/s 39587 (43242)	Loss/tok 2.8721 (3.1271)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][1340/7762]	Time 0.263 (0.330)	Data 1.08e-04 (3.35e-04)	Tok/s 39196 (43223)	Loss/tok 2.9950 (3.1267)	LR 1.250e-04
0: TRAIN [6][1350/7762]	Time 0.177 (0.330)	Data 1.30e-04 (3.33e-04)	Tok/s 29641 (43225)	Loss/tok 2.5325 (3.1264)	LR 1.250e-04
0: TRAIN [6][1360/7762]	Time 0.461 (0.330)	Data 1.02e-04 (3.31e-04)	Tok/s 50688 (43216)	Loss/tok 3.2976 (3.1261)	LR 1.250e-04
0: TRAIN [6][1370/7762]	Time 0.363 (0.330)	Data 1.03e-04 (3.30e-04)	Tok/s 46675 (43231)	Loss/tok 3.1420 (3.1265)	LR 1.250e-04
0: TRAIN [6][1380/7762]	Time 0.458 (0.330)	Data 9.89e-05 (3.28e-04)	Tok/s 51342 (43229)	Loss/tok 3.2577 (3.1263)	LR 1.250e-04
0: TRAIN [6][1390/7762]	Time 0.367 (0.330)	Data 1.02e-04 (3.26e-04)	Tok/s 46272 (43244)	Loss/tok 3.0646 (3.1267)	LR 1.250e-04
0: TRAIN [6][1400/7762]	Time 0.575 (0.330)	Data 1.03e-04 (3.25e-04)	Tok/s 51382 (43236)	Loss/tok 3.4292 (3.1271)	LR 1.250e-04
0: TRAIN [6][1410/7762]	Time 0.367 (0.330)	Data 1.10e-04 (3.23e-04)	Tok/s 45798 (43252)	Loss/tok 3.1034 (3.1272)	LR 1.250e-04
0: TRAIN [6][1420/7762]	Time 0.265 (0.330)	Data 1.09e-04 (3.22e-04)	Tok/s 38475 (43227)	Loss/tok 2.9193 (3.1263)	LR 1.250e-04
0: TRAIN [6][1430/7762]	Time 0.364 (0.330)	Data 1.10e-04 (3.20e-04)	Tok/s 45715 (43229)	Loss/tok 3.2255 (3.1262)	LR 1.250e-04
0: TRAIN [6][1440/7762]	Time 0.265 (0.330)	Data 2.35e-04 (3.19e-04)	Tok/s 38587 (43217)	Loss/tok 2.8544 (3.1255)	LR 1.250e-04
0: TRAIN [6][1450/7762]	Time 0.262 (0.330)	Data 1.04e-04 (3.18e-04)	Tok/s 39972 (43223)	Loss/tok 2.9070 (3.1253)	LR 1.250e-04
0: TRAIN [6][1460/7762]	Time 0.260 (0.330)	Data 1.00e-04 (3.16e-04)	Tok/s 39733 (43228)	Loss/tok 2.8523 (3.1251)	LR 1.250e-04
0: TRAIN [6][1470/7762]	Time 0.266 (0.330)	Data 1.85e-04 (3.15e-04)	Tok/s 38660 (43235)	Loss/tok 3.0111 (3.1248)	LR 1.250e-04
0: TRAIN [6][1480/7762]	Time 0.364 (0.330)	Data 1.18e-04 (3.14e-04)	Tok/s 45081 (43227)	Loss/tok 3.1940 (3.1248)	LR 1.250e-04
0: TRAIN [6][1490/7762]	Time 0.259 (0.330)	Data 1.05e-04 (3.12e-04)	Tok/s 39751 (43235)	Loss/tok 2.9278 (3.1247)	LR 1.250e-04
0: TRAIN [6][1500/7762]	Time 0.251 (0.329)	Data 9.85e-05 (3.11e-04)	Tok/s 41536 (43220)	Loss/tok 2.8882 (3.1243)	LR 1.250e-04
0: TRAIN [6][1510/7762]	Time 0.177 (0.329)	Data 1.03e-04 (3.09e-04)	Tok/s 30114 (43214)	Loss/tok 2.5057 (3.1237)	LR 1.250e-04
0: TRAIN [6][1520/7762]	Time 0.465 (0.329)	Data 1.20e-04 (3.08e-04)	Tok/s 50281 (43226)	Loss/tok 3.1939 (3.1241)	LR 1.250e-04
0: TRAIN [6][1530/7762]	Time 0.462 (0.329)	Data 9.94e-05 (3.07e-04)	Tok/s 50345 (43218)	Loss/tok 3.2292 (3.1242)	LR 1.250e-04
0: TRAIN [6][1540/7762]	Time 0.580 (0.329)	Data 9.56e-05 (3.05e-04)	Tok/s 51267 (43219)	Loss/tok 3.4356 (3.1248)	LR 1.250e-04
0: TRAIN [6][1550/7762]	Time 0.262 (0.329)	Data 1.04e-04 (3.04e-04)	Tok/s 39544 (43194)	Loss/tok 3.0355 (3.1244)	LR 1.250e-04
0: TRAIN [6][1560/7762]	Time 0.264 (0.329)	Data 1.03e-04 (3.03e-04)	Tok/s 39180 (43188)	Loss/tok 3.0130 (3.1240)	LR 1.250e-04
0: TRAIN [6][1570/7762]	Time 0.259 (0.329)	Data 9.56e-05 (3.02e-04)	Tok/s 40209 (43169)	Loss/tok 2.8633 (3.1235)	LR 1.250e-04
0: TRAIN [6][1580/7762]	Time 0.265 (0.329)	Data 1.03e-04 (3.00e-04)	Tok/s 39375 (43169)	Loss/tok 2.7847 (3.1238)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [6][1590/7762]	Time 0.264 (0.329)	Data 1.10e-04 (2.99e-04)	Tok/s 39263 (43184)	Loss/tok 2.8876 (3.1238)	LR 1.250e-04
0: TRAIN [6][1600/7762]	Time 0.266 (0.329)	Data 1.01e-04 (2.98e-04)	Tok/s 39627 (43180)	Loss/tok 2.9898 (3.1232)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][1610/7762]	Time 0.438 (0.329)	Data 9.68e-05 (2.97e-04)	Tok/s 53132 (43182)	Loss/tok 3.3085 (3.1234)	LR 1.250e-04
0: TRAIN [6][1620/7762]	Time 0.259 (0.329)	Data 1.09e-04 (2.95e-04)	Tok/s 40185 (43167)	Loss/tok 3.0012 (3.1229)	LR 1.250e-04
0: TRAIN [6][1630/7762]	Time 0.261 (0.329)	Data 9.99e-05 (2.94e-04)	Tok/s 39704 (43184)	Loss/tok 2.9895 (3.1234)	LR 1.250e-04
0: TRAIN [6][1640/7762]	Time 0.257 (0.329)	Data 1.01e-04 (2.93e-04)	Tok/s 39962 (43175)	Loss/tok 2.9714 (3.1231)	LR 1.250e-04
0: TRAIN [6][1650/7762]	Time 0.560 (0.329)	Data 1.08e-04 (2.92e-04)	Tok/s 52264 (43162)	Loss/tok 3.6619 (3.1235)	LR 1.250e-04
0: TRAIN [6][1660/7762]	Time 0.254 (0.328)	Data 1.01e-04 (2.91e-04)	Tok/s 39675 (43142)	Loss/tok 2.8406 (3.1234)	LR 1.250e-04
0: TRAIN [6][1670/7762]	Time 0.173 (0.328)	Data 1.06e-04 (2.90e-04)	Tok/s 30172 (43143)	Loss/tok 2.5719 (3.1234)	LR 1.250e-04
0: TRAIN [6][1680/7762]	Time 0.258 (0.328)	Data 1.20e-04 (2.89e-04)	Tok/s 39491 (43133)	Loss/tok 2.9255 (3.1231)	LR 1.250e-04
0: TRAIN [6][1690/7762]	Time 0.553 (0.329)	Data 1.05e-04 (2.88e-04)	Tok/s 53344 (43149)	Loss/tok 3.5908 (3.1246)	LR 1.250e-04
0: TRAIN [6][1700/7762]	Time 0.591 (0.329)	Data 1.03e-04 (2.86e-04)	Tok/s 50304 (43158)	Loss/tok 3.4764 (3.1250)	LR 1.250e-04
0: TRAIN [6][1710/7762]	Time 0.260 (0.329)	Data 1.12e-04 (2.85e-04)	Tok/s 39958 (43168)	Loss/tok 3.0249 (3.1252)	LR 1.250e-04
0: TRAIN [6][1720/7762]	Time 0.261 (0.329)	Data 9.97e-05 (2.84e-04)	Tok/s 39607 (43183)	Loss/tok 2.8348 (3.1254)	LR 1.250e-04
0: TRAIN [6][1730/7762]	Time 0.462 (0.330)	Data 1.05e-04 (2.83e-04)	Tok/s 50704 (43195)	Loss/tok 3.4103 (3.1265)	LR 1.250e-04
0: TRAIN [6][1740/7762]	Time 0.358 (0.330)	Data 1.04e-04 (2.82e-04)	Tok/s 46425 (43198)	Loss/tok 3.1744 (3.1264)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][1750/7762]	Time 0.263 (0.330)	Data 1.01e-04 (2.81e-04)	Tok/s 39155 (43196)	Loss/tok 3.0099 (3.1265)	LR 1.250e-04
0: TRAIN [6][1760/7762]	Time 0.260 (0.329)	Data 9.87e-05 (2.80e-04)	Tok/s 39859 (43187)	Loss/tok 2.9081 (3.1262)	LR 1.250e-04
0: TRAIN [6][1770/7762]	Time 0.262 (0.329)	Data 1.05e-04 (2.79e-04)	Tok/s 39998 (43175)	Loss/tok 2.9771 (3.1264)	LR 1.250e-04
0: TRAIN [6][1780/7762]	Time 0.462 (0.329)	Data 1.14e-04 (2.78e-04)	Tok/s 50778 (43176)	Loss/tok 3.1356 (3.1262)	LR 1.250e-04
0: TRAIN [6][1790/7762]	Time 0.257 (0.329)	Data 1.03e-04 (2.77e-04)	Tok/s 39360 (43162)	Loss/tok 3.0917 (3.1263)	LR 1.250e-04
0: TRAIN [6][1800/7762]	Time 0.264 (0.329)	Data 1.16e-04 (2.76e-04)	Tok/s 39696 (43175)	Loss/tok 2.9529 (3.1265)	LR 1.250e-04
0: TRAIN [6][1810/7762]	Time 0.347 (0.329)	Data 1.15e-04 (2.75e-04)	Tok/s 48348 (43172)	Loss/tok 3.1600 (3.1261)	LR 1.250e-04
0: TRAIN [6][1820/7762]	Time 0.461 (0.329)	Data 1.07e-04 (2.74e-04)	Tok/s 50613 (43190)	Loss/tok 3.3381 (3.1261)	LR 1.250e-04
0: TRAIN [6][1830/7762]	Time 0.356 (0.329)	Data 1.19e-04 (2.73e-04)	Tok/s 47330 (43182)	Loss/tok 3.2089 (3.1260)	LR 1.250e-04
0: TRAIN [6][1840/7762]	Time 0.356 (0.329)	Data 1.04e-04 (2.73e-04)	Tok/s 47524 (43181)	Loss/tok 3.0744 (3.1257)	LR 1.250e-04
0: TRAIN [6][1850/7762]	Time 0.177 (0.329)	Data 9.97e-05 (2.72e-04)	Tok/s 29724 (43166)	Loss/tok 2.4922 (3.1253)	LR 1.250e-04
0: TRAIN [6][1860/7762]	Time 0.363 (0.329)	Data 1.03e-04 (2.71e-04)	Tok/s 46544 (43167)	Loss/tok 3.0992 (3.1252)	LR 1.250e-04
0: TRAIN [6][1870/7762]	Time 0.361 (0.329)	Data 1.05e-04 (2.70e-04)	Tok/s 46920 (43152)	Loss/tok 3.1827 (3.1254)	LR 1.250e-04
0: TRAIN [6][1880/7762]	Time 0.176 (0.329)	Data 1.16e-04 (2.69e-04)	Tok/s 29871 (43142)	Loss/tok 2.6197 (3.1253)	LR 1.250e-04
0: TRAIN [6][1890/7762]	Time 0.360 (0.329)	Data 9.99e-05 (2.68e-04)	Tok/s 46657 (43151)	Loss/tok 3.0362 (3.1254)	LR 1.250e-04
0: TRAIN [6][1900/7762]	Time 0.362 (0.329)	Data 1.17e-04 (2.67e-04)	Tok/s 45891 (43163)	Loss/tok 3.1290 (3.1252)	LR 1.250e-04
0: TRAIN [6][1910/7762]	Time 0.261 (0.329)	Data 9.73e-05 (2.66e-04)	Tok/s 39412 (43160)	Loss/tok 2.9883 (3.1249)	LR 1.250e-04
0: TRAIN [6][1920/7762]	Time 0.256 (0.328)	Data 1.29e-04 (2.66e-04)	Tok/s 40620 (43144)	Loss/tok 2.8916 (3.1242)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][1930/7762]	Time 0.263 (0.328)	Data 1.22e-04 (2.65e-04)	Tok/s 39582 (43145)	Loss/tok 2.8255 (3.1242)	LR 1.250e-04
0: TRAIN [6][1940/7762]	Time 0.348 (0.328)	Data 9.82e-05 (2.64e-04)	Tok/s 48230 (43135)	Loss/tok 3.0454 (3.1237)	LR 1.250e-04
0: TRAIN [6][1950/7762]	Time 0.438 (0.328)	Data 1.11e-04 (2.63e-04)	Tok/s 54180 (43143)	Loss/tok 3.2074 (3.1237)	LR 1.250e-04
0: TRAIN [6][1960/7762]	Time 0.267 (0.328)	Data 1.00e-04 (2.62e-04)	Tok/s 39041 (43146)	Loss/tok 3.0090 (3.1245)	LR 1.250e-04
0: TRAIN [6][1970/7762]	Time 0.359 (0.328)	Data 1.03e-04 (2.61e-04)	Tok/s 47261 (43149)	Loss/tok 3.2059 (3.1246)	LR 1.250e-04
0: TRAIN [6][1980/7762]	Time 0.361 (0.328)	Data 1.02e-04 (2.61e-04)	Tok/s 46800 (43150)	Loss/tok 3.0938 (3.1243)	LR 1.250e-04
0: TRAIN [6][1990/7762]	Time 0.359 (0.328)	Data 1.01e-04 (2.60e-04)	Tok/s 46846 (43144)	Loss/tok 3.1279 (3.1245)	LR 1.250e-04
0: TRAIN [6][2000/7762]	Time 0.265 (0.328)	Data 1.02e-04 (2.59e-04)	Tok/s 39057 (43138)	Loss/tok 2.8459 (3.1239)	LR 1.250e-04
0: TRAIN [6][2010/7762]	Time 0.260 (0.328)	Data 1.01e-04 (2.58e-04)	Tok/s 38848 (43139)	Loss/tok 2.8767 (3.1236)	LR 1.250e-04
0: TRAIN [6][2020/7762]	Time 0.559 (0.328)	Data 1.06e-04 (2.58e-04)	Tok/s 54283 (43127)	Loss/tok 3.3180 (3.1235)	LR 1.250e-04
0: TRAIN [6][2030/7762]	Time 0.348 (0.328)	Data 1.02e-04 (2.57e-04)	Tok/s 48301 (43110)	Loss/tok 3.0513 (3.1231)	LR 1.250e-04
0: TRAIN [6][2040/7762]	Time 0.590 (0.328)	Data 1.04e-04 (2.56e-04)	Tok/s 50493 (43124)	Loss/tok 3.4232 (3.1236)	LR 1.250e-04
0: TRAIN [6][2050/7762]	Time 0.366 (0.328)	Data 1.18e-04 (2.55e-04)	Tok/s 45653 (43125)	Loss/tok 3.0774 (3.1239)	LR 1.250e-04
0: TRAIN [6][2060/7762]	Time 0.260 (0.328)	Data 1.02e-04 (2.55e-04)	Tok/s 38881 (43122)	Loss/tok 2.8315 (3.1236)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][2070/7762]	Time 0.253 (0.328)	Data 1.00e-04 (2.54e-04)	Tok/s 41389 (43113)	Loss/tok 2.8753 (3.1237)	LR 1.250e-04
0: TRAIN [6][2080/7762]	Time 0.466 (0.328)	Data 1.03e-04 (2.53e-04)	Tok/s 49624 (43128)	Loss/tok 3.2465 (3.1237)	LR 1.250e-04
0: TRAIN [6][2090/7762]	Time 0.266 (0.328)	Data 1.03e-04 (2.52e-04)	Tok/s 38402 (43120)	Loss/tok 2.9465 (3.1239)	LR 1.250e-04
0: TRAIN [6][2100/7762]	Time 0.358 (0.328)	Data 1.02e-04 (2.52e-04)	Tok/s 46651 (43123)	Loss/tok 3.2094 (3.1243)	LR 1.250e-04
0: TRAIN [6][2110/7762]	Time 0.265 (0.328)	Data 1.01e-04 (2.51e-04)	Tok/s 37642 (43127)	Loss/tok 2.8152 (3.1240)	LR 1.250e-04
0: TRAIN [6][2120/7762]	Time 0.356 (0.328)	Data 1.02e-04 (2.50e-04)	Tok/s 46787 (43133)	Loss/tok 3.1183 (3.1239)	LR 1.250e-04
0: TRAIN [6][2130/7762]	Time 0.369 (0.328)	Data 9.99e-05 (2.50e-04)	Tok/s 45452 (43139)	Loss/tok 3.1474 (3.1238)	LR 1.250e-04
0: TRAIN [6][2140/7762]	Time 0.360 (0.328)	Data 9.87e-05 (2.49e-04)	Tok/s 46612 (43135)	Loss/tok 3.1365 (3.1235)	LR 1.250e-04
0: TRAIN [6][2150/7762]	Time 0.253 (0.328)	Data 1.04e-04 (2.48e-04)	Tok/s 40541 (43144)	Loss/tok 2.9787 (3.1239)	LR 1.250e-04
0: TRAIN [6][2160/7762]	Time 0.275 (0.328)	Data 9.80e-05 (2.48e-04)	Tok/s 37351 (43142)	Loss/tok 2.8645 (3.1237)	LR 1.250e-04
0: TRAIN [6][2170/7762]	Time 0.264 (0.328)	Data 1.12e-04 (2.47e-04)	Tok/s 39074 (43140)	Loss/tok 2.9758 (3.1238)	LR 1.250e-04
0: TRAIN [6][2180/7762]	Time 0.265 (0.328)	Data 9.99e-05 (2.46e-04)	Tok/s 39730 (43134)	Loss/tok 3.0689 (3.1235)	LR 1.250e-04
0: TRAIN [6][2190/7762]	Time 0.435 (0.328)	Data 1.04e-04 (2.46e-04)	Tok/s 53869 (43136)	Loss/tok 3.3147 (3.1237)	LR 1.250e-04
0: TRAIN [6][2200/7762]	Time 0.595 (0.328)	Data 1.06e-04 (2.45e-04)	Tok/s 50518 (43146)	Loss/tok 3.5173 (3.1247)	LR 1.250e-04
0: TRAIN [6][2210/7762]	Time 0.266 (0.329)	Data 1.05e-04 (2.44e-04)	Tok/s 38069 (43153)	Loss/tok 2.9742 (3.1250)	LR 1.250e-04
0: TRAIN [6][2220/7762]	Time 0.265 (0.329)	Data 1.03e-04 (2.44e-04)	Tok/s 39389 (43163)	Loss/tok 2.9164 (3.1253)	LR 1.250e-04
0: TRAIN [6][2230/7762]	Time 0.362 (0.329)	Data 9.89e-05 (2.43e-04)	Tok/s 46900 (43167)	Loss/tok 3.0594 (3.1250)	LR 1.250e-04
0: TRAIN [6][2240/7762]	Time 0.174 (0.329)	Data 1.02e-04 (2.42e-04)	Tok/s 30110 (43156)	Loss/tok 2.5570 (3.1246)	LR 1.250e-04
0: TRAIN [6][2250/7762]	Time 0.263 (0.328)	Data 1.19e-04 (2.42e-04)	Tok/s 39528 (43143)	Loss/tok 2.8285 (3.1243)	LR 1.250e-04
0: TRAIN [6][2260/7762]	Time 0.344 (0.328)	Data 1.68e-04 (2.41e-04)	Tok/s 48956 (43144)	Loss/tok 3.1457 (3.1242)	LR 1.250e-04
0: TRAIN [6][2270/7762]	Time 0.355 (0.329)	Data 1.24e-04 (2.41e-04)	Tok/s 47421 (43152)	Loss/tok 3.1663 (3.1244)	LR 1.250e-04
0: TRAIN [6][2280/7762]	Time 0.357 (0.328)	Data 1.02e-04 (2.40e-04)	Tok/s 47360 (43149)	Loss/tok 3.0358 (3.1244)	LR 1.250e-04
0: TRAIN [6][2290/7762]	Time 0.450 (0.329)	Data 1.06e-04 (2.40e-04)	Tok/s 52505 (43151)	Loss/tok 3.2250 (3.1244)	LR 1.250e-04
0: TRAIN [6][2300/7762]	Time 0.366 (0.329)	Data 1.20e-04 (2.39e-04)	Tok/s 45850 (43159)	Loss/tok 3.1047 (3.1249)	LR 1.250e-04
0: TRAIN [6][2310/7762]	Time 0.261 (0.329)	Data 1.01e-04 (2.38e-04)	Tok/s 39260 (43152)	Loss/tok 2.9452 (3.1247)	LR 1.250e-04
0: TRAIN [6][2320/7762]	Time 0.462 (0.329)	Data 1.03e-04 (2.38e-04)	Tok/s 50693 (43154)	Loss/tok 3.2753 (3.1246)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [6][2330/7762]	Time 0.359 (0.329)	Data 1.06e-04 (2.37e-04)	Tok/s 46568 (43156)	Loss/tok 3.1023 (3.1248)	LR 1.250e-04
0: TRAIN [6][2340/7762]	Time 0.358 (0.329)	Data 1.08e-04 (2.37e-04)	Tok/s 46726 (43151)	Loss/tok 3.0287 (3.1244)	LR 1.250e-04
0: TRAIN [6][2350/7762]	Time 0.462 (0.329)	Data 1.02e-04 (2.36e-04)	Tok/s 50513 (43155)	Loss/tok 3.2799 (3.1248)	LR 1.250e-04
0: TRAIN [6][2360/7762]	Time 0.261 (0.329)	Data 1.02e-04 (2.36e-04)	Tok/s 39693 (43160)	Loss/tok 2.9965 (3.1248)	LR 1.250e-04
0: TRAIN [6][2370/7762]	Time 0.460 (0.329)	Data 1.03e-04 (2.35e-04)	Tok/s 49658 (43151)	Loss/tok 3.3372 (3.1246)	LR 1.250e-04
0: TRAIN [6][2380/7762]	Time 0.462 (0.329)	Data 1.19e-04 (2.34e-04)	Tok/s 51054 (43157)	Loss/tok 3.2307 (3.1246)	LR 1.250e-04
0: TRAIN [6][2390/7762]	Time 0.365 (0.329)	Data 9.89e-05 (2.34e-04)	Tok/s 46303 (43153)	Loss/tok 2.9228 (3.1244)	LR 1.250e-04
0: TRAIN [6][2400/7762]	Time 0.451 (0.329)	Data 1.02e-04 (2.33e-04)	Tok/s 52930 (43148)	Loss/tok 3.1051 (3.1242)	LR 1.250e-04
0: TRAIN [6][2410/7762]	Time 0.266 (0.329)	Data 1.05e-04 (2.33e-04)	Tok/s 39519 (43150)	Loss/tok 3.1156 (3.1243)	LR 1.250e-04
0: TRAIN [6][2420/7762]	Time 0.367 (0.328)	Data 1.00e-04 (2.32e-04)	Tok/s 45730 (43148)	Loss/tok 3.0716 (3.1241)	LR 1.250e-04
0: TRAIN [6][2430/7762]	Time 0.360 (0.328)	Data 9.89e-05 (2.32e-04)	Tok/s 46646 (43148)	Loss/tok 3.1385 (3.1240)	LR 1.250e-04
0: TRAIN [6][2440/7762]	Time 0.260 (0.328)	Data 1.05e-04 (2.31e-04)	Tok/s 39790 (43143)	Loss/tok 2.9365 (3.1239)	LR 1.250e-04
0: TRAIN [6][2450/7762]	Time 0.364 (0.328)	Data 9.99e-05 (2.31e-04)	Tok/s 46348 (43144)	Loss/tok 3.0786 (3.1239)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [6][2460/7762]	Time 0.263 (0.328)	Data 9.97e-05 (2.30e-04)	Tok/s 38900 (43140)	Loss/tok 2.8401 (3.1237)	LR 1.250e-04
0: TRAIN [6][2470/7762]	Time 0.262 (0.328)	Data 1.11e-04 (2.30e-04)	Tok/s 38917 (43136)	Loss/tok 2.9910 (3.1233)	LR 1.250e-04
0: TRAIN [6][2480/7762]	Time 0.349 (0.328)	Data 1.14e-04 (2.29e-04)	Tok/s 48718 (43141)	Loss/tok 3.2569 (3.1234)	LR 1.250e-04
0: TRAIN [6][2490/7762]	Time 0.266 (0.328)	Data 9.82e-05 (2.29e-04)	Tok/s 39592 (43128)	Loss/tok 2.9259 (3.1229)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][2500/7762]	Time 0.176 (0.328)	Data 9.63e-05 (2.28e-04)	Tok/s 30358 (43117)	Loss/tok 2.5942 (3.1228)	LR 1.250e-04
0: TRAIN [6][2510/7762]	Time 0.176 (0.328)	Data 1.02e-04 (2.28e-04)	Tok/s 30313 (43099)	Loss/tok 2.5671 (3.1225)	LR 1.250e-04
0: TRAIN [6][2520/7762]	Time 0.177 (0.327)	Data 1.01e-04 (2.27e-04)	Tok/s 29856 (43088)	Loss/tok 2.4765 (3.1225)	LR 1.250e-04
0: TRAIN [6][2530/7762]	Time 0.265 (0.327)	Data 9.56e-05 (2.27e-04)	Tok/s 38535 (43085)	Loss/tok 2.8359 (3.1222)	LR 1.250e-04
0: TRAIN [6][2540/7762]	Time 0.261 (0.327)	Data 1.03e-04 (2.26e-04)	Tok/s 40059 (43090)	Loss/tok 2.9002 (3.1225)	LR 1.250e-04
0: TRAIN [6][2550/7762]	Time 0.455 (0.327)	Data 1.05e-04 (2.26e-04)	Tok/s 51301 (43092)	Loss/tok 3.2678 (3.1225)	LR 1.250e-04
0: TRAIN [6][2560/7762]	Time 0.169 (0.327)	Data 1.04e-04 (2.25e-04)	Tok/s 32421 (43091)	Loss/tok 2.6596 (3.1224)	LR 1.250e-04
0: TRAIN [6][2570/7762]	Time 0.261 (0.327)	Data 9.87e-05 (2.25e-04)	Tok/s 39730 (43088)	Loss/tok 2.8923 (3.1224)	LR 1.250e-04
0: TRAIN [6][2580/7762]	Time 0.264 (0.327)	Data 1.02e-04 (2.24e-04)	Tok/s 39472 (43085)	Loss/tok 2.9000 (3.1224)	LR 1.250e-04
0: TRAIN [6][2590/7762]	Time 0.364 (0.327)	Data 1.01e-04 (2.24e-04)	Tok/s 45931 (43094)	Loss/tok 3.1351 (3.1226)	LR 1.250e-04
0: TRAIN [6][2600/7762]	Time 0.453 (0.328)	Data 2.42e-04 (2.23e-04)	Tok/s 50769 (43099)	Loss/tok 3.4160 (3.1229)	LR 1.250e-04
0: TRAIN [6][2610/7762]	Time 0.360 (0.328)	Data 9.87e-05 (2.23e-04)	Tok/s 46370 (43098)	Loss/tok 3.2042 (3.1231)	LR 1.250e-04
0: TRAIN [6][2620/7762]	Time 0.261 (0.327)	Data 1.04e-04 (2.22e-04)	Tok/s 39425 (43088)	Loss/tok 2.9778 (3.1227)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][2630/7762]	Time 0.177 (0.327)	Data 1.71e-04 (2.22e-04)	Tok/s 29442 (43078)	Loss/tok 2.4934 (3.1229)	LR 1.250e-04
0: TRAIN [6][2640/7762]	Time 0.259 (0.327)	Data 1.00e-04 (2.22e-04)	Tok/s 39688 (43081)	Loss/tok 2.9277 (3.1228)	LR 1.250e-04
0: TRAIN [6][2650/7762]	Time 0.261 (0.327)	Data 9.94e-05 (2.21e-04)	Tok/s 40034 (43077)	Loss/tok 2.9126 (3.1226)	LR 1.250e-04
0: TRAIN [6][2660/7762]	Time 0.172 (0.327)	Data 1.18e-04 (2.21e-04)	Tok/s 30963 (43087)	Loss/tok 2.5690 (3.1233)	LR 1.250e-04
0: TRAIN [6][2670/7762]	Time 0.175 (0.327)	Data 1.02e-04 (2.20e-04)	Tok/s 30652 (43087)	Loss/tok 2.5650 (3.1233)	LR 1.250e-04
0: TRAIN [6][2680/7762]	Time 0.363 (0.328)	Data 1.05e-04 (2.20e-04)	Tok/s 45803 (43093)	Loss/tok 3.1555 (3.1233)	LR 1.250e-04
0: TRAIN [6][2690/7762]	Time 0.254 (0.327)	Data 9.80e-05 (2.19e-04)	Tok/s 41207 (43085)	Loss/tok 2.9595 (3.1231)	LR 1.250e-04
0: TRAIN [6][2700/7762]	Time 0.359 (0.327)	Data 9.75e-05 (2.19e-04)	Tok/s 46615 (43100)	Loss/tok 3.1996 (3.1232)	LR 1.250e-04
0: TRAIN [6][2710/7762]	Time 0.365 (0.328)	Data 1.04e-04 (2.19e-04)	Tok/s 46324 (43102)	Loss/tok 3.1063 (3.1233)	LR 1.250e-04
0: TRAIN [6][2720/7762]	Time 0.266 (0.328)	Data 9.73e-05 (2.18e-04)	Tok/s 38233 (43109)	Loss/tok 2.9651 (3.1235)	LR 1.250e-04
0: TRAIN [6][2730/7762]	Time 0.265 (0.328)	Data 1.20e-04 (2.18e-04)	Tok/s 39215 (43114)	Loss/tok 3.0736 (3.1236)	LR 1.250e-04
0: TRAIN [6][2740/7762]	Time 0.353 (0.328)	Data 1.03e-04 (2.17e-04)	Tok/s 47436 (43114)	Loss/tok 3.0776 (3.1237)	LR 1.250e-04
0: TRAIN [6][2750/7762]	Time 0.360 (0.328)	Data 1.02e-04 (2.17e-04)	Tok/s 46566 (43111)	Loss/tok 3.1543 (3.1236)	LR 1.250e-04
0: TRAIN [6][2760/7762]	Time 0.353 (0.327)	Data 1.03e-04 (2.17e-04)	Tok/s 48109 (43096)	Loss/tok 3.0738 (3.1231)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][2770/7762]	Time 0.258 (0.327)	Data 1.03e-04 (2.16e-04)	Tok/s 39521 (43096)	Loss/tok 2.8949 (3.1230)	LR 1.250e-04
0: TRAIN [6][2780/7762]	Time 0.364 (0.327)	Data 1.01e-04 (2.16e-04)	Tok/s 45779 (43092)	Loss/tok 3.1517 (3.1227)	LR 1.250e-04
0: TRAIN [6][2790/7762]	Time 0.268 (0.327)	Data 9.94e-05 (2.15e-04)	Tok/s 38740 (43099)	Loss/tok 2.8247 (3.1227)	LR 1.250e-04
0: TRAIN [6][2800/7762]	Time 0.267 (0.327)	Data 1.02e-04 (2.15e-04)	Tok/s 38997 (43089)	Loss/tok 2.9596 (3.1224)	LR 1.250e-04
0: TRAIN [6][2810/7762]	Time 0.170 (0.327)	Data 1.04e-04 (2.15e-04)	Tok/s 31075 (43082)	Loss/tok 2.5611 (3.1224)	LR 1.250e-04
0: TRAIN [6][2820/7762]	Time 0.260 (0.327)	Data 1.02e-04 (2.14e-04)	Tok/s 39329 (43073)	Loss/tok 2.8977 (3.1221)	LR 1.250e-04
0: TRAIN [6][2830/7762]	Time 0.265 (0.327)	Data 1.01e-04 (2.14e-04)	Tok/s 39299 (43067)	Loss/tok 2.9708 (3.1216)	LR 1.250e-04
0: TRAIN [6][2840/7762]	Time 0.263 (0.327)	Data 1.29e-04 (2.13e-04)	Tok/s 38274 (43077)	Loss/tok 2.8643 (3.1218)	LR 1.250e-04
0: TRAIN [6][2850/7762]	Time 0.464 (0.327)	Data 1.23e-04 (2.13e-04)	Tok/s 50272 (43077)	Loss/tok 3.2291 (3.1217)	LR 1.250e-04
0: TRAIN [6][2860/7762]	Time 0.573 (0.327)	Data 1.01e-04 (2.13e-04)	Tok/s 51752 (43079)	Loss/tok 3.5166 (3.1222)	LR 1.250e-04
0: TRAIN [6][2870/7762]	Time 0.364 (0.327)	Data 9.42e-05 (2.12e-04)	Tok/s 46185 (43084)	Loss/tok 3.1083 (3.1224)	LR 1.250e-04
0: TRAIN [6][2880/7762]	Time 0.263 (0.327)	Data 1.04e-04 (2.12e-04)	Tok/s 38542 (43082)	Loss/tok 2.9638 (3.1223)	LR 1.250e-04
0: TRAIN [6][2890/7762]	Time 0.265 (0.327)	Data 1.04e-04 (2.12e-04)	Tok/s 39467 (43091)	Loss/tok 2.9478 (3.1227)	LR 1.250e-04
0: TRAIN [6][2900/7762]	Time 0.463 (0.327)	Data 1.35e-04 (2.11e-04)	Tok/s 50209 (43103)	Loss/tok 3.3048 (3.1231)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][2910/7762]	Time 0.363 (0.328)	Data 9.80e-05 (2.11e-04)	Tok/s 46478 (43103)	Loss/tok 3.1233 (3.1235)	LR 1.250e-04
0: TRAIN [6][2920/7762]	Time 0.271 (0.328)	Data 1.19e-04 (2.10e-04)	Tok/s 38554 (43105)	Loss/tok 2.9593 (3.1233)	LR 1.250e-04
0: TRAIN [6][2930/7762]	Time 0.590 (0.328)	Data 9.75e-05 (2.10e-04)	Tok/s 50270 (43106)	Loss/tok 3.3920 (3.1234)	LR 1.250e-04
0: TRAIN [6][2940/7762]	Time 0.268 (0.328)	Data 1.03e-04 (2.10e-04)	Tok/s 39469 (43108)	Loss/tok 2.9038 (3.1234)	LR 1.250e-04
0: TRAIN [6][2950/7762]	Time 0.359 (0.327)	Data 1.05e-04 (2.09e-04)	Tok/s 47629 (43100)	Loss/tok 3.1767 (3.1230)	LR 1.250e-04
0: TRAIN [6][2960/7762]	Time 0.364 (0.328)	Data 1.06e-04 (2.09e-04)	Tok/s 46494 (43108)	Loss/tok 3.2110 (3.1233)	LR 1.250e-04
0: TRAIN [6][2970/7762]	Time 0.459 (0.328)	Data 1.01e-04 (2.09e-04)	Tok/s 50707 (43119)	Loss/tok 3.2732 (3.1235)	LR 1.250e-04
0: TRAIN [6][2980/7762]	Time 0.257 (0.328)	Data 1.18e-04 (2.08e-04)	Tok/s 39886 (43112)	Loss/tok 2.9768 (3.1234)	LR 1.250e-04
0: TRAIN [6][2990/7762]	Time 0.267 (0.328)	Data 1.03e-04 (2.08e-04)	Tok/s 38499 (43109)	Loss/tok 2.9875 (3.1237)	LR 1.250e-04
0: TRAIN [6][3000/7762]	Time 0.264 (0.328)	Data 1.03e-04 (2.08e-04)	Tok/s 39989 (43114)	Loss/tok 2.9804 (3.1238)	LR 1.250e-04
0: TRAIN [6][3010/7762]	Time 0.342 (0.328)	Data 9.82e-05 (2.07e-04)	Tok/s 49272 (43106)	Loss/tok 3.1252 (3.1236)	LR 1.250e-04
0: TRAIN [6][3020/7762]	Time 0.175 (0.327)	Data 1.03e-04 (2.07e-04)	Tok/s 30524 (43100)	Loss/tok 2.4958 (3.1235)	LR 1.250e-04
0: TRAIN [6][3030/7762]	Time 0.353 (0.327)	Data 1.00e-04 (2.07e-04)	Tok/s 47233 (43100)	Loss/tok 3.1644 (3.1235)	LR 1.250e-04
0: TRAIN [6][3040/7762]	Time 0.459 (0.327)	Data 1.02e-04 (2.06e-04)	Tok/s 51442 (43093)	Loss/tok 3.3339 (3.1234)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][3050/7762]	Time 0.262 (0.328)	Data 1.19e-04 (2.06e-04)	Tok/s 39026 (43106)	Loss/tok 2.9111 (3.1240)	LR 1.250e-04
0: TRAIN [6][3060/7762]	Time 0.265 (0.327)	Data 1.15e-04 (2.06e-04)	Tok/s 39103 (43100)	Loss/tok 2.8311 (3.1238)	LR 1.250e-04
0: TRAIN [6][3070/7762]	Time 0.447 (0.328)	Data 1.04e-04 (2.05e-04)	Tok/s 51915 (43110)	Loss/tok 3.3199 (3.1240)	LR 1.250e-04
0: TRAIN [6][3080/7762]	Time 0.175 (0.328)	Data 1.01e-04 (2.05e-04)	Tok/s 30188 (43115)	Loss/tok 2.5936 (3.1246)	LR 1.250e-04
0: TRAIN [6][3090/7762]	Time 0.460 (0.328)	Data 1.16e-04 (2.05e-04)	Tok/s 50257 (43117)	Loss/tok 3.3600 (3.1247)	LR 1.250e-04
0: TRAIN [6][3100/7762]	Time 0.260 (0.328)	Data 9.66e-05 (2.04e-04)	Tok/s 40321 (43122)	Loss/tok 2.9906 (3.1250)	LR 1.250e-04
0: TRAIN [6][3110/7762]	Time 0.269 (0.328)	Data 1.01e-04 (2.04e-04)	Tok/s 37432 (43127)	Loss/tok 2.9864 (3.1254)	LR 1.250e-04
0: TRAIN [6][3120/7762]	Time 0.467 (0.328)	Data 1.02e-04 (2.04e-04)	Tok/s 49499 (43125)	Loss/tok 3.3144 (3.1254)	LR 1.250e-04
0: TRAIN [6][3130/7762]	Time 0.264 (0.328)	Data 9.73e-05 (2.03e-04)	Tok/s 39346 (43116)	Loss/tok 3.0069 (3.1252)	LR 1.250e-04
0: TRAIN [6][3140/7762]	Time 0.266 (0.328)	Data 1.14e-04 (2.03e-04)	Tok/s 38381 (43112)	Loss/tok 2.9537 (3.1251)	LR 1.250e-04
0: TRAIN [6][3150/7762]	Time 0.259 (0.328)	Data 1.02e-04 (2.03e-04)	Tok/s 39488 (43103)	Loss/tok 2.9415 (3.1249)	LR 1.250e-04
0: TRAIN [6][3160/7762]	Time 0.361 (0.328)	Data 1.04e-04 (2.03e-04)	Tok/s 46234 (43107)	Loss/tok 3.0476 (3.1252)	LR 1.250e-04
0: TRAIN [6][3170/7762]	Time 0.364 (0.328)	Data 1.06e-04 (2.02e-04)	Tok/s 45821 (43117)	Loss/tok 3.2359 (3.1258)	LR 1.250e-04
0: TRAIN [6][3180/7762]	Time 0.262 (0.328)	Data 1.03e-04 (2.02e-04)	Tok/s 39164 (43123)	Loss/tok 2.9220 (3.1264)	LR 1.250e-04
0: TRAIN [6][3190/7762]	Time 0.265 (0.328)	Data 1.16e-04 (2.02e-04)	Tok/s 38687 (43123)	Loss/tok 3.0212 (3.1263)	LR 1.250e-04
0: TRAIN [6][3200/7762]	Time 0.268 (0.328)	Data 1.16e-04 (2.01e-04)	Tok/s 37580 (43128)	Loss/tok 3.0246 (3.1264)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][3210/7762]	Time 0.463 (0.328)	Data 9.73e-05 (2.01e-04)	Tok/s 50976 (43119)	Loss/tok 3.2325 (3.1262)	LR 1.250e-04
0: TRAIN [6][3220/7762]	Time 0.353 (0.328)	Data 1.21e-04 (2.01e-04)	Tok/s 47727 (43123)	Loss/tok 3.1097 (3.1262)	LR 1.250e-04
0: TRAIN [6][3230/7762]	Time 0.264 (0.328)	Data 9.49e-05 (2.00e-04)	Tok/s 38028 (43114)	Loss/tok 2.9639 (3.1260)	LR 1.250e-04
0: TRAIN [6][3240/7762]	Time 0.344 (0.328)	Data 1.03e-04 (2.00e-04)	Tok/s 49380 (43104)	Loss/tok 3.0707 (3.1260)	LR 1.250e-04
0: TRAIN [6][3250/7762]	Time 0.266 (0.328)	Data 1.02e-04 (2.00e-04)	Tok/s 38596 (43099)	Loss/tok 2.8649 (3.1258)	LR 1.250e-04
0: TRAIN [6][3260/7762]	Time 0.176 (0.328)	Data 9.97e-05 (2.00e-04)	Tok/s 29961 (43089)	Loss/tok 2.5721 (3.1255)	LR 1.250e-04
0: TRAIN [6][3270/7762]	Time 0.586 (0.328)	Data 1.05e-04 (1.99e-04)	Tok/s 51264 (43095)	Loss/tok 3.4332 (3.1256)	LR 1.250e-04
0: TRAIN [6][3280/7762]	Time 0.175 (0.328)	Data 1.02e-04 (1.99e-04)	Tok/s 29968 (43086)	Loss/tok 2.5649 (3.1255)	LR 1.250e-04
0: TRAIN [6][3290/7762]	Time 0.353 (0.328)	Data 1.02e-04 (1.99e-04)	Tok/s 47442 (43083)	Loss/tok 3.2028 (3.1253)	LR 1.250e-04
0: TRAIN [6][3300/7762]	Time 0.265 (0.328)	Data 1.17e-04 (1.98e-04)	Tok/s 40363 (43077)	Loss/tok 2.9826 (3.1252)	LR 1.250e-04
0: TRAIN [6][3310/7762]	Time 0.460 (0.328)	Data 9.89e-05 (1.98e-04)	Tok/s 51375 (43087)	Loss/tok 3.2537 (3.1254)	LR 1.250e-04
0: TRAIN [6][3320/7762]	Time 0.462 (0.328)	Data 1.01e-04 (1.98e-04)	Tok/s 51119 (43089)	Loss/tok 3.3096 (3.1255)	LR 1.250e-04
0: TRAIN [6][3330/7762]	Time 0.355 (0.328)	Data 1.02e-04 (1.98e-04)	Tok/s 47215 (43082)	Loss/tok 3.1452 (3.1254)	LR 1.250e-04
0: TRAIN [6][3340/7762]	Time 0.366 (0.328)	Data 1.02e-04 (1.97e-04)	Tok/s 46744 (43084)	Loss/tok 3.2227 (3.1254)	LR 1.250e-04
0: TRAIN [6][3350/7762]	Time 0.175 (0.328)	Data 1.04e-04 (1.97e-04)	Tok/s 29721 (43082)	Loss/tok 2.4980 (3.1251)	LR 1.250e-04
0: TRAIN [6][3360/7762]	Time 0.342 (0.328)	Data 1.02e-04 (1.97e-04)	Tok/s 48734 (43083)	Loss/tok 3.1204 (3.1249)	LR 1.250e-04
0: TRAIN [6][3370/7762]	Time 0.254 (0.328)	Data 9.94e-05 (1.96e-04)	Tok/s 41194 (43087)	Loss/tok 2.9714 (3.1249)	LR 1.250e-04
0: TRAIN [6][3380/7762]	Time 0.256 (0.328)	Data 1.18e-04 (1.96e-04)	Tok/s 40793 (43090)	Loss/tok 2.9593 (3.1250)	LR 1.250e-04
0: TRAIN [6][3390/7762]	Time 0.262 (0.328)	Data 1.27e-04 (1.96e-04)	Tok/s 39999 (43091)	Loss/tok 2.8671 (3.1248)	LR 1.250e-04
0: TRAIN [6][3400/7762]	Time 0.455 (0.328)	Data 1.06e-04 (1.96e-04)	Tok/s 51134 (43098)	Loss/tok 3.2868 (3.1251)	LR 1.250e-04
0: TRAIN [6][3410/7762]	Time 0.266 (0.328)	Data 1.00e-04 (1.95e-04)	Tok/s 39243 (43099)	Loss/tok 2.9274 (3.1252)	LR 1.250e-04
0: TRAIN [6][3420/7762]	Time 0.265 (0.328)	Data 1.09e-04 (1.95e-04)	Tok/s 38394 (43105)	Loss/tok 2.9144 (3.1254)	LR 1.250e-04
0: TRAIN [6][3430/7762]	Time 0.256 (0.328)	Data 9.58e-05 (1.95e-04)	Tok/s 40504 (43102)	Loss/tok 2.9870 (3.1252)	LR 1.250e-04
0: TRAIN [6][3440/7762]	Time 0.259 (0.328)	Data 9.92e-05 (1.95e-04)	Tok/s 40123 (43102)	Loss/tok 2.8762 (3.1251)	LR 1.250e-04
0: TRAIN [6][3450/7762]	Time 0.263 (0.328)	Data 1.16e-04 (1.94e-04)	Tok/s 39301 (43108)	Loss/tok 2.9677 (3.1251)	LR 1.250e-04
0: TRAIN [6][3460/7762]	Time 0.261 (0.328)	Data 1.01e-04 (1.94e-04)	Tok/s 39697 (43101)	Loss/tok 2.9413 (3.1249)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [6][3470/7762]	Time 0.450 (0.328)	Data 1.14e-04 (1.94e-04)	Tok/s 52163 (43099)	Loss/tok 3.2733 (3.1249)	LR 1.250e-04
0: TRAIN [6][3480/7762]	Time 0.174 (0.328)	Data 1.06e-04 (1.94e-04)	Tok/s 30457 (43097)	Loss/tok 2.6630 (3.1247)	LR 1.250e-04
0: TRAIN [6][3490/7762]	Time 0.262 (0.328)	Data 1.17e-04 (1.93e-04)	Tok/s 40065 (43100)	Loss/tok 2.9151 (3.1247)	LR 1.250e-04
0: TRAIN [6][3500/7762]	Time 0.261 (0.328)	Data 1.06e-04 (1.93e-04)	Tok/s 39905 (43102)	Loss/tok 2.8576 (3.1246)	LR 1.250e-04
0: TRAIN [6][3510/7762]	Time 0.452 (0.327)	Data 9.94e-05 (1.93e-04)	Tok/s 52473 (43097)	Loss/tok 3.2102 (3.1245)	LR 1.250e-04
0: TRAIN [6][3520/7762]	Time 0.177 (0.327)	Data 9.70e-05 (1.93e-04)	Tok/s 29651 (43092)	Loss/tok 2.5067 (3.1243)	LR 1.250e-04
0: TRAIN [6][3530/7762]	Time 0.266 (0.327)	Data 9.92e-05 (1.92e-04)	Tok/s 38939 (43091)	Loss/tok 2.9757 (3.1242)	LR 1.250e-04
0: TRAIN [6][3540/7762]	Time 0.172 (0.327)	Data 1.01e-04 (1.92e-04)	Tok/s 31262 (43090)	Loss/tok 2.5511 (3.1244)	LR 1.250e-04
0: TRAIN [6][3550/7762]	Time 0.260 (0.328)	Data 9.99e-05 (1.92e-04)	Tok/s 40634 (43093)	Loss/tok 2.9477 (3.1247)	LR 1.250e-04
0: TRAIN [6][3560/7762]	Time 0.270 (0.328)	Data 1.04e-04 (1.92e-04)	Tok/s 38098 (43094)	Loss/tok 2.9219 (3.1247)	LR 1.250e-04
0: TRAIN [6][3570/7762]	Time 0.341 (0.327)	Data 1.17e-04 (1.91e-04)	Tok/s 48523 (43090)	Loss/tok 3.1420 (3.1246)	LR 1.250e-04
0: TRAIN [6][3580/7762]	Time 0.344 (0.328)	Data 9.99e-05 (1.91e-04)	Tok/s 48900 (43097)	Loss/tok 3.0398 (3.1245)	LR 1.250e-04
0: TRAIN [6][3590/7762]	Time 0.356 (0.328)	Data 1.03e-04 (1.91e-04)	Tok/s 46634 (43104)	Loss/tok 3.0842 (3.1247)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [6][3600/7762]	Time 0.255 (0.328)	Data 1.06e-04 (1.91e-04)	Tok/s 39721 (43104)	Loss/tok 3.0023 (3.1247)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][3610/7762]	Time 0.362 (0.328)	Data 9.85e-05 (1.90e-04)	Tok/s 46623 (43106)	Loss/tok 3.1533 (3.1248)	LR 1.250e-04
0: TRAIN [6][3620/7762]	Time 0.354 (0.328)	Data 9.11e-05 (1.90e-04)	Tok/s 47911 (43112)	Loss/tok 3.1121 (3.1248)	LR 1.250e-04
0: TRAIN [6][3630/7762]	Time 0.253 (0.328)	Data 1.04e-04 (1.90e-04)	Tok/s 41791 (43110)	Loss/tok 2.9502 (3.1247)	LR 1.250e-04
0: TRAIN [6][3640/7762]	Time 0.258 (0.328)	Data 1.03e-04 (1.90e-04)	Tok/s 40142 (43103)	Loss/tok 2.9800 (3.1245)	LR 1.250e-04
0: TRAIN [6][3650/7762]	Time 0.361 (0.327)	Data 1.19e-04 (1.89e-04)	Tok/s 46067 (43104)	Loss/tok 3.1955 (3.1244)	LR 1.250e-04
0: TRAIN [6][3660/7762]	Time 0.583 (0.328)	Data 1.01e-04 (1.89e-04)	Tok/s 50962 (43112)	Loss/tok 3.3433 (3.1247)	LR 1.250e-04
0: TRAIN [6][3670/7762]	Time 0.265 (0.328)	Data 1.03e-04 (1.89e-04)	Tok/s 38249 (43115)	Loss/tok 3.0679 (3.1248)	LR 1.250e-04
0: TRAIN [6][3680/7762]	Time 0.175 (0.328)	Data 1.05e-04 (1.89e-04)	Tok/s 30868 (43109)	Loss/tok 2.5718 (3.1247)	LR 1.250e-04
0: TRAIN [6][3690/7762]	Time 0.360 (0.327)	Data 1.03e-04 (1.88e-04)	Tok/s 46215 (43098)	Loss/tok 3.1899 (3.1244)	LR 1.250e-04
0: TRAIN [6][3700/7762]	Time 0.256 (0.327)	Data 1.03e-04 (1.88e-04)	Tok/s 40776 (43101)	Loss/tok 3.0184 (3.1243)	LR 1.250e-04
0: TRAIN [6][3710/7762]	Time 0.368 (0.328)	Data 1.03e-04 (1.88e-04)	Tok/s 45539 (43107)	Loss/tok 3.0496 (3.1246)	LR 1.250e-04
0: TRAIN [6][3720/7762]	Time 0.359 (0.327)	Data 1.03e-04 (1.88e-04)	Tok/s 47179 (43098)	Loss/tok 3.1527 (3.1244)	LR 1.250e-04
0: TRAIN [6][3730/7762]	Time 0.362 (0.327)	Data 9.68e-05 (1.87e-04)	Tok/s 46187 (43094)	Loss/tok 3.2420 (3.1244)	LR 1.250e-04
0: TRAIN [6][3740/7762]	Time 0.262 (0.327)	Data 1.07e-04 (1.87e-04)	Tok/s 38698 (43093)	Loss/tok 2.9177 (3.1245)	LR 1.250e-04
0: TRAIN [6][3750/7762]	Time 0.356 (0.328)	Data 1.03e-04 (1.87e-04)	Tok/s 46659 (43107)	Loss/tok 3.1747 (3.1249)	LR 1.250e-04
0: TRAIN [6][3760/7762]	Time 0.260 (0.328)	Data 1.04e-04 (1.87e-04)	Tok/s 39212 (43111)	Loss/tok 2.9774 (3.1250)	LR 1.250e-04
0: TRAIN [6][3770/7762]	Time 0.459 (0.328)	Data 1.18e-04 (1.87e-04)	Tok/s 51395 (43111)	Loss/tok 3.2173 (3.1247)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][3780/7762]	Time 0.356 (0.328)	Data 1.17e-04 (1.86e-04)	Tok/s 47053 (43106)	Loss/tok 3.1017 (3.1247)	LR 1.250e-04
0: TRAIN [6][3790/7762]	Time 0.361 (0.328)	Data 1.05e-04 (1.86e-04)	Tok/s 46618 (43112)	Loss/tok 3.1091 (3.1248)	LR 1.250e-04
0: TRAIN [6][3800/7762]	Time 0.366 (0.328)	Data 1.01e-04 (1.86e-04)	Tok/s 45979 (43117)	Loss/tok 3.0811 (3.1248)	LR 1.250e-04
0: TRAIN [6][3810/7762]	Time 0.264 (0.328)	Data 1.01e-04 (1.86e-04)	Tok/s 40413 (43116)	Loss/tok 2.9800 (3.1246)	LR 1.250e-04
0: TRAIN [6][3820/7762]	Time 0.265 (0.327)	Data 9.89e-05 (1.86e-04)	Tok/s 38910 (43116)	Loss/tok 2.9136 (3.1246)	LR 1.250e-04
0: TRAIN [6][3830/7762]	Time 0.361 (0.328)	Data 1.21e-04 (1.85e-04)	Tok/s 46689 (43126)	Loss/tok 3.1720 (3.1251)	LR 1.250e-04
0: TRAIN [6][3840/7762]	Time 0.363 (0.328)	Data 1.16e-04 (1.85e-04)	Tok/s 45399 (43129)	Loss/tok 3.1132 (3.1252)	LR 1.250e-04
0: TRAIN [6][3850/7762]	Time 0.464 (0.328)	Data 1.01e-04 (1.85e-04)	Tok/s 50763 (43125)	Loss/tok 3.2711 (3.1252)	LR 1.250e-04
0: TRAIN [6][3860/7762]	Time 0.351 (0.328)	Data 1.03e-04 (1.85e-04)	Tok/s 47940 (43128)	Loss/tok 3.1542 (3.1250)	LR 1.250e-04
0: TRAIN [6][3870/7762]	Time 0.269 (0.328)	Data 1.17e-04 (1.85e-04)	Tok/s 38241 (43127)	Loss/tok 2.8669 (3.1249)	LR 1.250e-04
0: TRAIN [6][3880/7762]	Time 0.454 (0.328)	Data 1.02e-04 (1.84e-04)	Tok/s 51585 (43131)	Loss/tok 3.2611 (3.1249)	LR 1.250e-04
0: TRAIN [6][3890/7762]	Time 0.592 (0.328)	Data 1.20e-04 (1.84e-04)	Tok/s 50672 (43137)	Loss/tok 3.3374 (3.1250)	LR 1.250e-04
0: TRAIN [6][3900/7762]	Time 0.364 (0.328)	Data 1.01e-04 (1.84e-04)	Tok/s 45718 (43146)	Loss/tok 3.2153 (3.1256)	LR 1.250e-04
0: TRAIN [6][3910/7762]	Time 0.355 (0.328)	Data 1.03e-04 (1.84e-04)	Tok/s 47476 (43143)	Loss/tok 3.2077 (3.1256)	LR 1.250e-04
0: TRAIN [6][3920/7762]	Time 0.451 (0.328)	Data 1.02e-04 (1.83e-04)	Tok/s 50698 (43140)	Loss/tok 3.4428 (3.1255)	LR 1.250e-04
0: TRAIN [6][3930/7762]	Time 0.355 (0.328)	Data 1.04e-04 (1.83e-04)	Tok/s 47925 (43139)	Loss/tok 3.2110 (3.1253)	LR 1.250e-04
0: TRAIN [6][3940/7762]	Time 0.262 (0.328)	Data 1.03e-04 (1.83e-04)	Tok/s 39669 (43134)	Loss/tok 2.8330 (3.1251)	LR 1.250e-04
0: TRAIN [6][3950/7762]	Time 0.457 (0.328)	Data 1.00e-04 (1.83e-04)	Tok/s 50970 (43135)	Loss/tok 3.3675 (3.1252)	LR 1.250e-04
0: TRAIN [6][3960/7762]	Time 0.265 (0.328)	Data 1.09e-04 (1.83e-04)	Tok/s 38944 (43129)	Loss/tok 2.8948 (3.1250)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][3970/7762]	Time 0.259 (0.328)	Data 1.04e-04 (1.83e-04)	Tok/s 40967 (43134)	Loss/tok 2.9555 (3.1250)	LR 1.250e-04
0: TRAIN [6][3980/7762]	Time 0.258 (0.328)	Data 1.03e-04 (1.82e-04)	Tok/s 39868 (43136)	Loss/tok 2.9480 (3.1250)	LR 1.250e-04
0: TRAIN [6][3990/7762]	Time 0.366 (0.328)	Data 1.07e-04 (1.82e-04)	Tok/s 47068 (43141)	Loss/tok 2.9604 (3.1250)	LR 1.250e-04
0: TRAIN [6][4000/7762]	Time 0.267 (0.328)	Data 1.07e-04 (1.82e-04)	Tok/s 38771 (43143)	Loss/tok 2.9590 (3.1250)	LR 1.250e-04
0: TRAIN [6][4010/7762]	Time 0.266 (0.328)	Data 1.01e-04 (1.82e-04)	Tok/s 39384 (43140)	Loss/tok 3.0432 (3.1249)	LR 1.250e-04
0: TRAIN [6][4020/7762]	Time 0.258 (0.328)	Data 9.87e-05 (1.82e-04)	Tok/s 39939 (43142)	Loss/tok 2.9259 (3.1249)	LR 1.250e-04
0: TRAIN [6][4030/7762]	Time 0.264 (0.328)	Data 1.20e-04 (1.81e-04)	Tok/s 39457 (43145)	Loss/tok 2.9908 (3.1248)	LR 1.250e-04
0: TRAIN [6][4040/7762]	Time 0.467 (0.328)	Data 1.02e-04 (1.81e-04)	Tok/s 50411 (43144)	Loss/tok 3.2497 (3.1247)	LR 1.250e-04
0: TRAIN [6][4050/7762]	Time 0.441 (0.328)	Data 1.05e-04 (1.81e-04)	Tok/s 53048 (43148)	Loss/tok 3.1888 (3.1247)	LR 1.250e-04
0: TRAIN [6][4060/7762]	Time 0.267 (0.328)	Data 1.10e-04 (1.81e-04)	Tok/s 38137 (43150)	Loss/tok 3.0657 (3.1248)	LR 1.250e-04
0: TRAIN [6][4070/7762]	Time 0.265 (0.328)	Data 1.02e-04 (1.81e-04)	Tok/s 38940 (43153)	Loss/tok 2.9432 (3.1248)	LR 1.250e-04
0: TRAIN [6][4080/7762]	Time 0.441 (0.328)	Data 1.03e-04 (1.80e-04)	Tok/s 53052 (43151)	Loss/tok 3.2904 (3.1247)	LR 1.250e-04
0: TRAIN [6][4090/7762]	Time 0.368 (0.328)	Data 1.03e-04 (1.80e-04)	Tok/s 45933 (43151)	Loss/tok 3.1645 (3.1247)	LR 1.250e-04
0: TRAIN [6][4100/7762]	Time 0.253 (0.328)	Data 1.03e-04 (1.80e-04)	Tok/s 40729 (43150)	Loss/tok 3.0351 (3.1247)	LR 1.250e-04
0: TRAIN [6][4110/7762]	Time 0.265 (0.328)	Data 1.05e-04 (1.80e-04)	Tok/s 39073 (43147)	Loss/tok 3.0094 (3.1246)	LR 1.250e-04
0: TRAIN [6][4120/7762]	Time 0.268 (0.328)	Data 9.78e-05 (1.80e-04)	Tok/s 39058 (43149)	Loss/tok 3.0089 (3.1246)	LR 1.250e-04
0: TRAIN [6][4130/7762]	Time 0.465 (0.328)	Data 1.17e-04 (1.80e-04)	Tok/s 50457 (43151)	Loss/tok 3.3652 (3.1248)	LR 1.250e-04
0: TRAIN [6][4140/7762]	Time 0.369 (0.328)	Data 1.19e-04 (1.79e-04)	Tok/s 45858 (43160)	Loss/tok 3.1295 (3.1251)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][4150/7762]	Time 0.356 (0.328)	Data 1.02e-04 (1.79e-04)	Tok/s 47605 (43162)	Loss/tok 3.2379 (3.1253)	LR 1.250e-04
0: TRAIN [6][4160/7762]	Time 0.361 (0.328)	Data 1.03e-04 (1.79e-04)	Tok/s 46119 (43151)	Loss/tok 3.1327 (3.1250)	LR 1.250e-04
0: TRAIN [6][4170/7762]	Time 0.360 (0.328)	Data 1.02e-04 (1.79e-04)	Tok/s 46564 (43149)	Loss/tok 3.1714 (3.1250)	LR 1.250e-04
0: TRAIN [6][4180/7762]	Time 0.344 (0.328)	Data 1.01e-04 (1.79e-04)	Tok/s 48826 (43147)	Loss/tok 3.2816 (3.1251)	LR 1.250e-04
0: TRAIN [6][4190/7762]	Time 0.586 (0.328)	Data 9.99e-05 (1.78e-04)	Tok/s 50367 (43139)	Loss/tok 3.4887 (3.1251)	LR 1.250e-04
0: TRAIN [6][4200/7762]	Time 0.365 (0.328)	Data 1.19e-04 (1.78e-04)	Tok/s 46254 (43147)	Loss/tok 2.9995 (3.1254)	LR 1.250e-04
0: TRAIN [6][4210/7762]	Time 0.348 (0.328)	Data 1.03e-04 (1.78e-04)	Tok/s 48387 (43147)	Loss/tok 3.1353 (3.1253)	LR 1.250e-04
0: TRAIN [6][4220/7762]	Time 0.453 (0.328)	Data 1.02e-04 (1.78e-04)	Tok/s 51455 (43135)	Loss/tok 3.2022 (3.1250)	LR 1.250e-04
0: TRAIN [6][4230/7762]	Time 0.363 (0.328)	Data 1.12e-04 (1.78e-04)	Tok/s 46272 (43123)	Loss/tok 3.1392 (3.1247)	LR 1.250e-04
0: TRAIN [6][4240/7762]	Time 0.259 (0.328)	Data 9.89e-05 (1.78e-04)	Tok/s 40291 (43129)	Loss/tok 2.8891 (3.1249)	LR 1.250e-04
0: TRAIN [6][4250/7762]	Time 0.265 (0.328)	Data 1.04e-04 (1.77e-04)	Tok/s 39225 (43137)	Loss/tok 2.9307 (3.1252)	LR 1.250e-04
0: TRAIN [6][4260/7762]	Time 0.355 (0.328)	Data 1.02e-04 (1.77e-04)	Tok/s 47851 (43136)	Loss/tok 3.1404 (3.1251)	LR 1.250e-04
0: TRAIN [6][4270/7762]	Time 0.256 (0.328)	Data 9.92e-05 (1.77e-04)	Tok/s 40649 (43133)	Loss/tok 2.9381 (3.1251)	LR 1.250e-04
0: TRAIN [6][4280/7762]	Time 0.177 (0.328)	Data 1.01e-04 (1.77e-04)	Tok/s 30345 (43134)	Loss/tok 2.5173 (3.1251)	LR 1.250e-04
0: TRAIN [6][4290/7762]	Time 0.369 (0.328)	Data 9.56e-05 (1.77e-04)	Tok/s 45379 (43138)	Loss/tok 3.1365 (3.1252)	LR 1.250e-04
0: TRAIN [6][4300/7762]	Time 0.263 (0.328)	Data 1.16e-04 (1.77e-04)	Tok/s 39736 (43140)	Loss/tok 2.9625 (3.1251)	LR 1.250e-04
0: TRAIN [6][4310/7762]	Time 0.267 (0.328)	Data 1.04e-04 (1.76e-04)	Tok/s 38396 (43140)	Loss/tok 2.8871 (3.1252)	LR 1.250e-04
0: TRAIN [6][4320/7762]	Time 0.265 (0.328)	Data 1.03e-04 (1.76e-04)	Tok/s 39119 (43141)	Loss/tok 3.0074 (3.1252)	LR 1.250e-04
0: TRAIN [6][4330/7762]	Time 0.267 (0.328)	Data 1.03e-04 (1.76e-04)	Tok/s 38593 (43141)	Loss/tok 2.8233 (3.1250)	LR 1.250e-04
0: TRAIN [6][4340/7762]	Time 0.466 (0.328)	Data 1.18e-04 (1.76e-04)	Tok/s 50767 (43144)	Loss/tok 3.2950 (3.1252)	LR 1.250e-04
0: TRAIN [6][4350/7762]	Time 0.264 (0.328)	Data 1.01e-04 (1.76e-04)	Tok/s 39063 (43142)	Loss/tok 2.8747 (3.1251)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][4360/7762]	Time 0.589 (0.328)	Data 1.04e-04 (1.76e-04)	Tok/s 51322 (43137)	Loss/tok 3.3819 (3.1250)	LR 1.250e-04
0: TRAIN [6][4370/7762]	Time 0.361 (0.328)	Data 1.00e-04 (1.75e-04)	Tok/s 46473 (43138)	Loss/tok 3.1641 (3.1250)	LR 1.250e-04
0: TRAIN [6][4380/7762]	Time 0.585 (0.328)	Data 1.03e-04 (1.75e-04)	Tok/s 50146 (43139)	Loss/tok 3.4591 (3.1250)	LR 1.250e-04
0: TRAIN [6][4390/7762]	Time 0.590 (0.328)	Data 1.03e-04 (1.75e-04)	Tok/s 49763 (43147)	Loss/tok 3.4953 (3.1253)	LR 1.250e-04
0: TRAIN [6][4400/7762]	Time 0.364 (0.328)	Data 1.06e-04 (1.75e-04)	Tok/s 46506 (43141)	Loss/tok 3.2239 (3.1251)	LR 1.250e-04
0: TRAIN [6][4410/7762]	Time 0.261 (0.328)	Data 1.02e-04 (1.75e-04)	Tok/s 38871 (43138)	Loss/tok 2.9186 (3.1250)	LR 1.250e-04
0: TRAIN [6][4420/7762]	Time 0.263 (0.328)	Data 1.02e-04 (1.75e-04)	Tok/s 38964 (43142)	Loss/tok 2.9372 (3.1250)	LR 1.250e-04
0: TRAIN [6][4430/7762]	Time 0.462 (0.328)	Data 1.15e-04 (1.74e-04)	Tok/s 50564 (43143)	Loss/tok 3.2934 (3.1250)	LR 1.250e-04
0: TRAIN [6][4440/7762]	Time 0.175 (0.328)	Data 1.01e-04 (1.74e-04)	Tok/s 30355 (43138)	Loss/tok 2.5253 (3.1249)	LR 1.250e-04
0: TRAIN [6][4450/7762]	Time 0.457 (0.328)	Data 9.70e-05 (1.74e-04)	Tok/s 50494 (43138)	Loss/tok 3.2616 (3.1249)	LR 1.250e-04
0: TRAIN [6][4460/7762]	Time 0.357 (0.328)	Data 9.94e-05 (1.74e-04)	Tok/s 47766 (43129)	Loss/tok 2.9590 (3.1246)	LR 1.250e-04
0: TRAIN [6][4470/7762]	Time 0.265 (0.328)	Data 1.03e-04 (1.74e-04)	Tok/s 39212 (43122)	Loss/tok 2.8996 (3.1244)	LR 1.250e-04
0: TRAIN [6][4480/7762]	Time 0.265 (0.328)	Data 1.04e-04 (1.74e-04)	Tok/s 38945 (43118)	Loss/tok 2.9265 (3.1243)	LR 1.250e-04
0: TRAIN [6][4490/7762]	Time 0.355 (0.328)	Data 1.01e-04 (1.74e-04)	Tok/s 46932 (43123)	Loss/tok 3.1502 (3.1244)	LR 1.250e-04
0: TRAIN [6][4500/7762]	Time 0.264 (0.328)	Data 1.07e-04 (1.73e-04)	Tok/s 38514 (43124)	Loss/tok 2.8975 (3.1245)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][4510/7762]	Time 0.173 (0.328)	Data 1.10e-04 (1.73e-04)	Tok/s 31200 (43114)	Loss/tok 2.5840 (3.1243)	LR 1.250e-04
0: TRAIN [6][4520/7762]	Time 0.265 (0.328)	Data 1.20e-04 (1.73e-04)	Tok/s 38900 (43118)	Loss/tok 2.8853 (3.1243)	LR 1.250e-04
0: TRAIN [6][4530/7762]	Time 0.263 (0.328)	Data 1.06e-04 (1.73e-04)	Tok/s 38984 (43118)	Loss/tok 2.9470 (3.1242)	LR 1.250e-04
0: TRAIN [6][4540/7762]	Time 0.451 (0.327)	Data 9.68e-05 (1.73e-04)	Tok/s 52102 (43115)	Loss/tok 3.2726 (3.1240)	LR 1.250e-04
0: TRAIN [6][4550/7762]	Time 0.265 (0.327)	Data 1.00e-04 (1.73e-04)	Tok/s 38591 (43111)	Loss/tok 2.9695 (3.1240)	LR 1.250e-04
0: TRAIN [6][4560/7762]	Time 0.460 (0.327)	Data 9.75e-05 (1.72e-04)	Tok/s 51169 (43110)	Loss/tok 3.2145 (3.1239)	LR 1.250e-04
0: TRAIN [6][4570/7762]	Time 0.365 (0.327)	Data 1.01e-04 (1.72e-04)	Tok/s 46181 (43108)	Loss/tok 3.2182 (3.1237)	LR 1.250e-04
0: TRAIN [6][4580/7762]	Time 0.366 (0.327)	Data 9.80e-05 (1.72e-04)	Tok/s 45862 (43105)	Loss/tok 3.0464 (3.1235)	LR 1.250e-04
0: TRAIN [6][4590/7762]	Time 0.362 (0.327)	Data 1.04e-04 (1.72e-04)	Tok/s 46130 (43102)	Loss/tok 3.0681 (3.1233)	LR 1.250e-04
0: TRAIN [6][4600/7762]	Time 0.360 (0.327)	Data 1.01e-04 (1.72e-04)	Tok/s 46733 (43106)	Loss/tok 3.0878 (3.1233)	LR 1.250e-04
0: TRAIN [6][4610/7762]	Time 0.264 (0.327)	Data 1.46e-04 (1.72e-04)	Tok/s 39171 (43104)	Loss/tok 2.9060 (3.1234)	LR 1.250e-04
0: TRAIN [6][4620/7762]	Time 0.465 (0.327)	Data 9.68e-05 (1.72e-04)	Tok/s 49679 (43103)	Loss/tok 3.4197 (3.1235)	LR 1.250e-04
0: TRAIN [6][4630/7762]	Time 0.462 (0.327)	Data 1.06e-04 (1.71e-04)	Tok/s 50473 (43109)	Loss/tok 3.2922 (3.1238)	LR 1.250e-04
0: TRAIN [6][4640/7762]	Time 0.259 (0.327)	Data 9.97e-05 (1.71e-04)	Tok/s 40568 (43106)	Loss/tok 2.9177 (3.1240)	LR 1.250e-04
0: TRAIN [6][4650/7762]	Time 0.584 (0.327)	Data 1.03e-04 (1.71e-04)	Tok/s 51102 (43107)	Loss/tok 3.4342 (3.1242)	LR 1.250e-04
0: TRAIN [6][4660/7762]	Time 0.458 (0.327)	Data 1.09e-04 (1.71e-04)	Tok/s 50993 (43106)	Loss/tok 3.4374 (3.1242)	LR 1.250e-04
0: TRAIN [6][4670/7762]	Time 0.342 (0.328)	Data 1.01e-04 (1.71e-04)	Tok/s 48974 (43113)	Loss/tok 3.1592 (3.1244)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][4680/7762]	Time 0.175 (0.328)	Data 1.21e-04 (1.71e-04)	Tok/s 30852 (43113)	Loss/tok 2.5876 (3.1246)	LR 1.250e-04
0: TRAIN [6][4690/7762]	Time 0.465 (0.328)	Data 1.01e-04 (1.71e-04)	Tok/s 50579 (43116)	Loss/tok 3.2724 (3.1245)	LR 1.250e-04
0: TRAIN [6][4700/7762]	Time 0.256 (0.328)	Data 1.01e-04 (1.70e-04)	Tok/s 40026 (43118)	Loss/tok 2.9944 (3.1245)	LR 1.250e-04
0: TRAIN [6][4710/7762]	Time 0.177 (0.327)	Data 1.01e-04 (1.70e-04)	Tok/s 29788 (43107)	Loss/tok 2.5394 (3.1242)	LR 1.250e-04
0: TRAIN [6][4720/7762]	Time 0.264 (0.327)	Data 1.19e-04 (1.70e-04)	Tok/s 39192 (43104)	Loss/tok 2.9950 (3.1242)	LR 1.250e-04
0: TRAIN [6][4730/7762]	Time 0.464 (0.327)	Data 9.82e-05 (1.70e-04)	Tok/s 51071 (43106)	Loss/tok 3.2480 (3.1242)	LR 1.250e-04
0: TRAIN [6][4740/7762]	Time 0.262 (0.328)	Data 1.03e-04 (1.70e-04)	Tok/s 40132 (43111)	Loss/tok 2.9009 (3.1245)	LR 1.250e-04
0: TRAIN [6][4750/7762]	Time 0.437 (0.327)	Data 1.04e-04 (1.70e-04)	Tok/s 53093 (43108)	Loss/tok 3.3501 (3.1244)	LR 1.250e-04
0: TRAIN [6][4760/7762]	Time 0.366 (0.327)	Data 1.04e-04 (1.70e-04)	Tok/s 45384 (43107)	Loss/tok 3.1069 (3.1245)	LR 1.250e-04
0: TRAIN [6][4770/7762]	Time 0.255 (0.327)	Data 9.61e-05 (1.70e-04)	Tok/s 39938 (43107)	Loss/tok 3.0331 (3.1245)	LR 1.250e-04
0: TRAIN [6][4780/7762]	Time 0.264 (0.328)	Data 1.91e-04 (1.69e-04)	Tok/s 39506 (43108)	Loss/tok 2.9582 (3.1245)	LR 1.250e-04
0: TRAIN [6][4790/7762]	Time 0.256 (0.327)	Data 1.03e-04 (1.69e-04)	Tok/s 39389 (43100)	Loss/tok 2.9923 (3.1243)	LR 1.250e-04
0: TRAIN [6][4800/7762]	Time 0.353 (0.327)	Data 1.03e-04 (1.69e-04)	Tok/s 47970 (43098)	Loss/tok 3.0961 (3.1242)	LR 1.250e-04
0: TRAIN [6][4810/7762]	Time 0.264 (0.327)	Data 9.89e-05 (1.69e-04)	Tok/s 39254 (43094)	Loss/tok 2.9741 (3.1242)	LR 1.250e-04
0: TRAIN [6][4820/7762]	Time 0.355 (0.327)	Data 1.02e-04 (1.69e-04)	Tok/s 47339 (43095)	Loss/tok 3.1193 (3.1242)	LR 1.250e-04
0: TRAIN [6][4830/7762]	Time 0.457 (0.327)	Data 1.01e-04 (1.69e-04)	Tok/s 52239 (43101)	Loss/tok 3.2281 (3.1244)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][4840/7762]	Time 0.574 (0.328)	Data 1.04e-04 (1.69e-04)	Tok/s 51931 (43105)	Loss/tok 3.4251 (3.1245)	LR 1.250e-04
0: TRAIN [6][4850/7762]	Time 0.585 (0.328)	Data 1.03e-04 (1.68e-04)	Tok/s 51326 (43110)	Loss/tok 3.4569 (3.1247)	LR 1.250e-04
0: TRAIN [6][4860/7762]	Time 0.260 (0.328)	Data 9.99e-05 (1.68e-04)	Tok/s 40484 (43109)	Loss/tok 3.0181 (3.1247)	LR 1.250e-04
0: TRAIN [6][4870/7762]	Time 0.177 (0.328)	Data 1.01e-04 (1.68e-04)	Tok/s 30018 (43100)	Loss/tok 2.5543 (3.1245)	LR 1.250e-04
0: TRAIN [6][4880/7762]	Time 0.264 (0.328)	Data 1.23e-04 (1.68e-04)	Tok/s 39469 (43100)	Loss/tok 2.9639 (3.1246)	LR 1.250e-04
0: TRAIN [6][4890/7762]	Time 0.351 (0.328)	Data 1.04e-04 (1.68e-04)	Tok/s 47239 (43106)	Loss/tok 3.0471 (3.1247)	LR 1.250e-04
0: TRAIN [6][4900/7762]	Time 0.352 (0.328)	Data 1.07e-04 (1.68e-04)	Tok/s 47688 (43107)	Loss/tok 3.1580 (3.1246)	LR 1.250e-04
0: TRAIN [6][4910/7762]	Time 0.269 (0.328)	Data 1.04e-04 (1.68e-04)	Tok/s 38559 (43104)	Loss/tok 2.9507 (3.1246)	LR 1.250e-04
0: TRAIN [6][4920/7762]	Time 0.364 (0.327)	Data 1.29e-04 (1.68e-04)	Tok/s 46549 (43098)	Loss/tok 3.1839 (3.1245)	LR 1.250e-04
0: TRAIN [6][4930/7762]	Time 0.259 (0.327)	Data 1.00e-04 (1.67e-04)	Tok/s 39651 (43094)	Loss/tok 3.0306 (3.1243)	LR 1.250e-04
0: TRAIN [6][4940/7762]	Time 0.177 (0.327)	Data 1.05e-04 (1.67e-04)	Tok/s 29389 (43091)	Loss/tok 2.4649 (3.1241)	LR 1.250e-04
0: TRAIN [6][4950/7762]	Time 0.257 (0.327)	Data 9.99e-05 (1.67e-04)	Tok/s 39360 (43085)	Loss/tok 2.8506 (3.1241)	LR 1.250e-04
0: TRAIN [6][4960/7762]	Time 0.262 (0.327)	Data 1.00e-04 (1.67e-04)	Tok/s 39316 (43090)	Loss/tok 2.9358 (3.1242)	LR 1.250e-04
0: TRAIN [6][4970/7762]	Time 0.363 (0.327)	Data 1.02e-04 (1.67e-04)	Tok/s 46475 (43091)	Loss/tok 3.0756 (3.1241)	LR 1.250e-04
0: TRAIN [6][4980/7762]	Time 0.463 (0.327)	Data 9.73e-05 (1.67e-04)	Tok/s 50493 (43097)	Loss/tok 3.3055 (3.1242)	LR 1.250e-04
0: TRAIN [6][4990/7762]	Time 0.342 (0.327)	Data 1.04e-04 (1.67e-04)	Tok/s 49313 (43097)	Loss/tok 3.0516 (3.1241)	LR 1.250e-04
0: TRAIN [6][5000/7762]	Time 0.355 (0.327)	Data 1.01e-04 (1.67e-04)	Tok/s 47063 (43100)	Loss/tok 3.0263 (3.1242)	LR 1.250e-04
0: TRAIN [6][5010/7762]	Time 0.263 (0.327)	Data 9.73e-05 (1.66e-04)	Tok/s 39299 (43099)	Loss/tok 2.9532 (3.1242)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][5020/7762]	Time 0.264 (0.327)	Data 9.97e-05 (1.66e-04)	Tok/s 38943 (43097)	Loss/tok 2.8897 (3.1242)	LR 1.250e-04
0: TRAIN [6][5030/7762]	Time 0.263 (0.327)	Data 9.99e-05 (1.66e-04)	Tok/s 38064 (43097)	Loss/tok 2.9916 (3.1244)	LR 1.250e-04
0: TRAIN [6][5040/7762]	Time 0.354 (0.327)	Data 9.66e-05 (1.66e-04)	Tok/s 47069 (43097)	Loss/tok 2.9955 (3.1244)	LR 1.250e-04
0: TRAIN [6][5050/7762]	Time 0.269 (0.327)	Data 1.01e-04 (1.66e-04)	Tok/s 38399 (43095)	Loss/tok 2.8793 (3.1243)	LR 1.250e-04
0: TRAIN [6][5060/7762]	Time 0.586 (0.327)	Data 1.02e-04 (1.66e-04)	Tok/s 50365 (43095)	Loss/tok 3.5635 (3.1245)	LR 1.250e-04
0: TRAIN [6][5070/7762]	Time 0.351 (0.327)	Data 1.03e-04 (1.66e-04)	Tok/s 47911 (43097)	Loss/tok 3.0639 (3.1243)	LR 1.250e-04
0: TRAIN [6][5080/7762]	Time 0.346 (0.327)	Data 9.94e-05 (1.65e-04)	Tok/s 48487 (43091)	Loss/tok 3.2534 (3.1242)	LR 1.250e-04
0: TRAIN [6][5090/7762]	Time 0.457 (0.327)	Data 9.75e-05 (1.65e-04)	Tok/s 50788 (43088)	Loss/tok 3.2696 (3.1242)	LR 1.250e-04
0: TRAIN [6][5100/7762]	Time 0.343 (0.327)	Data 1.05e-04 (1.65e-04)	Tok/s 48786 (43085)	Loss/tok 3.2082 (3.1240)	LR 1.250e-04
0: TRAIN [6][5110/7762]	Time 0.255 (0.327)	Data 1.04e-04 (1.65e-04)	Tok/s 40501 (43089)	Loss/tok 2.9015 (3.1241)	LR 1.250e-04
0: TRAIN [6][5120/7762]	Time 0.366 (0.327)	Data 1.04e-04 (1.65e-04)	Tok/s 46217 (43094)	Loss/tok 2.9829 (3.1242)	LR 1.250e-04
0: TRAIN [6][5130/7762]	Time 0.263 (0.327)	Data 1.03e-04 (1.65e-04)	Tok/s 39475 (43092)	Loss/tok 3.0072 (3.1241)	LR 1.250e-04
0: TRAIN [6][5140/7762]	Time 0.263 (0.327)	Data 1.03e-04 (1.65e-04)	Tok/s 39631 (43090)	Loss/tok 2.9504 (3.1239)	LR 1.250e-04
0: TRAIN [6][5150/7762]	Time 0.267 (0.327)	Data 9.85e-05 (1.65e-04)	Tok/s 38003 (43082)	Loss/tok 3.0213 (3.1237)	LR 1.250e-04
0: TRAIN [6][5160/7762]	Time 0.260 (0.327)	Data 1.03e-04 (1.65e-04)	Tok/s 40338 (43086)	Loss/tok 2.8206 (3.1239)	LR 1.250e-04
0: TRAIN [6][5170/7762]	Time 0.174 (0.327)	Data 1.03e-04 (1.64e-04)	Tok/s 30221 (43083)	Loss/tok 2.4786 (3.1240)	LR 1.250e-04
0: TRAIN [6][5180/7762]	Time 0.264 (0.327)	Data 1.02e-04 (1.64e-04)	Tok/s 39627 (43085)	Loss/tok 2.9268 (3.1239)	LR 1.250e-04
0: TRAIN [6][5190/7762]	Time 0.265 (0.327)	Data 1.03e-04 (1.64e-04)	Tok/s 39267 (43086)	Loss/tok 3.0341 (3.1239)	LR 1.250e-04
0: TRAIN [6][5200/7762]	Time 0.362 (0.327)	Data 9.66e-05 (1.64e-04)	Tok/s 46340 (43085)	Loss/tok 3.1148 (3.1239)	LR 1.250e-04
0: TRAIN [6][5210/7762]	Time 0.265 (0.327)	Data 9.97e-05 (1.64e-04)	Tok/s 39262 (43083)	Loss/tok 3.0111 (3.1238)	LR 1.250e-04
0: TRAIN [6][5220/7762]	Time 0.174 (0.327)	Data 1.01e-04 (1.64e-04)	Tok/s 30680 (43078)	Loss/tok 2.5058 (3.1238)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][5230/7762]	Time 0.360 (0.327)	Data 1.01e-04 (1.64e-04)	Tok/s 46654 (43073)	Loss/tok 3.1884 (3.1237)	LR 1.250e-04
0: TRAIN [6][5240/7762]	Time 0.577 (0.327)	Data 1.14e-04 (1.64e-04)	Tok/s 51209 (43076)	Loss/tok 3.4832 (3.1240)	LR 1.250e-04
0: TRAIN [6][5250/7762]	Time 0.363 (0.327)	Data 9.94e-05 (1.64e-04)	Tok/s 46893 (43073)	Loss/tok 3.1934 (3.1239)	LR 1.250e-04
0: TRAIN [6][5260/7762]	Time 0.443 (0.327)	Data 1.03e-04 (1.63e-04)	Tok/s 52948 (43077)	Loss/tok 3.2152 (3.1240)	LR 1.250e-04
0: TRAIN [6][5270/7762]	Time 0.464 (0.327)	Data 1.04e-04 (1.63e-04)	Tok/s 50496 (43079)	Loss/tok 3.3443 (3.1241)	LR 1.250e-04
0: TRAIN [6][5280/7762]	Time 0.342 (0.327)	Data 1.04e-04 (1.63e-04)	Tok/s 49344 (43084)	Loss/tok 3.0606 (3.1241)	LR 1.250e-04
0: TRAIN [6][5290/7762]	Time 0.178 (0.327)	Data 1.01e-04 (1.63e-04)	Tok/s 29395 (43079)	Loss/tok 2.6222 (3.1239)	LR 1.250e-04
0: TRAIN [6][5300/7762]	Time 0.259 (0.327)	Data 1.06e-04 (1.63e-04)	Tok/s 39919 (43080)	Loss/tok 2.9355 (3.1241)	LR 1.250e-04
0: TRAIN [6][5310/7762]	Time 0.174 (0.327)	Data 1.01e-04 (1.63e-04)	Tok/s 30607 (43075)	Loss/tok 2.5834 (3.1239)	LR 1.250e-04
0: TRAIN [6][5320/7762]	Time 0.458 (0.327)	Data 1.03e-04 (1.63e-04)	Tok/s 51070 (43073)	Loss/tok 3.2785 (3.1238)	LR 1.250e-04
0: TRAIN [6][5330/7762]	Time 0.265 (0.327)	Data 1.04e-04 (1.63e-04)	Tok/s 39091 (43077)	Loss/tok 2.9439 (3.1238)	LR 1.250e-04
0: TRAIN [6][5340/7762]	Time 0.455 (0.327)	Data 9.87e-05 (1.62e-04)	Tok/s 51007 (43077)	Loss/tok 3.3128 (3.1240)	LR 1.250e-04
0: TRAIN [6][5350/7762]	Time 0.264 (0.327)	Data 1.18e-04 (1.62e-04)	Tok/s 39087 (43075)	Loss/tok 2.9319 (3.1238)	LR 1.250e-04
0: TRAIN [6][5360/7762]	Time 0.351 (0.327)	Data 1.00e-04 (1.62e-04)	Tok/s 48428 (43074)	Loss/tok 3.0558 (3.1238)	LR 1.250e-04
0: TRAIN [6][5370/7762]	Time 0.262 (0.327)	Data 1.01e-04 (1.62e-04)	Tok/s 39215 (43076)	Loss/tok 3.0185 (3.1238)	LR 1.250e-04
0: TRAIN [6][5380/7762]	Time 0.266 (0.327)	Data 1.00e-04 (1.62e-04)	Tok/s 38981 (43079)	Loss/tok 2.9094 (3.1238)	LR 1.250e-04
0: TRAIN [6][5390/7762]	Time 0.266 (0.327)	Data 1.02e-04 (1.62e-04)	Tok/s 38930 (43080)	Loss/tok 2.9728 (3.1240)	LR 1.250e-04
0: TRAIN [6][5400/7762]	Time 0.177 (0.327)	Data 1.01e-04 (1.62e-04)	Tok/s 30024 (43082)	Loss/tok 2.6448 (3.1241)	LR 1.250e-04
0: TRAIN [6][5410/7762]	Time 0.262 (0.327)	Data 9.80e-05 (1.62e-04)	Tok/s 40477 (43076)	Loss/tok 2.8195 (3.1238)	LR 1.250e-04
0: TRAIN [6][5420/7762]	Time 0.172 (0.327)	Data 1.04e-04 (1.62e-04)	Tok/s 30969 (43072)	Loss/tok 2.5189 (3.1237)	LR 1.250e-04
0: TRAIN [6][5430/7762]	Time 0.173 (0.327)	Data 1.02e-04 (1.62e-04)	Tok/s 30984 (43066)	Loss/tok 2.4831 (3.1235)	LR 1.250e-04
0: TRAIN [6][5440/7762]	Time 0.260 (0.327)	Data 9.80e-05 (1.61e-04)	Tok/s 39729 (43064)	Loss/tok 2.9280 (3.1234)	LR 1.250e-04
0: TRAIN [6][5450/7762]	Time 0.463 (0.327)	Data 1.03e-04 (1.61e-04)	Tok/s 50281 (43070)	Loss/tok 3.3186 (3.1237)	LR 1.250e-04
0: TRAIN [6][5460/7762]	Time 0.451 (0.327)	Data 9.92e-05 (1.61e-04)	Tok/s 52000 (43070)	Loss/tok 3.3114 (3.1237)	LR 1.250e-04
0: TRAIN [6][5470/7762]	Time 0.262 (0.327)	Data 9.89e-05 (1.61e-04)	Tok/s 38903 (43075)	Loss/tok 2.9285 (3.1237)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][5480/7762]	Time 0.573 (0.327)	Data 1.08e-04 (1.61e-04)	Tok/s 51724 (43076)	Loss/tok 3.4255 (3.1238)	LR 1.250e-04
0: TRAIN [6][5490/7762]	Time 0.358 (0.327)	Data 1.02e-04 (1.61e-04)	Tok/s 47397 (43074)	Loss/tok 3.1062 (3.1237)	LR 1.250e-04
0: TRAIN [6][5500/7762]	Time 0.567 (0.327)	Data 9.94e-05 (1.61e-04)	Tok/s 52634 (43068)	Loss/tok 3.3807 (3.1236)	LR 1.250e-04
0: TRAIN [6][5510/7762]	Time 0.264 (0.327)	Data 9.97e-05 (1.61e-04)	Tok/s 39560 (43073)	Loss/tok 2.8730 (3.1238)	LR 1.250e-04
0: TRAIN [6][5520/7762]	Time 0.262 (0.327)	Data 1.01e-04 (1.61e-04)	Tok/s 40195 (43070)	Loss/tok 2.7823 (3.1238)	LR 1.250e-04
0: TRAIN [6][5530/7762]	Time 0.255 (0.327)	Data 1.15e-04 (1.60e-04)	Tok/s 40776 (43070)	Loss/tok 2.8452 (3.1236)	LR 1.250e-04
0: TRAIN [6][5540/7762]	Time 0.355 (0.327)	Data 1.01e-04 (1.60e-04)	Tok/s 46416 (43064)	Loss/tok 3.1698 (3.1235)	LR 1.250e-04
0: TRAIN [6][5550/7762]	Time 0.583 (0.327)	Data 1.13e-04 (1.60e-04)	Tok/s 51262 (43066)	Loss/tok 3.4122 (3.1235)	LR 1.250e-04
0: TRAIN [6][5560/7762]	Time 0.259 (0.327)	Data 1.03e-04 (1.60e-04)	Tok/s 40189 (43063)	Loss/tok 2.8810 (3.1234)	LR 1.250e-04
0: TRAIN [6][5570/7762]	Time 0.264 (0.327)	Data 1.00e-04 (1.60e-04)	Tok/s 39667 (43062)	Loss/tok 2.8869 (3.1233)	LR 1.250e-04
0: TRAIN [6][5580/7762]	Time 0.578 (0.327)	Data 1.13e-04 (1.60e-04)	Tok/s 51686 (43062)	Loss/tok 3.4869 (3.1234)	LR 1.250e-04
0: TRAIN [6][5590/7762]	Time 0.460 (0.327)	Data 1.02e-04 (1.60e-04)	Tok/s 50916 (43065)	Loss/tok 3.2622 (3.1236)	LR 1.250e-04
0: TRAIN [6][5600/7762]	Time 0.259 (0.327)	Data 1.18e-04 (1.60e-04)	Tok/s 40067 (43063)	Loss/tok 2.9899 (3.1235)	LR 1.250e-04
0: TRAIN [6][5610/7762]	Time 0.263 (0.327)	Data 9.92e-05 (1.60e-04)	Tok/s 39970 (43066)	Loss/tok 2.8080 (3.1236)	LR 1.250e-04
0: TRAIN [6][5620/7762]	Time 0.265 (0.327)	Data 1.05e-04 (1.60e-04)	Tok/s 39525 (43069)	Loss/tok 3.0001 (3.1238)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][5630/7762]	Time 0.263 (0.327)	Data 1.06e-04 (1.60e-04)	Tok/s 39287 (43066)	Loss/tok 2.8645 (3.1237)	LR 1.250e-04
0: TRAIN [6][5640/7762]	Time 0.172 (0.327)	Data 1.03e-04 (1.59e-04)	Tok/s 30262 (43060)	Loss/tok 2.5234 (3.1237)	LR 1.250e-04
0: TRAIN [6][5650/7762]	Time 0.262 (0.327)	Data 9.87e-05 (1.59e-04)	Tok/s 38933 (43056)	Loss/tok 2.8858 (3.1236)	LR 1.250e-04
0: TRAIN [6][5660/7762]	Time 0.263 (0.327)	Data 9.99e-05 (1.59e-04)	Tok/s 39375 (43052)	Loss/tok 2.9626 (3.1235)	LR 1.250e-04
0: TRAIN [6][5670/7762]	Time 0.257 (0.327)	Data 1.03e-04 (1.59e-04)	Tok/s 39844 (43047)	Loss/tok 3.0811 (3.1234)	LR 1.250e-04
0: TRAIN [6][5680/7762]	Time 0.268 (0.327)	Data 1.01e-04 (1.59e-04)	Tok/s 38809 (43044)	Loss/tok 3.0272 (3.1233)	LR 1.250e-04
0: TRAIN [6][5690/7762]	Time 0.460 (0.327)	Data 1.02e-04 (1.59e-04)	Tok/s 50460 (43044)	Loss/tok 3.3718 (3.1233)	LR 1.250e-04
0: TRAIN [6][5700/7762]	Time 0.449 (0.327)	Data 2.37e-04 (1.59e-04)	Tok/s 51717 (43049)	Loss/tok 3.2881 (3.1235)	LR 1.250e-04
0: TRAIN [6][5710/7762]	Time 0.587 (0.327)	Data 1.04e-04 (1.59e-04)	Tok/s 50655 (43047)	Loss/tok 3.3750 (3.1234)	LR 1.250e-04
0: TRAIN [6][5720/7762]	Time 0.355 (0.327)	Data 1.19e-04 (1.59e-04)	Tok/s 46495 (43047)	Loss/tok 3.1740 (3.1234)	LR 1.250e-04
0: TRAIN [6][5730/7762]	Time 0.268 (0.327)	Data 1.03e-04 (1.59e-04)	Tok/s 38426 (43046)	Loss/tok 2.9436 (3.1235)	LR 1.250e-04
0: TRAIN [6][5740/7762]	Time 0.343 (0.327)	Data 1.03e-04 (1.59e-04)	Tok/s 49221 (43048)	Loss/tok 3.2181 (3.1236)	LR 1.250e-04
0: TRAIN [6][5750/7762]	Time 0.264 (0.327)	Data 9.92e-05 (1.58e-04)	Tok/s 39343 (43051)	Loss/tok 2.9105 (3.1236)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][5760/7762]	Time 0.357 (0.327)	Data 9.78e-05 (1.58e-04)	Tok/s 46519 (43048)	Loss/tok 3.1867 (3.1236)	LR 1.250e-04
0: TRAIN [6][5770/7762]	Time 0.360 (0.327)	Data 1.01e-04 (1.58e-04)	Tok/s 46549 (43049)	Loss/tok 3.0338 (3.1235)	LR 1.250e-04
0: TRAIN [6][5780/7762]	Time 0.358 (0.327)	Data 1.01e-04 (1.58e-04)	Tok/s 46076 (43050)	Loss/tok 3.1517 (3.1236)	LR 1.250e-04
0: TRAIN [6][5790/7762]	Time 0.265 (0.327)	Data 1.03e-04 (1.58e-04)	Tok/s 38840 (43053)	Loss/tok 2.7390 (3.1236)	LR 1.250e-04
0: TRAIN [6][5800/7762]	Time 0.363 (0.327)	Data 1.26e-04 (1.58e-04)	Tok/s 46135 (43052)	Loss/tok 3.0705 (3.1235)	LR 1.250e-04
0: TRAIN [6][5810/7762]	Time 0.461 (0.327)	Data 9.99e-05 (1.58e-04)	Tok/s 50560 (43055)	Loss/tok 3.2387 (3.1236)	LR 1.250e-04
0: TRAIN [6][5820/7762]	Time 0.450 (0.327)	Data 1.04e-04 (1.58e-04)	Tok/s 51486 (43060)	Loss/tok 3.2629 (3.1238)	LR 1.250e-04
0: TRAIN [6][5830/7762]	Time 0.464 (0.327)	Data 1.05e-04 (1.58e-04)	Tok/s 50093 (43065)	Loss/tok 3.2619 (3.1238)	LR 1.250e-04
0: TRAIN [6][5840/7762]	Time 0.462 (0.327)	Data 1.04e-04 (1.58e-04)	Tok/s 50974 (43063)	Loss/tok 3.3191 (3.1239)	LR 1.250e-04
0: TRAIN [6][5850/7762]	Time 0.367 (0.327)	Data 9.68e-05 (1.57e-04)	Tok/s 45834 (43066)	Loss/tok 3.1579 (3.1240)	LR 1.250e-04
0: TRAIN [6][5860/7762]	Time 0.269 (0.327)	Data 1.03e-04 (1.57e-04)	Tok/s 39279 (43065)	Loss/tok 2.9368 (3.1241)	LR 1.250e-04
0: TRAIN [6][5870/7762]	Time 0.354 (0.327)	Data 9.94e-05 (1.57e-04)	Tok/s 47394 (43066)	Loss/tok 3.1945 (3.1242)	LR 1.250e-04
0: TRAIN [6][5880/7762]	Time 0.173 (0.327)	Data 1.01e-04 (1.57e-04)	Tok/s 30302 (43063)	Loss/tok 2.4786 (3.1241)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][5890/7762]	Time 0.464 (0.327)	Data 1.01e-04 (1.57e-04)	Tok/s 50498 (43064)	Loss/tok 3.3211 (3.1242)	LR 1.250e-04
0: TRAIN [6][5900/7762]	Time 0.590 (0.327)	Data 1.23e-04 (1.57e-04)	Tok/s 50228 (43065)	Loss/tok 3.5494 (3.1243)	LR 1.250e-04
0: TRAIN [6][5910/7762]	Time 0.261 (0.327)	Data 1.02e-04 (1.57e-04)	Tok/s 39185 (43069)	Loss/tok 2.8852 (3.1244)	LR 1.250e-04
0: TRAIN [6][5920/7762]	Time 0.265 (0.327)	Data 1.14e-04 (1.57e-04)	Tok/s 38579 (43069)	Loss/tok 2.9554 (3.1244)	LR 1.250e-04
0: TRAIN [6][5930/7762]	Time 0.361 (0.327)	Data 1.01e-04 (1.57e-04)	Tok/s 45904 (43069)	Loss/tok 3.1451 (3.1244)	LR 1.250e-04
0: TRAIN [6][5940/7762]	Time 0.353 (0.327)	Data 1.05e-04 (1.57e-04)	Tok/s 48316 (43071)	Loss/tok 3.0379 (3.1244)	LR 1.250e-04
0: TRAIN [6][5950/7762]	Time 0.343 (0.327)	Data 1.21e-04 (1.57e-04)	Tok/s 49024 (43076)	Loss/tok 3.1455 (3.1246)	LR 1.250e-04
0: TRAIN [6][5960/7762]	Time 0.262 (0.327)	Data 1.03e-04 (1.57e-04)	Tok/s 39800 (43076)	Loss/tok 2.9495 (3.1245)	LR 1.250e-04
0: TRAIN [6][5970/7762]	Time 0.261 (0.327)	Data 9.82e-05 (1.56e-04)	Tok/s 39688 (43071)	Loss/tok 2.9412 (3.1244)	LR 1.250e-04
0: TRAIN [6][5980/7762]	Time 0.263 (0.327)	Data 1.04e-04 (1.56e-04)	Tok/s 39763 (43068)	Loss/tok 2.8734 (3.1244)	LR 1.250e-04
0: TRAIN [6][5990/7762]	Time 0.252 (0.327)	Data 1.02e-04 (1.56e-04)	Tok/s 40474 (43071)	Loss/tok 2.9348 (3.1244)	LR 1.250e-04
0: TRAIN [6][6000/7762]	Time 0.588 (0.327)	Data 1.01e-04 (1.56e-04)	Tok/s 51431 (43074)	Loss/tok 3.2935 (3.1245)	LR 1.250e-04
0: TRAIN [6][6010/7762]	Time 0.345 (0.327)	Data 1.02e-04 (1.56e-04)	Tok/s 49411 (43074)	Loss/tok 3.0680 (3.1245)	LR 1.250e-04
0: TRAIN [6][6020/7762]	Time 0.263 (0.327)	Data 1.19e-04 (1.56e-04)	Tok/s 38765 (43073)	Loss/tok 2.8579 (3.1244)	LR 1.250e-04
0: TRAIN [6][6030/7762]	Time 0.266 (0.327)	Data 1.08e-04 (1.56e-04)	Tok/s 38842 (43070)	Loss/tok 2.9239 (3.1244)	LR 1.250e-04
0: TRAIN [6][6040/7762]	Time 0.179 (0.327)	Data 9.89e-05 (1.56e-04)	Tok/s 28914 (43070)	Loss/tok 2.4149 (3.1242)	LR 1.250e-04
0: TRAIN [6][6050/7762]	Time 0.266 (0.327)	Data 9.97e-05 (1.56e-04)	Tok/s 39290 (43071)	Loss/tok 3.0375 (3.1243)	LR 1.250e-04
0: TRAIN [6][6060/7762]	Time 0.362 (0.327)	Data 9.99e-05 (1.56e-04)	Tok/s 46127 (43070)	Loss/tok 3.1329 (3.1242)	LR 1.250e-04
0: TRAIN [6][6070/7762]	Time 0.357 (0.327)	Data 1.01e-04 (1.56e-04)	Tok/s 46966 (43074)	Loss/tok 3.1513 (3.1243)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][6080/7762]	Time 0.246 (0.327)	Data 1.26e-04 (1.55e-04)	Tok/s 42408 (43074)	Loss/tok 2.8345 (3.1242)	LR 1.250e-04
0: TRAIN [6][6090/7762]	Time 0.260 (0.327)	Data 9.92e-05 (1.55e-04)	Tok/s 40097 (43075)	Loss/tok 2.9079 (3.1242)	LR 1.250e-04
0: TRAIN [6][6100/7762]	Time 0.352 (0.327)	Data 1.05e-04 (1.55e-04)	Tok/s 48153 (43079)	Loss/tok 3.0217 (3.1242)	LR 1.250e-04
0: TRAIN [6][6110/7762]	Time 0.265 (0.327)	Data 1.03e-04 (1.55e-04)	Tok/s 39093 (43080)	Loss/tok 2.8614 (3.1242)	LR 1.250e-04
0: TRAIN [6][6120/7762]	Time 0.460 (0.327)	Data 1.02e-04 (1.55e-04)	Tok/s 50505 (43081)	Loss/tok 3.2622 (3.1242)	LR 1.250e-04
0: TRAIN [6][6130/7762]	Time 0.365 (0.327)	Data 1.17e-04 (1.55e-04)	Tok/s 46227 (43083)	Loss/tok 3.0951 (3.1243)	LR 1.250e-04
0: TRAIN [6][6140/7762]	Time 0.364 (0.327)	Data 1.21e-04 (1.55e-04)	Tok/s 46102 (43079)	Loss/tok 3.3323 (3.1241)	LR 1.250e-04
0: TRAIN [6][6150/7762]	Time 0.466 (0.327)	Data 1.18e-04 (1.55e-04)	Tok/s 49695 (43082)	Loss/tok 3.2737 (3.1244)	LR 1.250e-04
0: TRAIN [6][6160/7762]	Time 0.591 (0.327)	Data 9.61e-05 (1.55e-04)	Tok/s 49900 (43081)	Loss/tok 3.6636 (3.1244)	LR 1.250e-04
0: TRAIN [6][6170/7762]	Time 0.459 (0.327)	Data 1.01e-04 (1.55e-04)	Tok/s 50855 (43075)	Loss/tok 3.2744 (3.1243)	LR 1.250e-04
0: TRAIN [6][6180/7762]	Time 0.359 (0.327)	Data 1.18e-04 (1.55e-04)	Tok/s 46626 (43072)	Loss/tok 3.0969 (3.1242)	LR 1.250e-04
0: TRAIN [6][6190/7762]	Time 0.343 (0.327)	Data 1.04e-04 (1.55e-04)	Tok/s 48492 (43079)	Loss/tok 3.2232 (3.1244)	LR 1.250e-04
0: TRAIN [6][6200/7762]	Time 0.268 (0.327)	Data 9.85e-05 (1.55e-04)	Tok/s 38314 (43081)	Loss/tok 2.9823 (3.1244)	LR 1.250e-04
0: TRAIN [6][6210/7762]	Time 0.341 (0.327)	Data 1.07e-04 (1.54e-04)	Tok/s 49115 (43080)	Loss/tok 3.1580 (3.1245)	LR 1.250e-04
0: TRAIN [6][6220/7762]	Time 0.267 (0.327)	Data 9.70e-05 (1.54e-04)	Tok/s 38979 (43079)	Loss/tok 2.9394 (3.1245)	LR 1.250e-04
0: TRAIN [6][6230/7762]	Time 0.263 (0.327)	Data 9.85e-05 (1.54e-04)	Tok/s 39661 (43078)	Loss/tok 2.8408 (3.1244)	LR 1.250e-04
0: TRAIN [6][6240/7762]	Time 0.439 (0.327)	Data 9.58e-05 (1.54e-04)	Tok/s 53351 (43077)	Loss/tok 3.2839 (3.1245)	LR 1.250e-04
0: TRAIN [6][6250/7762]	Time 0.173 (0.327)	Data 9.44e-05 (1.54e-04)	Tok/s 30056 (43072)	Loss/tok 2.5738 (3.1244)	LR 1.250e-04
0: TRAIN [6][6260/7762]	Time 0.460 (0.327)	Data 1.01e-04 (1.54e-04)	Tok/s 51205 (43070)	Loss/tok 3.1891 (3.1243)	LR 1.250e-04
0: TRAIN [6][6270/7762]	Time 0.456 (0.327)	Data 9.94e-05 (1.54e-04)	Tok/s 50601 (43070)	Loss/tok 3.3402 (3.1242)	LR 1.250e-04
0: TRAIN [6][6280/7762]	Time 0.262 (0.327)	Data 9.94e-05 (1.54e-04)	Tok/s 40016 (43071)	Loss/tok 2.9431 (3.1244)	LR 1.250e-04
0: TRAIN [6][6290/7762]	Time 0.365 (0.327)	Data 1.01e-04 (1.54e-04)	Tok/s 45598 (43074)	Loss/tok 3.2093 (3.1244)	LR 1.250e-04
0: TRAIN [6][6300/7762]	Time 0.461 (0.327)	Data 1.02e-04 (1.54e-04)	Tok/s 51329 (43069)	Loss/tok 3.3046 (3.1243)	LR 1.250e-04
0: TRAIN [6][6310/7762]	Time 0.175 (0.327)	Data 1.03e-04 (1.54e-04)	Tok/s 30020 (43069)	Loss/tok 2.5254 (3.1242)	LR 1.250e-04
0: TRAIN [6][6320/7762]	Time 0.264 (0.327)	Data 1.03e-04 (1.54e-04)	Tok/s 39080 (43064)	Loss/tok 2.9441 (3.1240)	LR 1.250e-04
0: TRAIN [6][6330/7762]	Time 0.252 (0.327)	Data 1.03e-04 (1.53e-04)	Tok/s 41540 (43060)	Loss/tok 3.0146 (3.1240)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][6340/7762]	Time 0.577 (0.327)	Data 1.02e-04 (1.53e-04)	Tok/s 52813 (43058)	Loss/tok 3.4628 (3.1241)	LR 1.250e-04
0: TRAIN [6][6350/7762]	Time 0.362 (0.327)	Data 1.01e-04 (1.53e-04)	Tok/s 46960 (43058)	Loss/tok 3.0994 (3.1241)	LR 1.250e-04
0: TRAIN [6][6360/7762]	Time 0.365 (0.327)	Data 1.00e-04 (1.53e-04)	Tok/s 45331 (43058)	Loss/tok 3.1488 (3.1242)	LR 1.250e-04
0: TRAIN [6][6370/7762]	Time 0.255 (0.327)	Data 1.20e-04 (1.53e-04)	Tok/s 40546 (43062)	Loss/tok 2.9322 (3.1242)	LR 1.250e-04
0: TRAIN [6][6380/7762]	Time 0.259 (0.327)	Data 1.08e-04 (1.53e-04)	Tok/s 38462 (43057)	Loss/tok 2.9257 (3.1241)	LR 1.250e-04
0: TRAIN [6][6390/7762]	Time 0.445 (0.327)	Data 9.99e-05 (1.53e-04)	Tok/s 53076 (43060)	Loss/tok 3.2738 (3.1241)	LR 1.250e-04
0: TRAIN [6][6400/7762]	Time 0.259 (0.327)	Data 1.02e-04 (1.53e-04)	Tok/s 39804 (43055)	Loss/tok 2.9768 (3.1240)	LR 1.250e-04
0: TRAIN [6][6410/7762]	Time 0.350 (0.327)	Data 1.02e-04 (1.53e-04)	Tok/s 48324 (43058)	Loss/tok 3.1883 (3.1240)	LR 1.250e-04
0: TRAIN [6][6420/7762]	Time 0.263 (0.327)	Data 1.18e-04 (1.53e-04)	Tok/s 39514 (43059)	Loss/tok 2.9102 (3.1239)	LR 1.250e-04
0: TRAIN [6][6430/7762]	Time 0.262 (0.327)	Data 1.17e-04 (1.53e-04)	Tok/s 38944 (43061)	Loss/tok 2.8862 (3.1239)	LR 1.250e-04
0: TRAIN [6][6440/7762]	Time 0.254 (0.327)	Data 1.19e-04 (1.53e-04)	Tok/s 40212 (43060)	Loss/tok 2.9582 (3.1238)	LR 1.250e-04
0: TRAIN [6][6450/7762]	Time 0.178 (0.327)	Data 1.02e-04 (1.53e-04)	Tok/s 29166 (43059)	Loss/tok 2.5370 (3.1239)	LR 1.250e-04
0: TRAIN [6][6460/7762]	Time 0.174 (0.327)	Data 9.99e-05 (1.53e-04)	Tok/s 29805 (43056)	Loss/tok 2.4414 (3.1238)	LR 1.250e-04
0: TRAIN [6][6470/7762]	Time 0.259 (0.327)	Data 1.11e-04 (1.52e-04)	Tok/s 39896 (43055)	Loss/tok 2.9993 (3.1237)	LR 1.250e-04
0: TRAIN [6][6480/7762]	Time 0.258 (0.327)	Data 9.82e-05 (1.52e-04)	Tok/s 40387 (43054)	Loss/tok 2.9674 (3.1238)	LR 1.250e-04
0: TRAIN [6][6490/7762]	Time 0.261 (0.327)	Data 1.13e-04 (1.52e-04)	Tok/s 39914 (43050)	Loss/tok 2.8528 (3.1238)	LR 1.250e-04
0: TRAIN [6][6500/7762]	Time 0.463 (0.327)	Data 1.00e-04 (1.52e-04)	Tok/s 51043 (43049)	Loss/tok 3.1870 (3.1237)	LR 1.250e-04
0: TRAIN [6][6510/7762]	Time 0.463 (0.327)	Data 1.21e-04 (1.52e-04)	Tok/s 50271 (43050)	Loss/tok 3.2444 (3.1238)	LR 1.250e-04
0: TRAIN [6][6520/7762]	Time 0.263 (0.327)	Data 1.10e-04 (1.52e-04)	Tok/s 39240 (43049)	Loss/tok 2.8124 (3.1240)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][6530/7762]	Time 0.363 (0.327)	Data 9.80e-05 (1.52e-04)	Tok/s 45847 (43051)	Loss/tok 3.1649 (3.1242)	LR 1.250e-04
0: TRAIN [6][6540/7762]	Time 0.355 (0.327)	Data 1.03e-04 (1.52e-04)	Tok/s 47861 (43053)	Loss/tok 3.1177 (3.1242)	LR 1.250e-04
0: TRAIN [6][6550/7762]	Time 0.266 (0.327)	Data 1.17e-04 (1.52e-04)	Tok/s 39333 (43051)	Loss/tok 2.9698 (3.1241)	LR 1.250e-04
0: TRAIN [6][6560/7762]	Time 0.170 (0.327)	Data 1.30e-04 (1.52e-04)	Tok/s 30733 (43042)	Loss/tok 2.4918 (3.1239)	LR 1.250e-04
0: TRAIN [6][6570/7762]	Time 0.457 (0.327)	Data 1.06e-04 (1.52e-04)	Tok/s 51276 (43046)	Loss/tok 3.2031 (3.1240)	LR 1.250e-04
0: TRAIN [6][6580/7762]	Time 0.177 (0.327)	Data 1.12e-04 (1.52e-04)	Tok/s 30032 (43045)	Loss/tok 2.6296 (3.1240)	LR 1.250e-04
0: TRAIN [6][6590/7762]	Time 0.259 (0.327)	Data 1.04e-04 (1.52e-04)	Tok/s 39494 (43045)	Loss/tok 2.9182 (3.1240)	LR 1.250e-04
0: TRAIN [6][6600/7762]	Time 0.265 (0.327)	Data 9.68e-05 (1.52e-04)	Tok/s 39201 (43040)	Loss/tok 2.9573 (3.1238)	LR 1.250e-04
0: TRAIN [6][6610/7762]	Time 0.264 (0.327)	Data 1.28e-04 (1.51e-04)	Tok/s 39485 (43038)	Loss/tok 3.0753 (3.1238)	LR 1.250e-04
0: TRAIN [6][6620/7762]	Time 0.173 (0.327)	Data 1.19e-04 (1.51e-04)	Tok/s 30311 (43036)	Loss/tok 2.4567 (3.1238)	LR 1.250e-04
0: TRAIN [6][6630/7762]	Time 0.358 (0.327)	Data 1.04e-04 (1.51e-04)	Tok/s 48010 (43039)	Loss/tok 3.0253 (3.1238)	LR 1.250e-04
0: TRAIN [6][6640/7762]	Time 0.578 (0.327)	Data 1.37e-04 (1.51e-04)	Tok/s 52206 (43039)	Loss/tok 3.3990 (3.1238)	LR 1.250e-04
0: TRAIN [6][6650/7762]	Time 0.368 (0.327)	Data 1.01e-04 (1.51e-04)	Tok/s 45802 (43044)	Loss/tok 3.2085 (3.1239)	LR 1.250e-04
0: TRAIN [6][6660/7762]	Time 0.464 (0.327)	Data 1.22e-04 (1.51e-04)	Tok/s 50255 (43045)	Loss/tok 3.2128 (3.1239)	LR 1.250e-04
0: TRAIN [6][6670/7762]	Time 0.176 (0.327)	Data 1.01e-04 (1.51e-04)	Tok/s 29927 (43040)	Loss/tok 2.5123 (3.1238)	LR 1.250e-04
0: TRAIN [6][6680/7762]	Time 0.264 (0.327)	Data 9.78e-05 (1.51e-04)	Tok/s 38865 (43044)	Loss/tok 2.8531 (3.1240)	LR 1.250e-04
0: TRAIN [6][6690/7762]	Time 0.343 (0.327)	Data 1.05e-04 (1.51e-04)	Tok/s 48622 (43044)	Loss/tok 3.1336 (3.1241)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][6700/7762]	Time 0.265 (0.327)	Data 1.03e-04 (1.51e-04)	Tok/s 38228 (43045)	Loss/tok 2.9207 (3.1243)	LR 1.250e-04
0: TRAIN [6][6710/7762]	Time 0.265 (0.327)	Data 9.68e-05 (1.51e-04)	Tok/s 38918 (43040)	Loss/tok 2.9143 (3.1241)	LR 1.250e-04
0: TRAIN [6][6720/7762]	Time 0.557 (0.327)	Data 1.01e-04 (1.51e-04)	Tok/s 53379 (43044)	Loss/tok 3.4125 (3.1242)	LR 1.250e-04
0: TRAIN [6][6730/7762]	Time 0.357 (0.327)	Data 1.03e-04 (1.51e-04)	Tok/s 47626 (43044)	Loss/tok 3.0431 (3.1242)	LR 1.250e-04
0: TRAIN [6][6740/7762]	Time 0.457 (0.327)	Data 9.39e-05 (1.51e-04)	Tok/s 50739 (43041)	Loss/tok 3.3909 (3.1241)	LR 1.250e-04
0: TRAIN [6][6750/7762]	Time 0.463 (0.327)	Data 1.00e-04 (1.50e-04)	Tok/s 50319 (43047)	Loss/tok 3.3571 (3.1243)	LR 1.250e-04
0: TRAIN [6][6760/7762]	Time 0.360 (0.327)	Data 9.89e-05 (1.50e-04)	Tok/s 46681 (43043)	Loss/tok 3.1043 (3.1242)	LR 1.250e-04
0: TRAIN [6][6770/7762]	Time 0.458 (0.327)	Data 1.16e-04 (1.50e-04)	Tok/s 50553 (43044)	Loss/tok 3.4359 (3.1244)	LR 1.250e-04
0: TRAIN [6][6780/7762]	Time 0.173 (0.327)	Data 1.19e-04 (1.50e-04)	Tok/s 30422 (43042)	Loss/tok 2.5193 (3.1244)	LR 1.250e-04
0: TRAIN [6][6790/7762]	Time 0.175 (0.327)	Data 9.99e-05 (1.50e-04)	Tok/s 29938 (43045)	Loss/tok 2.5654 (3.1244)	LR 1.250e-04
0: TRAIN [6][6800/7762]	Time 0.592 (0.327)	Data 9.94e-05 (1.50e-04)	Tok/s 50099 (43044)	Loss/tok 3.4480 (3.1244)	LR 1.250e-04
0: TRAIN [6][6810/7762]	Time 0.268 (0.327)	Data 1.18e-04 (1.50e-04)	Tok/s 37954 (43047)	Loss/tok 2.8798 (3.1244)	LR 1.250e-04
0: TRAIN [6][6820/7762]	Time 0.344 (0.327)	Data 9.49e-05 (1.50e-04)	Tok/s 48125 (43042)	Loss/tok 3.1849 (3.1243)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][6830/7762]	Time 0.266 (0.327)	Data 1.04e-04 (1.50e-04)	Tok/s 38463 (43039)	Loss/tok 2.9524 (3.1242)	LR 1.250e-04
0: TRAIN [6][6840/7762]	Time 0.266 (0.327)	Data 9.89e-05 (1.50e-04)	Tok/s 38184 (43040)	Loss/tok 2.8857 (3.1243)	LR 1.250e-04
0: TRAIN [6][6850/7762]	Time 0.364 (0.327)	Data 9.94e-05 (1.50e-04)	Tok/s 46255 (43042)	Loss/tok 3.1278 (3.1244)	LR 1.250e-04
0: TRAIN [6][6860/7762]	Time 0.265 (0.327)	Data 1.02e-04 (1.50e-04)	Tok/s 38079 (43039)	Loss/tok 2.9021 (3.1242)	LR 1.250e-04
0: TRAIN [6][6870/7762]	Time 0.366 (0.327)	Data 9.80e-05 (1.50e-04)	Tok/s 45144 (43040)	Loss/tok 3.1437 (3.1242)	LR 1.250e-04
0: TRAIN [6][6880/7762]	Time 0.255 (0.327)	Data 1.03e-04 (1.50e-04)	Tok/s 40558 (43039)	Loss/tok 2.9281 (3.1241)	LR 1.250e-04
0: TRAIN [6][6890/7762]	Time 0.175 (0.327)	Data 9.85e-05 (1.50e-04)	Tok/s 30242 (43035)	Loss/tok 2.6035 (3.1240)	LR 1.250e-04
0: TRAIN [6][6900/7762]	Time 0.267 (0.327)	Data 1.02e-04 (1.49e-04)	Tok/s 38282 (43042)	Loss/tok 2.9710 (3.1241)	LR 1.250e-04
0: TRAIN [6][6910/7762]	Time 0.362 (0.327)	Data 9.78e-05 (1.49e-04)	Tok/s 46237 (43044)	Loss/tok 3.2308 (3.1242)	LR 1.250e-04
0: TRAIN [6][6920/7762]	Time 0.465 (0.327)	Data 9.82e-05 (1.49e-04)	Tok/s 50579 (43040)	Loss/tok 3.2370 (3.1241)	LR 1.250e-04
0: TRAIN [6][6930/7762]	Time 0.260 (0.327)	Data 1.06e-04 (1.49e-04)	Tok/s 39430 (43040)	Loss/tok 2.9558 (3.1240)	LR 1.250e-04
0: TRAIN [6][6940/7762]	Time 0.366 (0.327)	Data 1.00e-04 (1.49e-04)	Tok/s 45251 (43034)	Loss/tok 3.1054 (3.1238)	LR 1.250e-04
0: TRAIN [6][6950/7762]	Time 0.172 (0.327)	Data 1.03e-04 (1.49e-04)	Tok/s 31232 (43028)	Loss/tok 2.5212 (3.1237)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][6960/7762]	Time 0.252 (0.327)	Data 9.94e-05 (1.49e-04)	Tok/s 40882 (43030)	Loss/tok 2.9072 (3.1238)	LR 1.250e-04
0: TRAIN [6][6970/7762]	Time 0.269 (0.327)	Data 1.04e-04 (1.49e-04)	Tok/s 38285 (43029)	Loss/tok 2.8362 (3.1238)	LR 1.250e-04
0: TRAIN [6][6980/7762]	Time 0.179 (0.327)	Data 1.02e-04 (1.49e-04)	Tok/s 29597 (43029)	Loss/tok 2.5208 (3.1238)	LR 1.250e-04
0: TRAIN [6][6990/7762]	Time 0.340 (0.327)	Data 1.11e-04 (1.49e-04)	Tok/s 48070 (43026)	Loss/tok 3.2124 (3.1237)	LR 1.250e-04
0: TRAIN [6][7000/7762]	Time 0.368 (0.327)	Data 1.02e-04 (1.49e-04)	Tok/s 45966 (43030)	Loss/tok 3.1414 (3.1239)	LR 1.250e-04
0: TRAIN [6][7010/7762]	Time 0.369 (0.327)	Data 1.15e-04 (1.49e-04)	Tok/s 45452 (43032)	Loss/tok 3.2366 (3.1239)	LR 1.250e-04
0: TRAIN [6][7020/7762]	Time 0.270 (0.327)	Data 9.58e-05 (1.49e-04)	Tok/s 37922 (43028)	Loss/tok 2.7903 (3.1239)	LR 1.250e-04
0: TRAIN [6][7030/7762]	Time 0.263 (0.327)	Data 9.78e-05 (1.49e-04)	Tok/s 38542 (43023)	Loss/tok 2.8934 (3.1237)	LR 1.250e-04
0: TRAIN [6][7040/7762]	Time 0.357 (0.327)	Data 1.10e-04 (1.49e-04)	Tok/s 46882 (43024)	Loss/tok 3.0640 (3.1235)	LR 1.250e-04
0: TRAIN [6][7050/7762]	Time 0.265 (0.327)	Data 1.05e-04 (1.48e-04)	Tok/s 39141 (43022)	Loss/tok 2.8830 (3.1234)	LR 1.250e-04
0: TRAIN [6][7060/7762]	Time 0.458 (0.326)	Data 9.97e-05 (1.48e-04)	Tok/s 50720 (43021)	Loss/tok 3.3556 (3.1234)	LR 1.250e-04
0: TRAIN [6][7070/7762]	Time 0.556 (0.327)	Data 1.15e-04 (1.48e-04)	Tok/s 53500 (43024)	Loss/tok 3.4454 (3.1235)	LR 1.250e-04
0: TRAIN [6][7080/7762]	Time 0.364 (0.327)	Data 1.02e-04 (1.48e-04)	Tok/s 46823 (43024)	Loss/tok 3.0876 (3.1235)	LR 1.250e-04
0: TRAIN [6][7090/7762]	Time 0.261 (0.326)	Data 1.02e-04 (1.48e-04)	Tok/s 38905 (43023)	Loss/tok 2.8619 (3.1233)	LR 1.250e-04
0: TRAIN [6][7100/7762]	Time 0.268 (0.326)	Data 1.00e-04 (1.48e-04)	Tok/s 37984 (43021)	Loss/tok 2.9362 (3.1233)	LR 1.250e-04
0: TRAIN [6][7110/7762]	Time 0.264 (0.326)	Data 1.03e-04 (1.48e-04)	Tok/s 39069 (43022)	Loss/tok 2.9722 (3.1233)	LR 1.250e-04
0: TRAIN [6][7120/7762]	Time 0.360 (0.326)	Data 1.06e-04 (1.48e-04)	Tok/s 46833 (43019)	Loss/tok 3.0876 (3.1232)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][7130/7762]	Time 0.169 (0.326)	Data 9.99e-05 (1.48e-04)	Tok/s 31901 (43019)	Loss/tok 2.6472 (3.1232)	LR 1.250e-04
0: TRAIN [6][7140/7762]	Time 0.263 (0.326)	Data 1.19e-04 (1.48e-04)	Tok/s 39424 (43017)	Loss/tok 2.8076 (3.1231)	LR 1.250e-04
0: TRAIN [6][7150/7762]	Time 0.363 (0.326)	Data 1.03e-04 (1.48e-04)	Tok/s 46324 (43016)	Loss/tok 3.0345 (3.1231)	LR 1.250e-04
0: TRAIN [6][7160/7762]	Time 0.460 (0.326)	Data 1.09e-04 (1.48e-04)	Tok/s 50469 (43020)	Loss/tok 3.2117 (3.1232)	LR 1.250e-04
0: TRAIN [6][7170/7762]	Time 0.266 (0.326)	Data 1.01e-04 (1.48e-04)	Tok/s 39179 (43020)	Loss/tok 2.9934 (3.1231)	LR 1.250e-04
0: TRAIN [6][7180/7762]	Time 0.356 (0.326)	Data 1.18e-04 (1.48e-04)	Tok/s 46828 (43020)	Loss/tok 3.1314 (3.1232)	LR 1.250e-04
0: TRAIN [6][7190/7762]	Time 0.265 (0.326)	Data 1.17e-04 (1.48e-04)	Tok/s 39663 (43018)	Loss/tok 2.9532 (3.1231)	LR 1.250e-04
0: TRAIN [6][7200/7762]	Time 0.175 (0.326)	Data 1.14e-04 (1.48e-04)	Tok/s 30427 (43015)	Loss/tok 2.4697 (3.1230)	LR 1.250e-04
0: TRAIN [6][7210/7762]	Time 0.343 (0.326)	Data 1.18e-04 (1.48e-04)	Tok/s 49002 (43015)	Loss/tok 3.0240 (3.1230)	LR 1.250e-04
0: TRAIN [6][7220/7762]	Time 0.363 (0.326)	Data 1.06e-04 (1.47e-04)	Tok/s 46292 (43018)	Loss/tok 3.1218 (3.1231)	LR 1.250e-04
0: TRAIN [6][7230/7762]	Time 0.460 (0.326)	Data 1.04e-04 (1.47e-04)	Tok/s 51283 (43020)	Loss/tok 3.3375 (3.1231)	LR 1.250e-04
0: TRAIN [6][7240/7762]	Time 0.364 (0.326)	Data 1.74e-04 (1.47e-04)	Tok/s 46546 (43022)	Loss/tok 3.1684 (3.1232)	LR 1.250e-04
0: TRAIN [6][7250/7762]	Time 0.463 (0.326)	Data 1.05e-04 (1.47e-04)	Tok/s 50278 (43021)	Loss/tok 3.2667 (3.1232)	LR 1.250e-04
0: TRAIN [6][7260/7762]	Time 0.460 (0.326)	Data 1.03e-04 (1.47e-04)	Tok/s 50538 (43023)	Loss/tok 3.2936 (3.1232)	LR 1.250e-04
0: TRAIN [6][7270/7762]	Time 0.360 (0.326)	Data 1.02e-04 (1.47e-04)	Tok/s 46158 (43024)	Loss/tok 3.1011 (3.1231)	LR 1.250e-04
0: TRAIN [6][7280/7762]	Time 0.174 (0.326)	Data 1.06e-04 (1.47e-04)	Tok/s 30070 (43021)	Loss/tok 2.5247 (3.1230)	LR 1.250e-04
0: TRAIN [6][7290/7762]	Time 0.264 (0.326)	Data 1.02e-04 (1.47e-04)	Tok/s 38377 (43022)	Loss/tok 2.9707 (3.1230)	LR 1.250e-04
0: TRAIN [6][7300/7762]	Time 0.364 (0.326)	Data 1.21e-04 (1.47e-04)	Tok/s 45616 (43025)	Loss/tok 3.1217 (3.1229)	LR 1.250e-04
0: TRAIN [6][7310/7762]	Time 0.461 (0.326)	Data 1.05e-04 (1.47e-04)	Tok/s 50050 (43027)	Loss/tok 3.3254 (3.1229)	LR 1.250e-04
0: TRAIN [6][7320/7762]	Time 0.355 (0.326)	Data 1.01e-04 (1.47e-04)	Tok/s 47593 (43027)	Loss/tok 3.1421 (3.1230)	LR 1.250e-04
0: TRAIN [6][7330/7762]	Time 0.459 (0.326)	Data 9.92e-05 (1.47e-04)	Tok/s 50716 (43025)	Loss/tok 3.3038 (3.1229)	LR 1.250e-04
0: TRAIN [6][7340/7762]	Time 0.453 (0.326)	Data 1.03e-04 (1.47e-04)	Tok/s 51411 (43031)	Loss/tok 3.4334 (3.1232)	LR 1.250e-04
0: TRAIN [6][7350/7762]	Time 0.354 (0.326)	Data 9.85e-05 (1.47e-04)	Tok/s 47541 (43031)	Loss/tok 3.0590 (3.1232)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][7360/7762]	Time 0.266 (0.326)	Data 1.01e-04 (1.47e-04)	Tok/s 38511 (43030)	Loss/tok 2.8856 (3.1233)	LR 1.250e-04
0: TRAIN [6][7370/7762]	Time 0.258 (0.326)	Data 1.06e-04 (1.47e-04)	Tok/s 39691 (43028)	Loss/tok 2.9386 (3.1233)	LR 1.250e-04
0: TRAIN [6][7380/7762]	Time 0.361 (0.326)	Data 1.04e-04 (1.47e-04)	Tok/s 46972 (43031)	Loss/tok 3.1117 (3.1234)	LR 1.250e-04
0: TRAIN [6][7390/7762]	Time 0.262 (0.326)	Data 9.92e-05 (1.47e-04)	Tok/s 39068 (43031)	Loss/tok 2.9529 (3.1233)	LR 1.250e-04
0: TRAIN [6][7400/7762]	Time 0.267 (0.326)	Data 1.02e-04 (1.46e-04)	Tok/s 38669 (43030)	Loss/tok 2.9265 (3.1233)	LR 1.250e-04
0: TRAIN [6][7410/7762]	Time 0.268 (0.326)	Data 1.01e-04 (1.46e-04)	Tok/s 38336 (43030)	Loss/tok 2.7763 (3.1232)	LR 1.250e-04
0: TRAIN [6][7420/7762]	Time 0.452 (0.326)	Data 1.02e-04 (1.46e-04)	Tok/s 51098 (43034)	Loss/tok 3.3755 (3.1234)	LR 1.250e-04
0: TRAIN [6][7430/7762]	Time 0.263 (0.327)	Data 1.04e-04 (1.46e-04)	Tok/s 39528 (43038)	Loss/tok 2.8364 (3.1235)	LR 1.250e-04
0: TRAIN [6][7440/7762]	Time 0.345 (0.327)	Data 9.97e-05 (1.46e-04)	Tok/s 48427 (43039)	Loss/tok 3.1200 (3.1235)	LR 1.250e-04
0: TRAIN [6][7450/7762]	Time 0.352 (0.327)	Data 1.03e-04 (1.46e-04)	Tok/s 47526 (43038)	Loss/tok 2.9571 (3.1237)	LR 1.250e-04
0: TRAIN [6][7460/7762]	Time 0.256 (0.327)	Data 1.01e-04 (1.46e-04)	Tok/s 39970 (43038)	Loss/tok 2.9240 (3.1237)	LR 1.250e-04
0: TRAIN [6][7470/7762]	Time 0.262 (0.327)	Data 1.02e-04 (1.46e-04)	Tok/s 40204 (43037)	Loss/tok 2.8589 (3.1237)	LR 1.250e-04
0: TRAIN [6][7480/7762]	Time 0.261 (0.327)	Data 1.01e-04 (1.46e-04)	Tok/s 39174 (43034)	Loss/tok 2.8898 (3.1236)	LR 1.250e-04
0: TRAIN [6][7490/7762]	Time 0.265 (0.326)	Data 1.03e-04 (1.46e-04)	Tok/s 38793 (43031)	Loss/tok 2.9845 (3.1235)	LR 1.250e-04
0: TRAIN [6][7500/7762]	Time 0.178 (0.326)	Data 1.00e-04 (1.46e-04)	Tok/s 29376 (43027)	Loss/tok 2.6069 (3.1235)	LR 1.250e-04
0: TRAIN [6][7510/7762]	Time 0.176 (0.326)	Data 1.02e-04 (1.46e-04)	Tok/s 30326 (43027)	Loss/tok 2.5802 (3.1235)	LR 1.250e-04
0: TRAIN [6][7520/7762]	Time 0.363 (0.326)	Data 9.92e-05 (1.46e-04)	Tok/s 45886 (43030)	Loss/tok 3.0876 (3.1235)	LR 1.250e-04
0: TRAIN [6][7530/7762]	Time 0.263 (0.326)	Data 1.04e-04 (1.46e-04)	Tok/s 39657 (43033)	Loss/tok 2.9306 (3.1237)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][7540/7762]	Time 0.167 (0.326)	Data 1.07e-04 (1.46e-04)	Tok/s 32008 (43031)	Loss/tok 2.5041 (3.1237)	LR 1.250e-04
0: TRAIN [6][7550/7762]	Time 0.260 (0.326)	Data 9.82e-05 (1.46e-04)	Tok/s 38971 (43028)	Loss/tok 2.8601 (3.1236)	LR 1.250e-04
0: TRAIN [6][7560/7762]	Time 0.262 (0.326)	Data 1.04e-04 (1.46e-04)	Tok/s 39437 (43022)	Loss/tok 2.9289 (3.1235)	LR 1.250e-04
0: TRAIN [6][7570/7762]	Time 0.343 (0.326)	Data 1.01e-04 (1.46e-04)	Tok/s 48299 (43023)	Loss/tok 3.1618 (3.1235)	LR 1.250e-04
0: TRAIN [6][7580/7762]	Time 0.174 (0.326)	Data 1.03e-04 (1.45e-04)	Tok/s 30615 (43024)	Loss/tok 2.6881 (3.1235)	LR 1.250e-04
0: TRAIN [6][7590/7762]	Time 0.258 (0.326)	Data 1.01e-04 (1.45e-04)	Tok/s 39947 (43026)	Loss/tok 2.9571 (3.1238)	LR 1.250e-04
0: TRAIN [6][7600/7762]	Time 0.444 (0.326)	Data 1.06e-04 (1.45e-04)	Tok/s 51965 (43026)	Loss/tok 3.3538 (3.1238)	LR 1.250e-04
0: TRAIN [6][7610/7762]	Time 0.261 (0.326)	Data 1.02e-04 (1.45e-04)	Tok/s 39525 (43022)	Loss/tok 3.0165 (3.1236)	LR 1.250e-04
0: TRAIN [6][7620/7762]	Time 0.259 (0.326)	Data 1.01e-04 (1.45e-04)	Tok/s 39656 (43020)	Loss/tok 3.0601 (3.1236)	LR 1.250e-04
0: TRAIN [6][7630/7762]	Time 0.259 (0.326)	Data 1.05e-04 (1.45e-04)	Tok/s 39654 (43018)	Loss/tok 2.9433 (3.1235)	LR 1.250e-04
0: TRAIN [6][7640/7762]	Time 0.262 (0.326)	Data 1.10e-04 (1.45e-04)	Tok/s 40202 (43017)	Loss/tok 2.8939 (3.1235)	LR 1.250e-04
0: TRAIN [6][7650/7762]	Time 0.355 (0.326)	Data 1.04e-04 (1.45e-04)	Tok/s 47376 (43023)	Loss/tok 3.0082 (3.1235)	LR 1.250e-04
0: TRAIN [6][7660/7762]	Time 0.581 (0.326)	Data 1.19e-04 (1.45e-04)	Tok/s 51480 (43025)	Loss/tok 3.4763 (3.1237)	LR 1.250e-04
0: TRAIN [6][7670/7762]	Time 0.450 (0.326)	Data 1.14e-04 (1.45e-04)	Tok/s 51873 (43027)	Loss/tok 3.1912 (3.1237)	LR 1.250e-04
0: TRAIN [6][7680/7762]	Time 0.364 (0.326)	Data 9.75e-05 (1.45e-04)	Tok/s 45960 (43024)	Loss/tok 2.9937 (3.1235)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][7690/7762]	Time 0.176 (0.326)	Data 1.01e-04 (1.45e-04)	Tok/s 30493 (43027)	Loss/tok 2.6302 (3.1237)	LR 1.250e-04
0: TRAIN [6][7700/7762]	Time 0.260 (0.326)	Data 1.20e-04 (1.45e-04)	Tok/s 40373 (43027)	Loss/tok 2.8232 (3.1237)	LR 1.250e-04
0: TRAIN [6][7710/7762]	Time 0.465 (0.326)	Data 1.18e-04 (1.45e-04)	Tok/s 50932 (43029)	Loss/tok 3.2843 (3.1238)	LR 1.250e-04
0: TRAIN [6][7720/7762]	Time 0.363 (0.327)	Data 1.23e-04 (1.45e-04)	Tok/s 46539 (43035)	Loss/tok 3.1517 (3.1239)	LR 1.250e-04
0: TRAIN [6][7730/7762]	Time 0.259 (0.327)	Data 1.15e-04 (1.45e-04)	Tok/s 39169 (43033)	Loss/tok 2.9216 (3.1239)	LR 1.250e-04
0: TRAIN [6][7740/7762]	Time 0.175 (0.326)	Data 9.51e-05 (1.45e-04)	Tok/s 30440 (43029)	Loss/tok 2.4659 (3.1238)	LR 1.250e-04
0: TRAIN [6][7750/7762]	Time 0.268 (0.326)	Data 1.07e-04 (1.45e-04)	Tok/s 38310 (43029)	Loss/tok 2.9586 (3.1238)	LR 1.250e-04
0: TRAIN [6][7760/7762]	Time 0.268 (0.327)	Data 1.50e-02 (1.46e-04)	Tok/s 38669 (43033)	Loss/tok 2.8939 (3.1240)	LR 1.250e-04
:::MLL 1573763619.994 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 524}}
:::MLL 1573763619.995 eval_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [6][0/12]	Time 0.901 (0.901)	Decoder iters 149.0 (149.0)	Tok/s 18407 (18407)
0: TEST [6][10/12]	Time 0.117 (0.303)	Decoder iters 25.0 (55.3)	Tok/s 31831 (29073)
0: Running moses detokenizer
0: BLEU(score=23.41870221940592, counts=[36936, 18321, 10332, 6101], totals=[65961, 62958, 59956, 56959], precisions=[55.99672533769955, 29.100352616029735, 17.232637267329373, 10.71121332888569], bp=1.0, sys_len=65961, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1573763625.709 eval_accuracy: {"value": 23.42, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 535}}
:::MLL 1573763625.709 eval_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 6	Training Loss: 3.1238	Test BLEU: 23.42
0: Performance: Epoch: 6	Training: 86053 Tok/s
0: Finished epoch 6
:::MLL 1573763625.710 block_stop: {"value": null, "metadata": {"first_epoch_num": 7, "file": "train.py", "lineno": 557}}
:::MLL 1573763625.710 block_start: {"value": null, "metadata": {"first_epoch_num": 8, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1573763625.710 epoch_start: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 514}}
0: Starting epoch 7
0: Executing preallocation
0: Sampler for epoch 7 uses seed 3232097937
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [7][0/7762]	Time 0.474 (0.474)	Data 3.12e-01 (3.12e-01)	Tok/s 11210 (11210)	Loss/tok 2.5837 (2.5837)	LR 1.250e-04
0: TRAIN [7][10/7762]	Time 0.260 (0.376)	Data 1.04e-04 (2.84e-02)	Tok/s 39468 (42682)	Loss/tok 2.7364 (3.1871)	LR 1.250e-04
0: TRAIN [7][20/7762]	Time 0.354 (0.353)	Data 1.05e-04 (1.50e-02)	Tok/s 47341 (43436)	Loss/tok 3.1192 (3.1448)	LR 1.250e-04
0: TRAIN [7][30/7762]	Time 0.444 (0.347)	Data 9.82e-05 (1.02e-02)	Tok/s 52826 (44037)	Loss/tok 3.3143 (3.1359)	LR 1.250e-04
0: TRAIN [7][40/7762]	Time 0.444 (0.339)	Data 1.06e-04 (7.71e-03)	Tok/s 52973 (44199)	Loss/tok 3.1373 (3.1162)	LR 1.250e-04
0: TRAIN [7][50/7762]	Time 0.264 (0.335)	Data 9.89e-05 (6.22e-03)	Tok/s 38635 (44162)	Loss/tok 2.9052 (3.1040)	LR 1.250e-04
0: TRAIN [7][60/7762]	Time 0.341 (0.336)	Data 1.03e-04 (5.21e-03)	Tok/s 50236 (44657)	Loss/tok 3.0677 (3.0988)	LR 1.250e-04
0: TRAIN [7][70/7762]	Time 0.571 (0.341)	Data 9.92e-05 (4.49e-03)	Tok/s 51983 (44680)	Loss/tok 3.4317 (3.1144)	LR 1.250e-04
0: TRAIN [7][80/7762]	Time 0.465 (0.347)	Data 1.04e-04 (3.95e-03)	Tok/s 50193 (44980)	Loss/tok 3.3297 (3.1268)	LR 1.250e-04
0: TRAIN [7][90/7762]	Time 0.359 (0.343)	Data 9.75e-05 (3.53e-03)	Tok/s 46867 (44717)	Loss/tok 3.0875 (3.1191)	LR 1.250e-04
0: TRAIN [7][100/7762]	Time 0.341 (0.339)	Data 1.00e-04 (3.19e-03)	Tok/s 48926 (44506)	Loss/tok 3.1831 (3.1092)	LR 1.250e-04
0: TRAIN [7][110/7762]	Time 0.174 (0.334)	Data 9.56e-05 (2.91e-03)	Tok/s 29882 (44127)	Loss/tok 2.4354 (3.1005)	LR 1.250e-04
0: TRAIN [7][120/7762]	Time 0.365 (0.334)	Data 1.03e-04 (2.68e-03)	Tok/s 45779 (44012)	Loss/tok 3.1932 (3.1041)	LR 1.250e-04
0: TRAIN [7][130/7762]	Time 0.178 (0.336)	Data 1.02e-04 (2.48e-03)	Tok/s 29632 (44037)	Loss/tok 2.5773 (3.1079)	LR 1.250e-04
0: TRAIN [7][140/7762]	Time 0.257 (0.334)	Data 1.02e-04 (2.31e-03)	Tok/s 40646 (43951)	Loss/tok 2.9197 (3.1029)	LR 1.250e-04
0: TRAIN [7][150/7762]	Time 0.264 (0.337)	Data 1.06e-04 (2.17e-03)	Tok/s 38689 (44052)	Loss/tok 2.8101 (3.1108)	LR 1.250e-04
0: TRAIN [7][160/7762]	Time 0.460 (0.337)	Data 1.00e-04 (2.04e-03)	Tok/s 50583 (43994)	Loss/tok 3.2927 (3.1096)	LR 1.250e-04
0: TRAIN [7][170/7762]	Time 0.264 (0.338)	Data 1.03e-04 (1.93e-03)	Tok/s 38958 (44003)	Loss/tok 2.8354 (3.1131)	LR 1.250e-04
0: TRAIN [7][180/7762]	Time 0.353 (0.339)	Data 1.33e-04 (1.83e-03)	Tok/s 47671 (44044)	Loss/tok 3.0175 (3.1127)	LR 1.250e-04
0: TRAIN [7][190/7762]	Time 0.344 (0.338)	Data 9.97e-05 (1.74e-03)	Tok/s 48427 (44016)	Loss/tok 3.1645 (3.1098)	LR 1.250e-04
0: TRAIN [7][200/7762]	Time 0.254 (0.336)	Data 1.14e-04 (1.65e-03)	Tok/s 39901 (43937)	Loss/tok 2.9424 (3.1054)	LR 1.250e-04
0: TRAIN [7][210/7762]	Time 0.259 (0.334)	Data 1.02e-04 (1.58e-03)	Tok/s 40308 (43791)	Loss/tok 2.9237 (3.1026)	LR 1.250e-04
0: TRAIN [7][220/7762]	Time 0.263 (0.333)	Data 1.02e-04 (1.51e-03)	Tok/s 38612 (43705)	Loss/tok 3.0270 (3.1013)	LR 1.250e-04
0: TRAIN [7][230/7762]	Time 0.463 (0.332)	Data 1.01e-04 (1.45e-03)	Tok/s 51415 (43569)	Loss/tok 3.2366 (3.0979)	LR 1.250e-04
0: TRAIN [7][240/7762]	Time 0.262 (0.334)	Data 9.92e-05 (1.40e-03)	Tok/s 39127 (43663)	Loss/tok 2.9106 (3.1052)	LR 1.250e-04
0: TRAIN [7][250/7762]	Time 0.265 (0.333)	Data 1.03e-04 (1.35e-03)	Tok/s 38380 (43590)	Loss/tok 2.9415 (3.1043)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][260/7762]	Time 0.354 (0.333)	Data 1.01e-04 (1.30e-03)	Tok/s 47538 (43624)	Loss/tok 3.0689 (3.1049)	LR 1.250e-04
0: TRAIN [7][270/7762]	Time 0.366 (0.335)	Data 1.05e-04 (1.25e-03)	Tok/s 46159 (43668)	Loss/tok 3.1792 (3.1085)	LR 1.250e-04
0: TRAIN [7][280/7762]	Time 0.365 (0.337)	Data 1.03e-04 (1.21e-03)	Tok/s 46687 (43715)	Loss/tok 3.1512 (3.1120)	LR 1.250e-04
0: TRAIN [7][290/7762]	Time 0.260 (0.335)	Data 9.97e-05 (1.17e-03)	Tok/s 39440 (43634)	Loss/tok 2.8052 (3.1075)	LR 1.250e-04
0: TRAIN [7][300/7762]	Time 0.169 (0.333)	Data 1.00e-04 (1.14e-03)	Tok/s 31566 (43522)	Loss/tok 2.5313 (3.1039)	LR 1.250e-04
0: TRAIN [7][310/7762]	Time 0.451 (0.334)	Data 9.97e-05 (1.11e-03)	Tok/s 52030 (43577)	Loss/tok 3.2769 (3.1072)	LR 1.250e-04
0: TRAIN [7][320/7762]	Time 0.363 (0.337)	Data 1.19e-04 (1.07e-03)	Tok/s 46540 (43687)	Loss/tok 3.2109 (3.1136)	LR 1.250e-04
0: TRAIN [7][330/7762]	Time 0.586 (0.337)	Data 9.97e-05 (1.05e-03)	Tok/s 50810 (43707)	Loss/tok 3.4196 (3.1148)	LR 1.250e-04
0: TRAIN [7][340/7762]	Time 0.588 (0.339)	Data 1.22e-04 (1.02e-03)	Tok/s 50314 (43759)	Loss/tok 3.5380 (3.1217)	LR 1.250e-04
0: TRAIN [7][350/7762]	Time 0.264 (0.338)	Data 1.00e-04 (9.92e-04)	Tok/s 39258 (43683)	Loss/tok 2.9342 (3.1198)	LR 1.250e-04
0: TRAIN [7][360/7762]	Time 0.341 (0.337)	Data 9.51e-05 (9.67e-04)	Tok/s 48867 (43648)	Loss/tok 3.1391 (3.1193)	LR 1.250e-04
0: TRAIN [7][370/7762]	Time 0.260 (0.336)	Data 9.87e-05 (9.44e-04)	Tok/s 40722 (43607)	Loss/tok 3.0447 (3.1173)	LR 1.250e-04
0: TRAIN [7][380/7762]	Time 0.457 (0.336)	Data 9.97e-05 (9.22e-04)	Tok/s 50878 (43606)	Loss/tok 3.2831 (3.1160)	LR 1.250e-04
0: TRAIN [7][390/7762]	Time 0.265 (0.336)	Data 9.87e-05 (9.01e-04)	Tok/s 39140 (43605)	Loss/tok 2.8451 (3.1137)	LR 1.250e-04
0: TRAIN [7][400/7762]	Time 0.264 (0.336)	Data 1.02e-04 (8.81e-04)	Tok/s 38644 (43578)	Loss/tok 3.0083 (3.1141)	LR 1.250e-04
0: TRAIN [7][410/7762]	Time 0.351 (0.336)	Data 1.16e-04 (8.63e-04)	Tok/s 48441 (43616)	Loss/tok 2.9845 (3.1135)	LR 1.250e-04
0: TRAIN [7][420/7762]	Time 0.462 (0.337)	Data 1.03e-04 (8.45e-04)	Tok/s 50525 (43682)	Loss/tok 3.2235 (3.1144)	LR 1.250e-04
0: TRAIN [7][430/7762]	Time 0.354 (0.337)	Data 1.03e-04 (8.28e-04)	Tok/s 47578 (43728)	Loss/tok 3.0527 (3.1158)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][440/7762]	Time 0.265 (0.338)	Data 1.06e-04 (8.12e-04)	Tok/s 38704 (43757)	Loss/tok 2.9303 (3.1176)	LR 1.250e-04
0: TRAIN [7][450/7762]	Time 0.367 (0.338)	Data 9.61e-05 (7.96e-04)	Tok/s 46302 (43773)	Loss/tok 3.1005 (3.1182)	LR 1.250e-04
0: TRAIN [7][460/7762]	Time 0.178 (0.337)	Data 1.05e-04 (7.81e-04)	Tok/s 30100 (43718)	Loss/tok 2.4785 (3.1174)	LR 1.250e-04
0: TRAIN [7][470/7762]	Time 0.263 (0.337)	Data 1.01e-04 (7.66e-04)	Tok/s 39841 (43679)	Loss/tok 2.7862 (3.1157)	LR 1.250e-04
0: TRAIN [7][480/7762]	Time 0.261 (0.335)	Data 1.14e-04 (7.53e-04)	Tok/s 38897 (43608)	Loss/tok 2.8372 (3.1129)	LR 1.250e-04
0: TRAIN [7][490/7762]	Time 0.261 (0.334)	Data 9.85e-05 (7.40e-04)	Tok/s 38569 (43538)	Loss/tok 2.8709 (3.1114)	LR 1.250e-04
0: TRAIN [7][500/7762]	Time 0.261 (0.334)	Data 1.01e-04 (7.27e-04)	Tok/s 39297 (43502)	Loss/tok 2.9316 (3.1099)	LR 1.250e-04
0: TRAIN [7][510/7762]	Time 0.364 (0.334)	Data 1.00e-04 (7.15e-04)	Tok/s 46044 (43517)	Loss/tok 3.1570 (3.1107)	LR 1.250e-04
0: TRAIN [7][520/7762]	Time 0.570 (0.336)	Data 1.02e-04 (7.03e-04)	Tok/s 52204 (43561)	Loss/tok 3.4200 (3.1142)	LR 1.250e-04
0: TRAIN [7][530/7762]	Time 0.361 (0.336)	Data 1.05e-04 (6.92e-04)	Tok/s 47342 (43568)	Loss/tok 2.9933 (3.1155)	LR 1.250e-04
0: TRAIN [7][540/7762]	Time 0.257 (0.336)	Data 1.01e-04 (6.81e-04)	Tok/s 39667 (43588)	Loss/tok 2.9016 (3.1159)	LR 1.250e-04
0: TRAIN [7][550/7762]	Time 0.461 (0.337)	Data 1.02e-04 (6.70e-04)	Tok/s 50546 (43607)	Loss/tok 3.2201 (3.1162)	LR 1.250e-04
0: TRAIN [7][560/7762]	Time 0.351 (0.335)	Data 9.54e-05 (6.60e-04)	Tok/s 46942 (43522)	Loss/tok 3.1402 (3.1145)	LR 1.250e-04
0: TRAIN [7][570/7762]	Time 0.254 (0.335)	Data 1.11e-04 (6.51e-04)	Tok/s 40551 (43508)	Loss/tok 2.9890 (3.1137)	LR 1.250e-04
0: TRAIN [7][580/7762]	Time 0.366 (0.336)	Data 1.02e-04 (6.41e-04)	Tok/s 45901 (43522)	Loss/tok 3.0873 (3.1147)	LR 1.250e-04
0: TRAIN [7][590/7762]	Time 0.457 (0.336)	Data 9.94e-05 (6.32e-04)	Tok/s 50674 (43555)	Loss/tok 3.2471 (3.1145)	LR 1.250e-04
0: TRAIN [7][600/7762]	Time 0.464 (0.336)	Data 1.06e-04 (6.23e-04)	Tok/s 49880 (43557)	Loss/tok 3.2251 (3.1140)	LR 1.250e-04
0: TRAIN [7][610/7762]	Time 0.258 (0.335)	Data 1.04e-04 (6.15e-04)	Tok/s 39973 (43526)	Loss/tok 2.8963 (3.1130)	LR 1.250e-04
0: TRAIN [7][620/7762]	Time 0.264 (0.335)	Data 1.01e-04 (6.06e-04)	Tok/s 38698 (43514)	Loss/tok 2.9378 (3.1132)	LR 1.250e-04
0: TRAIN [7][630/7762]	Time 0.268 (0.334)	Data 1.03e-04 (5.98e-04)	Tok/s 38666 (43480)	Loss/tok 3.0025 (3.1119)	LR 1.250e-04
0: TRAIN [7][640/7762]	Time 0.342 (0.335)	Data 1.04e-04 (5.91e-04)	Tok/s 49921 (43523)	Loss/tok 3.0573 (3.1134)	LR 1.250e-04
0: TRAIN [7][650/7762]	Time 0.176 (0.335)	Data 1.17e-04 (5.83e-04)	Tok/s 30282 (43506)	Loss/tok 2.5238 (3.1123)	LR 1.250e-04
0: TRAIN [7][660/7762]	Time 0.463 (0.335)	Data 1.19e-04 (5.76e-04)	Tok/s 50399 (43512)	Loss/tok 3.3367 (3.1120)	LR 1.250e-04
0: TRAIN [7][670/7762]	Time 0.173 (0.334)	Data 1.01e-04 (5.69e-04)	Tok/s 30868 (43485)	Loss/tok 2.5642 (3.1120)	LR 1.250e-04
0: TRAIN [7][680/7762]	Time 0.350 (0.334)	Data 9.66e-05 (5.62e-04)	Tok/s 47824 (43477)	Loss/tok 3.1932 (3.1122)	LR 1.250e-04
0: TRAIN [7][690/7762]	Time 0.262 (0.334)	Data 1.07e-04 (5.56e-04)	Tok/s 39272 (43472)	Loss/tok 2.9258 (3.1120)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [7][700/7762]	Time 0.264 (0.334)	Data 1.23e-04 (5.49e-04)	Tok/s 38895 (43480)	Loss/tok 2.9872 (3.1124)	LR 1.250e-04
0: TRAIN [7][710/7762]	Time 0.462 (0.334)	Data 1.25e-04 (5.43e-04)	Tok/s 50141 (43480)	Loss/tok 3.3364 (3.1140)	LR 1.250e-04
0: TRAIN [7][720/7762]	Time 0.454 (0.334)	Data 9.87e-05 (5.37e-04)	Tok/s 51302 (43469)	Loss/tok 3.3486 (3.1136)	LR 1.250e-04
0: TRAIN [7][730/7762]	Time 0.354 (0.333)	Data 1.02e-04 (5.31e-04)	Tok/s 47497 (43449)	Loss/tok 3.1384 (3.1130)	LR 1.250e-04
0: TRAIN [7][740/7762]	Time 0.265 (0.333)	Data 9.99e-05 (5.25e-04)	Tok/s 38705 (43418)	Loss/tok 2.8840 (3.1134)	LR 1.250e-04
0: TRAIN [7][750/7762]	Time 0.348 (0.333)	Data 1.00e-04 (5.19e-04)	Tok/s 47876 (43388)	Loss/tok 3.1317 (3.1129)	LR 1.250e-04
0: TRAIN [7][760/7762]	Time 0.260 (0.333)	Data 1.02e-04 (5.14e-04)	Tok/s 41615 (43371)	Loss/tok 3.0067 (3.1133)	LR 1.250e-04
0: TRAIN [7][770/7762]	Time 0.256 (0.332)	Data 1.03e-04 (5.09e-04)	Tok/s 40330 (43356)	Loss/tok 2.9518 (3.1128)	LR 1.250e-04
0: TRAIN [7][780/7762]	Time 0.175 (0.333)	Data 1.03e-04 (5.03e-04)	Tok/s 29908 (43351)	Loss/tok 2.4830 (3.1134)	LR 1.250e-04
0: TRAIN [7][790/7762]	Time 0.582 (0.333)	Data 9.92e-05 (4.98e-04)	Tok/s 51931 (43353)	Loss/tok 3.4119 (3.1138)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][800/7762]	Time 0.363 (0.333)	Data 1.04e-04 (4.94e-04)	Tok/s 46406 (43392)	Loss/tok 3.0448 (3.1160)	LR 1.250e-04
0: TRAIN [7][810/7762]	Time 0.459 (0.333)	Data 1.21e-04 (4.89e-04)	Tok/s 51140 (43401)	Loss/tok 3.2428 (3.1158)	LR 1.250e-04
0: TRAIN [7][820/7762]	Time 0.253 (0.333)	Data 9.97e-05 (4.84e-04)	Tok/s 41144 (43374)	Loss/tok 2.9107 (3.1157)	LR 1.250e-04
0: TRAIN [7][830/7762]	Time 0.263 (0.333)	Data 9.99e-05 (4.79e-04)	Tok/s 39311 (43359)	Loss/tok 2.8696 (3.1151)	LR 1.250e-04
0: TRAIN [7][840/7762]	Time 0.361 (0.333)	Data 1.04e-04 (4.75e-04)	Tok/s 46612 (43368)	Loss/tok 3.1070 (3.1155)	LR 1.250e-04
0: TRAIN [7][850/7762]	Time 0.264 (0.332)	Data 1.03e-04 (4.70e-04)	Tok/s 40153 (43332)	Loss/tok 2.9095 (3.1139)	LR 1.250e-04
0: TRAIN [7][860/7762]	Time 0.360 (0.332)	Data 1.06e-04 (4.66e-04)	Tok/s 46038 (43355)	Loss/tok 3.0547 (3.1140)	LR 1.250e-04
0: TRAIN [7][870/7762]	Time 0.173 (0.333)	Data 1.09e-04 (4.62e-04)	Tok/s 30500 (43358)	Loss/tok 2.5502 (3.1142)	LR 1.250e-04
0: TRAIN [7][880/7762]	Time 0.354 (0.333)	Data 1.02e-04 (4.58e-04)	Tok/s 46979 (43352)	Loss/tok 3.0833 (3.1147)	LR 1.250e-04
0: TRAIN [7][890/7762]	Time 0.269 (0.333)	Data 9.58e-05 (4.54e-04)	Tok/s 38081 (43361)	Loss/tok 2.8277 (3.1139)	LR 1.250e-04
0: TRAIN [7][900/7762]	Time 0.177 (0.332)	Data 1.01e-04 (4.50e-04)	Tok/s 29669 (43337)	Loss/tok 2.5005 (3.1140)	LR 1.250e-04
0: TRAIN [7][910/7762]	Time 0.265 (0.333)	Data 9.87e-05 (4.46e-04)	Tok/s 38542 (43345)	Loss/tok 2.8400 (3.1147)	LR 1.250e-04
0: TRAIN [7][920/7762]	Time 0.260 (0.333)	Data 1.06e-04 (4.43e-04)	Tok/s 39904 (43343)	Loss/tok 2.9694 (3.1144)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][930/7762]	Time 0.578 (0.333)	Data 1.20e-04 (4.39e-04)	Tok/s 51282 (43361)	Loss/tok 3.3946 (3.1154)	LR 1.250e-04
0: TRAIN [7][940/7762]	Time 0.265 (0.333)	Data 1.01e-04 (4.35e-04)	Tok/s 39254 (43355)	Loss/tok 2.9717 (3.1149)	LR 1.250e-04
0: TRAIN [7][950/7762]	Time 0.256 (0.333)	Data 9.87e-05 (4.32e-04)	Tok/s 40285 (43316)	Loss/tok 2.9862 (3.1147)	LR 1.250e-04
0: TRAIN [7][960/7762]	Time 0.269 (0.332)	Data 1.25e-04 (4.28e-04)	Tok/s 38296 (43282)	Loss/tok 2.8889 (3.1139)	LR 1.250e-04
0: TRAIN [7][970/7762]	Time 0.264 (0.331)	Data 1.06e-04 (4.25e-04)	Tok/s 40123 (43219)	Loss/tok 2.9372 (3.1126)	LR 1.250e-04
0: TRAIN [7][980/7762]	Time 0.453 (0.331)	Data 1.28e-04 (4.22e-04)	Tok/s 51514 (43235)	Loss/tok 3.3489 (3.1127)	LR 1.250e-04
0: TRAIN [7][990/7762]	Time 0.264 (0.331)	Data 1.18e-04 (4.19e-04)	Tok/s 39198 (43200)	Loss/tok 2.9165 (3.1115)	LR 1.250e-04
0: TRAIN [7][1000/7762]	Time 0.259 (0.330)	Data 1.21e-04 (4.16e-04)	Tok/s 39712 (43180)	Loss/tok 2.9279 (3.1105)	LR 1.250e-04
0: TRAIN [7][1010/7762]	Time 0.260 (0.330)	Data 1.15e-04 (4.13e-04)	Tok/s 39562 (43165)	Loss/tok 2.7865 (3.1095)	LR 1.250e-04
0: TRAIN [7][1020/7762]	Time 0.363 (0.330)	Data 1.04e-04 (4.10e-04)	Tok/s 46762 (43202)	Loss/tok 3.0428 (3.1102)	LR 1.250e-04
0: TRAIN [7][1030/7762]	Time 0.350 (0.331)	Data 1.07e-04 (4.07e-04)	Tok/s 47389 (43207)	Loss/tok 3.1756 (3.1105)	LR 1.250e-04
0: TRAIN [7][1040/7762]	Time 0.264 (0.330)	Data 1.05e-04 (4.04e-04)	Tok/s 39164 (43203)	Loss/tok 2.8783 (3.1106)	LR 1.250e-04
0: TRAIN [7][1050/7762]	Time 0.363 (0.330)	Data 1.02e-04 (4.01e-04)	Tok/s 46210 (43179)	Loss/tok 3.0939 (3.1096)	LR 1.250e-04
0: TRAIN [7][1060/7762]	Time 0.452 (0.330)	Data 1.15e-04 (3.98e-04)	Tok/s 52002 (43214)	Loss/tok 3.2399 (3.1103)	LR 1.250e-04
0: TRAIN [7][1070/7762]	Time 0.267 (0.330)	Data 1.02e-04 (3.95e-04)	Tok/s 38567 (43220)	Loss/tok 2.8703 (3.1101)	LR 1.250e-04
0: TRAIN [7][1080/7762]	Time 0.259 (0.330)	Data 1.04e-04 (3.93e-04)	Tok/s 40340 (43216)	Loss/tok 2.7692 (3.1098)	LR 1.250e-04
0: TRAIN [7][1090/7762]	Time 0.265 (0.330)	Data 1.07e-04 (3.90e-04)	Tok/s 38328 (43196)	Loss/tok 2.9033 (3.1091)	LR 1.250e-04
0: TRAIN [7][1100/7762]	Time 0.261 (0.330)	Data 9.99e-05 (3.87e-04)	Tok/s 38560 (43180)	Loss/tok 2.9268 (3.1089)	LR 1.250e-04
0: TRAIN [7][1110/7762]	Time 0.264 (0.330)	Data 1.04e-04 (3.85e-04)	Tok/s 39385 (43177)	Loss/tok 2.9666 (3.1092)	LR 1.250e-04
0: TRAIN [7][1120/7762]	Time 0.260 (0.329)	Data 1.02e-04 (3.82e-04)	Tok/s 39858 (43161)	Loss/tok 2.8853 (3.1088)	LR 1.250e-04
0: TRAIN [7][1130/7762]	Time 0.174 (0.329)	Data 1.02e-04 (3.80e-04)	Tok/s 30842 (43150)	Loss/tok 2.5825 (3.1087)	LR 1.250e-04
0: TRAIN [7][1140/7762]	Time 0.456 (0.329)	Data 1.17e-04 (3.77e-04)	Tok/s 50926 (43157)	Loss/tok 3.2777 (3.1086)	LR 1.250e-04
0: TRAIN [7][1150/7762]	Time 0.450 (0.330)	Data 1.01e-04 (3.75e-04)	Tok/s 52368 (43198)	Loss/tok 3.2284 (3.1097)	LR 1.250e-04
0: TRAIN [7][1160/7762]	Time 0.254 (0.330)	Data 1.03e-04 (3.73e-04)	Tok/s 41261 (43209)	Loss/tok 2.8667 (3.1099)	LR 1.250e-04
0: TRAIN [7][1170/7762]	Time 0.453 (0.330)	Data 1.05e-04 (3.71e-04)	Tok/s 51216 (43215)	Loss/tok 3.2308 (3.1101)	LR 1.250e-04
0: TRAIN [7][1180/7762]	Time 0.461 (0.330)	Data 1.01e-04 (3.68e-04)	Tok/s 50324 (43214)	Loss/tok 3.3793 (3.1102)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [7][1190/7762]	Time 0.366 (0.330)	Data 1.10e-04 (3.66e-04)	Tok/s 45415 (43206)	Loss/tok 3.1843 (3.1103)	LR 1.250e-04
0: TRAIN [7][1200/7762]	Time 0.169 (0.330)	Data 1.01e-04 (3.64e-04)	Tok/s 31065 (43194)	Loss/tok 2.4850 (3.1102)	LR 1.250e-04
0: TRAIN [7][1210/7762]	Time 0.259 (0.330)	Data 1.24e-04 (3.62e-04)	Tok/s 39981 (43206)	Loss/tok 3.0210 (3.1109)	LR 1.250e-04
0: TRAIN [7][1220/7762]	Time 0.253 (0.330)	Data 9.63e-05 (3.60e-04)	Tok/s 41340 (43176)	Loss/tok 2.9604 (3.1103)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][1230/7762]	Time 0.577 (0.330)	Data 1.04e-04 (3.58e-04)	Tok/s 51051 (43193)	Loss/tok 3.4325 (3.1108)	LR 1.250e-04
0: TRAIN [7][1240/7762]	Time 0.356 (0.330)	Data 1.03e-04 (3.56e-04)	Tok/s 46812 (43216)	Loss/tok 3.2003 (3.1107)	LR 1.250e-04
0: TRAIN [7][1250/7762]	Time 0.262 (0.331)	Data 1.06e-04 (3.54e-04)	Tok/s 39137 (43242)	Loss/tok 2.9782 (3.1113)	LR 1.250e-04
0: TRAIN [7][1260/7762]	Time 0.454 (0.331)	Data 9.97e-05 (3.52e-04)	Tok/s 51177 (43260)	Loss/tok 3.2690 (3.1116)	LR 1.250e-04
0: TRAIN [7][1270/7762]	Time 0.263 (0.331)	Data 1.90e-04 (3.50e-04)	Tok/s 39429 (43254)	Loss/tok 2.9615 (3.1108)	LR 1.250e-04
0: TRAIN [7][1280/7762]	Time 0.251 (0.331)	Data 1.04e-04 (3.48e-04)	Tok/s 42008 (43244)	Loss/tok 2.9460 (3.1104)	LR 1.250e-04
0: TRAIN [7][1290/7762]	Time 0.365 (0.330)	Data 1.00e-04 (3.46e-04)	Tok/s 46157 (43240)	Loss/tok 3.1004 (3.1099)	LR 1.250e-04
0: TRAIN [7][1300/7762]	Time 0.463 (0.330)	Data 1.00e-04 (3.44e-04)	Tok/s 50393 (43238)	Loss/tok 3.3079 (3.1100)	LR 1.250e-04
0: TRAIN [7][1310/7762]	Time 0.257 (0.331)	Data 9.97e-05 (3.42e-04)	Tok/s 39885 (43238)	Loss/tok 2.8490 (3.1101)	LR 1.250e-04
0: TRAIN [7][1320/7762]	Time 0.178 (0.330)	Data 1.06e-04 (3.41e-04)	Tok/s 29078 (43219)	Loss/tok 2.5729 (3.1098)	LR 1.250e-04
0: TRAIN [7][1330/7762]	Time 0.357 (0.330)	Data 1.03e-04 (3.39e-04)	Tok/s 47131 (43229)	Loss/tok 3.1099 (3.1097)	LR 1.250e-04
0: TRAIN [7][1340/7762]	Time 0.253 (0.330)	Data 1.04e-04 (3.37e-04)	Tok/s 40613 (43207)	Loss/tok 2.7982 (3.1089)	LR 1.250e-04
0: TRAIN [7][1350/7762]	Time 0.461 (0.330)	Data 1.06e-04 (3.35e-04)	Tok/s 50795 (43192)	Loss/tok 3.3962 (3.1090)	LR 1.250e-04
0: TRAIN [7][1360/7762]	Time 0.264 (0.329)	Data 1.07e-04 (3.34e-04)	Tok/s 38936 (43185)	Loss/tok 2.9663 (3.1085)	LR 1.250e-04
0: TRAIN [7][1370/7762]	Time 0.583 (0.330)	Data 1.02e-04 (3.32e-04)	Tok/s 50316 (43185)	Loss/tok 3.4560 (3.1091)	LR 1.250e-04
0: TRAIN [7][1380/7762]	Time 0.254 (0.329)	Data 1.01e-04 (3.30e-04)	Tok/s 40448 (43182)	Loss/tok 3.0289 (3.1086)	LR 1.250e-04
0: TRAIN [7][1390/7762]	Time 0.176 (0.329)	Data 1.02e-04 (3.29e-04)	Tok/s 30271 (43168)	Loss/tok 2.5050 (3.1085)	LR 1.250e-04
0: TRAIN [7][1400/7762]	Time 0.364 (0.329)	Data 1.03e-04 (3.27e-04)	Tok/s 45478 (43176)	Loss/tok 3.0889 (3.1083)	LR 1.250e-04
0: TRAIN [7][1410/7762]	Time 0.174 (0.329)	Data 1.02e-04 (3.26e-04)	Tok/s 30852 (43179)	Loss/tok 2.5582 (3.1083)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][1420/7762]	Time 0.558 (0.329)	Data 1.06e-04 (3.24e-04)	Tok/s 54079 (43179)	Loss/tok 3.3889 (3.1085)	LR 1.250e-04
0: TRAIN [7][1430/7762]	Time 0.453 (0.329)	Data 1.02e-04 (3.22e-04)	Tok/s 51904 (43178)	Loss/tok 3.2734 (3.1088)	LR 1.250e-04
0: TRAIN [7][1440/7762]	Time 0.358 (0.329)	Data 1.02e-04 (3.21e-04)	Tok/s 46821 (43180)	Loss/tok 3.0876 (3.1087)	LR 1.250e-04
0: TRAIN [7][1450/7762]	Time 0.265 (0.329)	Data 1.23e-04 (3.19e-04)	Tok/s 38859 (43180)	Loss/tok 2.8312 (3.1082)	LR 1.250e-04
0: TRAIN [7][1460/7762]	Time 0.264 (0.329)	Data 1.06e-04 (3.18e-04)	Tok/s 39562 (43193)	Loss/tok 2.9458 (3.1085)	LR 1.250e-04
0: TRAIN [7][1470/7762]	Time 0.261 (0.329)	Data 1.01e-04 (3.16e-04)	Tok/s 39371 (43172)	Loss/tok 2.9310 (3.1081)	LR 1.250e-04
0: TRAIN [7][1480/7762]	Time 0.263 (0.329)	Data 1.05e-04 (3.15e-04)	Tok/s 38997 (43199)	Loss/tok 3.0433 (3.1092)	LR 1.250e-04
0: TRAIN [7][1490/7762]	Time 0.266 (0.329)	Data 9.87e-05 (3.14e-04)	Tok/s 38888 (43197)	Loss/tok 2.8656 (3.1088)	LR 1.250e-04
0: TRAIN [7][1500/7762]	Time 0.462 (0.330)	Data 1.24e-04 (3.12e-04)	Tok/s 51269 (43200)	Loss/tok 3.3037 (3.1094)	LR 1.250e-04
0: TRAIN [7][1510/7762]	Time 0.263 (0.329)	Data 1.08e-04 (3.11e-04)	Tok/s 37977 (43187)	Loss/tok 2.9094 (3.1092)	LR 1.250e-04
0: TRAIN [7][1520/7762]	Time 0.363 (0.329)	Data 9.92e-05 (3.09e-04)	Tok/s 45513 (43188)	Loss/tok 3.1372 (3.1089)	LR 1.250e-04
0: TRAIN [7][1530/7762]	Time 0.365 (0.330)	Data 1.07e-04 (3.08e-04)	Tok/s 46098 (43212)	Loss/tok 3.0593 (3.1092)	LR 1.250e-04
0: TRAIN [7][1540/7762]	Time 0.359 (0.330)	Data 1.02e-04 (3.07e-04)	Tok/s 46445 (43220)	Loss/tok 3.0756 (3.1091)	LR 1.250e-04
0: TRAIN [7][1550/7762]	Time 0.173 (0.330)	Data 1.01e-04 (3.06e-04)	Tok/s 29757 (43217)	Loss/tok 2.4429 (3.1093)	LR 1.250e-04
0: TRAIN [7][1560/7762]	Time 0.266 (0.329)	Data 9.99e-05 (3.04e-04)	Tok/s 38653 (43204)	Loss/tok 2.8946 (3.1088)	LR 1.250e-04
0: TRAIN [7][1570/7762]	Time 0.265 (0.329)	Data 1.15e-04 (3.03e-04)	Tok/s 38610 (43185)	Loss/tok 2.9010 (3.1082)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][1580/7762]	Time 0.367 (0.329)	Data 1.18e-04 (3.02e-04)	Tok/s 45464 (43193)	Loss/tok 3.0352 (3.1083)	LR 1.250e-04
0: TRAIN [7][1590/7762]	Time 0.265 (0.329)	Data 1.03e-04 (3.01e-04)	Tok/s 38664 (43182)	Loss/tok 2.9628 (3.1078)	LR 1.250e-04
0: TRAIN [7][1600/7762]	Time 0.262 (0.329)	Data 1.19e-04 (2.99e-04)	Tok/s 39019 (43178)	Loss/tok 3.0026 (3.1075)	LR 1.250e-04
0: TRAIN [7][1610/7762]	Time 0.467 (0.329)	Data 1.07e-04 (2.98e-04)	Tok/s 49202 (43173)	Loss/tok 3.2161 (3.1072)	LR 1.250e-04
0: TRAIN [7][1620/7762]	Time 0.259 (0.329)	Data 1.00e-04 (2.97e-04)	Tok/s 41123 (43190)	Loss/tok 2.8637 (3.1077)	LR 1.250e-04
0: TRAIN [7][1630/7762]	Time 0.178 (0.329)	Data 1.05e-04 (2.96e-04)	Tok/s 29821 (43169)	Loss/tok 2.5646 (3.1069)	LR 1.250e-04
0: TRAIN [7][1640/7762]	Time 0.354 (0.329)	Data 1.00e-04 (2.95e-04)	Tok/s 47073 (43165)	Loss/tok 3.0332 (3.1067)	LR 1.250e-04
0: TRAIN [7][1650/7762]	Time 0.462 (0.329)	Data 1.05e-04 (2.94e-04)	Tok/s 50081 (43182)	Loss/tok 3.2954 (3.1076)	LR 1.250e-04
0: TRAIN [7][1660/7762]	Time 0.261 (0.329)	Data 1.12e-04 (2.92e-04)	Tok/s 39333 (43190)	Loss/tok 2.8745 (3.1080)	LR 1.250e-04
0: TRAIN [7][1670/7762]	Time 0.175 (0.329)	Data 1.04e-04 (2.91e-04)	Tok/s 30250 (43182)	Loss/tok 2.5380 (3.1075)	LR 1.250e-04
0: TRAIN [7][1680/7762]	Time 0.357 (0.329)	Data 1.02e-04 (2.90e-04)	Tok/s 46935 (43176)	Loss/tok 3.1193 (3.1073)	LR 1.250e-04
0: TRAIN [7][1690/7762]	Time 0.264 (0.329)	Data 9.82e-05 (2.89e-04)	Tok/s 39283 (43182)	Loss/tok 2.8948 (3.1075)	LR 1.250e-04
0: TRAIN [7][1700/7762]	Time 0.258 (0.329)	Data 1.01e-04 (2.88e-04)	Tok/s 39908 (43175)	Loss/tok 2.9847 (3.1075)	LR 1.250e-04
0: TRAIN [7][1710/7762]	Time 0.442 (0.329)	Data 9.94e-05 (2.87e-04)	Tok/s 52278 (43164)	Loss/tok 3.3605 (3.1079)	LR 1.250e-04
0: TRAIN [7][1720/7762]	Time 0.359 (0.329)	Data 1.03e-04 (2.86e-04)	Tok/s 45688 (43178)	Loss/tok 3.1786 (3.1086)	LR 1.250e-04
0: TRAIN [7][1730/7762]	Time 0.261 (0.329)	Data 1.12e-04 (2.85e-04)	Tok/s 39203 (43173)	Loss/tok 2.9217 (3.1081)	LR 1.250e-04
0: TRAIN [7][1740/7762]	Time 0.264 (0.329)	Data 1.02e-04 (2.84e-04)	Tok/s 38827 (43171)	Loss/tok 2.8501 (3.1080)	LR 1.250e-04
0: TRAIN [7][1750/7762]	Time 0.261 (0.329)	Data 1.01e-04 (2.83e-04)	Tok/s 39397 (43153)	Loss/tok 2.9487 (3.1077)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][1760/7762]	Time 0.176 (0.329)	Data 1.03e-04 (2.82e-04)	Tok/s 29914 (43150)	Loss/tok 2.5792 (3.1076)	LR 1.250e-04
0: TRAIN [7][1770/7762]	Time 0.175 (0.328)	Data 1.02e-04 (2.81e-04)	Tok/s 30761 (43131)	Loss/tok 2.6012 (3.1071)	LR 1.250e-04
0: TRAIN [7][1780/7762]	Time 0.356 (0.328)	Data 9.68e-05 (2.80e-04)	Tok/s 47046 (43120)	Loss/tok 3.1259 (3.1071)	LR 1.250e-04
0: TRAIN [7][1790/7762]	Time 0.259 (0.328)	Data 1.02e-04 (2.79e-04)	Tok/s 38858 (43128)	Loss/tok 2.9433 (3.1072)	LR 1.250e-04
0: TRAIN [7][1800/7762]	Time 0.261 (0.328)	Data 1.03e-04 (2.78e-04)	Tok/s 39012 (43139)	Loss/tok 2.8742 (3.1071)	LR 1.250e-04
0: TRAIN [7][1810/7762]	Time 0.453 (0.328)	Data 1.03e-04 (2.77e-04)	Tok/s 51073 (43129)	Loss/tok 3.1489 (3.1069)	LR 1.250e-04
0: TRAIN [7][1820/7762]	Time 0.252 (0.328)	Data 1.02e-04 (2.76e-04)	Tok/s 41364 (43135)	Loss/tok 2.8565 (3.1067)	LR 1.250e-04
0: TRAIN [7][1830/7762]	Time 0.359 (0.328)	Data 1.14e-04 (2.75e-04)	Tok/s 46984 (43120)	Loss/tok 3.1534 (3.1063)	LR 1.250e-04
0: TRAIN [7][1840/7762]	Time 0.177 (0.328)	Data 1.19e-04 (2.74e-04)	Tok/s 29468 (43112)	Loss/tok 2.5562 (3.1058)	LR 1.250e-04
0: TRAIN [7][1850/7762]	Time 0.364 (0.328)	Data 1.02e-04 (2.73e-04)	Tok/s 46261 (43099)	Loss/tok 3.1174 (3.1059)	LR 1.250e-04
0: TRAIN [7][1860/7762]	Time 0.362 (0.328)	Data 1.14e-04 (2.72e-04)	Tok/s 46762 (43116)	Loss/tok 3.0579 (3.1058)	LR 1.250e-04
0: TRAIN [7][1870/7762]	Time 0.261 (0.328)	Data 1.07e-04 (2.71e-04)	Tok/s 39496 (43119)	Loss/tok 2.9526 (3.1059)	LR 1.250e-04
0: TRAIN [7][1880/7762]	Time 0.260 (0.328)	Data 1.13e-04 (2.71e-04)	Tok/s 39314 (43104)	Loss/tok 2.9343 (3.1055)	LR 1.250e-04
0: TRAIN [7][1890/7762]	Time 0.344 (0.328)	Data 1.10e-04 (2.70e-04)	Tok/s 49202 (43091)	Loss/tok 3.1603 (3.1050)	LR 1.250e-04
0: TRAIN [7][1900/7762]	Time 0.457 (0.328)	Data 1.02e-04 (2.69e-04)	Tok/s 50829 (43111)	Loss/tok 3.1452 (3.1056)	LR 1.250e-04
0: TRAIN [7][1910/7762]	Time 0.364 (0.328)	Data 1.06e-04 (2.68e-04)	Tok/s 46294 (43122)	Loss/tok 2.9257 (3.1054)	LR 1.250e-04
0: TRAIN [7][1920/7762]	Time 0.452 (0.328)	Data 1.05e-04 (2.67e-04)	Tok/s 51003 (43124)	Loss/tok 3.2845 (3.1056)	LR 1.250e-04
0: TRAIN [7][1930/7762]	Time 0.464 (0.328)	Data 1.06e-04 (2.66e-04)	Tok/s 50014 (43133)	Loss/tok 3.2829 (3.1055)	LR 1.250e-04
0: TRAIN [7][1940/7762]	Time 0.251 (0.328)	Data 1.22e-04 (2.65e-04)	Tok/s 40088 (43140)	Loss/tok 2.8910 (3.1053)	LR 1.250e-04
0: TRAIN [7][1950/7762]	Time 0.267 (0.328)	Data 9.78e-05 (2.65e-04)	Tok/s 37926 (43129)	Loss/tok 2.9508 (3.1053)	LR 1.250e-04
0: TRAIN [7][1960/7762]	Time 0.585 (0.328)	Data 1.03e-04 (2.64e-04)	Tok/s 51700 (43148)	Loss/tok 3.3614 (3.1058)	LR 1.250e-04
0: TRAIN [7][1970/7762]	Time 0.262 (0.328)	Data 9.39e-05 (2.63e-04)	Tok/s 39592 (43133)	Loss/tok 2.9651 (3.1055)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][1980/7762]	Time 0.176 (0.328)	Data 1.01e-04 (2.62e-04)	Tok/s 29444 (43137)	Loss/tok 2.6429 (3.1057)	LR 1.250e-04
0: TRAIN [7][1990/7762]	Time 0.357 (0.328)	Data 9.92e-05 (2.61e-04)	Tok/s 47371 (43130)	Loss/tok 3.0848 (3.1053)	LR 1.250e-04
0: TRAIN [7][2000/7762]	Time 0.263 (0.328)	Data 1.02e-04 (2.61e-04)	Tok/s 38767 (43123)	Loss/tok 2.9323 (3.1051)	LR 1.250e-04
0: TRAIN [7][2010/7762]	Time 0.253 (0.328)	Data 1.01e-04 (2.60e-04)	Tok/s 40715 (43130)	Loss/tok 2.9992 (3.1055)	LR 1.250e-04
0: TRAIN [7][2020/7762]	Time 0.264 (0.328)	Data 9.97e-05 (2.59e-04)	Tok/s 38657 (43142)	Loss/tok 2.9255 (3.1059)	LR 1.250e-04
0: TRAIN [7][2030/7762]	Time 0.461 (0.328)	Data 1.03e-04 (2.58e-04)	Tok/s 50791 (43150)	Loss/tok 3.3161 (3.1061)	LR 1.250e-04
0: TRAIN [7][2040/7762]	Time 0.363 (0.328)	Data 9.61e-05 (2.58e-04)	Tok/s 46512 (43146)	Loss/tok 3.0729 (3.1057)	LR 1.250e-04
0: TRAIN [7][2050/7762]	Time 0.262 (0.328)	Data 1.13e-04 (2.57e-04)	Tok/s 39634 (43140)	Loss/tok 2.9249 (3.1055)	LR 1.250e-04
0: TRAIN [7][2060/7762]	Time 0.265 (0.328)	Data 1.18e-04 (2.56e-04)	Tok/s 38470 (43147)	Loss/tok 2.9503 (3.1065)	LR 1.250e-04
0: TRAIN [7][2070/7762]	Time 0.266 (0.328)	Data 1.17e-04 (2.55e-04)	Tok/s 38907 (43148)	Loss/tok 2.8587 (3.1063)	LR 1.250e-04
0: TRAIN [7][2080/7762]	Time 0.269 (0.328)	Data 9.94e-05 (2.55e-04)	Tok/s 39276 (43135)	Loss/tok 3.0025 (3.1059)	LR 1.250e-04
0: TRAIN [7][2090/7762]	Time 0.266 (0.328)	Data 1.05e-04 (2.54e-04)	Tok/s 38828 (43136)	Loss/tok 2.9952 (3.1058)	LR 1.250e-04
0: TRAIN [7][2100/7762]	Time 0.178 (0.328)	Data 1.04e-04 (2.53e-04)	Tok/s 29921 (43134)	Loss/tok 2.4560 (3.1059)	LR 1.250e-04
0: TRAIN [7][2110/7762]	Time 0.269 (0.328)	Data 1.01e-04 (2.53e-04)	Tok/s 37718 (43121)	Loss/tok 2.8060 (3.1054)	LR 1.250e-04
0: TRAIN [7][2120/7762]	Time 0.465 (0.328)	Data 9.63e-05 (2.52e-04)	Tok/s 50201 (43123)	Loss/tok 3.2576 (3.1053)	LR 1.250e-04
0: TRAIN [7][2130/7762]	Time 0.362 (0.328)	Data 1.17e-04 (2.51e-04)	Tok/s 46312 (43118)	Loss/tok 3.1177 (3.1049)	LR 1.250e-04
0: TRAIN [7][2140/7762]	Time 0.262 (0.328)	Data 1.21e-04 (2.51e-04)	Tok/s 38919 (43109)	Loss/tok 2.9719 (3.1048)	LR 1.250e-04
0: TRAIN [7][2150/7762]	Time 0.269 (0.328)	Data 1.02e-04 (2.50e-04)	Tok/s 37944 (43106)	Loss/tok 2.9472 (3.1048)	LR 1.250e-04
0: TRAIN [7][2160/7762]	Time 0.348 (0.328)	Data 1.06e-04 (2.49e-04)	Tok/s 48007 (43113)	Loss/tok 3.0698 (3.1051)	LR 1.250e-04
0: TRAIN [7][2170/7762]	Time 0.348 (0.328)	Data 1.04e-04 (2.49e-04)	Tok/s 48448 (43109)	Loss/tok 3.1232 (3.1050)	LR 1.250e-04
0: TRAIN [7][2180/7762]	Time 0.256 (0.328)	Data 1.04e-04 (2.48e-04)	Tok/s 41008 (43125)	Loss/tok 2.9418 (3.1052)	LR 1.250e-04
0: TRAIN [7][2190/7762]	Time 0.173 (0.328)	Data 9.85e-05 (2.47e-04)	Tok/s 30431 (43115)	Loss/tok 2.5794 (3.1050)	LR 1.250e-04
0: TRAIN [7][2200/7762]	Time 0.359 (0.328)	Data 1.13e-04 (2.47e-04)	Tok/s 47269 (43106)	Loss/tok 3.0797 (3.1045)	LR 1.250e-04
0: TRAIN [7][2210/7762]	Time 0.265 (0.327)	Data 9.97e-05 (2.46e-04)	Tok/s 39566 (43091)	Loss/tok 2.9182 (3.1039)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][2220/7762]	Time 0.254 (0.327)	Data 1.02e-04 (2.45e-04)	Tok/s 40004 (43092)	Loss/tok 2.9518 (3.1042)	LR 1.250e-04
0: TRAIN [7][2230/7762]	Time 0.174 (0.327)	Data 1.11e-04 (2.45e-04)	Tok/s 30561 (43087)	Loss/tok 2.5946 (3.1039)	LR 1.250e-04
0: TRAIN [7][2240/7762]	Time 0.178 (0.327)	Data 1.10e-04 (2.44e-04)	Tok/s 30139 (43076)	Loss/tok 2.5053 (3.1039)	LR 1.250e-04
0: TRAIN [7][2250/7762]	Time 0.352 (0.327)	Data 1.09e-04 (2.43e-04)	Tok/s 47810 (43082)	Loss/tok 3.0524 (3.1039)	LR 1.250e-04
0: TRAIN [7][2260/7762]	Time 0.266 (0.327)	Data 1.02e-04 (2.43e-04)	Tok/s 38689 (43083)	Loss/tok 2.9556 (3.1040)	LR 1.250e-04
0: TRAIN [7][2270/7762]	Time 0.365 (0.327)	Data 1.00e-04 (2.42e-04)	Tok/s 45995 (43077)	Loss/tok 3.0933 (3.1037)	LR 1.250e-04
0: TRAIN [7][2280/7762]	Time 0.357 (0.327)	Data 1.00e-04 (2.42e-04)	Tok/s 47777 (43085)	Loss/tok 3.0202 (3.1036)	LR 1.250e-04
0: TRAIN [7][2290/7762]	Time 0.265 (0.327)	Data 1.00e-04 (2.41e-04)	Tok/s 39129 (43075)	Loss/tok 3.0806 (3.1032)	LR 1.250e-04
0: TRAIN [7][2300/7762]	Time 0.264 (0.327)	Data 1.17e-04 (2.41e-04)	Tok/s 38875 (43068)	Loss/tok 2.9169 (3.1034)	LR 1.250e-04
0: TRAIN [7][2310/7762]	Time 0.357 (0.327)	Data 1.13e-04 (2.40e-04)	Tok/s 46621 (43073)	Loss/tok 3.0857 (3.1031)	LR 1.250e-04
0: TRAIN [7][2320/7762]	Time 0.572 (0.327)	Data 9.35e-05 (2.39e-04)	Tok/s 52479 (43078)	Loss/tok 3.4250 (3.1035)	LR 1.250e-04
0: TRAIN [7][2330/7762]	Time 0.462 (0.327)	Data 1.01e-04 (2.39e-04)	Tok/s 50341 (43076)	Loss/tok 3.2413 (3.1037)	LR 1.250e-04
0: TRAIN [7][2340/7762]	Time 0.255 (0.327)	Data 1.03e-04 (2.38e-04)	Tok/s 40074 (43085)	Loss/tok 2.9310 (3.1038)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][2350/7762]	Time 0.365 (0.327)	Data 1.15e-04 (2.38e-04)	Tok/s 46601 (43094)	Loss/tok 3.1529 (3.1039)	LR 1.250e-04
0: TRAIN [7][2360/7762]	Time 0.464 (0.327)	Data 9.47e-05 (2.37e-04)	Tok/s 50443 (43096)	Loss/tok 3.3248 (3.1038)	LR 1.250e-04
0: TRAIN [7][2370/7762]	Time 0.264 (0.327)	Data 1.02e-04 (2.37e-04)	Tok/s 38847 (43093)	Loss/tok 2.9155 (3.1038)	LR 1.250e-04
0: TRAIN [7][2380/7762]	Time 0.360 (0.327)	Data 1.06e-04 (2.36e-04)	Tok/s 46993 (43090)	Loss/tok 3.0747 (3.1036)	LR 1.250e-04
0: TRAIN [7][2390/7762]	Time 0.463 (0.327)	Data 1.01e-04 (2.35e-04)	Tok/s 49941 (43097)	Loss/tok 3.2342 (3.1039)	LR 1.250e-04
0: TRAIN [7][2400/7762]	Time 0.178 (0.327)	Data 1.05e-04 (2.35e-04)	Tok/s 28926 (43104)	Loss/tok 2.5700 (3.1043)	LR 1.250e-04
0: TRAIN [7][2410/7762]	Time 0.367 (0.327)	Data 1.22e-04 (2.34e-04)	Tok/s 45683 (43099)	Loss/tok 3.1968 (3.1043)	LR 1.250e-04
0: TRAIN [7][2420/7762]	Time 0.356 (0.328)	Data 1.19e-04 (2.34e-04)	Tok/s 47149 (43106)	Loss/tok 3.0354 (3.1050)	LR 1.250e-04
0: TRAIN [7][2430/7762]	Time 0.574 (0.328)	Data 1.05e-04 (2.33e-04)	Tok/s 52263 (43110)	Loss/tok 3.4487 (3.1054)	LR 1.250e-04
0: TRAIN [7][2440/7762]	Time 0.259 (0.328)	Data 1.03e-04 (2.33e-04)	Tok/s 39754 (43104)	Loss/tok 2.8060 (3.1052)	LR 1.250e-04
0: TRAIN [7][2450/7762]	Time 0.269 (0.328)	Data 1.05e-04 (2.32e-04)	Tok/s 37514 (43110)	Loss/tok 2.9656 (3.1055)	LR 1.250e-04
0: TRAIN [7][2460/7762]	Time 0.361 (0.328)	Data 1.02e-04 (2.32e-04)	Tok/s 45884 (43109)	Loss/tok 3.1965 (3.1057)	LR 1.250e-04
0: TRAIN [7][2470/7762]	Time 0.268 (0.328)	Data 1.19e-04 (2.31e-04)	Tok/s 39190 (43108)	Loss/tok 2.9933 (3.1057)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][2480/7762]	Time 0.170 (0.328)	Data 9.66e-05 (2.31e-04)	Tok/s 30583 (43101)	Loss/tok 2.3908 (3.1057)	LR 1.250e-04
0: TRAIN [7][2490/7762]	Time 0.260 (0.328)	Data 1.01e-04 (2.30e-04)	Tok/s 39485 (43104)	Loss/tok 2.9322 (3.1058)	LR 1.250e-04
0: TRAIN [7][2500/7762]	Time 0.588 (0.328)	Data 9.78e-05 (2.30e-04)	Tok/s 50596 (43098)	Loss/tok 3.4324 (3.1058)	LR 1.250e-04
0: TRAIN [7][2510/7762]	Time 0.253 (0.328)	Data 1.03e-04 (2.29e-04)	Tok/s 41630 (43091)	Loss/tok 2.8819 (3.1054)	LR 1.250e-04
0: TRAIN [7][2520/7762]	Time 0.362 (0.328)	Data 1.03e-04 (2.29e-04)	Tok/s 46750 (43105)	Loss/tok 3.0122 (3.1057)	LR 1.250e-04
0: TRAIN [7][2530/7762]	Time 0.267 (0.328)	Data 1.34e-04 (2.28e-04)	Tok/s 39334 (43104)	Loss/tok 2.8065 (3.1056)	LR 1.250e-04
0: TRAIN [7][2540/7762]	Time 0.364 (0.328)	Data 9.68e-05 (2.28e-04)	Tok/s 46758 (43099)	Loss/tok 3.0436 (3.1052)	LR 1.250e-04
0: TRAIN [7][2550/7762]	Time 0.177 (0.327)	Data 9.99e-05 (2.27e-04)	Tok/s 30304 (43088)	Loss/tok 2.5049 (3.1049)	LR 1.250e-04
0: TRAIN [7][2560/7762]	Time 0.265 (0.327)	Data 1.03e-04 (2.27e-04)	Tok/s 38909 (43077)	Loss/tok 2.8994 (3.1049)	LR 1.250e-04
0: TRAIN [7][2570/7762]	Time 0.555 (0.328)	Data 1.02e-04 (2.26e-04)	Tok/s 53176 (43085)	Loss/tok 3.5396 (3.1054)	LR 1.250e-04
0: TRAIN [7][2580/7762]	Time 0.366 (0.327)	Data 1.00e-04 (2.26e-04)	Tok/s 45511 (43076)	Loss/tok 3.0842 (3.1049)	LR 1.250e-04
0: TRAIN [7][2590/7762]	Time 0.465 (0.327)	Data 1.10e-04 (2.25e-04)	Tok/s 49413 (43067)	Loss/tok 3.3485 (3.1046)	LR 1.250e-04
0: TRAIN [7][2600/7762]	Time 0.176 (0.327)	Data 1.05e-04 (2.25e-04)	Tok/s 29640 (43067)	Loss/tok 2.5769 (3.1046)	LR 1.250e-04
0: TRAIN [7][2610/7762]	Time 0.264 (0.327)	Data 1.04e-04 (2.25e-04)	Tok/s 39280 (43066)	Loss/tok 2.9470 (3.1046)	LR 1.250e-04
0: TRAIN [7][2620/7762]	Time 0.177 (0.327)	Data 1.02e-04 (2.24e-04)	Tok/s 30022 (43064)	Loss/tok 2.5249 (3.1049)	LR 1.250e-04
0: TRAIN [7][2630/7762]	Time 0.268 (0.327)	Data 1.25e-04 (2.24e-04)	Tok/s 39312 (43061)	Loss/tok 2.8765 (3.1045)	LR 1.250e-04
0: TRAIN [7][2640/7762]	Time 0.458 (0.327)	Data 1.22e-04 (2.23e-04)	Tok/s 50848 (43074)	Loss/tok 3.2161 (3.1048)	LR 1.250e-04
0: TRAIN [7][2650/7762]	Time 0.455 (0.327)	Data 2.31e-04 (2.23e-04)	Tok/s 50843 (43080)	Loss/tok 3.3275 (3.1051)	LR 1.250e-04
0: TRAIN [7][2660/7762]	Time 0.267 (0.327)	Data 1.04e-04 (2.22e-04)	Tok/s 38183 (43074)	Loss/tok 2.8812 (3.1047)	LR 1.250e-04
0: TRAIN [7][2670/7762]	Time 0.359 (0.327)	Data 1.03e-04 (2.22e-04)	Tok/s 47138 (43078)	Loss/tok 3.0635 (3.1046)	LR 1.250e-04
0: TRAIN [7][2680/7762]	Time 0.464 (0.328)	Data 9.49e-05 (2.22e-04)	Tok/s 50166 (43086)	Loss/tok 3.2924 (3.1047)	LR 1.250e-04
0: TRAIN [7][2690/7762]	Time 0.178 (0.328)	Data 1.04e-04 (2.21e-04)	Tok/s 29352 (43085)	Loss/tok 2.5496 (3.1047)	LR 1.250e-04
0: TRAIN [7][2700/7762]	Time 0.579 (0.328)	Data 1.07e-04 (2.21e-04)	Tok/s 50762 (43097)	Loss/tok 3.5207 (3.1052)	LR 1.250e-04
0: TRAIN [7][2710/7762]	Time 0.265 (0.328)	Data 1.05e-04 (2.20e-04)	Tok/s 38491 (43105)	Loss/tok 2.9656 (3.1054)	LR 1.250e-04
0: TRAIN [7][2720/7762]	Time 0.462 (0.328)	Data 1.00e-04 (2.20e-04)	Tok/s 51131 (43100)	Loss/tok 3.1408 (3.1054)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][2730/7762]	Time 0.460 (0.328)	Data 1.01e-04 (2.20e-04)	Tok/s 50160 (43108)	Loss/tok 3.2630 (3.1056)	LR 1.250e-04
0: TRAIN [7][2740/7762]	Time 0.455 (0.328)	Data 1.18e-04 (2.19e-04)	Tok/s 51561 (43112)	Loss/tok 3.2573 (3.1055)	LR 1.250e-04
0: TRAIN [7][2750/7762]	Time 0.268 (0.328)	Data 1.02e-04 (2.19e-04)	Tok/s 39013 (43110)	Loss/tok 2.9488 (3.1054)	LR 1.250e-04
0: TRAIN [7][2760/7762]	Time 0.363 (0.328)	Data 1.21e-04 (2.18e-04)	Tok/s 46574 (43103)	Loss/tok 3.0841 (3.1052)	LR 1.250e-04
0: TRAIN [7][2770/7762]	Time 0.262 (0.328)	Data 1.03e-04 (2.18e-04)	Tok/s 39866 (43106)	Loss/tok 2.9295 (3.1051)	LR 1.250e-04
0: TRAIN [7][2780/7762]	Time 0.358 (0.328)	Data 1.14e-04 (2.17e-04)	Tok/s 46778 (43111)	Loss/tok 2.9973 (3.1051)	LR 1.250e-04
0: TRAIN [7][2790/7762]	Time 0.362 (0.328)	Data 1.10e-04 (2.17e-04)	Tok/s 46604 (43120)	Loss/tok 3.1932 (3.1054)	LR 1.250e-04
0: TRAIN [7][2800/7762]	Time 0.265 (0.328)	Data 9.92e-05 (2.17e-04)	Tok/s 39391 (43120)	Loss/tok 3.0236 (3.1053)	LR 1.250e-04
0: TRAIN [7][2810/7762]	Time 0.265 (0.328)	Data 1.04e-04 (2.16e-04)	Tok/s 39474 (43119)	Loss/tok 2.9019 (3.1053)	LR 1.250e-04
0: TRAIN [7][2820/7762]	Time 0.344 (0.328)	Data 9.87e-05 (2.16e-04)	Tok/s 48452 (43120)	Loss/tok 3.2034 (3.1053)	LR 1.250e-04
0: TRAIN [7][2830/7762]	Time 0.450 (0.328)	Data 1.02e-04 (2.15e-04)	Tok/s 51217 (43127)	Loss/tok 3.3639 (3.1057)	LR 1.250e-04
0: TRAIN [7][2840/7762]	Time 0.262 (0.328)	Data 1.01e-04 (2.15e-04)	Tok/s 39885 (43119)	Loss/tok 2.9902 (3.1055)	LR 1.250e-04
0: TRAIN [7][2850/7762]	Time 0.454 (0.328)	Data 1.02e-04 (2.15e-04)	Tok/s 51802 (43126)	Loss/tok 3.2433 (3.1061)	LR 1.250e-04
0: TRAIN [7][2860/7762]	Time 0.349 (0.328)	Data 1.04e-04 (2.14e-04)	Tok/s 48528 (43128)	Loss/tok 3.1349 (3.1060)	LR 1.250e-04
0: TRAIN [7][2870/7762]	Time 0.364 (0.328)	Data 9.99e-05 (2.14e-04)	Tok/s 45429 (43123)	Loss/tok 3.1359 (3.1056)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][2880/7762]	Time 0.179 (0.328)	Data 9.80e-05 (2.13e-04)	Tok/s 29385 (43120)	Loss/tok 2.5403 (3.1057)	LR 1.250e-04
0: TRAIN [7][2890/7762]	Time 0.363 (0.328)	Data 1.16e-04 (2.13e-04)	Tok/s 45997 (43115)	Loss/tok 3.1263 (3.1053)	LR 1.250e-04
0: TRAIN [7][2900/7762]	Time 0.268 (0.328)	Data 1.01e-04 (2.13e-04)	Tok/s 38918 (43115)	Loss/tok 2.9002 (3.1054)	LR 1.250e-04
0: TRAIN [7][2910/7762]	Time 0.266 (0.328)	Data 1.03e-04 (2.12e-04)	Tok/s 38191 (43112)	Loss/tok 2.9212 (3.1053)	LR 1.250e-04
0: TRAIN [7][2920/7762]	Time 0.269 (0.328)	Data 9.94e-05 (2.12e-04)	Tok/s 39014 (43104)	Loss/tok 2.8176 (3.1050)	LR 1.250e-04
0: TRAIN [7][2930/7762]	Time 0.346 (0.328)	Data 1.07e-04 (2.12e-04)	Tok/s 47956 (43099)	Loss/tok 3.0719 (3.1048)	LR 1.250e-04
0: TRAIN [7][2940/7762]	Time 0.359 (0.328)	Data 1.02e-04 (2.11e-04)	Tok/s 46850 (43085)	Loss/tok 3.2317 (3.1043)	LR 1.250e-04
0: TRAIN [7][2950/7762]	Time 0.460 (0.328)	Data 1.04e-04 (2.11e-04)	Tok/s 50816 (43083)	Loss/tok 3.1966 (3.1044)	LR 1.250e-04
0: TRAIN [7][2960/7762]	Time 0.265 (0.328)	Data 1.10e-04 (2.11e-04)	Tok/s 38721 (43086)	Loss/tok 2.9270 (3.1043)	LR 1.250e-04
0: TRAIN [7][2970/7762]	Time 0.265 (0.327)	Data 1.03e-04 (2.10e-04)	Tok/s 38598 (43083)	Loss/tok 2.8989 (3.1041)	LR 1.250e-04
0: TRAIN [7][2980/7762]	Time 0.450 (0.328)	Data 1.03e-04 (2.10e-04)	Tok/s 51760 (43080)	Loss/tok 3.2773 (3.1044)	LR 1.250e-04
0: TRAIN [7][2990/7762]	Time 0.457 (0.327)	Data 1.04e-04 (2.09e-04)	Tok/s 50524 (43075)	Loss/tok 3.3560 (3.1044)	LR 1.250e-04
0: TRAIN [7][3000/7762]	Time 0.358 (0.327)	Data 1.00e-04 (2.09e-04)	Tok/s 46581 (43077)	Loss/tok 3.0786 (3.1044)	LR 1.250e-04
0: TRAIN [7][3010/7762]	Time 0.362 (0.327)	Data 9.39e-05 (2.09e-04)	Tok/s 46841 (43073)	Loss/tok 3.0465 (3.1045)	LR 1.250e-04
0: TRAIN [7][3020/7762]	Time 0.177 (0.327)	Data 9.92e-05 (2.08e-04)	Tok/s 29620 (43071)	Loss/tok 2.4846 (3.1047)	LR 1.250e-04
0: TRAIN [7][3030/7762]	Time 0.462 (0.328)	Data 1.01e-04 (2.08e-04)	Tok/s 50332 (43083)	Loss/tok 3.0866 (3.1051)	LR 1.250e-04
0: TRAIN [7][3040/7762]	Time 0.265 (0.328)	Data 1.22e-04 (2.08e-04)	Tok/s 38469 (43079)	Loss/tok 2.7851 (3.1047)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][3050/7762]	Time 0.344 (0.327)	Data 1.02e-04 (2.07e-04)	Tok/s 48557 (43072)	Loss/tok 3.0727 (3.1046)	LR 1.250e-04
0: TRAIN [7][3060/7762]	Time 0.365 (0.327)	Data 1.02e-04 (2.07e-04)	Tok/s 46443 (43066)	Loss/tok 3.1937 (3.1045)	LR 1.250e-04
0: TRAIN [7][3070/7762]	Time 0.261 (0.327)	Data 9.99e-05 (2.07e-04)	Tok/s 39440 (43064)	Loss/tok 2.8914 (3.1044)	LR 1.250e-04
0: TRAIN [7][3080/7762]	Time 0.457 (0.327)	Data 1.15e-04 (2.06e-04)	Tok/s 51459 (43059)	Loss/tok 3.2711 (3.1043)	LR 1.250e-04
0: TRAIN [7][3090/7762]	Time 0.462 (0.327)	Data 1.01e-04 (2.06e-04)	Tok/s 50300 (43064)	Loss/tok 3.2345 (3.1043)	LR 1.250e-04
0: TRAIN [7][3100/7762]	Time 0.265 (0.327)	Data 1.03e-04 (2.06e-04)	Tok/s 38544 (43071)	Loss/tok 2.8902 (3.1045)	LR 1.250e-04
0: TRAIN [7][3110/7762]	Time 0.354 (0.327)	Data 1.02e-04 (2.05e-04)	Tok/s 47854 (43071)	Loss/tok 3.2335 (3.1046)	LR 1.250e-04
0: TRAIN [7][3120/7762]	Time 0.435 (0.327)	Data 1.03e-04 (2.05e-04)	Tok/s 53836 (43072)	Loss/tok 3.2877 (3.1044)	LR 1.250e-04
0: TRAIN [7][3130/7762]	Time 0.463 (0.327)	Data 1.02e-04 (2.05e-04)	Tok/s 50690 (43082)	Loss/tok 3.2168 (3.1046)	LR 1.250e-04
0: TRAIN [7][3140/7762]	Time 0.267 (0.328)	Data 1.02e-04 (2.04e-04)	Tok/s 37984 (43088)	Loss/tok 2.8853 (3.1046)	LR 1.250e-04
0: TRAIN [7][3150/7762]	Time 0.270 (0.328)	Data 9.99e-05 (2.04e-04)	Tok/s 38541 (43100)	Loss/tok 2.7412 (3.1049)	LR 1.250e-04
0: TRAIN [7][3160/7762]	Time 0.463 (0.328)	Data 1.02e-04 (2.04e-04)	Tok/s 50616 (43104)	Loss/tok 3.2822 (3.1049)	LR 1.250e-04
0: TRAIN [7][3170/7762]	Time 0.363 (0.328)	Data 1.25e-04 (2.04e-04)	Tok/s 46346 (43102)	Loss/tok 3.1226 (3.1050)	LR 1.250e-04
0: TRAIN [7][3180/7762]	Time 0.259 (0.328)	Data 1.03e-04 (2.03e-04)	Tok/s 40334 (43094)	Loss/tok 2.8592 (3.1048)	LR 1.250e-04
0: TRAIN [7][3190/7762]	Time 0.363 (0.328)	Data 1.01e-04 (2.03e-04)	Tok/s 45476 (43100)	Loss/tok 3.2123 (3.1050)	LR 1.250e-04
0: TRAIN [7][3200/7762]	Time 0.267 (0.328)	Data 9.75e-05 (2.03e-04)	Tok/s 38337 (43098)	Loss/tok 2.8490 (3.1047)	LR 1.250e-04
0: TRAIN [7][3210/7762]	Time 0.258 (0.327)	Data 9.99e-05 (2.02e-04)	Tok/s 39912 (43084)	Loss/tok 2.9472 (3.1044)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][3220/7762]	Time 0.462 (0.328)	Data 1.03e-04 (2.02e-04)	Tok/s 51056 (43091)	Loss/tok 3.1523 (3.1045)	LR 1.250e-04
0: TRAIN [7][3230/7762]	Time 0.358 (0.328)	Data 1.03e-04 (2.02e-04)	Tok/s 47066 (43096)	Loss/tok 3.1611 (3.1048)	LR 1.250e-04
0: TRAIN [7][3240/7762]	Time 0.358 (0.328)	Data 1.16e-04 (2.01e-04)	Tok/s 47428 (43103)	Loss/tok 3.0523 (3.1048)	LR 1.250e-04
0: TRAIN [7][3250/7762]	Time 0.268 (0.328)	Data 1.01e-04 (2.01e-04)	Tok/s 38656 (43103)	Loss/tok 2.9115 (3.1047)	LR 1.250e-04
0: TRAIN [7][3260/7762]	Time 0.269 (0.328)	Data 1.01e-04 (2.01e-04)	Tok/s 38364 (43099)	Loss/tok 2.9967 (3.1046)	LR 1.250e-04
0: TRAIN [7][3270/7762]	Time 0.363 (0.328)	Data 9.97e-05 (2.01e-04)	Tok/s 46376 (43101)	Loss/tok 3.0224 (3.1045)	LR 1.250e-04
0: TRAIN [7][3280/7762]	Time 0.361 (0.328)	Data 9.85e-05 (2.00e-04)	Tok/s 46836 (43104)	Loss/tok 3.0959 (3.1045)	LR 1.250e-04
0: TRAIN [7][3290/7762]	Time 0.352 (0.328)	Data 1.03e-04 (2.00e-04)	Tok/s 47200 (43103)	Loss/tok 3.1597 (3.1044)	LR 1.250e-04
0: TRAIN [7][3300/7762]	Time 0.341 (0.328)	Data 1.17e-04 (2.00e-04)	Tok/s 49287 (43115)	Loss/tok 3.1088 (3.1046)	LR 1.250e-04
0: TRAIN [7][3310/7762]	Time 0.460 (0.328)	Data 1.03e-04 (1.99e-04)	Tok/s 50515 (43110)	Loss/tok 3.2707 (3.1046)	LR 1.250e-04
0: TRAIN [7][3320/7762]	Time 0.348 (0.328)	Data 1.44e-04 (1.99e-04)	Tok/s 48378 (43105)	Loss/tok 3.1547 (3.1047)	LR 1.250e-04
0: TRAIN [7][3330/7762]	Time 0.169 (0.328)	Data 1.04e-04 (1.99e-04)	Tok/s 31311 (43107)	Loss/tok 2.5505 (3.1047)	LR 1.250e-04
0: TRAIN [7][3340/7762]	Time 0.456 (0.328)	Data 9.92e-05 (1.98e-04)	Tok/s 50926 (43103)	Loss/tok 3.2911 (3.1049)	LR 1.250e-04
0: TRAIN [7][3350/7762]	Time 0.178 (0.328)	Data 9.73e-05 (1.98e-04)	Tok/s 29664 (43088)	Loss/tok 2.4973 (3.1045)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][3360/7762]	Time 0.356 (0.328)	Data 1.63e-04 (1.98e-04)	Tok/s 47533 (43100)	Loss/tok 3.2061 (3.1047)	LR 1.250e-04
0: TRAIN [7][3370/7762]	Time 0.268 (0.328)	Data 1.03e-04 (1.98e-04)	Tok/s 39134 (43103)	Loss/tok 2.9479 (3.1047)	LR 1.250e-04
0: TRAIN [7][3380/7762]	Time 0.268 (0.328)	Data 1.04e-04 (1.97e-04)	Tok/s 38047 (43100)	Loss/tok 2.9481 (3.1047)	LR 1.250e-04
0: TRAIN [7][3390/7762]	Time 0.266 (0.328)	Data 1.02e-04 (1.97e-04)	Tok/s 38774 (43100)	Loss/tok 2.9428 (3.1046)	LR 1.250e-04
0: TRAIN [7][3400/7762]	Time 0.356 (0.327)	Data 1.12e-04 (1.97e-04)	Tok/s 47291 (43091)	Loss/tok 3.1154 (3.1043)	LR 1.250e-04
0: TRAIN [7][3410/7762]	Time 0.264 (0.327)	Data 1.04e-04 (1.97e-04)	Tok/s 39636 (43087)	Loss/tok 2.9047 (3.1040)	LR 1.250e-04
0: TRAIN [7][3420/7762]	Time 0.359 (0.327)	Data 1.02e-04 (1.96e-04)	Tok/s 47312 (43093)	Loss/tok 3.0045 (3.1041)	LR 1.250e-04
0: TRAIN [7][3430/7762]	Time 0.260 (0.327)	Data 1.01e-04 (1.96e-04)	Tok/s 39256 (43093)	Loss/tok 2.9161 (3.1040)	LR 1.250e-04
0: TRAIN [7][3440/7762]	Time 0.365 (0.327)	Data 9.85e-05 (1.96e-04)	Tok/s 45251 (43095)	Loss/tok 3.0981 (3.1041)	LR 1.250e-04
0: TRAIN [7][3450/7762]	Time 0.359 (0.327)	Data 1.01e-04 (1.95e-04)	Tok/s 46473 (43095)	Loss/tok 3.1069 (3.1041)	LR 1.250e-04
0: TRAIN [7][3460/7762]	Time 0.462 (0.327)	Data 1.14e-04 (1.95e-04)	Tok/s 50281 (43095)	Loss/tok 3.4099 (3.1041)	LR 1.250e-04
0: TRAIN [7][3470/7762]	Time 0.263 (0.327)	Data 9.99e-05 (1.95e-04)	Tok/s 38635 (43081)	Loss/tok 2.9216 (3.1038)	LR 1.250e-04
0: TRAIN [7][3480/7762]	Time 0.265 (0.327)	Data 1.05e-04 (1.95e-04)	Tok/s 38626 (43081)	Loss/tok 2.9581 (3.1039)	LR 1.250e-04
0: TRAIN [7][3490/7762]	Time 0.359 (0.327)	Data 1.04e-04 (1.94e-04)	Tok/s 46824 (43086)	Loss/tok 2.9911 (3.1041)	LR 1.250e-04
0: TRAIN [7][3500/7762]	Time 0.456 (0.327)	Data 1.02e-04 (1.94e-04)	Tok/s 51110 (43083)	Loss/tok 3.3601 (3.1041)	LR 1.250e-04
0: TRAIN [7][3510/7762]	Time 0.559 (0.327)	Data 9.75e-05 (1.94e-04)	Tok/s 53179 (43083)	Loss/tok 3.4547 (3.1042)	LR 1.250e-04
0: TRAIN [7][3520/7762]	Time 0.357 (0.327)	Data 1.03e-04 (1.94e-04)	Tok/s 47623 (43076)	Loss/tok 3.1137 (3.1040)	LR 1.250e-04
0: TRAIN [7][3530/7762]	Time 0.262 (0.327)	Data 1.06e-04 (1.93e-04)	Tok/s 39816 (43074)	Loss/tok 2.8686 (3.1043)	LR 1.250e-04
0: TRAIN [7][3540/7762]	Time 0.463 (0.327)	Data 1.03e-04 (1.93e-04)	Tok/s 50268 (43075)	Loss/tok 3.1968 (3.1041)	LR 1.250e-04
0: TRAIN [7][3550/7762]	Time 0.460 (0.327)	Data 1.01e-04 (1.93e-04)	Tok/s 50841 (43061)	Loss/tok 3.2151 (3.1039)	LR 1.250e-04
0: TRAIN [7][3560/7762]	Time 0.261 (0.327)	Data 1.20e-04 (1.93e-04)	Tok/s 39526 (43058)	Loss/tok 2.9249 (3.1038)	LR 1.250e-04
0: TRAIN [7][3570/7762]	Time 0.253 (0.327)	Data 1.17e-04 (1.92e-04)	Tok/s 41046 (43057)	Loss/tok 2.9017 (3.1037)	LR 1.250e-04
0: TRAIN [7][3580/7762]	Time 0.359 (0.327)	Data 1.07e-04 (1.92e-04)	Tok/s 47051 (43064)	Loss/tok 3.1076 (3.1037)	LR 1.250e-04
0: TRAIN [7][3590/7762]	Time 0.358 (0.327)	Data 1.05e-04 (1.92e-04)	Tok/s 46591 (43062)	Loss/tok 3.0069 (3.1036)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][3600/7762]	Time 0.258 (0.327)	Data 1.14e-04 (1.92e-04)	Tok/s 39971 (43061)	Loss/tok 2.9198 (3.1037)	LR 1.250e-04
0: TRAIN [7][3610/7762]	Time 0.447 (0.327)	Data 1.02e-04 (1.91e-04)	Tok/s 51612 (43058)	Loss/tok 3.4508 (3.1037)	LR 1.250e-04
0: TRAIN [7][3620/7762]	Time 0.178 (0.327)	Data 1.06e-04 (1.91e-04)	Tok/s 29890 (43055)	Loss/tok 2.5755 (3.1041)	LR 1.250e-04
0: TRAIN [7][3630/7762]	Time 0.178 (0.327)	Data 1.14e-04 (1.91e-04)	Tok/s 30219 (43058)	Loss/tok 2.5813 (3.1041)	LR 1.250e-04
0: TRAIN [7][3640/7762]	Time 0.358 (0.327)	Data 1.03e-04 (1.91e-04)	Tok/s 47473 (43050)	Loss/tok 3.1781 (3.1041)	LR 1.250e-04
0: TRAIN [7][3650/7762]	Time 0.174 (0.327)	Data 1.03e-04 (1.90e-04)	Tok/s 30533 (43055)	Loss/tok 2.5122 (3.1041)	LR 1.250e-04
0: TRAIN [7][3660/7762]	Time 0.359 (0.327)	Data 1.02e-04 (1.90e-04)	Tok/s 47393 (43058)	Loss/tok 3.1332 (3.1042)	LR 1.250e-04
0: TRAIN [7][3670/7762]	Time 0.265 (0.327)	Data 9.87e-05 (1.90e-04)	Tok/s 39072 (43052)	Loss/tok 3.0107 (3.1040)	LR 1.250e-04
0: TRAIN [7][3680/7762]	Time 0.360 (0.327)	Data 1.00e-04 (1.90e-04)	Tok/s 46283 (43056)	Loss/tok 3.0338 (3.1039)	LR 1.250e-04
0: TRAIN [7][3690/7762]	Time 0.260 (0.327)	Data 1.02e-04 (1.90e-04)	Tok/s 39802 (43048)	Loss/tok 2.7990 (3.1038)	LR 1.250e-04
0: TRAIN [7][3700/7762]	Time 0.584 (0.327)	Data 1.01e-04 (1.89e-04)	Tok/s 51570 (43046)	Loss/tok 3.4365 (3.1037)	LR 1.250e-04
0: TRAIN [7][3710/7762]	Time 0.256 (0.327)	Data 1.09e-04 (1.89e-04)	Tok/s 40129 (43046)	Loss/tok 2.9038 (3.1039)	LR 1.250e-04
0: TRAIN [7][3720/7762]	Time 0.352 (0.327)	Data 1.06e-04 (1.89e-04)	Tok/s 47723 (43054)	Loss/tok 3.1021 (3.1043)	LR 1.250e-04
0: TRAIN [7][3730/7762]	Time 0.446 (0.327)	Data 1.05e-04 (1.89e-04)	Tok/s 52174 (43057)	Loss/tok 3.3102 (3.1043)	LR 1.250e-04
0: TRAIN [7][3740/7762]	Time 0.258 (0.327)	Data 1.10e-04 (1.88e-04)	Tok/s 40027 (43059)	Loss/tok 2.9386 (3.1046)	LR 1.250e-04
0: TRAIN [7][3750/7762]	Time 0.362 (0.327)	Data 1.03e-04 (1.88e-04)	Tok/s 45937 (43059)	Loss/tok 3.1467 (3.1047)	LR 1.250e-04
0: TRAIN [7][3760/7762]	Time 0.260 (0.327)	Data 1.02e-04 (1.88e-04)	Tok/s 40286 (43064)	Loss/tok 2.9186 (3.1047)	LR 1.250e-04
0: TRAIN [7][3770/7762]	Time 0.462 (0.327)	Data 1.01e-04 (1.88e-04)	Tok/s 50826 (43063)	Loss/tok 3.2613 (3.1049)	LR 1.250e-04
0: TRAIN [7][3780/7762]	Time 0.269 (0.327)	Data 1.24e-04 (1.88e-04)	Tok/s 38570 (43067)	Loss/tok 3.0018 (3.1050)	LR 1.250e-04
0: TRAIN [7][3790/7762]	Time 0.447 (0.327)	Data 1.03e-04 (1.87e-04)	Tok/s 51532 (43063)	Loss/tok 3.3007 (3.1049)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][3800/7762]	Time 0.582 (0.327)	Data 1.03e-04 (1.87e-04)	Tok/s 50562 (43066)	Loss/tok 3.6230 (3.1052)	LR 1.250e-04
0: TRAIN [7][3810/7762]	Time 0.178 (0.327)	Data 1.05e-04 (1.87e-04)	Tok/s 29249 (43060)	Loss/tok 2.5237 (3.1050)	LR 1.250e-04
0: TRAIN [7][3820/7762]	Time 0.265 (0.327)	Data 9.73e-05 (1.87e-04)	Tok/s 38632 (43055)	Loss/tok 2.9297 (3.1049)	LR 1.250e-04
0: TRAIN [7][3830/7762]	Time 0.257 (0.327)	Data 1.18e-04 (1.86e-04)	Tok/s 39596 (43051)	Loss/tok 2.8716 (3.1049)	LR 1.250e-04
0: TRAIN [7][3840/7762]	Time 0.259 (0.327)	Data 9.87e-05 (1.86e-04)	Tok/s 40435 (43049)	Loss/tok 2.8664 (3.1049)	LR 1.250e-04
0: TRAIN [7][3850/7762]	Time 0.356 (0.327)	Data 1.16e-04 (1.86e-04)	Tok/s 47401 (43054)	Loss/tok 3.2474 (3.1052)	LR 1.250e-04
0: TRAIN [7][3860/7762]	Time 0.356 (0.327)	Data 1.01e-04 (1.86e-04)	Tok/s 48053 (43056)	Loss/tok 3.0332 (3.1052)	LR 1.250e-04
0: TRAIN [7][3870/7762]	Time 0.466 (0.327)	Data 1.02e-04 (1.86e-04)	Tok/s 50175 (43061)	Loss/tok 3.1947 (3.1052)	LR 1.250e-04
0: TRAIN [7][3880/7762]	Time 0.266 (0.327)	Data 2.35e-04 (1.85e-04)	Tok/s 38469 (43065)	Loss/tok 2.9060 (3.1052)	LR 1.250e-04
0: TRAIN [7][3890/7762]	Time 0.178 (0.327)	Data 1.01e-04 (1.85e-04)	Tok/s 30095 (43062)	Loss/tok 2.5843 (3.1051)	LR 1.250e-04
0: TRAIN [7][3900/7762]	Time 0.264 (0.327)	Data 9.63e-05 (1.85e-04)	Tok/s 38993 (43061)	Loss/tok 2.8673 (3.1050)	LR 1.250e-04
0: TRAIN [7][3910/7762]	Time 0.260 (0.327)	Data 1.17e-04 (1.85e-04)	Tok/s 39295 (43058)	Loss/tok 2.9033 (3.1049)	LR 1.250e-04
0: TRAIN [7][3920/7762]	Time 0.356 (0.327)	Data 1.01e-04 (1.85e-04)	Tok/s 47252 (43064)	Loss/tok 3.1305 (3.1050)	LR 1.250e-04
0: TRAIN [7][3930/7762]	Time 0.466 (0.327)	Data 1.03e-04 (1.84e-04)	Tok/s 50150 (43066)	Loss/tok 3.2134 (3.1049)	LR 1.250e-04
0: TRAIN [7][3940/7762]	Time 0.259 (0.327)	Data 1.01e-04 (1.84e-04)	Tok/s 39865 (43060)	Loss/tok 2.9297 (3.1047)	LR 1.250e-04
0: TRAIN [7][3950/7762]	Time 0.259 (0.327)	Data 1.05e-04 (1.84e-04)	Tok/s 40235 (43063)	Loss/tok 2.8685 (3.1049)	LR 1.250e-04
0: TRAIN [7][3960/7762]	Time 0.268 (0.327)	Data 1.14e-04 (1.84e-04)	Tok/s 38800 (43058)	Loss/tok 2.9374 (3.1048)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][3970/7762]	Time 0.575 (0.327)	Data 1.02e-04 (1.84e-04)	Tok/s 52368 (43063)	Loss/tok 3.4941 (3.1053)	LR 1.250e-04
0: TRAIN [7][3980/7762]	Time 0.354 (0.327)	Data 1.04e-04 (1.83e-04)	Tok/s 47737 (43064)	Loss/tok 3.0395 (3.1051)	LR 1.250e-04
0: TRAIN [7][3990/7762]	Time 0.357 (0.327)	Data 1.17e-04 (1.83e-04)	Tok/s 46227 (43064)	Loss/tok 3.1160 (3.1051)	LR 1.250e-04
0: TRAIN [7][4000/7762]	Time 0.587 (0.327)	Data 1.03e-04 (1.83e-04)	Tok/s 49883 (43065)	Loss/tok 3.6664 (3.1054)	LR 1.250e-04
0: TRAIN [7][4010/7762]	Time 0.260 (0.327)	Data 1.04e-04 (1.83e-04)	Tok/s 39739 (43061)	Loss/tok 2.8404 (3.1051)	LR 1.250e-04
0: TRAIN [7][4020/7762]	Time 0.444 (0.327)	Data 1.05e-04 (1.83e-04)	Tok/s 52121 (43067)	Loss/tok 3.2458 (3.1052)	LR 1.250e-04
0: TRAIN [7][4030/7762]	Time 0.586 (0.327)	Data 1.01e-04 (1.82e-04)	Tok/s 50416 (43074)	Loss/tok 3.4734 (3.1056)	LR 1.250e-04
0: TRAIN [7][4040/7762]	Time 0.263 (0.327)	Data 1.01e-04 (1.82e-04)	Tok/s 38643 (43074)	Loss/tok 2.9823 (3.1055)	LR 1.250e-04
0: TRAIN [7][4050/7762]	Time 0.356 (0.327)	Data 1.01e-04 (1.82e-04)	Tok/s 47964 (43077)	Loss/tok 3.0633 (3.1055)	LR 1.250e-04
0: TRAIN [7][4060/7762]	Time 0.365 (0.327)	Data 1.02e-04 (1.82e-04)	Tok/s 45742 (43073)	Loss/tok 3.1227 (3.1053)	LR 1.250e-04
0: TRAIN [7][4070/7762]	Time 0.362 (0.327)	Data 1.03e-04 (1.82e-04)	Tok/s 46521 (43074)	Loss/tok 3.1655 (3.1053)	LR 1.250e-04
0: TRAIN [7][4080/7762]	Time 0.177 (0.327)	Data 1.07e-04 (1.81e-04)	Tok/s 29508 (43070)	Loss/tok 2.4880 (3.1054)	LR 1.250e-04
0: TRAIN [7][4090/7762]	Time 0.362 (0.327)	Data 1.10e-04 (1.81e-04)	Tok/s 46122 (43076)	Loss/tok 3.1605 (3.1056)	LR 1.250e-04
0: TRAIN [7][4100/7762]	Time 0.343 (0.327)	Data 1.01e-04 (1.81e-04)	Tok/s 48990 (43083)	Loss/tok 3.1499 (3.1056)	LR 1.250e-04
0: TRAIN [7][4110/7762]	Time 0.267 (0.327)	Data 9.97e-05 (1.81e-04)	Tok/s 37204 (43074)	Loss/tok 2.9769 (3.1053)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][4120/7762]	Time 0.464 (0.327)	Data 1.16e-04 (1.81e-04)	Tok/s 50532 (43086)	Loss/tok 3.3178 (3.1058)	LR 1.250e-04
0: TRAIN [7][4130/7762]	Time 0.264 (0.327)	Data 9.89e-05 (1.81e-04)	Tok/s 38651 (43086)	Loss/tok 2.8253 (3.1057)	LR 1.250e-04
0: TRAIN [7][4140/7762]	Time 0.174 (0.327)	Data 9.78e-05 (1.80e-04)	Tok/s 30615 (43085)	Loss/tok 2.5145 (3.1057)	LR 1.250e-04
0: TRAIN [7][4150/7762]	Time 0.360 (0.327)	Data 1.04e-04 (1.80e-04)	Tok/s 46066 (43088)	Loss/tok 3.1830 (3.1057)	LR 1.250e-04
0: TRAIN [7][4160/7762]	Time 0.259 (0.327)	Data 1.01e-04 (1.80e-04)	Tok/s 40330 (43083)	Loss/tok 2.8755 (3.1056)	LR 1.250e-04
0: TRAIN [7][4170/7762]	Time 0.262 (0.327)	Data 1.06e-04 (1.80e-04)	Tok/s 39347 (43086)	Loss/tok 2.9076 (3.1055)	LR 1.250e-04
0: TRAIN [7][4180/7762]	Time 0.259 (0.327)	Data 1.07e-04 (1.80e-04)	Tok/s 39868 (43088)	Loss/tok 2.9549 (3.1056)	LR 1.250e-04
0: TRAIN [7][4190/7762]	Time 0.261 (0.327)	Data 1.00e-04 (1.80e-04)	Tok/s 39332 (43088)	Loss/tok 3.0416 (3.1057)	LR 1.250e-04
0: TRAIN [7][4200/7762]	Time 0.264 (0.327)	Data 9.89e-05 (1.79e-04)	Tok/s 39290 (43084)	Loss/tok 2.9277 (3.1056)	LR 1.250e-04
0: TRAIN [7][4210/7762]	Time 0.264 (0.327)	Data 9.92e-05 (1.79e-04)	Tok/s 39395 (43085)	Loss/tok 2.8328 (3.1054)	LR 1.250e-04
0: TRAIN [7][4220/7762]	Time 0.362 (0.327)	Data 1.03e-04 (1.79e-04)	Tok/s 47058 (43082)	Loss/tok 3.0283 (3.1055)	LR 1.250e-04
0: TRAIN [7][4230/7762]	Time 0.585 (0.327)	Data 1.05e-04 (1.79e-04)	Tok/s 51035 (43081)	Loss/tok 3.3991 (3.1055)	LR 1.250e-04
0: TRAIN [7][4240/7762]	Time 0.174 (0.327)	Data 1.02e-04 (1.79e-04)	Tok/s 29925 (43076)	Loss/tok 2.6270 (3.1054)	LR 1.250e-04
0: TRAIN [7][4250/7762]	Time 0.262 (0.327)	Data 1.04e-04 (1.78e-04)	Tok/s 39038 (43072)	Loss/tok 2.9289 (3.1053)	LR 1.250e-04
0: TRAIN [7][4260/7762]	Time 0.262 (0.327)	Data 1.03e-04 (1.78e-04)	Tok/s 38776 (43072)	Loss/tok 2.9398 (3.1053)	LR 1.250e-04
0: TRAIN [7][4270/7762]	Time 0.590 (0.327)	Data 1.01e-04 (1.78e-04)	Tok/s 50044 (43075)	Loss/tok 3.4304 (3.1054)	LR 1.250e-04
0: TRAIN [7][4280/7762]	Time 0.438 (0.327)	Data 1.17e-04 (1.78e-04)	Tok/s 53124 (43075)	Loss/tok 3.3197 (3.1055)	LR 1.250e-04
0: TRAIN [7][4290/7762]	Time 0.461 (0.327)	Data 1.15e-04 (1.78e-04)	Tok/s 50379 (43069)	Loss/tok 3.2297 (3.1053)	LR 1.250e-04
0: TRAIN [7][4300/7762]	Time 0.589 (0.327)	Data 1.05e-04 (1.78e-04)	Tok/s 51322 (43072)	Loss/tok 3.3488 (3.1054)	LR 1.250e-04
0: TRAIN [7][4310/7762]	Time 0.262 (0.327)	Data 9.99e-05 (1.77e-04)	Tok/s 39771 (43074)	Loss/tok 2.9892 (3.1052)	LR 1.250e-04
0: TRAIN [7][4320/7762]	Time 0.363 (0.327)	Data 9.94e-05 (1.77e-04)	Tok/s 46605 (43076)	Loss/tok 3.1305 (3.1053)	LR 1.250e-04
0: TRAIN [7][4330/7762]	Time 0.466 (0.327)	Data 1.01e-04 (1.77e-04)	Tok/s 50184 (43080)	Loss/tok 3.2708 (3.1054)	LR 1.250e-04
0: TRAIN [7][4340/7762]	Time 0.266 (0.327)	Data 1.17e-04 (1.77e-04)	Tok/s 39584 (43077)	Loss/tok 2.9457 (3.1051)	LR 1.250e-04
0: TRAIN [7][4350/7762]	Time 0.366 (0.327)	Data 9.92e-05 (1.77e-04)	Tok/s 46311 (43076)	Loss/tok 3.0872 (3.1051)	LR 1.250e-04
0: TRAIN [7][4360/7762]	Time 0.265 (0.327)	Data 1.03e-04 (1.77e-04)	Tok/s 39041 (43082)	Loss/tok 2.9585 (3.1052)	LR 1.250e-04
0: TRAIN [7][4370/7762]	Time 0.342 (0.327)	Data 1.06e-04 (1.76e-04)	Tok/s 48219 (43086)	Loss/tok 3.1378 (3.1053)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [7][4380/7762]	Time 0.368 (0.327)	Data 9.80e-05 (1.76e-04)	Tok/s 44747 (43085)	Loss/tok 3.1202 (3.1053)	LR 1.250e-04
0: TRAIN [7][4390/7762]	Time 0.260 (0.327)	Data 1.03e-04 (1.76e-04)	Tok/s 39468 (43081)	Loss/tok 2.8095 (3.1052)	LR 1.250e-04
0: TRAIN [7][4400/7762]	Time 0.354 (0.327)	Data 1.02e-04 (1.76e-04)	Tok/s 47417 (43084)	Loss/tok 3.1492 (3.1052)	LR 1.250e-04
0: TRAIN [7][4410/7762]	Time 0.369 (0.327)	Data 1.19e-04 (1.76e-04)	Tok/s 45624 (43082)	Loss/tok 3.1389 (3.1054)	LR 1.250e-04
0: TRAIN [7][4420/7762]	Time 0.172 (0.327)	Data 1.01e-04 (1.76e-04)	Tok/s 30651 (43085)	Loss/tok 2.5700 (3.1055)	LR 1.250e-04
0: TRAIN [7][4430/7762]	Time 0.462 (0.327)	Data 9.97e-05 (1.75e-04)	Tok/s 50704 (43088)	Loss/tok 3.3267 (3.1056)	LR 1.250e-04
0: TRAIN [7][4440/7762]	Time 0.360 (0.327)	Data 1.04e-04 (1.75e-04)	Tok/s 47079 (43089)	Loss/tok 3.0111 (3.1056)	LR 1.250e-04
0: TRAIN [7][4450/7762]	Time 0.263 (0.327)	Data 1.15e-04 (1.75e-04)	Tok/s 39398 (43084)	Loss/tok 2.8777 (3.1053)	LR 1.250e-04
0: TRAIN [7][4460/7762]	Time 0.260 (0.327)	Data 1.02e-04 (1.75e-04)	Tok/s 39613 (43084)	Loss/tok 3.0179 (3.1054)	LR 1.250e-04
0: TRAIN [7][4470/7762]	Time 0.347 (0.327)	Data 1.12e-04 (1.75e-04)	Tok/s 48110 (43077)	Loss/tok 3.1363 (3.1052)	LR 1.250e-04
0: TRAIN [7][4480/7762]	Time 0.262 (0.327)	Data 1.00e-04 (1.75e-04)	Tok/s 39409 (43075)	Loss/tok 2.9920 (3.1051)	LR 1.250e-04
0: TRAIN [7][4490/7762]	Time 0.258 (0.327)	Data 1.01e-04 (1.74e-04)	Tok/s 40026 (43070)	Loss/tok 3.0078 (3.1049)	LR 1.250e-04
0: TRAIN [7][4500/7762]	Time 0.357 (0.327)	Data 9.80e-05 (1.74e-04)	Tok/s 47329 (43068)	Loss/tok 3.0021 (3.1047)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][4510/7762]	Time 0.364 (0.327)	Data 1.00e-04 (1.74e-04)	Tok/s 46136 (43073)	Loss/tok 3.2061 (3.1047)	LR 1.250e-04
0: TRAIN [7][4520/7762]	Time 0.260 (0.327)	Data 1.03e-04 (1.74e-04)	Tok/s 39751 (43074)	Loss/tok 2.8847 (3.1047)	LR 1.250e-04
0: TRAIN [7][4530/7762]	Time 0.364 (0.327)	Data 1.02e-04 (1.74e-04)	Tok/s 45320 (43078)	Loss/tok 3.1743 (3.1047)	LR 1.250e-04
0: TRAIN [7][4540/7762]	Time 0.264 (0.327)	Data 1.01e-04 (1.74e-04)	Tok/s 39308 (43076)	Loss/tok 2.9151 (3.1047)	LR 1.250e-04
0: TRAIN [7][4550/7762]	Time 0.363 (0.327)	Data 1.15e-04 (1.74e-04)	Tok/s 46382 (43076)	Loss/tok 3.0901 (3.1047)	LR 1.250e-04
0: TRAIN [7][4560/7762]	Time 0.265 (0.327)	Data 1.01e-04 (1.73e-04)	Tok/s 39327 (43078)	Loss/tok 2.7833 (3.1048)	LR 1.250e-04
0: TRAIN [7][4570/7762]	Time 0.253 (0.327)	Data 9.68e-05 (1.73e-04)	Tok/s 40240 (43072)	Loss/tok 2.9978 (3.1046)	LR 1.250e-04
0: TRAIN [7][4580/7762]	Time 0.458 (0.327)	Data 1.02e-04 (1.73e-04)	Tok/s 50888 (43071)	Loss/tok 3.2653 (3.1046)	LR 1.250e-04
0: TRAIN [7][4590/7762]	Time 0.267 (0.327)	Data 9.73e-05 (1.73e-04)	Tok/s 38798 (43062)	Loss/tok 2.8804 (3.1045)	LR 1.250e-04
0: TRAIN [7][4600/7762]	Time 0.354 (0.327)	Data 9.80e-05 (1.73e-04)	Tok/s 47358 (43062)	Loss/tok 3.0686 (3.1044)	LR 1.250e-04
0: TRAIN [7][4610/7762]	Time 0.255 (0.327)	Data 1.02e-04 (1.73e-04)	Tok/s 39775 (43063)	Loss/tok 3.0215 (3.1044)	LR 1.250e-04
0: TRAIN [7][4620/7762]	Time 0.363 (0.327)	Data 9.89e-05 (1.72e-04)	Tok/s 46750 (43063)	Loss/tok 3.0324 (3.1045)	LR 1.250e-04
0: TRAIN [7][4630/7762]	Time 0.358 (0.327)	Data 1.01e-04 (1.72e-04)	Tok/s 46968 (43067)	Loss/tok 3.0868 (3.1046)	LR 1.250e-04
0: TRAIN [7][4640/7762]	Time 0.262 (0.327)	Data 1.05e-04 (1.72e-04)	Tok/s 40337 (43067)	Loss/tok 2.8928 (3.1046)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][4650/7762]	Time 0.363 (0.327)	Data 1.18e-04 (1.72e-04)	Tok/s 47492 (43074)	Loss/tok 3.1069 (3.1049)	LR 1.250e-04
0: TRAIN [7][4660/7762]	Time 0.175 (0.327)	Data 1.15e-04 (1.72e-04)	Tok/s 30165 (43074)	Loss/tok 2.5751 (3.1049)	LR 1.250e-04
0: TRAIN [7][4670/7762]	Time 0.460 (0.327)	Data 1.17e-04 (1.72e-04)	Tok/s 51026 (43075)	Loss/tok 3.2202 (3.1049)	LR 1.250e-04
0: TRAIN [7][4680/7762]	Time 0.586 (0.327)	Data 9.78e-05 (1.72e-04)	Tok/s 50958 (43078)	Loss/tok 3.4575 (3.1049)	LR 1.250e-04
0: TRAIN [7][4690/7762]	Time 0.262 (0.327)	Data 1.16e-04 (1.72e-04)	Tok/s 39347 (43077)	Loss/tok 2.8588 (3.1050)	LR 1.250e-04
0: TRAIN [7][4700/7762]	Time 0.457 (0.327)	Data 9.87e-05 (1.71e-04)	Tok/s 51638 (43079)	Loss/tok 3.3230 (3.1052)	LR 1.250e-04
0: TRAIN [7][4710/7762]	Time 0.363 (0.327)	Data 1.04e-04 (1.71e-04)	Tok/s 46044 (43082)	Loss/tok 3.1700 (3.1053)	LR 1.250e-04
0: TRAIN [7][4720/7762]	Time 0.176 (0.327)	Data 1.07e-04 (1.71e-04)	Tok/s 29840 (43072)	Loss/tok 2.6299 (3.1051)	LR 1.250e-04
0: TRAIN [7][4730/7762]	Time 0.354 (0.327)	Data 1.08e-04 (1.71e-04)	Tok/s 47865 (43070)	Loss/tok 3.0023 (3.1050)	LR 1.250e-04
0: TRAIN [7][4740/7762]	Time 0.584 (0.327)	Data 1.03e-04 (1.71e-04)	Tok/s 50974 (43080)	Loss/tok 3.4483 (3.1053)	LR 1.250e-04
0: TRAIN [7][4750/7762]	Time 0.463 (0.327)	Data 1.09e-04 (1.71e-04)	Tok/s 50090 (43085)	Loss/tok 3.3069 (3.1056)	LR 1.250e-04
0: TRAIN [7][4760/7762]	Time 0.365 (0.327)	Data 1.02e-04 (1.71e-04)	Tok/s 45774 (43088)	Loss/tok 3.0539 (3.1056)	LR 1.250e-04
0: TRAIN [7][4770/7762]	Time 0.459 (0.327)	Data 9.56e-05 (1.70e-04)	Tok/s 51612 (43093)	Loss/tok 3.2534 (3.1057)	LR 1.250e-04
0: TRAIN [7][4780/7762]	Time 0.363 (0.327)	Data 1.01e-04 (1.70e-04)	Tok/s 45501 (43088)	Loss/tok 3.0905 (3.1054)	LR 1.250e-04
0: TRAIN [7][4790/7762]	Time 0.341 (0.327)	Data 1.02e-04 (1.70e-04)	Tok/s 48532 (43085)	Loss/tok 3.1307 (3.1053)	LR 1.250e-04
0: TRAIN [7][4800/7762]	Time 0.257 (0.327)	Data 1.07e-04 (1.70e-04)	Tok/s 41124 (43090)	Loss/tok 2.9045 (3.1054)	LR 1.250e-04
0: TRAIN [7][4810/7762]	Time 0.265 (0.327)	Data 1.01e-04 (1.70e-04)	Tok/s 39073 (43083)	Loss/tok 2.8874 (3.1052)	LR 1.250e-04
0: TRAIN [7][4820/7762]	Time 0.175 (0.327)	Data 9.92e-05 (1.70e-04)	Tok/s 30191 (43080)	Loss/tok 2.5010 (3.1052)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][4830/7762]	Time 0.260 (0.327)	Data 9.75e-05 (1.70e-04)	Tok/s 40204 (43074)	Loss/tok 2.9027 (3.1050)	LR 1.250e-04
0: TRAIN [7][4840/7762]	Time 0.366 (0.327)	Data 1.00e-04 (1.69e-04)	Tok/s 45459 (43075)	Loss/tok 3.2133 (3.1053)	LR 1.250e-04
0: TRAIN [7][4850/7762]	Time 0.266 (0.327)	Data 1.03e-04 (1.69e-04)	Tok/s 39775 (43079)	Loss/tok 2.9682 (3.1055)	LR 1.250e-04
0: TRAIN [7][4860/7762]	Time 0.263 (0.327)	Data 9.94e-05 (1.69e-04)	Tok/s 39871 (43070)	Loss/tok 2.9030 (3.1052)	LR 1.250e-04
0: TRAIN [7][4870/7762]	Time 0.465 (0.327)	Data 9.94e-05 (1.69e-04)	Tok/s 49826 (43072)	Loss/tok 3.3081 (3.1052)	LR 1.250e-04
0: TRAIN [7][4880/7762]	Time 0.265 (0.327)	Data 9.97e-05 (1.69e-04)	Tok/s 38470 (43067)	Loss/tok 3.0076 (3.1051)	LR 1.250e-04
0: TRAIN [7][4890/7762]	Time 0.353 (0.327)	Data 9.97e-05 (1.69e-04)	Tok/s 48067 (43070)	Loss/tok 3.1065 (3.1050)	LR 1.250e-04
0: TRAIN [7][4900/7762]	Time 0.350 (0.327)	Data 1.05e-04 (1.69e-04)	Tok/s 47985 (43078)	Loss/tok 3.0835 (3.1053)	LR 1.250e-04
0: TRAIN [7][4910/7762]	Time 0.349 (0.327)	Data 1.01e-04 (1.68e-04)	Tok/s 47769 (43078)	Loss/tok 3.1520 (3.1053)	LR 1.250e-04
0: TRAIN [7][4920/7762]	Time 0.264 (0.327)	Data 1.09e-04 (1.68e-04)	Tok/s 37696 (43079)	Loss/tok 2.9000 (3.1053)	LR 1.250e-04
0: TRAIN [7][4930/7762]	Time 0.265 (0.327)	Data 9.94e-05 (1.68e-04)	Tok/s 39693 (43075)	Loss/tok 2.9430 (3.1050)	LR 1.250e-04
0: TRAIN [7][4940/7762]	Time 0.464 (0.327)	Data 1.01e-04 (1.68e-04)	Tok/s 50091 (43081)	Loss/tok 3.2662 (3.1051)	LR 1.250e-04
0: TRAIN [7][4950/7762]	Time 0.262 (0.327)	Data 9.94e-05 (1.68e-04)	Tok/s 39529 (43079)	Loss/tok 2.9365 (3.1050)	LR 1.250e-04
0: TRAIN [7][4960/7762]	Time 0.460 (0.327)	Data 9.78e-05 (1.68e-04)	Tok/s 51104 (43073)	Loss/tok 3.2582 (3.1049)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][4970/7762]	Time 0.361 (0.327)	Data 1.14e-04 (1.68e-04)	Tok/s 46789 (43072)	Loss/tok 3.0916 (3.1050)	LR 1.250e-04
0: TRAIN [7][4980/7762]	Time 0.357 (0.327)	Data 1.22e-04 (1.68e-04)	Tok/s 46459 (43076)	Loss/tok 3.1269 (3.1052)	LR 1.250e-04
0: TRAIN [7][4990/7762]	Time 0.460 (0.327)	Data 1.02e-04 (1.67e-04)	Tok/s 50141 (43081)	Loss/tok 3.2803 (3.1055)	LR 1.250e-04
0: TRAIN [7][5000/7762]	Time 0.457 (0.327)	Data 1.03e-04 (1.67e-04)	Tok/s 51080 (43089)	Loss/tok 3.2430 (3.1057)	LR 1.250e-04
0: TRAIN [7][5010/7762]	Time 0.264 (0.327)	Data 9.89e-05 (1.67e-04)	Tok/s 38687 (43088)	Loss/tok 2.8859 (3.1056)	LR 1.250e-04
0: TRAIN [7][5020/7762]	Time 0.589 (0.327)	Data 1.04e-04 (1.67e-04)	Tok/s 50685 (43092)	Loss/tok 3.3146 (3.1059)	LR 1.250e-04
0: TRAIN [7][5030/7762]	Time 0.174 (0.327)	Data 2.32e-04 (1.67e-04)	Tok/s 30868 (43095)	Loss/tok 2.6155 (3.1059)	LR 1.250e-04
0: TRAIN [7][5040/7762]	Time 0.266 (0.327)	Data 9.97e-05 (1.67e-04)	Tok/s 38638 (43086)	Loss/tok 2.8683 (3.1057)	LR 1.250e-04
0: TRAIN [7][5050/7762]	Time 0.261 (0.328)	Data 1.08e-04 (1.67e-04)	Tok/s 39687 (43092)	Loss/tok 2.9395 (3.1061)	LR 1.250e-04
0: TRAIN [7][5060/7762]	Time 0.262 (0.327)	Data 1.21e-04 (1.67e-04)	Tok/s 40137 (43089)	Loss/tok 2.9392 (3.1059)	LR 1.250e-04
0: TRAIN [7][5070/7762]	Time 0.364 (0.327)	Data 1.05e-04 (1.66e-04)	Tok/s 45764 (43084)	Loss/tok 3.0834 (3.1058)	LR 1.250e-04
0: TRAIN [7][5080/7762]	Time 0.253 (0.327)	Data 1.02e-04 (1.66e-04)	Tok/s 41218 (43086)	Loss/tok 3.0135 (3.1057)	LR 1.250e-04
0: TRAIN [7][5090/7762]	Time 0.352 (0.327)	Data 1.04e-04 (1.66e-04)	Tok/s 47841 (43083)	Loss/tok 3.1099 (3.1057)	LR 1.250e-04
0: TRAIN [7][5100/7762]	Time 0.265 (0.327)	Data 1.02e-04 (1.66e-04)	Tok/s 39420 (43083)	Loss/tok 2.8893 (3.1059)	LR 1.250e-04
0: TRAIN [7][5110/7762]	Time 0.340 (0.327)	Data 1.01e-04 (1.66e-04)	Tok/s 49827 (43076)	Loss/tok 3.1688 (3.1057)	LR 1.250e-04
0: TRAIN [7][5120/7762]	Time 0.261 (0.327)	Data 1.11e-04 (1.66e-04)	Tok/s 39419 (43078)	Loss/tok 2.8847 (3.1056)	LR 1.250e-04
0: TRAIN [7][5130/7762]	Time 0.355 (0.327)	Data 1.04e-04 (1.66e-04)	Tok/s 47576 (43071)	Loss/tok 3.1030 (3.1054)	LR 1.250e-04
0: TRAIN [7][5140/7762]	Time 0.262 (0.327)	Data 1.03e-04 (1.66e-04)	Tok/s 39382 (43076)	Loss/tok 2.8914 (3.1057)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][5150/7762]	Time 0.260 (0.327)	Data 1.18e-04 (1.66e-04)	Tok/s 39349 (43075)	Loss/tok 2.8932 (3.1057)	LR 1.250e-04
0: TRAIN [7][5160/7762]	Time 0.359 (0.327)	Data 1.00e-04 (1.65e-04)	Tok/s 46501 (43074)	Loss/tok 3.1907 (3.1056)	LR 1.250e-04
0: TRAIN [7][5170/7762]	Time 0.178 (0.327)	Data 1.04e-04 (1.65e-04)	Tok/s 29688 (43079)	Loss/tok 2.5237 (3.1059)	LR 1.250e-04
0: TRAIN [7][5180/7762]	Time 0.267 (0.327)	Data 1.01e-04 (1.65e-04)	Tok/s 38434 (43074)	Loss/tok 3.0124 (3.1059)	LR 1.250e-04
0: TRAIN [7][5190/7762]	Time 0.562 (0.327)	Data 1.01e-04 (1.65e-04)	Tok/s 53441 (43077)	Loss/tok 3.4040 (3.1061)	LR 1.250e-04
0: TRAIN [7][5200/7762]	Time 0.264 (0.327)	Data 9.87e-05 (1.65e-04)	Tok/s 39159 (43073)	Loss/tok 3.0234 (3.1060)	LR 1.250e-04
0: TRAIN [7][5210/7762]	Time 0.260 (0.327)	Data 1.05e-04 (1.65e-04)	Tok/s 41000 (43072)	Loss/tok 2.9436 (3.1060)	LR 1.250e-04
0: TRAIN [7][5220/7762]	Time 0.587 (0.327)	Data 1.03e-04 (1.65e-04)	Tok/s 49840 (43073)	Loss/tok 3.5458 (3.1061)	LR 1.250e-04
0: TRAIN [7][5230/7762]	Time 0.359 (0.327)	Data 1.03e-04 (1.65e-04)	Tok/s 46382 (43075)	Loss/tok 3.0859 (3.1062)	LR 1.250e-04
0: TRAIN [7][5240/7762]	Time 0.461 (0.327)	Data 1.02e-04 (1.64e-04)	Tok/s 50712 (43076)	Loss/tok 3.2262 (3.1061)	LR 1.250e-04
0: TRAIN [7][5250/7762]	Time 0.264 (0.327)	Data 1.02e-04 (1.64e-04)	Tok/s 39451 (43076)	Loss/tok 2.9637 (3.1061)	LR 1.250e-04
0: TRAIN [7][5260/7762]	Time 0.270 (0.327)	Data 1.16e-04 (1.64e-04)	Tok/s 38494 (43078)	Loss/tok 2.9782 (3.1061)	LR 1.250e-04
0: TRAIN [7][5270/7762]	Time 0.265 (0.327)	Data 9.99e-05 (1.64e-04)	Tok/s 39573 (43080)	Loss/tok 2.9895 (3.1060)	LR 1.250e-04
0: TRAIN [7][5280/7762]	Time 0.558 (0.327)	Data 1.01e-04 (1.64e-04)	Tok/s 53310 (43080)	Loss/tok 3.4617 (3.1061)	LR 1.250e-04
0: TRAIN [7][5290/7762]	Time 0.259 (0.327)	Data 1.05e-04 (1.64e-04)	Tok/s 40067 (43079)	Loss/tok 2.8558 (3.1062)	LR 1.250e-04
0: TRAIN [7][5300/7762]	Time 0.356 (0.327)	Data 1.01e-04 (1.64e-04)	Tok/s 47143 (43078)	Loss/tok 3.1991 (3.1060)	LR 1.250e-04
0: TRAIN [7][5310/7762]	Time 0.343 (0.327)	Data 1.22e-04 (1.64e-04)	Tok/s 48857 (43079)	Loss/tok 3.1073 (3.1060)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][5320/7762]	Time 0.265 (0.327)	Data 1.02e-04 (1.64e-04)	Tok/s 39616 (43078)	Loss/tok 2.8440 (3.1061)	LR 1.250e-04
0: TRAIN [7][5330/7762]	Time 0.262 (0.327)	Data 1.01e-04 (1.63e-04)	Tok/s 39870 (43072)	Loss/tok 2.8858 (3.1060)	LR 1.250e-04
0: TRAIN [7][5340/7762]	Time 0.259 (0.327)	Data 1.03e-04 (1.63e-04)	Tok/s 40069 (43069)	Loss/tok 2.8909 (3.1059)	LR 1.250e-04
0: TRAIN [7][5350/7762]	Time 0.268 (0.327)	Data 1.03e-04 (1.63e-04)	Tok/s 38898 (43069)	Loss/tok 2.9197 (3.1061)	LR 1.250e-04
0: TRAIN [7][5360/7762]	Time 0.174 (0.327)	Data 1.01e-04 (1.63e-04)	Tok/s 29937 (43066)	Loss/tok 2.5622 (3.1061)	LR 1.250e-04
0: TRAIN [7][5370/7762]	Time 0.262 (0.327)	Data 1.01e-04 (1.63e-04)	Tok/s 38791 (43068)	Loss/tok 2.8199 (3.1061)	LR 1.250e-04
0: TRAIN [7][5380/7762]	Time 0.260 (0.327)	Data 1.05e-04 (1.63e-04)	Tok/s 39699 (43063)	Loss/tok 2.9271 (3.1060)	LR 1.250e-04
0: TRAIN [7][5390/7762]	Time 0.263 (0.327)	Data 1.03e-04 (1.63e-04)	Tok/s 39417 (43063)	Loss/tok 2.9069 (3.1058)	LR 1.250e-04
0: TRAIN [7][5400/7762]	Time 0.252 (0.327)	Data 1.01e-04 (1.63e-04)	Tok/s 40972 (43062)	Loss/tok 2.8897 (3.1058)	LR 1.250e-04
0: TRAIN [7][5410/7762]	Time 0.267 (0.327)	Data 1.08e-04 (1.63e-04)	Tok/s 38470 (43068)	Loss/tok 2.7861 (3.1062)	LR 1.250e-04
0: TRAIN [7][5420/7762]	Time 0.447 (0.327)	Data 9.97e-05 (1.63e-04)	Tok/s 52099 (43072)	Loss/tok 3.2467 (3.1062)	LR 1.250e-04
0: TRAIN [7][5430/7762]	Time 0.171 (0.327)	Data 1.02e-04 (1.62e-04)	Tok/s 31475 (43070)	Loss/tok 2.6527 (3.1062)	LR 1.250e-04
0: TRAIN [7][5440/7762]	Time 0.173 (0.327)	Data 1.05e-04 (1.62e-04)	Tok/s 30003 (43069)	Loss/tok 2.6161 (3.1062)	LR 1.250e-04
0: TRAIN [7][5450/7762]	Time 0.260 (0.327)	Data 1.10e-04 (1.62e-04)	Tok/s 39711 (43078)	Loss/tok 2.8319 (3.1065)	LR 1.250e-04
0: TRAIN [7][5460/7762]	Time 0.263 (0.327)	Data 9.97e-05 (1.62e-04)	Tok/s 39839 (43075)	Loss/tok 2.8788 (3.1063)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][5470/7762]	Time 0.266 (0.327)	Data 1.04e-04 (1.62e-04)	Tok/s 39003 (43071)	Loss/tok 2.8789 (3.1065)	LR 1.250e-04
0: TRAIN [7][5480/7762]	Time 0.354 (0.327)	Data 1.15e-04 (1.62e-04)	Tok/s 46859 (43073)	Loss/tok 3.1409 (3.1066)	LR 1.250e-04
0: TRAIN [7][5490/7762]	Time 0.363 (0.327)	Data 1.21e-04 (1.62e-04)	Tok/s 46306 (43073)	Loss/tok 3.0699 (3.1066)	LR 1.250e-04
0: TRAIN [7][5500/7762]	Time 0.263 (0.327)	Data 9.82e-05 (1.62e-04)	Tok/s 39746 (43071)	Loss/tok 2.9512 (3.1064)	LR 1.250e-04
0: TRAIN [7][5510/7762]	Time 0.362 (0.327)	Data 1.05e-04 (1.62e-04)	Tok/s 46769 (43069)	Loss/tok 3.1378 (3.1063)	LR 1.250e-04
0: TRAIN [7][5520/7762]	Time 0.259 (0.327)	Data 1.03e-04 (1.62e-04)	Tok/s 40170 (43072)	Loss/tok 3.0193 (3.1062)	LR 1.250e-04
0: TRAIN [7][5530/7762]	Time 0.360 (0.327)	Data 1.08e-04 (1.61e-04)	Tok/s 47541 (43075)	Loss/tok 3.0330 (3.1062)	LR 1.250e-04
0: TRAIN [7][5540/7762]	Time 0.366 (0.327)	Data 1.05e-04 (1.61e-04)	Tok/s 46250 (43077)	Loss/tok 3.0628 (3.1061)	LR 1.250e-04
0: TRAIN [7][5550/7762]	Time 0.342 (0.327)	Data 1.11e-04 (1.61e-04)	Tok/s 49282 (43084)	Loss/tok 3.0628 (3.1064)	LR 1.250e-04
0: TRAIN [7][5560/7762]	Time 0.359 (0.327)	Data 9.89e-05 (1.61e-04)	Tok/s 47051 (43083)	Loss/tok 3.1453 (3.1063)	LR 1.250e-04
0: TRAIN [7][5570/7762]	Time 0.262 (0.327)	Data 1.08e-04 (1.61e-04)	Tok/s 39976 (43082)	Loss/tok 2.9184 (3.1063)	LR 1.250e-04
0: TRAIN [7][5580/7762]	Time 0.263 (0.327)	Data 1.17e-04 (1.61e-04)	Tok/s 38904 (43083)	Loss/tok 3.0764 (3.1063)	LR 1.250e-04
0: TRAIN [7][5590/7762]	Time 0.179 (0.327)	Data 9.78e-05 (1.61e-04)	Tok/s 29303 (43084)	Loss/tok 2.5479 (3.1063)	LR 1.250e-04
0: TRAIN [7][5600/7762]	Time 0.263 (0.327)	Data 2.34e-04 (1.61e-04)	Tok/s 39660 (43086)	Loss/tok 2.8875 (3.1063)	LR 1.250e-04
0: TRAIN [7][5610/7762]	Time 0.263 (0.327)	Data 1.04e-04 (1.61e-04)	Tok/s 38818 (43087)	Loss/tok 2.9808 (3.1063)	LR 1.250e-04
0: TRAIN [7][5620/7762]	Time 0.174 (0.327)	Data 1.07e-04 (1.61e-04)	Tok/s 29865 (43087)	Loss/tok 2.6041 (3.1062)	LR 1.250e-04
0: TRAIN [7][5630/7762]	Time 0.264 (0.327)	Data 9.85e-05 (1.60e-04)	Tok/s 39947 (43081)	Loss/tok 3.0051 (3.1061)	LR 1.250e-04
0: TRAIN [7][5640/7762]	Time 0.467 (0.327)	Data 1.09e-04 (1.60e-04)	Tok/s 50007 (43083)	Loss/tok 3.1838 (3.1061)	LR 1.250e-04
0: TRAIN [7][5650/7762]	Time 0.263 (0.327)	Data 1.02e-04 (1.60e-04)	Tok/s 40385 (43084)	Loss/tok 2.8791 (3.1061)	LR 1.250e-04
0: TRAIN [7][5660/7762]	Time 0.262 (0.327)	Data 9.68e-05 (1.60e-04)	Tok/s 38896 (43076)	Loss/tok 2.9709 (3.1060)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][5670/7762]	Time 0.265 (0.327)	Data 1.04e-04 (1.60e-04)	Tok/s 38970 (43078)	Loss/tok 2.8635 (3.1062)	LR 1.250e-04
0: TRAIN [7][5680/7762]	Time 0.263 (0.327)	Data 1.17e-04 (1.60e-04)	Tok/s 39225 (43079)	Loss/tok 2.8806 (3.1063)	LR 1.250e-04
0: TRAIN [7][5690/7762]	Time 0.266 (0.327)	Data 9.89e-05 (1.60e-04)	Tok/s 38993 (43083)	Loss/tok 3.0226 (3.1065)	LR 1.250e-04
0: TRAIN [7][5700/7762]	Time 0.366 (0.327)	Data 1.04e-04 (1.60e-04)	Tok/s 45756 (43086)	Loss/tok 3.0836 (3.1067)	LR 1.250e-04
0: TRAIN [7][5710/7762]	Time 0.254 (0.327)	Data 1.00e-04 (1.60e-04)	Tok/s 41444 (43081)	Loss/tok 2.9097 (3.1066)	LR 1.250e-04
0: TRAIN [7][5720/7762]	Time 0.465 (0.327)	Data 9.97e-05 (1.60e-04)	Tok/s 50372 (43085)	Loss/tok 3.3089 (3.1066)	LR 1.250e-04
0: TRAIN [7][5730/7762]	Time 0.458 (0.327)	Data 9.68e-05 (1.59e-04)	Tok/s 50198 (43083)	Loss/tok 3.2948 (3.1066)	LR 1.250e-04
0: TRAIN [7][5740/7762]	Time 0.174 (0.327)	Data 1.33e-04 (1.59e-04)	Tok/s 30028 (43079)	Loss/tok 2.5823 (3.1065)	LR 1.250e-04
0: TRAIN [7][5750/7762]	Time 0.175 (0.327)	Data 1.01e-04 (1.59e-04)	Tok/s 30327 (43073)	Loss/tok 2.5214 (3.1063)	LR 1.250e-04
0: TRAIN [7][5760/7762]	Time 0.172 (0.327)	Data 1.01e-04 (1.59e-04)	Tok/s 30392 (43075)	Loss/tok 2.5323 (3.1063)	LR 1.250e-04
0: TRAIN [7][5770/7762]	Time 0.254 (0.327)	Data 9.85e-05 (1.59e-04)	Tok/s 40471 (43075)	Loss/tok 2.9234 (3.1065)	LR 1.250e-04
0: TRAIN [7][5780/7762]	Time 0.463 (0.327)	Data 9.99e-05 (1.59e-04)	Tok/s 50678 (43079)	Loss/tok 3.1774 (3.1064)	LR 1.250e-04
0: TRAIN [7][5790/7762]	Time 0.258 (0.327)	Data 1.02e-04 (1.59e-04)	Tok/s 39757 (43078)	Loss/tok 2.9048 (3.1063)	LR 1.250e-04
0: TRAIN [7][5800/7762]	Time 0.257 (0.327)	Data 1.02e-04 (1.59e-04)	Tok/s 39494 (43080)	Loss/tok 2.9583 (3.1065)	LR 1.250e-04
0: TRAIN [7][5810/7762]	Time 0.264 (0.327)	Data 9.85e-05 (1.59e-04)	Tok/s 39467 (43076)	Loss/tok 2.9008 (3.1064)	LR 1.250e-04
0: TRAIN [7][5820/7762]	Time 0.444 (0.327)	Data 1.03e-04 (1.59e-04)	Tok/s 52268 (43074)	Loss/tok 3.2655 (3.1063)	LR 1.250e-04
0: TRAIN [7][5830/7762]	Time 0.261 (0.327)	Data 1.28e-04 (1.58e-04)	Tok/s 39344 (43072)	Loss/tok 2.7752 (3.1062)	LR 1.250e-04
0: TRAIN [7][5840/7762]	Time 0.258 (0.327)	Data 1.03e-04 (1.58e-04)	Tok/s 39964 (43072)	Loss/tok 2.8552 (3.1063)	LR 1.250e-04
0: TRAIN [7][5850/7762]	Time 0.363 (0.327)	Data 1.17e-04 (1.58e-04)	Tok/s 46320 (43072)	Loss/tok 3.1234 (3.1063)	LR 1.250e-04
0: TRAIN [7][5860/7762]	Time 0.177 (0.327)	Data 9.92e-05 (1.58e-04)	Tok/s 29524 (43064)	Loss/tok 2.4775 (3.1061)	LR 1.250e-04
0: TRAIN [7][5870/7762]	Time 0.174 (0.327)	Data 1.14e-04 (1.58e-04)	Tok/s 30269 (43062)	Loss/tok 2.5968 (3.1060)	LR 1.250e-04
0: TRAIN [7][5880/7762]	Time 0.357 (0.327)	Data 9.89e-05 (1.58e-04)	Tok/s 47003 (43057)	Loss/tok 3.1480 (3.1059)	LR 1.250e-04
0: TRAIN [7][5890/7762]	Time 0.180 (0.327)	Data 1.01e-04 (1.58e-04)	Tok/s 29282 (43051)	Loss/tok 2.4585 (3.1057)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][5900/7762]	Time 0.174 (0.327)	Data 1.01e-04 (1.58e-04)	Tok/s 29760 (43049)	Loss/tok 2.5000 (3.1057)	LR 1.250e-04
0: TRAIN [7][5910/7762]	Time 0.360 (0.327)	Data 1.18e-04 (1.58e-04)	Tok/s 47952 (43054)	Loss/tok 3.0505 (3.1057)	LR 1.250e-04
0: TRAIN [7][5920/7762]	Time 0.175 (0.327)	Data 1.07e-04 (1.58e-04)	Tok/s 30219 (43056)	Loss/tok 2.5557 (3.1058)	LR 1.250e-04
0: TRAIN [7][5930/7762]	Time 0.253 (0.327)	Data 9.85e-05 (1.58e-04)	Tok/s 40692 (43056)	Loss/tok 2.9259 (3.1057)	LR 1.250e-04
0: TRAIN [7][5940/7762]	Time 0.461 (0.327)	Data 1.02e-04 (1.57e-04)	Tok/s 51142 (43056)	Loss/tok 3.3334 (3.1057)	LR 1.250e-04
0: TRAIN [7][5950/7762]	Time 0.365 (0.327)	Data 1.00e-04 (1.57e-04)	Tok/s 46298 (43051)	Loss/tok 3.0709 (3.1057)	LR 1.250e-04
0: TRAIN [7][5960/7762]	Time 0.258 (0.327)	Data 1.06e-04 (1.57e-04)	Tok/s 40305 (43049)	Loss/tok 2.9052 (3.1056)	LR 1.250e-04
0: TRAIN [7][5970/7762]	Time 0.363 (0.327)	Data 1.02e-04 (1.57e-04)	Tok/s 45121 (43052)	Loss/tok 3.0543 (3.1057)	LR 1.250e-04
0: TRAIN [7][5980/7762]	Time 0.365 (0.327)	Data 1.17e-04 (1.57e-04)	Tok/s 45249 (43054)	Loss/tok 3.1798 (3.1059)	LR 1.250e-04
0: TRAIN [7][5990/7762]	Time 0.259 (0.327)	Data 1.03e-04 (1.57e-04)	Tok/s 39532 (43048)	Loss/tok 2.9268 (3.1057)	LR 1.250e-04
0: TRAIN [7][6000/7762]	Time 0.178 (0.327)	Data 9.75e-05 (1.57e-04)	Tok/s 29954 (43044)	Loss/tok 2.5554 (3.1056)	LR 1.250e-04
0: TRAIN [7][6010/7762]	Time 0.347 (0.327)	Data 1.08e-04 (1.57e-04)	Tok/s 48384 (43048)	Loss/tok 3.0614 (3.1057)	LR 1.250e-04
0: TRAIN [7][6020/7762]	Time 0.446 (0.327)	Data 1.02e-04 (1.57e-04)	Tok/s 52662 (43049)	Loss/tok 3.2737 (3.1057)	LR 1.250e-04
0: TRAIN [7][6030/7762]	Time 0.260 (0.327)	Data 9.92e-05 (1.57e-04)	Tok/s 39724 (43044)	Loss/tok 2.9240 (3.1056)	LR 1.250e-04
0: TRAIN [7][6040/7762]	Time 0.255 (0.327)	Data 9.66e-05 (1.57e-04)	Tok/s 40443 (43048)	Loss/tok 3.0597 (3.1057)	LR 1.250e-04
0: TRAIN [7][6050/7762]	Time 0.178 (0.327)	Data 1.01e-04 (1.56e-04)	Tok/s 29230 (43043)	Loss/tok 2.5054 (3.1056)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][6060/7762]	Time 0.256 (0.327)	Data 1.02e-04 (1.56e-04)	Tok/s 39839 (43041)	Loss/tok 2.9097 (3.1056)	LR 1.250e-04
0: TRAIN [7][6070/7762]	Time 0.354 (0.327)	Data 1.00e-04 (1.56e-04)	Tok/s 47263 (43038)	Loss/tok 3.2323 (3.1055)	LR 1.250e-04
0: TRAIN [7][6080/7762]	Time 0.174 (0.327)	Data 1.04e-04 (1.56e-04)	Tok/s 30075 (43041)	Loss/tok 2.6049 (3.1055)	LR 1.250e-04
0: TRAIN [7][6090/7762]	Time 0.357 (0.327)	Data 1.05e-04 (1.56e-04)	Tok/s 46571 (43043)	Loss/tok 3.1784 (3.1055)	LR 1.250e-04
0: TRAIN [7][6100/7762]	Time 0.267 (0.327)	Data 1.06e-04 (1.56e-04)	Tok/s 38620 (43042)	Loss/tok 3.0053 (3.1055)	LR 1.250e-04
0: TRAIN [7][6110/7762]	Time 0.265 (0.327)	Data 1.06e-04 (1.56e-04)	Tok/s 39229 (43047)	Loss/tok 2.9113 (3.1056)	LR 1.250e-04
0: TRAIN [7][6120/7762]	Time 0.261 (0.327)	Data 9.89e-05 (1.56e-04)	Tok/s 39320 (43039)	Loss/tok 3.0053 (3.1054)	LR 1.250e-04
0: TRAIN [7][6130/7762]	Time 0.365 (0.327)	Data 9.94e-05 (1.56e-04)	Tok/s 46403 (43041)	Loss/tok 3.0498 (3.1053)	LR 1.250e-04
0: TRAIN [7][6140/7762]	Time 0.263 (0.327)	Data 1.17e-04 (1.56e-04)	Tok/s 39732 (43039)	Loss/tok 3.0272 (3.1053)	LR 1.250e-04
0: TRAIN [7][6150/7762]	Time 0.258 (0.327)	Data 1.03e-04 (1.56e-04)	Tok/s 39818 (43041)	Loss/tok 3.0149 (3.1053)	LR 1.250e-04
0: TRAIN [7][6160/7762]	Time 0.263 (0.327)	Data 9.92e-05 (1.56e-04)	Tok/s 39456 (43037)	Loss/tok 2.8198 (3.1052)	LR 1.250e-04
0: TRAIN [7][6170/7762]	Time 0.353 (0.326)	Data 9.89e-05 (1.55e-04)	Tok/s 46688 (43030)	Loss/tok 3.1086 (3.1049)	LR 1.250e-04
0: TRAIN [7][6180/7762]	Time 0.362 (0.326)	Data 1.02e-04 (1.55e-04)	Tok/s 46287 (43029)	Loss/tok 3.1797 (3.1049)	LR 1.250e-04
0: TRAIN [7][6190/7762]	Time 0.168 (0.326)	Data 9.82e-05 (1.55e-04)	Tok/s 31406 (43029)	Loss/tok 2.5560 (3.1050)	LR 1.250e-04
0: TRAIN [7][6200/7762]	Time 0.364 (0.326)	Data 1.18e-04 (1.55e-04)	Tok/s 45923 (43032)	Loss/tok 3.1203 (3.1052)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][6210/7762]	Time 0.254 (0.326)	Data 1.02e-04 (1.55e-04)	Tok/s 39809 (43029)	Loss/tok 2.8550 (3.1051)	LR 1.250e-04
0: TRAIN [7][6220/7762]	Time 0.357 (0.326)	Data 1.03e-04 (1.55e-04)	Tok/s 47880 (43025)	Loss/tok 3.1527 (3.1050)	LR 1.250e-04
0: TRAIN [7][6230/7762]	Time 0.367 (0.326)	Data 1.21e-04 (1.55e-04)	Tok/s 46092 (43023)	Loss/tok 3.0805 (3.1049)	LR 1.250e-04
0: TRAIN [7][6240/7762]	Time 0.358 (0.326)	Data 1.01e-04 (1.55e-04)	Tok/s 46678 (43026)	Loss/tok 3.0827 (3.1049)	LR 1.250e-04
0: TRAIN [7][6250/7762]	Time 0.459 (0.326)	Data 2.40e-04 (1.55e-04)	Tok/s 51143 (43026)	Loss/tok 3.2444 (3.1049)	LR 1.250e-04
0: TRAIN [7][6260/7762]	Time 0.347 (0.326)	Data 9.92e-05 (1.55e-04)	Tok/s 47245 (43030)	Loss/tok 3.2392 (3.1051)	LR 1.250e-04
0: TRAIN [7][6270/7762]	Time 0.363 (0.327)	Data 1.21e-04 (1.55e-04)	Tok/s 46775 (43034)	Loss/tok 3.0485 (3.1052)	LR 1.250e-04
0: TRAIN [7][6280/7762]	Time 0.365 (0.327)	Data 1.03e-04 (1.55e-04)	Tok/s 44765 (43038)	Loss/tok 3.1470 (3.1053)	LR 1.250e-04
0: TRAIN [7][6290/7762]	Time 0.256 (0.327)	Data 1.00e-04 (1.55e-04)	Tok/s 40481 (43034)	Loss/tok 2.9196 (3.1051)	LR 1.250e-04
0: TRAIN [7][6300/7762]	Time 0.176 (0.327)	Data 1.02e-04 (1.54e-04)	Tok/s 30649 (43036)	Loss/tok 2.5967 (3.1051)	LR 1.250e-04
0: TRAIN [7][6310/7762]	Time 0.261 (0.327)	Data 1.05e-04 (1.54e-04)	Tok/s 39211 (43038)	Loss/tok 2.8899 (3.1051)	LR 1.250e-04
0: TRAIN [7][6320/7762]	Time 0.261 (0.327)	Data 2.32e-04 (1.54e-04)	Tok/s 39872 (43037)	Loss/tok 2.9290 (3.1052)	LR 1.250e-04
0: TRAIN [7][6330/7762]	Time 0.342 (0.327)	Data 1.01e-04 (1.54e-04)	Tok/s 49042 (43037)	Loss/tok 3.1128 (3.1051)	LR 1.250e-04
0: TRAIN [7][6340/7762]	Time 0.262 (0.326)	Data 1.17e-04 (1.54e-04)	Tok/s 39140 (43033)	Loss/tok 2.9735 (3.1049)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][6350/7762]	Time 0.588 (0.327)	Data 9.99e-05 (1.54e-04)	Tok/s 50405 (43035)	Loss/tok 3.4355 (3.1051)	LR 1.250e-04
0: TRAIN [7][6360/7762]	Time 0.266 (0.327)	Data 1.17e-04 (1.54e-04)	Tok/s 39477 (43037)	Loss/tok 2.9565 (3.1052)	LR 1.250e-04
0: TRAIN [7][6370/7762]	Time 0.438 (0.327)	Data 1.03e-04 (1.54e-04)	Tok/s 53216 (43037)	Loss/tok 3.2180 (3.1051)	LR 1.250e-04
0: TRAIN [7][6380/7762]	Time 0.344 (0.327)	Data 1.03e-04 (1.54e-04)	Tok/s 48422 (43041)	Loss/tok 3.2027 (3.1051)	LR 1.250e-04
0: TRAIN [7][6390/7762]	Time 0.361 (0.327)	Data 9.97e-05 (1.54e-04)	Tok/s 46570 (43040)	Loss/tok 3.1486 (3.1051)	LR 1.250e-04
0: TRAIN [7][6400/7762]	Time 0.357 (0.327)	Data 1.03e-04 (1.54e-04)	Tok/s 46831 (43039)	Loss/tok 3.1773 (3.1052)	LR 1.250e-04
0: TRAIN [7][6410/7762]	Time 0.176 (0.327)	Data 1.21e-04 (1.54e-04)	Tok/s 30059 (43036)	Loss/tok 2.5893 (3.1052)	LR 1.250e-04
0: TRAIN [7][6420/7762]	Time 0.584 (0.327)	Data 1.03e-04 (1.54e-04)	Tok/s 51135 (43036)	Loss/tok 3.5296 (3.1054)	LR 1.250e-04
0: TRAIN [7][6430/7762]	Time 0.258 (0.327)	Data 9.94e-05 (1.53e-04)	Tok/s 39970 (43033)	Loss/tok 2.9715 (3.1052)	LR 1.250e-04
0: TRAIN [7][6440/7762]	Time 0.264 (0.326)	Data 9.58e-05 (1.53e-04)	Tok/s 39149 (43026)	Loss/tok 2.8704 (3.1050)	LR 1.250e-04
0: TRAIN [7][6450/7762]	Time 0.363 (0.327)	Data 1.07e-04 (1.53e-04)	Tok/s 46243 (43031)	Loss/tok 3.1760 (3.1052)	LR 1.250e-04
0: TRAIN [7][6460/7762]	Time 0.589 (0.327)	Data 1.05e-04 (1.53e-04)	Tok/s 51033 (43032)	Loss/tok 3.3234 (3.1051)	LR 1.250e-04
0: TRAIN [7][6470/7762]	Time 0.350 (0.326)	Data 1.03e-04 (1.53e-04)	Tok/s 47690 (43031)	Loss/tok 3.0666 (3.1051)	LR 1.250e-04
0: TRAIN [7][6480/7762]	Time 0.365 (0.327)	Data 1.06e-04 (1.53e-04)	Tok/s 45825 (43037)	Loss/tok 3.1864 (3.1052)	LR 1.250e-04
0: TRAIN [7][6490/7762]	Time 0.361 (0.327)	Data 1.00e-04 (1.53e-04)	Tok/s 46258 (43034)	Loss/tok 3.1240 (3.1051)	LR 1.250e-04
0: TRAIN [7][6500/7762]	Time 0.254 (0.327)	Data 1.16e-04 (1.53e-04)	Tok/s 40760 (43035)	Loss/tok 2.9743 (3.1051)	LR 1.250e-04
0: TRAIN [7][6510/7762]	Time 0.174 (0.326)	Data 1.02e-04 (1.53e-04)	Tok/s 30657 (43032)	Loss/tok 2.5334 (3.1051)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][6520/7762]	Time 0.462 (0.327)	Data 1.02e-04 (1.53e-04)	Tok/s 50956 (43040)	Loss/tok 3.2235 (3.1053)	LR 1.250e-04
0: TRAIN [7][6530/7762]	Time 0.263 (0.327)	Data 1.01e-04 (1.53e-04)	Tok/s 39256 (43043)	Loss/tok 2.9188 (3.1054)	LR 1.250e-04
0: TRAIN [7][6540/7762]	Time 0.363 (0.327)	Data 9.75e-05 (1.53e-04)	Tok/s 46663 (43041)	Loss/tok 3.1885 (3.1053)	LR 1.250e-04
0: TRAIN [7][6550/7762]	Time 0.465 (0.327)	Data 9.99e-05 (1.53e-04)	Tok/s 50503 (43043)	Loss/tok 3.3827 (3.1053)	LR 1.250e-04
0: TRAIN [7][6560/7762]	Time 0.267 (0.327)	Data 9.87e-05 (1.53e-04)	Tok/s 38796 (43044)	Loss/tok 2.9010 (3.1053)	LR 1.250e-04
0: TRAIN [7][6570/7762]	Time 0.264 (0.327)	Data 1.00e-04 (1.52e-04)	Tok/s 40021 (43039)	Loss/tok 2.9377 (3.1052)	LR 1.250e-04
0: TRAIN [7][6580/7762]	Time 0.466 (0.327)	Data 1.19e-04 (1.52e-04)	Tok/s 49509 (43039)	Loss/tok 3.3059 (3.1051)	LR 1.250e-04
0: TRAIN [7][6590/7762]	Time 0.581 (0.327)	Data 9.75e-05 (1.52e-04)	Tok/s 51788 (43041)	Loss/tok 3.4032 (3.1053)	LR 1.250e-04
0: TRAIN [7][6600/7762]	Time 0.261 (0.327)	Data 1.02e-04 (1.52e-04)	Tok/s 39534 (43040)	Loss/tok 2.9242 (3.1054)	LR 1.250e-04
0: TRAIN [7][6610/7762]	Time 0.260 (0.327)	Data 1.04e-04 (1.52e-04)	Tok/s 39367 (43036)	Loss/tok 2.9871 (3.1053)	LR 1.250e-04
0: TRAIN [7][6620/7762]	Time 0.460 (0.326)	Data 9.89e-05 (1.52e-04)	Tok/s 50772 (43033)	Loss/tok 3.2249 (3.1053)	LR 1.250e-04
0: TRAIN [7][6630/7762]	Time 0.462 (0.327)	Data 1.05e-04 (1.52e-04)	Tok/s 50209 (43036)	Loss/tok 3.3150 (3.1054)	LR 1.250e-04
0: TRAIN [7][6640/7762]	Time 0.449 (0.327)	Data 1.04e-04 (1.52e-04)	Tok/s 52544 (43038)	Loss/tok 3.2476 (3.1053)	LR 1.250e-04
0: TRAIN [7][6650/7762]	Time 0.584 (0.327)	Data 9.94e-05 (1.52e-04)	Tok/s 50455 (43040)	Loss/tok 3.5147 (3.1055)	LR 1.250e-04
0: TRAIN [7][6660/7762]	Time 0.265 (0.327)	Data 1.19e-04 (1.52e-04)	Tok/s 39474 (43045)	Loss/tok 2.8182 (3.1056)	LR 1.250e-04
0: TRAIN [7][6670/7762]	Time 0.345 (0.327)	Data 1.12e-04 (1.52e-04)	Tok/s 48735 (43046)	Loss/tok 3.2315 (3.1056)	LR 1.250e-04
0: TRAIN [7][6680/7762]	Time 0.173 (0.327)	Data 1.01e-04 (1.52e-04)	Tok/s 30455 (43047)	Loss/tok 2.6159 (3.1057)	LR 1.250e-04
0: TRAIN [7][6690/7762]	Time 0.344 (0.327)	Data 1.18e-04 (1.52e-04)	Tok/s 48774 (43047)	Loss/tok 3.0221 (3.1056)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][6700/7762]	Time 0.267 (0.327)	Data 9.63e-05 (1.52e-04)	Tok/s 38497 (43047)	Loss/tok 2.8792 (3.1056)	LR 1.250e-04
0: TRAIN [7][6710/7762]	Time 0.259 (0.327)	Data 1.05e-04 (1.51e-04)	Tok/s 40026 (43052)	Loss/tok 2.9195 (3.1057)	LR 1.250e-04
0: TRAIN [7][6720/7762]	Time 0.269 (0.327)	Data 1.48e-04 (1.51e-04)	Tok/s 38218 (43052)	Loss/tok 2.9234 (3.1059)	LR 1.250e-04
0: TRAIN [7][6730/7762]	Time 0.261 (0.327)	Data 9.89e-05 (1.51e-04)	Tok/s 39118 (43049)	Loss/tok 2.9270 (3.1058)	LR 1.250e-04
0: TRAIN [7][6740/7762]	Time 0.262 (0.327)	Data 1.02e-04 (1.51e-04)	Tok/s 38672 (43048)	Loss/tok 2.9461 (3.1059)	LR 1.250e-04
0: TRAIN [7][6750/7762]	Time 0.267 (0.327)	Data 1.05e-04 (1.51e-04)	Tok/s 38670 (43045)	Loss/tok 2.8309 (3.1057)	LR 1.250e-04
0: TRAIN [7][6760/7762]	Time 0.352 (0.327)	Data 9.97e-05 (1.51e-04)	Tok/s 46858 (43047)	Loss/tok 3.1219 (3.1059)	LR 1.250e-04
0: TRAIN [7][6770/7762]	Time 0.178 (0.327)	Data 1.01e-04 (1.51e-04)	Tok/s 29013 (43047)	Loss/tok 2.5914 (3.1060)	LR 1.250e-04
0: TRAIN [7][6780/7762]	Time 0.266 (0.327)	Data 1.16e-04 (1.51e-04)	Tok/s 38993 (43050)	Loss/tok 2.9502 (3.1060)	LR 1.250e-04
0: TRAIN [7][6790/7762]	Time 0.454 (0.327)	Data 1.04e-04 (1.51e-04)	Tok/s 51884 (43050)	Loss/tok 3.3182 (3.1060)	LR 1.250e-04
0: TRAIN [7][6800/7762]	Time 0.458 (0.327)	Data 1.04e-04 (1.51e-04)	Tok/s 50553 (43049)	Loss/tok 3.1559 (3.1059)	LR 1.250e-04
0: TRAIN [7][6810/7762]	Time 0.260 (0.327)	Data 1.06e-04 (1.51e-04)	Tok/s 39953 (43046)	Loss/tok 2.8485 (3.1058)	LR 1.250e-04
0: TRAIN [7][6820/7762]	Time 0.354 (0.327)	Data 1.21e-04 (1.51e-04)	Tok/s 47727 (43051)	Loss/tok 3.0893 (3.1060)	LR 1.250e-04
0: TRAIN [7][6830/7762]	Time 0.259 (0.327)	Data 1.03e-04 (1.51e-04)	Tok/s 40264 (43047)	Loss/tok 2.9043 (3.1058)	LR 1.250e-04
0: TRAIN [7][6840/7762]	Time 0.265 (0.327)	Data 1.00e-04 (1.51e-04)	Tok/s 38361 (43046)	Loss/tok 2.9281 (3.1058)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][6850/7762]	Time 0.261 (0.327)	Data 1.04e-04 (1.51e-04)	Tok/s 40439 (43045)	Loss/tok 2.9073 (3.1059)	LR 1.250e-04
0: TRAIN [7][6860/7762]	Time 0.466 (0.327)	Data 1.06e-04 (1.50e-04)	Tok/s 50380 (43051)	Loss/tok 3.3001 (3.1060)	LR 1.250e-04
0: TRAIN [7][6870/7762]	Time 0.265 (0.327)	Data 1.02e-04 (1.50e-04)	Tok/s 39291 (43048)	Loss/tok 2.8869 (3.1059)	LR 1.250e-04
0: TRAIN [7][6880/7762]	Time 0.263 (0.327)	Data 1.08e-04 (1.50e-04)	Tok/s 39418 (43050)	Loss/tok 2.8255 (3.1060)	LR 1.250e-04
0: TRAIN [7][6890/7762]	Time 0.361 (0.327)	Data 9.54e-05 (1.50e-04)	Tok/s 46166 (43048)	Loss/tok 3.1625 (3.1058)	LR 1.250e-04
0: TRAIN [7][6900/7762]	Time 0.362 (0.327)	Data 1.10e-04 (1.50e-04)	Tok/s 46403 (43050)	Loss/tok 3.0175 (3.1060)	LR 1.250e-04
0: TRAIN [7][6910/7762]	Time 0.355 (0.327)	Data 1.07e-04 (1.50e-04)	Tok/s 46661 (43046)	Loss/tok 3.0949 (3.1060)	LR 1.250e-04
0: TRAIN [7][6920/7762]	Time 0.342 (0.327)	Data 1.00e-04 (1.50e-04)	Tok/s 48430 (43050)	Loss/tok 3.2093 (3.1060)	LR 1.250e-04
0: TRAIN [7][6930/7762]	Time 0.264 (0.327)	Data 1.11e-04 (1.50e-04)	Tok/s 39104 (43056)	Loss/tok 2.9269 (3.1062)	LR 1.250e-04
0: TRAIN [7][6940/7762]	Time 0.265 (0.327)	Data 9.73e-05 (1.50e-04)	Tok/s 38897 (43056)	Loss/tok 3.0339 (3.1062)	LR 1.250e-04
0: TRAIN [7][6950/7762]	Time 0.269 (0.327)	Data 9.66e-05 (1.50e-04)	Tok/s 37556 (43051)	Loss/tok 2.9696 (3.1061)	LR 1.250e-04
0: TRAIN [7][6960/7762]	Time 0.364 (0.327)	Data 1.05e-04 (1.50e-04)	Tok/s 45542 (43048)	Loss/tok 3.2454 (3.1060)	LR 1.250e-04
0: TRAIN [7][6970/7762]	Time 0.352 (0.327)	Data 1.07e-04 (1.50e-04)	Tok/s 47542 (43049)	Loss/tok 3.1448 (3.1061)	LR 1.250e-04
0: TRAIN [7][6980/7762]	Time 0.453 (0.327)	Data 1.01e-04 (1.50e-04)	Tok/s 51545 (43048)	Loss/tok 3.3177 (3.1061)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][6990/7762]	Time 0.266 (0.327)	Data 1.02e-04 (1.50e-04)	Tok/s 39088 (43043)	Loss/tok 2.9458 (3.1060)	LR 1.250e-04
0: TRAIN [7][7000/7762]	Time 0.450 (0.327)	Data 1.16e-04 (1.50e-04)	Tok/s 52339 (43048)	Loss/tok 3.1688 (3.1062)	LR 1.250e-04
0: TRAIN [7][7010/7762]	Time 0.364 (0.327)	Data 1.03e-04 (1.49e-04)	Tok/s 45743 (43045)	Loss/tok 3.0990 (3.1060)	LR 1.250e-04
0: TRAIN [7][7020/7762]	Time 0.365 (0.327)	Data 1.04e-04 (1.49e-04)	Tok/s 45814 (43044)	Loss/tok 3.1133 (3.1060)	LR 1.250e-04
0: TRAIN [7][7030/7762]	Time 0.345 (0.327)	Data 1.00e-04 (1.49e-04)	Tok/s 48664 (43043)	Loss/tok 3.0733 (3.1059)	LR 1.250e-04
0: TRAIN [7][7040/7762]	Time 0.264 (0.327)	Data 1.04e-04 (1.49e-04)	Tok/s 39058 (43044)	Loss/tok 2.8873 (3.1059)	LR 1.250e-04
0: TRAIN [7][7050/7762]	Time 0.178 (0.327)	Data 1.09e-04 (1.49e-04)	Tok/s 29646 (43042)	Loss/tok 2.5455 (3.1059)	LR 1.250e-04
0: TRAIN [7][7060/7762]	Time 0.259 (0.327)	Data 1.21e-04 (1.49e-04)	Tok/s 39738 (43041)	Loss/tok 2.9661 (3.1060)	LR 1.250e-04
0: TRAIN [7][7070/7762]	Time 0.265 (0.327)	Data 1.02e-04 (1.49e-04)	Tok/s 39094 (43040)	Loss/tok 2.8738 (3.1061)	LR 1.250e-04
0: TRAIN [7][7080/7762]	Time 0.264 (0.327)	Data 1.03e-04 (1.49e-04)	Tok/s 38634 (43042)	Loss/tok 2.8768 (3.1062)	LR 1.250e-04
0: TRAIN [7][7090/7762]	Time 0.363 (0.327)	Data 9.94e-05 (1.49e-04)	Tok/s 46617 (43042)	Loss/tok 3.1198 (3.1061)	LR 1.250e-04
0: TRAIN [7][7100/7762]	Time 0.362 (0.327)	Data 1.01e-04 (1.49e-04)	Tok/s 46097 (43047)	Loss/tok 3.1873 (3.1063)	LR 1.250e-04
0: TRAIN [7][7110/7762]	Time 0.361 (0.327)	Data 1.03e-04 (1.49e-04)	Tok/s 46047 (43052)	Loss/tok 3.0784 (3.1063)	LR 1.250e-04
0: TRAIN [7][7120/7762]	Time 0.173 (0.327)	Data 1.05e-04 (1.49e-04)	Tok/s 29674 (43046)	Loss/tok 2.4671 (3.1062)	LR 1.250e-04
0: TRAIN [7][7130/7762]	Time 0.259 (0.327)	Data 1.03e-04 (1.49e-04)	Tok/s 39782 (43043)	Loss/tok 2.9374 (3.1062)	LR 1.250e-04
0: TRAIN [7][7140/7762]	Time 0.458 (0.327)	Data 9.94e-05 (1.49e-04)	Tok/s 50626 (43041)	Loss/tok 3.3219 (3.1061)	LR 1.250e-04
0: TRAIN [7][7150/7762]	Time 0.260 (0.327)	Data 1.19e-04 (1.49e-04)	Tok/s 39124 (43043)	Loss/tok 2.9634 (3.1061)	LR 1.250e-04
0: TRAIN [7][7160/7762]	Time 0.262 (0.327)	Data 1.03e-04 (1.49e-04)	Tok/s 39611 (43043)	Loss/tok 2.8726 (3.1061)	LR 1.250e-04
0: TRAIN [7][7170/7762]	Time 0.342 (0.327)	Data 1.02e-04 (1.48e-04)	Tok/s 49991 (43045)	Loss/tok 3.0612 (3.1061)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][7180/7762]	Time 0.262 (0.327)	Data 1.20e-04 (1.48e-04)	Tok/s 39893 (43042)	Loss/tok 2.8541 (3.1060)	LR 1.250e-04
0: TRAIN [7][7190/7762]	Time 0.592 (0.327)	Data 9.87e-05 (1.48e-04)	Tok/s 50713 (43038)	Loss/tok 3.3819 (3.1060)	LR 1.250e-04
0: TRAIN [7][7200/7762]	Time 0.366 (0.327)	Data 1.00e-04 (1.48e-04)	Tok/s 45998 (43037)	Loss/tok 3.1017 (3.1059)	LR 1.250e-04
0: TRAIN [7][7210/7762]	Time 0.363 (0.327)	Data 1.01e-04 (1.48e-04)	Tok/s 45924 (43035)	Loss/tok 3.1589 (3.1058)	LR 1.250e-04
0: TRAIN [7][7220/7762]	Time 0.259 (0.327)	Data 1.07e-04 (1.48e-04)	Tok/s 39643 (43039)	Loss/tok 3.0003 (3.1059)	LR 1.250e-04
0: TRAIN [7][7230/7762]	Time 0.177 (0.327)	Data 9.80e-05 (1.48e-04)	Tok/s 29021 (43036)	Loss/tok 2.5745 (3.1058)	LR 1.250e-04
0: TRAIN [7][7240/7762]	Time 0.265 (0.327)	Data 1.05e-04 (1.48e-04)	Tok/s 39027 (43033)	Loss/tok 3.0726 (3.1058)	LR 1.250e-04
0: TRAIN [7][7250/7762]	Time 0.463 (0.327)	Data 1.18e-04 (1.48e-04)	Tok/s 51099 (43033)	Loss/tok 3.2949 (3.1057)	LR 1.250e-04
0: TRAIN [7][7260/7762]	Time 0.258 (0.326)	Data 1.22e-04 (1.48e-04)	Tok/s 39458 (43032)	Loss/tok 2.9229 (3.1057)	LR 1.250e-04
0: TRAIN [7][7270/7762]	Time 0.364 (0.327)	Data 1.29e-04 (1.48e-04)	Tok/s 45589 (43034)	Loss/tok 3.1081 (3.1057)	LR 1.250e-04
0: TRAIN [7][7280/7762]	Time 0.266 (0.327)	Data 1.01e-04 (1.48e-04)	Tok/s 39122 (43034)	Loss/tok 2.9425 (3.1057)	LR 1.250e-04
0: TRAIN [7][7290/7762]	Time 0.362 (0.327)	Data 1.04e-04 (1.48e-04)	Tok/s 45767 (43036)	Loss/tok 3.0702 (3.1057)	LR 1.250e-04
0: TRAIN [7][7300/7762]	Time 0.255 (0.327)	Data 1.02e-04 (1.48e-04)	Tok/s 40376 (43037)	Loss/tok 2.9502 (3.1057)	LR 1.250e-04
0: TRAIN [7][7310/7762]	Time 0.463 (0.327)	Data 1.02e-04 (1.48e-04)	Tok/s 50012 (43039)	Loss/tok 3.2790 (3.1058)	LR 1.250e-04
0: TRAIN [7][7320/7762]	Time 0.252 (0.327)	Data 1.01e-04 (1.48e-04)	Tok/s 40264 (43044)	Loss/tok 2.9353 (3.1060)	LR 1.250e-04
0: TRAIN [7][7330/7762]	Time 0.581 (0.327)	Data 1.17e-04 (1.47e-04)	Tok/s 51211 (43046)	Loss/tok 3.5161 (3.1062)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][7340/7762]	Time 0.259 (0.327)	Data 9.97e-05 (1.47e-04)	Tok/s 40010 (43040)	Loss/tok 2.8998 (3.1061)	LR 1.250e-04
0: TRAIN [7][7350/7762]	Time 0.255 (0.327)	Data 1.02e-04 (1.47e-04)	Tok/s 40895 (43040)	Loss/tok 2.9062 (3.1061)	LR 1.250e-04
0: TRAIN [7][7360/7762]	Time 0.265 (0.327)	Data 1.25e-04 (1.47e-04)	Tok/s 39188 (43043)	Loss/tok 2.9877 (3.1062)	LR 1.250e-04
0: TRAIN [7][7370/7762]	Time 0.355 (0.327)	Data 1.00e-04 (1.47e-04)	Tok/s 47694 (43042)	Loss/tok 3.0546 (3.1062)	LR 1.250e-04
0: TRAIN [7][7380/7762]	Time 0.363 (0.327)	Data 1.05e-04 (1.47e-04)	Tok/s 45981 (43042)	Loss/tok 3.1334 (3.1062)	LR 1.250e-04
0: TRAIN [7][7390/7762]	Time 0.260 (0.327)	Data 1.02e-04 (1.47e-04)	Tok/s 40347 (43047)	Loss/tok 2.8695 (3.1064)	LR 1.250e-04
0: TRAIN [7][7400/7762]	Time 0.463 (0.327)	Data 1.03e-04 (1.47e-04)	Tok/s 50397 (43047)	Loss/tok 3.3119 (3.1065)	LR 1.250e-04
0: TRAIN [7][7410/7762]	Time 0.355 (0.327)	Data 1.03e-04 (1.47e-04)	Tok/s 47384 (43046)	Loss/tok 3.0913 (3.1065)	LR 1.250e-04
0: TRAIN [7][7420/7762]	Time 0.264 (0.327)	Data 1.04e-04 (1.47e-04)	Tok/s 39727 (43047)	Loss/tok 2.9857 (3.1065)	LR 1.250e-04
0: TRAIN [7][7430/7762]	Time 0.580 (0.327)	Data 9.99e-05 (1.47e-04)	Tok/s 51492 (43044)	Loss/tok 3.4276 (3.1065)	LR 1.250e-04
0: TRAIN [7][7440/7762]	Time 0.462 (0.327)	Data 2.36e-04 (1.47e-04)	Tok/s 50692 (43047)	Loss/tok 3.2192 (3.1067)	LR 1.250e-04
0: TRAIN [7][7450/7762]	Time 0.265 (0.327)	Data 1.06e-04 (1.47e-04)	Tok/s 38167 (43047)	Loss/tok 2.8712 (3.1067)	LR 1.250e-04
0: TRAIN [7][7460/7762]	Time 0.365 (0.327)	Data 1.07e-04 (1.47e-04)	Tok/s 45788 (43049)	Loss/tok 3.0166 (3.1067)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][7470/7762]	Time 0.354 (0.327)	Data 1.07e-04 (1.47e-04)	Tok/s 47848 (43049)	Loss/tok 3.1489 (3.1066)	LR 1.250e-04
0: TRAIN [7][7480/7762]	Time 0.171 (0.327)	Data 1.10e-04 (1.47e-04)	Tok/s 31008 (43050)	Loss/tok 2.5963 (3.1066)	LR 1.250e-04
0: TRAIN [7][7490/7762]	Time 0.341 (0.327)	Data 1.01e-04 (1.47e-04)	Tok/s 50109 (43049)	Loss/tok 3.1686 (3.1066)	LR 1.250e-04
0: TRAIN [7][7500/7762]	Time 0.361 (0.327)	Data 1.20e-04 (1.47e-04)	Tok/s 47157 (43051)	Loss/tok 3.0371 (3.1067)	LR 1.250e-04
0: TRAIN [7][7510/7762]	Time 0.590 (0.327)	Data 1.06e-04 (1.46e-04)	Tok/s 50255 (43049)	Loss/tok 3.4690 (3.1067)	LR 1.250e-04
0: TRAIN [7][7520/7762]	Time 0.356 (0.327)	Data 1.03e-04 (1.46e-04)	Tok/s 47320 (43046)	Loss/tok 3.1657 (3.1066)	LR 1.250e-04
0: TRAIN [7][7530/7762]	Time 0.255 (0.327)	Data 1.18e-04 (1.46e-04)	Tok/s 40052 (43045)	Loss/tok 2.8351 (3.1065)	LR 1.250e-04
0: TRAIN [7][7540/7762]	Time 0.454 (0.327)	Data 1.19e-04 (1.46e-04)	Tok/s 51882 (43043)	Loss/tok 3.2634 (3.1064)	LR 1.250e-04
0: TRAIN [7][7550/7762]	Time 0.261 (0.327)	Data 1.03e-04 (1.46e-04)	Tok/s 39715 (43046)	Loss/tok 2.8840 (3.1064)	LR 1.250e-04
0: TRAIN [7][7560/7762]	Time 0.365 (0.327)	Data 9.82e-05 (1.46e-04)	Tok/s 46721 (43047)	Loss/tok 3.0386 (3.1064)	LR 1.250e-04
0: TRAIN [7][7570/7762]	Time 0.265 (0.327)	Data 9.85e-05 (1.46e-04)	Tok/s 38274 (43047)	Loss/tok 2.9089 (3.1064)	LR 1.250e-04
0: TRAIN [7][7580/7762]	Time 0.463 (0.327)	Data 1.14e-04 (1.46e-04)	Tok/s 50054 (43050)	Loss/tok 3.3080 (3.1064)	LR 1.250e-04
0: TRAIN [7][7590/7762]	Time 0.365 (0.327)	Data 9.92e-05 (1.46e-04)	Tok/s 45657 (43047)	Loss/tok 3.1015 (3.1064)	LR 1.250e-04
0: TRAIN [7][7600/7762]	Time 0.266 (0.327)	Data 1.00e-04 (1.46e-04)	Tok/s 38853 (43047)	Loss/tok 2.9036 (3.1063)	LR 1.250e-04
0: TRAIN [7][7610/7762]	Time 0.367 (0.327)	Data 1.18e-04 (1.46e-04)	Tok/s 45488 (43044)	Loss/tok 3.1506 (3.1062)	LR 1.250e-04
0: TRAIN [7][7620/7762]	Time 0.352 (0.327)	Data 1.03e-04 (1.46e-04)	Tok/s 47853 (43042)	Loss/tok 3.0654 (3.1062)	LR 1.250e-04
0: TRAIN [7][7630/7762]	Time 0.265 (0.327)	Data 1.16e-04 (1.46e-04)	Tok/s 39808 (43043)	Loss/tok 2.8440 (3.1062)	LR 1.250e-04
0: TRAIN [7][7640/7762]	Time 0.360 (0.327)	Data 9.73e-05 (1.46e-04)	Tok/s 46692 (43042)	Loss/tok 3.0562 (3.1061)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][7650/7762]	Time 0.265 (0.327)	Data 9.70e-05 (1.46e-04)	Tok/s 38088 (43040)	Loss/tok 3.0196 (3.1061)	LR 1.250e-04
0: TRAIN [7][7660/7762]	Time 0.267 (0.327)	Data 1.04e-04 (1.46e-04)	Tok/s 38890 (43038)	Loss/tok 2.9101 (3.1062)	LR 1.250e-04
0: TRAIN [7][7670/7762]	Time 0.352 (0.327)	Data 1.20e-04 (1.46e-04)	Tok/s 48179 (43037)	Loss/tok 3.1855 (3.1061)	LR 1.250e-04
0: TRAIN [7][7680/7762]	Time 0.263 (0.327)	Data 9.82e-05 (1.46e-04)	Tok/s 38468 (43034)	Loss/tok 2.8013 (3.1060)	LR 1.250e-04
0: TRAIN [7][7690/7762]	Time 0.360 (0.327)	Data 9.97e-05 (1.46e-04)	Tok/s 46860 (43034)	Loss/tok 3.1608 (3.1060)	LR 1.250e-04
0: TRAIN [7][7700/7762]	Time 0.178 (0.327)	Data 9.73e-05 (1.45e-04)	Tok/s 29604 (43033)	Loss/tok 2.4820 (3.1060)	LR 1.250e-04
0: TRAIN [7][7710/7762]	Time 0.263 (0.327)	Data 1.01e-04 (1.45e-04)	Tok/s 39349 (43035)	Loss/tok 2.9046 (3.1061)	LR 1.250e-04
0: TRAIN [7][7720/7762]	Time 0.261 (0.326)	Data 9.82e-05 (1.45e-04)	Tok/s 39626 (43031)	Loss/tok 2.8842 (3.1059)	LR 1.250e-04
0: TRAIN [7][7730/7762]	Time 0.263 (0.326)	Data 1.01e-04 (1.45e-04)	Tok/s 39533 (43031)	Loss/tok 2.8710 (3.1058)	LR 1.250e-04
0: TRAIN [7][7740/7762]	Time 0.258 (0.327)	Data 9.73e-05 (1.45e-04)	Tok/s 40035 (43034)	Loss/tok 2.9215 (3.1059)	LR 1.250e-04
0: TRAIN [7][7750/7762]	Time 0.359 (0.326)	Data 1.03e-04 (1.45e-04)	Tok/s 46323 (43028)	Loss/tok 3.0921 (3.1058)	LR 1.250e-04
0: TRAIN [7][7760/7762]	Time 0.176 (0.326)	Data 1.50e-02 (1.47e-04)	Tok/s 30274 (43029)	Loss/tok 2.4461 (3.1058)	LR 1.250e-04
:::MLL 1573766160.871 epoch_stop: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 524}}
:::MLL 1573766160.872 eval_start: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [7][0/12]	Time 0.800 (0.800)	Decoder iters 101.0 (101.0)	Tok/s 20572 (20572)
0: TEST [7][10/12]	Time 0.118 (0.290)	Decoder iters 25.0 (48.8)	Tok/s 31532 (29743)
0: Running moses detokenizer
0: BLEU(score=23.835653238911604, counts=[36953, 18467, 10441, 6169], totals=[65342, 62339, 59337, 56340], precisions=[56.55321232897677, 29.62351016217777, 17.596103611574566, 10.94959176428825], bp=1.0, sys_len=65342, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1573766166.420 eval_accuracy: {"value": 23.84, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 535}}
:::MLL 1573766166.420 eval_stop: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 7	Training Loss: 3.1057	Test BLEU: 23.84
0: Performance: Epoch: 7	Training: 86066 Tok/s
0: Finished epoch 7
:::MLL 1573766166.421 block_stop: {"value": null, "metadata": {"first_epoch_num": 8, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1573766166.421 run_stop: {"value": null, "metadata": {"status": "aborted", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-11-14 09:16:10 PM
RESULT,RNN_TRANSLATOR,,20321,nvidia,2019-11-14 03:37:29 PM

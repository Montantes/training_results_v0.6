Beginning trial 1 of 1
Gathering sys log on lambda-server
:::MLL 1572986650.762 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1572986650.763 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1572986650.763 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1572986650.764 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1572986650.765 submission_platform: {"value": "1xSYS-4029GP-TRT", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1572986650.765 submission_entry: {"value": "{'hardware': 'SYS-4029GP-TRT', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': ' ', 'os': 'Ubuntu 18.04.3 LTS / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.4-1.0.0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz', 'num_cores': '24', 'num_vcpus': '48', 'accelerator': 'GeForce RTX 2080 Ti', 'num_accelerators': '8', 'sys_mem_size': '503 GB', 'sys_storage_type': 'SATA SSD', 'sys_storage_size': '1x 3.7T', 'cpu_accel_interconnect': 'UPI', 'network_card': '', 'num_network_cards': '0', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1572986650.766 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1572986650.767 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1572986655.301 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node lambda-server
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=LambdaBlade2080Ti -e 'MULTI_NODE= --master_port=4759' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191105124223851542128 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191105124223851542128 ./run_and_time.sh
Run vars: id 191105124223851542128 gpus 8 mparams  --master_port=4759
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
STARTING TIMING RUN AT 2019-11-05 08:44:16 PM
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 12 --nproc_per_node 8  --master_port=4759'
running benchmark
+ echo 'running benchmark'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 12 --nproc_per_node 8 --master_port=4759 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1572986658.621 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1572986658.622 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1572986658.626 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1572986658.626 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1572986658.630 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1572986658.662 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1572986658.664 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1572986658.693 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 2236547535
0: Worker 0 is using worker seed: 2800973355
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1572986677.602 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1572986680.133 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1572986680.134 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1572986680.135 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1572986680.650 global_batch_size: {"value": 2048, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1572986680.652 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1572986680.653 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1572986680.653 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1572986680.654 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1572986680.654 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1572986680.655 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1572986680.655 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1572986680.680 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1572986680.680 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 2480394575
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/1938]	Time 0.866 (0.866)	Data 5.07e-01 (5.07e-01)	Tok/s 19310 (19310)	Loss/tok 10.6298 (10.6298)	LR 2.000e-05
0: TRAIN [0][10/1938]	Time 0.275 (0.383)	Data 1.39e-04 (4.62e-02)	Tok/s 38408 (41201)	Loss/tok 9.6736 (10.2031)	LR 2.518e-05
0: TRAIN [0][20/1938]	Time 0.274 (0.348)	Data 1.28e-04 (2.43e-02)	Tok/s 37749 (41048)	Loss/tok 9.2520 (9.8886)	LR 3.170e-05
0: TRAIN [0][30/1938]	Time 0.363 (0.341)	Data 1.48e-04 (1.65e-02)	Tok/s 46004 (41467)	Loss/tok 9.0625 (9.6536)	LR 3.991e-05
0: TRAIN [0][40/1938]	Time 0.281 (0.337)	Data 1.40e-04 (1.25e-02)	Tok/s 36589 (41382)	Loss/tok 8.7291 (9.4757)	LR 5.024e-05
0: TRAIN [0][50/1938]	Time 0.280 (0.329)	Data 1.32e-04 (1.01e-02)	Tok/s 36028 (40642)	Loss/tok 8.5163 (9.3408)	LR 6.325e-05
0: TRAIN [0][60/1938]	Time 0.361 (0.331)	Data 1.54e-04 (8.45e-03)	Tok/s 46137 (41085)	Loss/tok 8.3739 (9.1891)	LR 7.962e-05
0: TRAIN [0][70/1938]	Time 0.361 (0.335)	Data 1.45e-04 (7.28e-03)	Tok/s 46959 (41450)	Loss/tok 8.2294 (9.0482)	LR 1.002e-04
0: TRAIN [0][80/1938]	Time 0.281 (0.334)	Data 1.50e-04 (6.40e-03)	Tok/s 36468 (41515)	Loss/tok 7.9629 (8.9342)	LR 1.262e-04
0: TRAIN [0][90/1938]	Time 0.282 (0.332)	Data 1.49e-04 (5.71e-03)	Tok/s 37641 (41362)	Loss/tok 7.9037 (8.8410)	LR 1.589e-04
0: TRAIN [0][100/1938]	Time 0.202 (0.330)	Data 2.11e-04 (5.16e-03)	Tok/s 26674 (41203)	Loss/tok 7.4957 (8.7603)	LR 2.000e-04
0: TRAIN [0][110/1938]	Time 0.282 (0.330)	Data 1.32e-04 (4.71e-03)	Tok/s 35554 (41056)	Loss/tok 7.8130 (8.6873)	LR 2.518e-04
0: TRAIN [0][120/1938]	Time 0.282 (0.329)	Data 1.60e-04 (4.33e-03)	Tok/s 36535 (40893)	Loss/tok 7.8201 (8.6269)	LR 3.170e-04
0: TRAIN [0][130/1938]	Time 0.564 (0.333)	Data 1.45e-04 (4.01e-03)	Tok/s 52590 (41026)	Loss/tok 8.0727 (8.5647)	LR 3.991e-04
0: TRAIN [0][140/1938]	Time 0.450 (0.333)	Data 1.30e-04 (3.74e-03)	Tok/s 51928 (41066)	Loss/tok 8.0165 (8.5117)	LR 5.024e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][150/1938]	Time 0.284 (0.332)	Data 1.26e-04 (3.50e-03)	Tok/s 35817 (41017)	Loss/tok 7.6128 (8.4678)	LR 6.181e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][160/1938]	Time 0.284 (0.334)	Data 1.44e-04 (3.29e-03)	Tok/s 36210 (41165)	Loss/tok 7.5871 (8.4208)	LR 7.604e-04
0: TRAIN [0][170/1938]	Time 0.284 (0.332)	Data 1.35e-04 (3.11e-03)	Tok/s 35777 (40935)	Loss/tok 7.3169 (8.3784)	LR 9.573e-04
0: TRAIN [0][180/1938]	Time 0.201 (0.329)	Data 1.29e-04 (2.94e-03)	Tok/s 26333 (40618)	Loss/tok 6.5076 (8.3359)	LR 1.205e-03
0: TRAIN [0][190/1938]	Time 0.203 (0.328)	Data 1.31e-04 (2.80e-03)	Tok/s 25460 (40510)	Loss/tok 6.3016 (8.2877)	LR 1.517e-03
0: TRAIN [0][200/1938]	Time 0.453 (0.330)	Data 1.30e-04 (2.66e-03)	Tok/s 51498 (40750)	Loss/tok 7.1956 (8.2212)	LR 1.910e-03
0: TRAIN [0][210/1938]	Time 0.367 (0.329)	Data 1.53e-04 (2.54e-03)	Tok/s 46172 (40611)	Loss/tok 7.1074 (8.1694)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.457 (0.331)	Data 1.46e-04 (2.44e-03)	Tok/s 51500 (40710)	Loss/tok 6.9504 (8.1035)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.367 (0.331)	Data 1.27e-04 (2.34e-03)	Tok/s 45720 (40745)	Loss/tok 6.6583 (8.0418)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.203 (0.332)	Data 1.31e-04 (2.24e-03)	Tok/s 26014 (40820)	Loss/tok 5.4625 (7.9753)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.203 (0.332)	Data 1.40e-04 (2.16e-03)	Tok/s 25593 (40769)	Loss/tok 5.2652 (7.9192)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.372 (0.334)	Data 1.49e-04 (2.08e-03)	Tok/s 44801 (40904)	Loss/tok 6.2326 (7.8479)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.369 (0.333)	Data 1.40e-04 (2.01e-03)	Tok/s 46176 (40867)	Loss/tok 6.2265 (7.7902)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.283 (0.334)	Data 1.37e-04 (1.95e-03)	Tok/s 36122 (40918)	Loss/tok 5.7517 (7.7259)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.285 (0.333)	Data 1.30e-04 (1.88e-03)	Tok/s 36961 (40775)	Loss/tok 5.9230 (7.6774)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.283 (0.334)	Data 1.58e-04 (1.83e-03)	Tok/s 36075 (40852)	Loss/tok 5.6672 (7.6176)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.203 (0.334)	Data 1.58e-04 (1.77e-03)	Tok/s 25902 (40802)	Loss/tok 4.7634 (7.5621)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.284 (0.334)	Data 1.52e-04 (1.72e-03)	Tok/s 35798 (40741)	Loss/tok 5.5775 (7.5079)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.285 (0.333)	Data 1.49e-04 (1.67e-03)	Tok/s 36488 (40684)	Loss/tok 5.4798 (7.4544)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.458 (0.335)	Data 1.51e-04 (1.63e-03)	Tok/s 50490 (40810)	Loss/tok 5.8422 (7.3908)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.284 (0.335)	Data 1.30e-04 (1.59e-03)	Tok/s 36507 (40752)	Loss/tok 5.1872 (7.3401)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.284 (0.334)	Data 1.59e-04 (1.55e-03)	Tok/s 36537 (40696)	Loss/tok 5.0896 (7.2914)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.283 (0.333)	Data 1.27e-04 (1.51e-03)	Tok/s 36286 (40604)	Loss/tok 5.0628 (7.2455)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.285 (0.332)	Data 1.47e-04 (1.47e-03)	Tok/s 36509 (40530)	Loss/tok 5.2977 (7.1997)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.370 (0.332)	Data 1.44e-04 (1.44e-03)	Tok/s 45395 (40530)	Loss/tok 5.2301 (7.1499)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.283 (0.332)	Data 1.42e-04 (1.41e-03)	Tok/s 36381 (40530)	Loss/tok 4.8053 (7.1002)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.370 (0.332)	Data 1.29e-04 (1.37e-03)	Tok/s 45375 (40575)	Loss/tok 4.9319 (7.0464)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.369 (0.333)	Data 1.27e-04 (1.34e-03)	Tok/s 45812 (40610)	Loss/tok 4.8678 (6.9932)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.284 (0.333)	Data 1.40e-04 (1.32e-03)	Tok/s 37142 (40611)	Loss/tok 4.7143 (6.9429)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.285 (0.334)	Data 1.49e-04 (1.29e-03)	Tok/s 36516 (40603)	Loss/tok 4.5461 (6.8939)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.571 (0.335)	Data 1.42e-04 (1.26e-03)	Tok/s 52486 (40688)	Loss/tok 5.0659 (6.8382)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.570 (0.335)	Data 1.45e-04 (1.24e-03)	Tok/s 52136 (40751)	Loss/tok 5.0459 (6.7858)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.371 (0.335)	Data 1.28e-04 (1.22e-03)	Tok/s 44902 (40715)	Loss/tok 4.8448 (6.7425)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.370 (0.335)	Data 1.49e-04 (1.19e-03)	Tok/s 45214 (40707)	Loss/tok 4.5750 (6.6981)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.287 (0.335)	Data 1.34e-04 (1.17e-03)	Tok/s 36139 (40687)	Loss/tok 4.2138 (6.6562)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.370 (0.335)	Data 1.31e-04 (1.15e-03)	Tok/s 45286 (40658)	Loss/tok 4.5684 (6.6163)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.457 (0.335)	Data 1.55e-04 (1.13e-03)	Tok/s 51364 (40719)	Loss/tok 4.7792 (6.5688)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.457 (0.337)	Data 1.35e-04 (1.11e-03)	Tok/s 50639 (40833)	Loss/tok 4.6608 (6.5172)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.458 (0.336)	Data 1.44e-04 (1.10e-03)	Tok/s 50200 (40805)	Loss/tok 4.7400 (6.4803)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.371 (0.336)	Data 1.45e-04 (1.08e-03)	Tok/s 45538 (40828)	Loss/tok 4.3425 (6.4396)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.284 (0.336)	Data 1.46e-04 (1.06e-03)	Tok/s 36448 (40843)	Loss/tok 4.0239 (6.4011)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.285 (0.337)	Data 1.35e-04 (1.04e-03)	Tok/s 36210 (40862)	Loss/tok 3.9763 (6.3614)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.369 (0.336)	Data 1.53e-04 (1.03e-03)	Tok/s 45311 (40819)	Loss/tok 4.3428 (6.3278)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.458 (0.336)	Data 1.39e-04 (1.01e-03)	Tok/s 50744 (40785)	Loss/tok 4.5149 (6.2942)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][590/1938]	Time 0.206 (0.336)	Data 1.39e-04 (9.99e-04)	Tok/s 25618 (40807)	Loss/tok 3.3161 (6.2556)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.369 (0.337)	Data 1.49e-04 (9.85e-04)	Tok/s 45732 (40843)	Loss/tok 4.2045 (6.2183)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.370 (0.337)	Data 1.51e-04 (9.71e-04)	Tok/s 45725 (40877)	Loss/tok 4.3176 (6.1828)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.369 (0.338)	Data 1.34e-04 (9.58e-04)	Tok/s 45403 (40974)	Loss/tok 4.4005 (6.1431)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.457 (0.338)	Data 1.20e-04 (9.45e-04)	Tok/s 50978 (40978)	Loss/tok 4.3899 (6.1104)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.286 (0.338)	Data 1.44e-04 (9.32e-04)	Tok/s 35646 (40945)	Loss/tok 3.7744 (6.0812)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.457 (0.338)	Data 1.42e-04 (9.20e-04)	Tok/s 51304 (40927)	Loss/tok 4.3563 (6.0521)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.370 (0.338)	Data 1.34e-04 (9.08e-04)	Tok/s 45421 (40878)	Loss/tok 4.1634 (6.0263)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.284 (0.337)	Data 1.52e-04 (8.97e-04)	Tok/s 35814 (40824)	Loss/tok 3.6435 (6.0008)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.458 (0.337)	Data 1.34e-04 (8.86e-04)	Tok/s 51211 (40832)	Loss/tok 4.2626 (5.9722)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.369 (0.338)	Data 1.56e-04 (8.75e-04)	Tok/s 45267 (40878)	Loss/tok 4.0245 (5.9407)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.457 (0.337)	Data 1.29e-04 (8.65e-04)	Tok/s 51480 (40832)	Loss/tok 4.2794 (5.9171)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.288 (0.337)	Data 1.41e-04 (8.54e-04)	Tok/s 36197 (40823)	Loss/tok 3.6541 (5.8920)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.571 (0.338)	Data 1.31e-04 (8.44e-04)	Tok/s 52647 (40861)	Loss/tok 4.3922 (5.8627)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.374 (0.338)	Data 1.28e-04 (8.35e-04)	Tok/s 44917 (40853)	Loss/tok 4.0674 (5.8373)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.204 (0.338)	Data 1.48e-04 (8.25e-04)	Tok/s 26140 (40815)	Loss/tok 3.2661 (5.8146)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.286 (0.337)	Data 1.46e-04 (8.16e-04)	Tok/s 35675 (40777)	Loss/tok 3.8115 (5.7935)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.286 (0.337)	Data 1.45e-04 (8.07e-04)	Tok/s 36448 (40758)	Loss/tok 3.7925 (5.7708)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.285 (0.337)	Data 1.44e-04 (7.99e-04)	Tok/s 36698 (40753)	Loss/tok 3.6706 (5.7485)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.285 (0.337)	Data 1.59e-04 (7.91e-04)	Tok/s 35929 (40727)	Loss/tok 3.6697 (5.7275)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.457 (0.337)	Data 1.59e-04 (7.82e-04)	Tok/s 51754 (40761)	Loss/tok 4.1906 (5.7025)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.203 (0.337)	Data 1.42e-04 (7.74e-04)	Tok/s 25572 (40719)	Loss/tok 3.0921 (5.6830)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.369 (0.337)	Data 1.40e-04 (7.67e-04)	Tok/s 45388 (40720)	Loss/tok 4.0115 (5.6618)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.205 (0.337)	Data 1.57e-04 (7.59e-04)	Tok/s 25470 (40712)	Loss/tok 3.1205 (5.6410)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.571 (0.337)	Data 1.54e-04 (7.52e-04)	Tok/s 51896 (40736)	Loss/tok 4.4021 (5.6182)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.287 (0.337)	Data 1.28e-04 (7.45e-04)	Tok/s 36165 (40722)	Loss/tok 3.6941 (5.5989)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.286 (0.337)	Data 1.40e-04 (7.38e-04)	Tok/s 35866 (40710)	Loss/tok 3.7051 (5.5802)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.370 (0.337)	Data 1.89e-04 (7.31e-04)	Tok/s 45091 (40668)	Loss/tok 3.9533 (5.5633)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.369 (0.337)	Data 1.51e-04 (7.24e-04)	Tok/s 45778 (40649)	Loss/tok 3.8888 (5.5449)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][880/1938]	Time 0.202 (0.336)	Data 1.48e-04 (7.18e-04)	Tok/s 26042 (40618)	Loss/tok 3.1612 (5.5283)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.571 (0.337)	Data 1.57e-04 (7.11e-04)	Tok/s 51937 (40658)	Loss/tok 4.3903 (5.5069)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.572 (0.337)	Data 1.30e-04 (7.05e-04)	Tok/s 52554 (40656)	Loss/tok 4.2785 (5.4890)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.287 (0.337)	Data 1.41e-04 (6.99e-04)	Tok/s 36229 (40638)	Loss/tok 3.6723 (5.4721)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.285 (0.337)	Data 1.55e-04 (6.93e-04)	Tok/s 35861 (40630)	Loss/tok 3.5730 (5.4549)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.203 (0.337)	Data 1.36e-04 (6.87e-04)	Tok/s 25758 (40618)	Loss/tok 3.0910 (5.4380)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.373 (0.337)	Data 1.43e-04 (6.81e-04)	Tok/s 44508 (40614)	Loss/tok 4.0786 (5.4220)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.287 (0.337)	Data 1.29e-04 (6.75e-04)	Tok/s 36096 (40618)	Loss/tok 3.4844 (5.4052)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.286 (0.337)	Data 1.59e-04 (6.70e-04)	Tok/s 36058 (40640)	Loss/tok 3.6462 (5.3875)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.458 (0.337)	Data 1.59e-04 (6.64e-04)	Tok/s 50838 (40644)	Loss/tok 4.0794 (5.3708)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.286 (0.337)	Data 1.41e-04 (6.59e-04)	Tok/s 35559 (40635)	Loss/tok 3.6359 (5.3558)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.459 (0.338)	Data 1.34e-04 (6.54e-04)	Tok/s 50949 (40675)	Loss/tok 3.9465 (5.3376)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.288 (0.338)	Data 1.49e-04 (6.49e-04)	Tok/s 35527 (40714)	Loss/tok 3.6301 (5.3198)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1010/1938]	Time 0.374 (0.339)	Data 1.45e-04 (6.44e-04)	Tok/s 44881 (40761)	Loss/tok 3.8481 (5.3029)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.285 (0.339)	Data 1.52e-04 (6.39e-04)	Tok/s 36626 (40768)	Loss/tok 3.5927 (5.2876)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.372 (0.339)	Data 1.31e-04 (6.34e-04)	Tok/s 45386 (40818)	Loss/tok 3.7821 (5.2704)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.205 (0.340)	Data 1.49e-04 (6.29e-04)	Tok/s 25818 (40825)	Loss/tok 3.0070 (5.2556)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.455 (0.340)	Data 1.33e-04 (6.25e-04)	Tok/s 51396 (40852)	Loss/tok 4.0069 (5.2399)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.287 (0.340)	Data 1.31e-04 (6.20e-04)	Tok/s 35583 (40871)	Loss/tok 3.6279 (5.2243)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.457 (0.340)	Data 1.44e-04 (6.16e-04)	Tok/s 51539 (40875)	Loss/tok 4.0436 (5.2102)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.205 (0.340)	Data 1.49e-04 (6.11e-04)	Tok/s 25770 (40861)	Loss/tok 2.9681 (5.1973)	LR 2.000e-03
0: TRAIN [0][1090/1938]	Time 0.287 (0.341)	Data 1.32e-04 (6.07e-04)	Tok/s 35223 (40891)	Loss/tok 3.5092 (5.1830)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.285 (0.341)	Data 1.30e-04 (6.03e-04)	Tok/s 36983 (40906)	Loss/tok 3.5441 (5.1693)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.284 (0.340)	Data 1.34e-04 (5.99e-04)	Tok/s 36454 (40887)	Loss/tok 3.4092 (5.1577)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.285 (0.340)	Data 1.48e-04 (5.95e-04)	Tok/s 36334 (40884)	Loss/tok 3.3853 (5.1455)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.284 (0.340)	Data 1.34e-04 (5.91e-04)	Tok/s 36689 (40875)	Loss/tok 3.6547 (5.1335)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.458 (0.340)	Data 1.23e-04 (5.87e-04)	Tok/s 51684 (40893)	Loss/tok 3.8793 (5.1203)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.285 (0.340)	Data 1.27e-04 (5.83e-04)	Tok/s 35857 (40878)	Loss/tok 3.5774 (5.1092)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.572 (0.341)	Data 1.29e-04 (5.79e-04)	Tok/s 51656 (40907)	Loss/tok 4.1228 (5.0958)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.369 (0.341)	Data 1.46e-04 (5.75e-04)	Tok/s 45526 (40928)	Loss/tok 3.6799 (5.0830)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.285 (0.341)	Data 1.46e-04 (5.71e-04)	Tok/s 35784 (40935)	Loss/tok 3.5368 (5.0712)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.369 (0.341)	Data 1.35e-04 (5.68e-04)	Tok/s 45920 (40940)	Loss/tok 3.7758 (5.0596)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.202 (0.341)	Data 1.47e-04 (5.64e-04)	Tok/s 25942 (40975)	Loss/tok 2.9380 (5.0466)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.370 (0.341)	Data 1.48e-04 (5.61e-04)	Tok/s 45748 (40962)	Loss/tok 3.7002 (5.0360)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.368 (0.341)	Data 1.49e-04 (5.57e-04)	Tok/s 45163 (40996)	Loss/tok 3.6492 (5.0242)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.205 (0.341)	Data 1.45e-04 (5.54e-04)	Tok/s 26102 (40946)	Loss/tok 2.9821 (5.0154)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.459 (0.341)	Data 1.44e-04 (5.51e-04)	Tok/s 50496 (40961)	Loss/tok 3.8597 (5.0041)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.287 (0.341)	Data 1.40e-04 (5.47e-04)	Tok/s 34416 (40958)	Loss/tok 3.5052 (4.9941)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1260/1938]	Time 0.566 (0.341)	Data 1.38e-04 (5.44e-04)	Tok/s 52364 (40949)	Loss/tok 4.2210 (4.9845)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.370 (0.341)	Data 1.42e-04 (5.41e-04)	Tok/s 45641 (40942)	Loss/tok 3.5585 (4.9746)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.202 (0.341)	Data 1.44e-04 (5.38e-04)	Tok/s 26067 (40936)	Loss/tok 2.9650 (4.9647)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.373 (0.341)	Data 1.43e-04 (5.35e-04)	Tok/s 44937 (40932)	Loss/tok 3.6414 (4.9546)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.286 (0.341)	Data 1.57e-04 (5.32e-04)	Tok/s 36735 (40936)	Loss/tok 3.4097 (4.9445)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.284 (0.341)	Data 1.30e-04 (5.29e-04)	Tok/s 36640 (40929)	Loss/tok 3.5350 (4.9352)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.283 (0.341)	Data 1.48e-04 (5.26e-04)	Tok/s 36781 (40917)	Loss/tok 3.4267 (4.9260)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.204 (0.340)	Data 1.29e-04 (5.23e-04)	Tok/s 25816 (40907)	Loss/tok 2.8554 (4.9169)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.287 (0.341)	Data 1.43e-04 (5.20e-04)	Tok/s 35854 (40899)	Loss/tok 3.5329 (4.9076)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.285 (0.341)	Data 1.49e-04 (5.18e-04)	Tok/s 36677 (40917)	Loss/tok 3.5826 (4.8979)	LR 2.000e-03
0: TRAIN [0][1360/1938]	Time 0.458 (0.341)	Data 1.48e-04 (5.15e-04)	Tok/s 51086 (40930)	Loss/tok 3.8664 (4.8886)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.287 (0.341)	Data 1.33e-04 (5.12e-04)	Tok/s 35392 (40940)	Loss/tok 3.4012 (4.8793)	LR 2.000e-03
0: TRAIN [0][1380/1938]	Time 0.370 (0.341)	Data 1.44e-04 (5.09e-04)	Tok/s 44349 (40922)	Loss/tok 3.6763 (4.8713)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.286 (0.341)	Data 1.32e-04 (5.07e-04)	Tok/s 36034 (40930)	Loss/tok 3.5537 (4.8624)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.457 (0.341)	Data 1.45e-04 (5.04e-04)	Tok/s 50910 (40939)	Loss/tok 3.7454 (4.8533)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.286 (0.341)	Data 1.41e-04 (5.02e-04)	Tok/s 36073 (40947)	Loss/tok 3.5616 (4.8444)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.285 (0.341)	Data 1.35e-04 (4.99e-04)	Tok/s 36580 (40955)	Loss/tok 3.4167 (4.8355)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.369 (0.341)	Data 1.39e-04 (4.97e-04)	Tok/s 45099 (40948)	Loss/tok 3.7384 (4.8271)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.572 (0.341)	Data 1.38e-04 (4.94e-04)	Tok/s 51821 (40949)	Loss/tok 4.1513 (4.8190)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.370 (0.341)	Data 1.48e-04 (4.92e-04)	Tok/s 45656 (40941)	Loss/tok 3.6363 (4.8112)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.459 (0.341)	Data 1.37e-04 (4.89e-04)	Tok/s 50354 (40940)	Loss/tok 3.9024 (4.8031)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.573 (0.342)	Data 1.49e-04 (4.87e-04)	Tok/s 51371 (40963)	Loss/tok 4.1139 (4.7943)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.374 (0.342)	Data 1.35e-04 (4.85e-04)	Tok/s 44553 (40980)	Loss/tok 3.6091 (4.7856)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.284 (0.342)	Data 1.46e-04 (4.82e-04)	Tok/s 36701 (40976)	Loss/tok 3.3784 (4.7781)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.374 (0.342)	Data 1.34e-04 (4.80e-04)	Tok/s 44559 (41002)	Loss/tok 3.7691 (4.7691)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.290 (0.342)	Data 1.47e-04 (4.78e-04)	Tok/s 35899 (41002)	Loss/tok 3.4302 (4.7615)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1520/1938]	Time 0.569 (0.342)	Data 1.44e-04 (4.76e-04)	Tok/s 52431 (41026)	Loss/tok 3.8945 (4.7529)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.290 (0.342)	Data 1.51e-04 (4.73e-04)	Tok/s 35536 (41013)	Loss/tok 3.3268 (4.7456)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.462 (0.342)	Data 1.29e-04 (4.71e-04)	Tok/s 50480 (40998)	Loss/tok 3.8203 (4.7387)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.284 (0.342)	Data 1.36e-04 (4.69e-04)	Tok/s 36300 (40987)	Loss/tok 3.4771 (4.7317)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.371 (0.342)	Data 1.23e-04 (4.67e-04)	Tok/s 45517 (40989)	Loss/tok 3.6762 (4.7243)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.290 (0.342)	Data 1.50e-04 (4.65e-04)	Tok/s 34858 (40972)	Loss/tok 3.4504 (4.7175)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.370 (0.342)	Data 1.46e-04 (4.63e-04)	Tok/s 45103 (40955)	Loss/tok 3.7722 (4.7113)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.289 (0.342)	Data 1.52e-04 (4.61e-04)	Tok/s 35396 (40964)	Loss/tok 3.4690 (4.7039)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.290 (0.342)	Data 1.49e-04 (4.59e-04)	Tok/s 35919 (40959)	Loss/tok 3.2518 (4.6970)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.458 (0.342)	Data 1.34e-04 (4.57e-04)	Tok/s 50875 (40954)	Loss/tok 3.6634 (4.6900)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.205 (0.341)	Data 1.36e-04 (4.55e-04)	Tok/s 25676 (40937)	Loss/tok 2.7801 (4.6837)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.458 (0.342)	Data 1.52e-04 (4.53e-04)	Tok/s 51621 (40941)	Loss/tok 3.8086 (4.6768)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.369 (0.342)	Data 1.40e-04 (4.51e-04)	Tok/s 45615 (40970)	Loss/tok 3.5281 (4.6689)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.570 (0.342)	Data 1.51e-04 (4.50e-04)	Tok/s 52155 (40964)	Loss/tok 3.9495 (4.6625)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.459 (0.342)	Data 1.52e-04 (4.48e-04)	Tok/s 50251 (40944)	Loss/tok 3.9786 (4.6569)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.373 (0.342)	Data 1.33e-04 (4.46e-04)	Tok/s 44636 (40932)	Loss/tok 3.6877 (4.6509)	LR 2.000e-03
0: TRAIN [0][1680/1938]	Time 0.285 (0.341)	Data 1.32e-04 (4.44e-04)	Tok/s 36466 (40914)	Loss/tok 3.2831 (4.6450)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.460 (0.342)	Data 1.50e-04 (4.42e-04)	Tok/s 51329 (40920)	Loss/tok 3.6836 (4.6384)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.285 (0.342)	Data 1.34e-04 (4.41e-04)	Tok/s 36190 (40926)	Loss/tok 3.3362 (4.6318)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.284 (0.342)	Data 1.56e-04 (4.39e-04)	Tok/s 36294 (40926)	Loss/tok 3.3556 (4.6254)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.287 (0.341)	Data 1.32e-04 (4.37e-04)	Tok/s 36210 (40913)	Loss/tok 3.2038 (4.6193)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.285 (0.342)	Data 1.31e-04 (4.35e-04)	Tok/s 36287 (40920)	Loss/tok 3.2755 (4.6129)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1740/1938]	Time 0.285 (0.342)	Data 1.34e-04 (4.34e-04)	Tok/s 35847 (40932)	Loss/tok 3.2695 (4.6067)	LR 2.000e-03
0: TRAIN [0][1750/1938]	Time 0.284 (0.342)	Data 1.31e-04 (4.32e-04)	Tok/s 36657 (40947)	Loss/tok 3.4053 (4.6002)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.370 (0.342)	Data 1.64e-04 (4.30e-04)	Tok/s 45658 (40940)	Loss/tok 3.5637 (4.5946)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.571 (0.342)	Data 1.34e-04 (4.29e-04)	Tok/s 51987 (40944)	Loss/tok 4.0581 (4.5890)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.285 (0.342)	Data 1.43e-04 (4.27e-04)	Tok/s 35978 (40921)	Loss/tok 3.2721 (4.5839)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.285 (0.342)	Data 1.31e-04 (4.26e-04)	Tok/s 36725 (40923)	Loss/tok 3.3222 (4.5781)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.285 (0.342)	Data 1.25e-04 (4.24e-04)	Tok/s 36052 (40919)	Loss/tok 3.2615 (4.5728)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.285 (0.342)	Data 1.27e-04 (4.22e-04)	Tok/s 35909 (40907)	Loss/tok 3.3458 (4.5677)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.458 (0.342)	Data 1.36e-04 (4.21e-04)	Tok/s 51299 (40929)	Loss/tok 3.7483 (4.5614)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.370 (0.342)	Data 1.85e-04 (4.19e-04)	Tok/s 45666 (40934)	Loss/tok 3.5537 (4.5559)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.458 (0.343)	Data 1.29e-04 (4.18e-04)	Tok/s 51199 (40961)	Loss/tok 3.6625 (4.5494)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.370 (0.343)	Data 1.24e-04 (4.16e-04)	Tok/s 45863 (40967)	Loss/tok 3.5305 (4.5437)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.285 (0.342)	Data 1.39e-04 (4.15e-04)	Tok/s 36052 (40955)	Loss/tok 3.2901 (4.5387)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.285 (0.342)	Data 1.33e-04 (4.14e-04)	Tok/s 35998 (40946)	Loss/tok 3.2739 (4.5338)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.287 (0.342)	Data 1.47e-04 (4.12e-04)	Tok/s 36806 (40928)	Loss/tok 3.2718 (4.5292)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.371 (0.342)	Data 1.54e-04 (4.11e-04)	Tok/s 44942 (40922)	Loss/tok 3.6356 (4.5240)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.372 (0.342)	Data 1.42e-04 (4.09e-04)	Tok/s 44879 (40919)	Loss/tok 3.4989 (4.5186)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.370 (0.342)	Data 1.46e-04 (4.08e-04)	Tok/s 44742 (40900)	Loss/tok 3.6642 (4.5140)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.288 (0.342)	Data 1.42e-04 (4.07e-04)	Tok/s 35880 (40882)	Loss/tok 3.3820 (4.5094)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.285 (0.342)	Data 1.61e-04 (4.05e-04)	Tok/s 36112 (40876)	Loss/tok 3.1759 (4.5044)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
:::MLL 1572987344.613 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1572987344.613 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.970 (0.970)	Decoder iters 149.0 (149.0)	Tok/s 17318 (17318)
0: Running moses detokenizer
0: BLEU(score=19.769295542298995, counts=[35035, 16110, 8575, 4768], totals=[66939, 63936, 60933, 57933], precisions=[52.33869642510345, 25.19707207207207, 14.072834096466611, 8.230196951651045], bp=1.0, sys_len=66939, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1572987347.240 eval_accuracy: {"value": 19.77, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1572987347.241 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.5016	Test BLEU: 19.77
0: Performance: Epoch: 0	Training: 327191 Tok/s
0: Finished epoch 0
:::MLL 1572987347.242 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1572987347.242 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1572987347.242 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 1789559884
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][0/1938]	Time 0.869 (0.869)	Data 4.10e-01 (4.10e-01)	Tok/s 26825 (26825)	Loss/tok 3.6374 (3.6374)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.285 (0.418)	Data 1.36e-04 (3.74e-02)	Tok/s 36038 (42085)	Loss/tok 3.2442 (3.5707)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.457 (0.391)	Data 1.44e-04 (1.97e-02)	Tok/s 51522 (42762)	Loss/tok 3.6156 (3.5321)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.285 (0.379)	Data 1.46e-04 (1.34e-02)	Tok/s 37031 (42796)	Loss/tok 3.3393 (3.5214)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.368 (0.364)	Data 1.29e-04 (1.01e-02)	Tok/s 45044 (42091)	Loss/tok 3.4864 (3.4817)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.369 (0.357)	Data 1.61e-04 (8.18e-03)	Tok/s 45510 (41829)	Loss/tok 3.4100 (3.4648)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.370 (0.359)	Data 1.34e-04 (6.86e-03)	Tok/s 45461 (41914)	Loss/tok 3.5219 (3.4794)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.283 (0.356)	Data 1.37e-04 (5.92e-03)	Tok/s 36213 (41710)	Loss/tok 3.3346 (3.4779)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.284 (0.351)	Data 1.50e-04 (5.21e-03)	Tok/s 37223 (41408)	Loss/tok 3.2624 (3.4737)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.285 (0.350)	Data 1.46e-04 (4.65e-03)	Tok/s 36334 (41051)	Loss/tok 3.2693 (3.4740)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.458 (0.350)	Data 1.49e-04 (4.20e-03)	Tok/s 51249 (41130)	Loss/tok 3.5639 (3.4747)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.284 (0.350)	Data 1.31e-04 (3.84e-03)	Tok/s 35500 (41230)	Loss/tok 3.4119 (3.4700)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.283 (0.348)	Data 1.42e-04 (3.53e-03)	Tok/s 35782 (41109)	Loss/tok 3.2148 (3.4657)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.284 (0.348)	Data 1.29e-04 (3.27e-03)	Tok/s 35672 (41138)	Loss/tok 3.2269 (3.4655)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.570 (0.348)	Data 1.42e-04 (3.05e-03)	Tok/s 52770 (41162)	Loss/tok 3.7929 (3.4667)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.459 (0.348)	Data 1.34e-04 (2.86e-03)	Tok/s 50429 (41202)	Loss/tok 3.5483 (3.4646)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.371 (0.347)	Data 1.33e-04 (2.69e-03)	Tok/s 45488 (41157)	Loss/tok 3.4334 (3.4606)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.204 (0.344)	Data 1.47e-04 (2.54e-03)	Tok/s 25834 (40871)	Loss/tok 2.7259 (3.4537)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.287 (0.344)	Data 1.34e-04 (2.41e-03)	Tok/s 35081 (40755)	Loss/tok 3.2440 (3.4573)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.285 (0.344)	Data 1.44e-04 (2.29e-03)	Tok/s 36356 (40809)	Loss/tok 3.2132 (3.4572)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.369 (0.343)	Data 1.52e-04 (2.18e-03)	Tok/s 45409 (40736)	Loss/tok 3.3588 (3.4533)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.286 (0.343)	Data 1.49e-04 (2.09e-03)	Tok/s 36052 (40765)	Loss/tok 3.1798 (3.4551)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.369 (0.342)	Data 1.25e-04 (2.00e-03)	Tok/s 45738 (40669)	Loss/tok 3.5373 (3.4525)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.370 (0.342)	Data 1.55e-04 (1.92e-03)	Tok/s 45238 (40739)	Loss/tok 3.4795 (3.4522)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.371 (0.343)	Data 1.59e-04 (1.84e-03)	Tok/s 44668 (40799)	Loss/tok 3.5065 (3.4554)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.286 (0.341)	Data 1.61e-04 (1.78e-03)	Tok/s 36144 (40636)	Loss/tok 3.2996 (3.4506)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.285 (0.340)	Data 1.47e-04 (1.71e-03)	Tok/s 37229 (40583)	Loss/tok 3.2463 (3.4476)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.369 (0.338)	Data 1.35e-04 (1.66e-03)	Tok/s 45465 (40450)	Loss/tok 3.4487 (3.4419)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.285 (0.339)	Data 1.33e-04 (1.60e-03)	Tok/s 35547 (40550)	Loss/tok 3.1305 (3.4426)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][290/1938]	Time 0.571 (0.339)	Data 1.55e-04 (1.55e-03)	Tok/s 53078 (40458)	Loss/tok 3.7907 (3.4457)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][300/1938]	Time 0.285 (0.341)	Data 1.38e-04 (1.51e-03)	Tok/s 36223 (40600)	Loss/tok 3.2599 (3.4513)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.571 (0.341)	Data 1.55e-04 (1.46e-03)	Tok/s 52747 (40654)	Loss/tok 3.7105 (3.4538)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.370 (0.340)	Data 1.62e-04 (1.42e-03)	Tok/s 45766 (40561)	Loss/tok 3.4361 (3.4508)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.457 (0.340)	Data 1.48e-04 (1.38e-03)	Tok/s 50664 (40616)	Loss/tok 3.7096 (3.4495)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.458 (0.341)	Data 1.40e-04 (1.35e-03)	Tok/s 50390 (40623)	Loss/tok 3.6835 (3.4497)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.286 (0.340)	Data 1.41e-04 (1.31e-03)	Tok/s 36208 (40508)	Loss/tok 3.1066 (3.4469)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.284 (0.341)	Data 1.31e-04 (1.28e-03)	Tok/s 35906 (40635)	Loss/tok 3.1735 (3.4521)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.370 (0.340)	Data 1.55e-04 (1.25e-03)	Tok/s 45634 (40525)	Loss/tok 3.4236 (3.4486)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.285 (0.340)	Data 1.45e-04 (1.22e-03)	Tok/s 36745 (40577)	Loss/tok 3.2504 (3.4491)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.373 (0.340)	Data 1.28e-04 (1.19e-03)	Tok/s 44148 (40554)	Loss/tok 3.5762 (3.4467)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.286 (0.340)	Data 1.47e-04 (1.17e-03)	Tok/s 35503 (40604)	Loss/tok 3.2239 (3.4461)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.286 (0.339)	Data 1.32e-04 (1.14e-03)	Tok/s 35905 (40521)	Loss/tok 3.1206 (3.4437)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.458 (0.340)	Data 1.46e-04 (1.12e-03)	Tok/s 50601 (40611)	Loss/tok 3.6317 (3.4469)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.369 (0.340)	Data 1.40e-04 (1.09e-03)	Tok/s 45601 (40637)	Loss/tok 3.4149 (3.4451)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.283 (0.340)	Data 1.44e-04 (1.07e-03)	Tok/s 36733 (40634)	Loss/tok 3.2053 (3.4434)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.284 (0.339)	Data 1.45e-04 (1.05e-03)	Tok/s 36257 (40576)	Loss/tok 3.2194 (3.4407)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.458 (0.339)	Data 1.48e-04 (1.03e-03)	Tok/s 50294 (40591)	Loss/tok 3.7219 (3.4401)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.203 (0.339)	Data 1.43e-04 (1.01e-03)	Tok/s 25632 (40635)	Loss/tok 2.8725 (3.4391)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.371 (0.340)	Data 1.35e-04 (9.96e-04)	Tok/s 45471 (40718)	Loss/tok 3.5086 (3.4419)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.370 (0.340)	Data 1.33e-04 (9.78e-04)	Tok/s 45028 (40736)	Loss/tok 3.5000 (3.4407)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.285 (0.340)	Data 1.29e-04 (9.62e-04)	Tok/s 35962 (40712)	Loss/tok 3.3158 (3.4402)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.286 (0.340)	Data 1.29e-04 (9.46e-04)	Tok/s 36591 (40676)	Loss/tok 3.1140 (3.4380)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.287 (0.339)	Data 1.56e-04 (9.30e-04)	Tok/s 35807 (40645)	Loss/tok 3.3525 (3.4377)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.285 (0.339)	Data 1.49e-04 (9.15e-04)	Tok/s 36322 (40646)	Loss/tok 3.2041 (3.4385)	LR 2.000e-03
0: TRAIN [1][540/1938]	Time 0.459 (0.340)	Data 1.46e-04 (9.01e-04)	Tok/s 50980 (40676)	Loss/tok 3.6145 (3.4397)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.287 (0.340)	Data 1.45e-04 (8.87e-04)	Tok/s 35478 (40700)	Loss/tok 3.2785 (3.4416)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.369 (0.341)	Data 1.37e-04 (8.74e-04)	Tok/s 45908 (40768)	Loss/tok 3.4070 (3.4426)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.369 (0.341)	Data 1.46e-04 (8.61e-04)	Tok/s 45753 (40772)	Loss/tok 3.3287 (3.4422)	LR 2.000e-03
0: TRAIN [1][580/1938]	Time 0.370 (0.342)	Data 1.27e-04 (8.49e-04)	Tok/s 44852 (40826)	Loss/tok 3.3615 (3.4427)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.459 (0.342)	Data 1.55e-04 (8.37e-04)	Tok/s 51388 (40833)	Loss/tok 3.5389 (3.4429)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.457 (0.342)	Data 1.47e-04 (8.25e-04)	Tok/s 51529 (40836)	Loss/tok 3.5576 (3.4423)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.458 (0.341)	Data 1.54e-04 (8.14e-04)	Tok/s 50994 (40819)	Loss/tok 3.6078 (3.4410)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.369 (0.341)	Data 1.51e-04 (8.03e-04)	Tok/s 44680 (40840)	Loss/tok 3.4843 (3.4402)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.370 (0.341)	Data 1.46e-04 (7.93e-04)	Tok/s 45681 (40805)	Loss/tok 3.5228 (3.4387)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.370 (0.341)	Data 1.41e-04 (7.83e-04)	Tok/s 45171 (40820)	Loss/tok 3.3449 (3.4382)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.283 (0.341)	Data 1.29e-04 (7.73e-04)	Tok/s 35744 (40804)	Loss/tok 3.1630 (3.4386)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.458 (0.341)	Data 1.32e-04 (7.63e-04)	Tok/s 50862 (40833)	Loss/tok 3.6783 (3.4387)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.285 (0.341)	Data 1.48e-04 (7.54e-04)	Tok/s 36479 (40826)	Loss/tok 3.0701 (3.4384)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.287 (0.341)	Data 1.49e-04 (7.45e-04)	Tok/s 35833 (40806)	Loss/tok 3.2062 (3.4377)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][690/1938]	Time 0.571 (0.342)	Data 1.58e-04 (7.36e-04)	Tok/s 52049 (40897)	Loss/tok 3.9037 (3.4404)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.369 (0.342)	Data 1.31e-04 (7.28e-04)	Tok/s 45640 (40879)	Loss/tok 3.3730 (3.4394)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.285 (0.342)	Data 1.42e-04 (7.20e-04)	Tok/s 35861 (40873)	Loss/tok 3.2607 (3.4382)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.282 (0.342)	Data 1.61e-04 (7.12e-04)	Tok/s 36031 (40871)	Loss/tok 3.1668 (3.4369)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.371 (0.341)	Data 1.58e-04 (7.04e-04)	Tok/s 45540 (40853)	Loss/tok 3.3226 (3.4357)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.369 (0.341)	Data 1.48e-04 (6.97e-04)	Tok/s 45428 (40860)	Loss/tok 3.4991 (3.4346)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.370 (0.342)	Data 1.38e-04 (6.89e-04)	Tok/s 45229 (40925)	Loss/tok 3.4258 (3.4363)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.285 (0.342)	Data 1.46e-04 (6.82e-04)	Tok/s 36094 (40903)	Loss/tok 3.1518 (3.4363)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.286 (0.341)	Data 1.48e-04 (6.75e-04)	Tok/s 36562 (40865)	Loss/tok 3.2560 (3.4345)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.284 (0.343)	Data 1.54e-04 (6.69e-04)	Tok/s 36899 (40941)	Loss/tok 3.1979 (3.4373)	LR 2.000e-03
0: TRAIN [1][790/1938]	Time 0.283 (0.342)	Data 1.49e-04 (6.62e-04)	Tok/s 36846 (40924)	Loss/tok 3.1824 (3.4366)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.373 (0.342)	Data 1.49e-04 (6.55e-04)	Tok/s 45134 (40927)	Loss/tok 3.4751 (3.4363)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.570 (0.342)	Data 1.48e-04 (6.49e-04)	Tok/s 53036 (40922)	Loss/tok 3.6452 (3.4353)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.286 (0.342)	Data 1.54e-04 (6.43e-04)	Tok/s 36031 (40898)	Loss/tok 3.1730 (3.4343)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.284 (0.342)	Data 1.46e-04 (6.37e-04)	Tok/s 36727 (40904)	Loss/tok 3.2251 (3.4337)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.285 (0.341)	Data 1.31e-04 (6.31e-04)	Tok/s 36684 (40848)	Loss/tok 3.1520 (3.4324)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.286 (0.341)	Data 1.42e-04 (6.26e-04)	Tok/s 35347 (40808)	Loss/tok 3.1320 (3.4309)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.287 (0.341)	Data 1.57e-04 (6.20e-04)	Tok/s 35897 (40807)	Loss/tok 3.3348 (3.4299)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.282 (0.341)	Data 1.33e-04 (6.15e-04)	Tok/s 36480 (40835)	Loss/tok 3.1451 (3.4307)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.285 (0.341)	Data 1.29e-04 (6.09e-04)	Tok/s 36352 (40821)	Loss/tok 3.1064 (3.4297)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.287 (0.341)	Data 1.49e-04 (6.04e-04)	Tok/s 35985 (40805)	Loss/tok 3.2355 (3.4286)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.285 (0.341)	Data 1.52e-04 (5.99e-04)	Tok/s 36525 (40807)	Loss/tok 3.1065 (3.4273)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.459 (0.341)	Data 1.27e-04 (5.94e-04)	Tok/s 50763 (40795)	Loss/tok 3.5747 (3.4266)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.283 (0.340)	Data 1.53e-04 (5.89e-04)	Tok/s 36026 (40782)	Loss/tok 3.2697 (3.4259)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.203 (0.341)	Data 1.45e-04 (5.85e-04)	Tok/s 26101 (40785)	Loss/tok 2.6818 (3.4263)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.458 (0.341)	Data 1.43e-04 (5.80e-04)	Tok/s 50688 (40831)	Loss/tok 3.6267 (3.4283)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.370 (0.341)	Data 1.91e-04 (5.75e-04)	Tok/s 44807 (40834)	Loss/tok 3.3633 (3.4277)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.371 (0.341)	Data 1.53e-04 (5.71e-04)	Tok/s 45693 (40830)	Loss/tok 3.4193 (3.4267)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.572 (0.341)	Data 1.41e-04 (5.67e-04)	Tok/s 51934 (40799)	Loss/tok 3.8015 (3.4267)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.374 (0.341)	Data 1.40e-04 (5.62e-04)	Tok/s 45059 (40783)	Loss/tok 3.3802 (3.4256)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.205 (0.341)	Data 1.53e-04 (5.58e-04)	Tok/s 25790 (40792)	Loss/tok 2.6197 (3.4258)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.288 (0.341)	Data 1.38e-04 (5.54e-04)	Tok/s 35863 (40768)	Loss/tok 3.1763 (3.4245)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.372 (0.341)	Data 1.42e-04 (5.50e-04)	Tok/s 45201 (40769)	Loss/tok 3.4551 (3.4237)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1020/1938]	Time 0.372 (0.341)	Data 1.47e-04 (5.46e-04)	Tok/s 44437 (40772)	Loss/tok 3.4632 (3.4240)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.284 (0.341)	Data 1.51e-04 (5.42e-04)	Tok/s 35224 (40753)	Loss/tok 3.1348 (3.4223)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.572 (0.341)	Data 1.39e-04 (5.38e-04)	Tok/s 52837 (40763)	Loss/tok 3.7193 (3.4227)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.285 (0.341)	Data 1.50e-04 (5.34e-04)	Tok/s 36120 (40762)	Loss/tok 3.3181 (3.4218)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.284 (0.341)	Data 1.49e-04 (5.31e-04)	Tok/s 37395 (40791)	Loss/tok 3.1831 (3.4229)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.460 (0.341)	Data 1.63e-04 (5.27e-04)	Tok/s 50101 (40790)	Loss/tok 3.6987 (3.4229)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1080/1938]	Time 0.372 (0.341)	Data 1.49e-04 (5.24e-04)	Tok/s 45564 (40803)	Loss/tok 3.3680 (3.4233)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.284 (0.342)	Data 1.49e-04 (5.20e-04)	Tok/s 36286 (40815)	Loss/tok 3.1202 (3.4235)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.284 (0.341)	Data 1.42e-04 (5.17e-04)	Tok/s 35327 (40796)	Loss/tok 3.1422 (3.4225)	LR 2.000e-03
0: TRAIN [1][1110/1938]	Time 0.370 (0.342)	Data 1.51e-04 (5.13e-04)	Tok/s 45527 (40800)	Loss/tok 3.2955 (3.4227)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.370 (0.342)	Data 1.26e-04 (5.10e-04)	Tok/s 45177 (40835)	Loss/tok 3.3630 (3.4238)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.284 (0.342)	Data 1.58e-04 (5.07e-04)	Tok/s 35793 (40852)	Loss/tok 3.1038 (3.4236)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.284 (0.342)	Data 1.29e-04 (5.04e-04)	Tok/s 35788 (40841)	Loss/tok 3.2760 (3.4230)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.285 (0.342)	Data 1.47e-04 (5.01e-04)	Tok/s 35837 (40856)	Loss/tok 3.1256 (3.4230)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.286 (0.342)	Data 1.33e-04 (4.98e-04)	Tok/s 35946 (40852)	Loss/tok 3.2710 (3.4226)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.284 (0.342)	Data 1.51e-04 (4.94e-04)	Tok/s 36069 (40856)	Loss/tok 3.1862 (3.4226)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.573 (0.343)	Data 1.36e-04 (4.92e-04)	Tok/s 51876 (40872)	Loss/tok 3.8010 (3.4226)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.287 (0.342)	Data 1.27e-04 (4.89e-04)	Tok/s 35601 (40839)	Loss/tok 3.0841 (3.4215)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.286 (0.342)	Data 1.42e-04 (4.86e-04)	Tok/s 36364 (40834)	Loss/tok 3.1506 (3.4207)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.285 (0.342)	Data 1.34e-04 (4.83e-04)	Tok/s 37317 (40845)	Loss/tok 3.1350 (3.4206)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.370 (0.343)	Data 1.46e-04 (4.80e-04)	Tok/s 44726 (40856)	Loss/tok 3.3631 (3.4212)	LR 2.000e-03
0: TRAIN [1][1230/1938]	Time 0.370 (0.343)	Data 1.43e-04 (4.77e-04)	Tok/s 45558 (40871)	Loss/tok 3.2950 (3.4209)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.574 (0.343)	Data 1.44e-04 (4.75e-04)	Tok/s 52215 (40903)	Loss/tok 3.7312 (3.4218)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1250/1938]	Time 0.371 (0.343)	Data 1.31e-04 (4.72e-04)	Tok/s 45596 (40883)	Loss/tok 3.4281 (3.4209)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.287 (0.343)	Data 1.35e-04 (4.70e-04)	Tok/s 35490 (40888)	Loss/tok 3.1906 (3.4210)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.288 (0.343)	Data 1.31e-04 (4.67e-04)	Tok/s 36189 (40921)	Loss/tok 3.1453 (3.4213)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.287 (0.343)	Data 1.40e-04 (4.64e-04)	Tok/s 35527 (40913)	Loss/tok 3.0781 (3.4207)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.284 (0.343)	Data 1.48e-04 (4.62e-04)	Tok/s 36383 (40905)	Loss/tok 3.1850 (3.4201)	LR 2.000e-03
0: TRAIN [1][1300/1938]	Time 0.285 (0.343)	Data 1.38e-04 (4.60e-04)	Tok/s 36707 (40894)	Loss/tok 3.0386 (3.4189)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.459 (0.343)	Data 1.29e-04 (4.57e-04)	Tok/s 50553 (40911)	Loss/tok 3.5209 (3.4197)	LR 2.000e-03
0: TRAIN [1][1320/1938]	Time 0.287 (0.344)	Data 1.40e-04 (4.55e-04)	Tok/s 35952 (40914)	Loss/tok 3.1847 (3.4195)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.457 (0.343)	Data 1.25e-04 (4.52e-04)	Tok/s 50683 (40887)	Loss/tok 3.5649 (3.4189)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.459 (0.343)	Data 1.36e-04 (4.50e-04)	Tok/s 50373 (40873)	Loss/tok 3.6178 (3.4182)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.374 (0.343)	Data 1.45e-04 (4.48e-04)	Tok/s 45792 (40889)	Loss/tok 3.3157 (3.4175)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.283 (0.343)	Data 1.41e-04 (4.46e-04)	Tok/s 36236 (40889)	Loss/tok 3.1345 (3.4169)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.459 (0.343)	Data 1.42e-04 (4.43e-04)	Tok/s 51761 (40883)	Loss/tok 3.6101 (3.4168)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.287 (0.343)	Data 1.37e-04 (4.41e-04)	Tok/s 35696 (40882)	Loss/tok 3.1808 (3.4165)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.285 (0.343)	Data 1.50e-04 (4.39e-04)	Tok/s 36261 (40885)	Loss/tok 3.2353 (3.4158)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.571 (0.343)	Data 1.46e-04 (4.37e-04)	Tok/s 51799 (40896)	Loss/tok 3.7827 (3.4159)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.371 (0.343)	Data 1.53e-04 (4.35e-04)	Tok/s 45317 (40886)	Loss/tok 3.4054 (3.4149)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.285 (0.344)	Data 1.91e-04 (4.33e-04)	Tok/s 36268 (40919)	Loss/tok 3.1815 (3.4153)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.287 (0.343)	Data 1.44e-04 (4.31e-04)	Tok/s 36231 (40908)	Loss/tok 3.0993 (3.4144)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.286 (0.344)	Data 1.31e-04 (4.29e-04)	Tok/s 36504 (40905)	Loss/tok 3.1268 (3.4145)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.287 (0.344)	Data 1.34e-04 (4.27e-04)	Tok/s 36840 (40911)	Loss/tok 3.1599 (3.4148)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.459 (0.344)	Data 1.31e-04 (4.25e-04)	Tok/s 51254 (40920)	Loss/tok 3.4664 (3.4145)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.287 (0.344)	Data 1.24e-04 (4.23e-04)	Tok/s 35981 (40917)	Loss/tok 3.1728 (3.4138)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.370 (0.344)	Data 1.46e-04 (4.21e-04)	Tok/s 45622 (40914)	Loss/tok 3.3720 (3.4130)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.460 (0.344)	Data 1.40e-04 (4.19e-04)	Tok/s 50468 (40901)	Loss/tok 3.5219 (3.4128)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.458 (0.344)	Data 1.35e-04 (4.17e-04)	Tok/s 50930 (40896)	Loss/tok 3.4637 (3.4126)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.371 (0.343)	Data 1.29e-04 (4.16e-04)	Tok/s 45034 (40888)	Loss/tok 3.3931 (3.4119)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.370 (0.343)	Data 1.86e-04 (4.14e-04)	Tok/s 45132 (40859)	Loss/tok 3.3204 (3.4108)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.284 (0.343)	Data 1.28e-04 (4.12e-04)	Tok/s 36446 (40851)	Loss/tok 3.3355 (3.4103)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1540/1938]	Time 0.202 (0.343)	Data 1.33e-04 (4.10e-04)	Tok/s 26269 (40838)	Loss/tok 2.5985 (3.4099)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.205 (0.343)	Data 1.45e-04 (4.09e-04)	Tok/s 26142 (40834)	Loss/tok 2.6732 (3.4098)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.288 (0.343)	Data 1.43e-04 (4.07e-04)	Tok/s 36542 (40817)	Loss/tok 3.1270 (3.4092)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.286 (0.342)	Data 1.29e-04 (4.05e-04)	Tok/s 36410 (40802)	Loss/tok 3.0113 (3.4085)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.286 (0.342)	Data 1.35e-04 (4.03e-04)	Tok/s 35981 (40779)	Loss/tok 3.2004 (3.4080)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.203 (0.342)	Data 1.43e-04 (4.02e-04)	Tok/s 25870 (40792)	Loss/tok 2.6859 (3.4076)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.288 (0.343)	Data 1.27e-04 (4.00e-04)	Tok/s 35698 (40808)	Loss/tok 3.1233 (3.4078)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.287 (0.342)	Data 1.37e-04 (3.98e-04)	Tok/s 36337 (40794)	Loss/tok 3.0758 (3.4070)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.204 (0.342)	Data 1.44e-04 (3.97e-04)	Tok/s 26253 (40786)	Loss/tok 2.7610 (3.4065)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.287 (0.342)	Data 1.57e-04 (3.95e-04)	Tok/s 35992 (40756)	Loss/tok 3.2092 (3.4057)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.376 (0.342)	Data 1.55e-04 (3.94e-04)	Tok/s 44535 (40761)	Loss/tok 3.2747 (3.4057)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.374 (0.342)	Data 1.27e-04 (3.92e-04)	Tok/s 44691 (40720)	Loss/tok 3.3226 (3.4046)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.287 (0.342)	Data 1.44e-04 (3.91e-04)	Tok/s 35214 (40724)	Loss/tok 3.0731 (3.4044)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.573 (0.342)	Data 1.42e-04 (3.89e-04)	Tok/s 51937 (40752)	Loss/tok 3.6836 (3.4052)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1680/1938]	Time 0.203 (0.342)	Data 1.53e-04 (3.88e-04)	Tok/s 26301 (40735)	Loss/tok 2.5426 (3.4048)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.374 (0.342)	Data 1.60e-04 (3.86e-04)	Tok/s 45251 (40756)	Loss/tok 3.2846 (3.4052)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.372 (0.342)	Data 1.42e-04 (3.85e-04)	Tok/s 44796 (40753)	Loss/tok 3.3884 (3.4048)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.459 (0.342)	Data 1.34e-04 (3.83e-04)	Tok/s 50095 (40759)	Loss/tok 3.5028 (3.4043)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.284 (0.342)	Data 1.33e-04 (3.82e-04)	Tok/s 36922 (40746)	Loss/tok 3.1684 (3.4036)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.459 (0.342)	Data 1.32e-04 (3.81e-04)	Tok/s 51040 (40762)	Loss/tok 3.4638 (3.4032)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.372 (0.342)	Data 1.30e-04 (3.79e-04)	Tok/s 44638 (40764)	Loss/tok 3.3627 (3.4027)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.459 (0.342)	Data 1.49e-04 (3.78e-04)	Tok/s 51379 (40777)	Loss/tok 3.4812 (3.4025)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.371 (0.342)	Data 1.49e-04 (3.77e-04)	Tok/s 45173 (40786)	Loss/tok 3.4091 (3.4024)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.373 (0.342)	Data 1.51e-04 (3.75e-04)	Tok/s 45273 (40771)	Loss/tok 3.2841 (3.4016)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.459 (0.343)	Data 1.30e-04 (3.74e-04)	Tok/s 51248 (40797)	Loss/tok 3.4928 (3.4021)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.286 (0.343)	Data 1.54e-04 (3.73e-04)	Tok/s 36538 (40803)	Loss/tok 3.1254 (3.4019)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.287 (0.343)	Data 1.35e-04 (3.71e-04)	Tok/s 36388 (40790)	Loss/tok 3.0162 (3.4014)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.287 (0.343)	Data 1.32e-04 (3.70e-04)	Tok/s 36529 (40788)	Loss/tok 3.1852 (3.4010)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.370 (0.342)	Data 1.42e-04 (3.69e-04)	Tok/s 45458 (40779)	Loss/tok 3.2983 (3.4004)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.287 (0.342)	Data 1.55e-04 (3.68e-04)	Tok/s 37050 (40762)	Loss/tok 3.2082 (3.3996)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.368 (0.342)	Data 1.77e-04 (3.66e-04)	Tok/s 45857 (40759)	Loss/tok 3.3086 (3.3990)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1850/1938]	Time 0.284 (0.342)	Data 1.32e-04 (3.65e-04)	Tok/s 36217 (40753)	Loss/tok 3.1883 (3.3983)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.204 (0.342)	Data 1.50e-04 (3.64e-04)	Tok/s 25974 (40771)	Loss/tok 2.7388 (3.3992)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1870/1938]	Time 0.459 (0.343)	Data 1.66e-04 (3.63e-04)	Tok/s 50676 (40790)	Loss/tok 3.5498 (3.3994)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.371 (0.343)	Data 1.48e-04 (3.62e-04)	Tok/s 45713 (40814)	Loss/tok 3.3101 (3.3996)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.370 (0.343)	Data 1.78e-04 (3.61e-04)	Tok/s 45414 (40801)	Loss/tok 3.3109 (3.3988)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.203 (0.342)	Data 1.49e-04 (3.59e-04)	Tok/s 26333 (40789)	Loss/tok 2.7450 (3.3980)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.460 (0.343)	Data 1.58e-04 (3.58e-04)	Tok/s 50358 (40798)	Loss/tok 3.4569 (3.3981)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.458 (0.343)	Data 1.33e-04 (3.57e-04)	Tok/s 51013 (40811)	Loss/tok 3.4907 (3.3978)	LR 2.000e-03
0: TRAIN [1][1930/1938]	Time 0.370 (0.343)	Data 1.43e-04 (3.56e-04)	Tok/s 45656 (40812)	Loss/tok 3.3815 (3.3976)	LR 2.000e-03
:::MLL 1572988012.623 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1572988012.624 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.828 (0.828)	Decoder iters 106.0 (106.0)	Tok/s 19828 (19828)
0: Running moses detokenizer
0: BLEU(score=22.120770259933117, counts=[35647, 17074, 9416, 5434], totals=[64305, 61302, 58300, 55301], precisions=[55.434258611305495, 27.852272356529966, 16.150943396226417, 9.826223757255747], bp=0.9942472306172374, sys_len=64305, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1572988015.044 eval_accuracy: {"value": 22.12, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1572988015.045 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.3974	Test BLEU: 22.12
0: Performance: Epoch: 1	Training: 326478 Tok/s
0: Finished epoch 1
:::MLL 1572988015.045 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1572988015.046 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1572988015.046 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 3292842447
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [2][0/1938]	Time 0.618 (0.618)	Data 4.18e-01 (4.18e-01)	Tok/s 8629 (8629)	Loss/tok 2.6873 (2.6873)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.286 (0.374)	Data 1.32e-04 (3.82e-02)	Tok/s 35581 (38193)	Loss/tok 3.0367 (3.2272)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.368 (0.362)	Data 1.31e-04 (2.01e-02)	Tok/s 46287 (39495)	Loss/tok 3.2289 (3.2579)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.369 (0.345)	Data 1.60e-04 (1.36e-02)	Tok/s 44776 (39273)	Loss/tok 3.2901 (3.2168)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.285 (0.355)	Data 1.38e-04 (1.03e-02)	Tok/s 36321 (40129)	Loss/tok 3.0431 (3.2641)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.287 (0.355)	Data 1.32e-04 (8.34e-03)	Tok/s 35820 (40560)	Loss/tok 3.0773 (3.2690)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.458 (0.355)	Data 1.48e-04 (7.00e-03)	Tok/s 50711 (40973)	Loss/tok 3.5322 (3.2663)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.285 (0.347)	Data 1.54e-04 (6.03e-03)	Tok/s 36860 (40594)	Loss/tok 2.9871 (3.2477)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.284 (0.346)	Data 1.37e-04 (5.31e-03)	Tok/s 36254 (40660)	Loss/tok 2.9140 (3.2407)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.285 (0.345)	Data 1.39e-04 (4.74e-03)	Tok/s 36080 (40580)	Loss/tok 3.0900 (3.2394)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.284 (0.341)	Data 1.53e-04 (4.28e-03)	Tok/s 36565 (40324)	Loss/tok 3.0937 (3.2281)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.287 (0.338)	Data 1.53e-04 (3.91e-03)	Tok/s 36311 (40159)	Loss/tok 3.0771 (3.2243)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.284 (0.340)	Data 1.34e-04 (3.60e-03)	Tok/s 36578 (40383)	Loss/tok 3.0141 (3.2270)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.286 (0.336)	Data 1.39e-04 (3.34e-03)	Tok/s 36446 (40080)	Loss/tok 3.1879 (3.2222)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.285 (0.339)	Data 1.32e-04 (3.11e-03)	Tok/s 35585 (40256)	Loss/tok 3.0911 (3.2324)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.458 (0.345)	Data 1.50e-04 (2.91e-03)	Tok/s 50996 (40678)	Loss/tok 3.3764 (3.2471)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.373 (0.348)	Data 1.56e-04 (2.74e-03)	Tok/s 44849 (40957)	Loss/tok 3.1972 (3.2543)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.371 (0.347)	Data 1.67e-04 (2.59e-03)	Tok/s 45180 (40924)	Loss/tok 3.2101 (3.2520)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.370 (0.348)	Data 1.31e-04 (2.46e-03)	Tok/s 45112 (41084)	Loss/tok 3.2443 (3.2572)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.286 (0.350)	Data 1.29e-04 (2.33e-03)	Tok/s 36508 (41188)	Loss/tok 3.1481 (3.2657)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.288 (0.349)	Data 1.44e-04 (2.23e-03)	Tok/s 35963 (41065)	Loss/tok 3.0326 (3.2622)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.370 (0.348)	Data 1.47e-04 (2.13e-03)	Tok/s 46029 (41037)	Loss/tok 3.2365 (3.2597)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.369 (0.347)	Data 1.53e-04 (2.04e-03)	Tok/s 45791 (41031)	Loss/tok 3.2644 (3.2585)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.286 (0.349)	Data 1.41e-04 (1.96e-03)	Tok/s 36921 (41161)	Loss/tok 2.9737 (3.2634)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.204 (0.348)	Data 1.29e-04 (1.88e-03)	Tok/s 26070 (41122)	Loss/tok 2.6441 (3.2615)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.370 (0.348)	Data 1.27e-04 (1.81e-03)	Tok/s 44830 (41138)	Loss/tok 3.3400 (3.2616)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.372 (0.350)	Data 1.31e-04 (1.75e-03)	Tok/s 45320 (41316)	Loss/tok 3.2521 (3.2668)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.286 (0.349)	Data 1.47e-04 (1.69e-03)	Tok/s 35981 (41258)	Loss/tok 3.2309 (3.2636)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.286 (0.348)	Data 1.39e-04 (1.63e-03)	Tok/s 35852 (41177)	Loss/tok 2.9744 (3.2628)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.459 (0.348)	Data 1.34e-04 (1.58e-03)	Tok/s 50799 (41201)	Loss/tok 3.3945 (3.2627)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.285 (0.348)	Data 1.34e-04 (1.53e-03)	Tok/s 35767 (41147)	Loss/tok 2.9487 (3.2632)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.203 (0.348)	Data 1.26e-04 (1.49e-03)	Tok/s 26083 (41180)	Loss/tok 2.5541 (3.2636)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.203 (0.346)	Data 1.43e-04 (1.45e-03)	Tok/s 25886 (41026)	Loss/tok 2.5783 (3.2595)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.284 (0.346)	Data 1.34e-04 (1.41e-03)	Tok/s 36735 (41012)	Loss/tok 2.9343 (3.2591)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.458 (0.346)	Data 1.47e-04 (1.37e-03)	Tok/s 50696 (41034)	Loss/tok 3.4473 (3.2588)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.369 (0.346)	Data 1.61e-04 (1.34e-03)	Tok/s 45770 (41047)	Loss/tok 3.1606 (3.2576)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.287 (0.346)	Data 1.51e-04 (1.30e-03)	Tok/s 36075 (41088)	Loss/tok 2.9869 (3.2576)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.458 (0.346)	Data 1.60e-04 (1.27e-03)	Tok/s 51304 (41115)	Loss/tok 3.3433 (3.2565)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.287 (0.346)	Data 1.36e-04 (1.24e-03)	Tok/s 36255 (41066)	Loss/tok 3.1724 (3.2573)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][390/1938]	Time 0.375 (0.345)	Data 1.36e-04 (1.21e-03)	Tok/s 45200 (40947)	Loss/tok 3.3174 (3.2566)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.203 (0.344)	Data 1.31e-04 (1.19e-03)	Tok/s 26704 (40909)	Loss/tok 2.6921 (3.2571)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.286 (0.345)	Data 1.58e-04 (1.16e-03)	Tok/s 36277 (40962)	Loss/tok 3.0836 (3.2569)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.205 (0.344)	Data 1.38e-04 (1.14e-03)	Tok/s 26019 (40831)	Loss/tok 2.5809 (3.2549)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.572 (0.344)	Data 1.33e-04 (1.12e-03)	Tok/s 52956 (40813)	Loss/tok 3.5668 (3.2554)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.206 (0.344)	Data 1.36e-04 (1.09e-03)	Tok/s 26039 (40838)	Loss/tok 2.6419 (3.2556)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.287 (0.342)	Data 1.45e-04 (1.07e-03)	Tok/s 36302 (40705)	Loss/tok 2.9950 (3.2531)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.369 (0.343)	Data 1.92e-04 (1.05e-03)	Tok/s 46062 (40727)	Loss/tok 3.2060 (3.2532)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.372 (0.342)	Data 1.53e-04 (1.03e-03)	Tok/s 44452 (40677)	Loss/tok 3.2223 (3.2520)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.286 (0.341)	Data 1.51e-04 (1.01e-03)	Tok/s 37093 (40638)	Loss/tok 3.0169 (3.2499)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.285 (0.340)	Data 1.46e-04 (9.96e-04)	Tok/s 35751 (40558)	Loss/tok 3.1469 (3.2478)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.369 (0.340)	Data 1.52e-04 (9.79e-04)	Tok/s 44722 (40517)	Loss/tok 3.4330 (3.2481)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.205 (0.340)	Data 1.34e-04 (9.63e-04)	Tok/s 26065 (40490)	Loss/tok 2.5324 (3.2500)	LR 2.000e-03
0: TRAIN [2][520/1938]	Time 0.375 (0.340)	Data 1.30e-04 (9.47e-04)	Tok/s 43857 (40470)	Loss/tok 3.3307 (3.2500)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.284 (0.340)	Data 1.50e-04 (9.32e-04)	Tok/s 35829 (40456)	Loss/tok 2.9754 (3.2485)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.284 (0.340)	Data 1.33e-04 (9.18e-04)	Tok/s 36306 (40477)	Loss/tok 3.0669 (3.2507)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.285 (0.340)	Data 1.50e-04 (9.04e-04)	Tok/s 36752 (40464)	Loss/tok 3.0129 (3.2502)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.287 (0.340)	Data 1.57e-04 (8.90e-04)	Tok/s 36196 (40451)	Loss/tok 3.0155 (3.2493)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.369 (0.340)	Data 1.50e-04 (8.77e-04)	Tok/s 45824 (40457)	Loss/tok 3.2796 (3.2485)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.287 (0.339)	Data 1.29e-04 (8.64e-04)	Tok/s 36035 (40409)	Loss/tok 3.0841 (3.2470)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.287 (0.338)	Data 1.35e-04 (8.52e-04)	Tok/s 36157 (40357)	Loss/tok 3.0960 (3.2454)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][600/1938]	Time 0.371 (0.339)	Data 1.43e-04 (8.40e-04)	Tok/s 45723 (40444)	Loss/tok 3.1983 (3.2473)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.370 (0.339)	Data 1.51e-04 (8.29e-04)	Tok/s 45385 (40477)	Loss/tok 3.2574 (3.2468)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.372 (0.339)	Data 1.43e-04 (8.18e-04)	Tok/s 45582 (40419)	Loss/tok 3.2816 (3.2451)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.370 (0.339)	Data 1.34e-04 (8.07e-04)	Tok/s 45087 (40426)	Loss/tok 3.1876 (3.2456)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.371 (0.339)	Data 1.61e-04 (7.97e-04)	Tok/s 44822 (40399)	Loss/tok 3.2619 (3.2460)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.571 (0.340)	Data 1.39e-04 (7.87e-04)	Tok/s 51906 (40473)	Loss/tok 3.7224 (3.2489)	LR 2.000e-03
0: TRAIN [2][660/1938]	Time 0.286 (0.339)	Data 1.34e-04 (7.78e-04)	Tok/s 36229 (40455)	Loss/tok 3.0560 (3.2488)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.458 (0.340)	Data 1.44e-04 (7.68e-04)	Tok/s 50949 (40524)	Loss/tok 3.4052 (3.2494)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.285 (0.340)	Data 1.53e-04 (7.59e-04)	Tok/s 36730 (40523)	Loss/tok 3.0316 (3.2490)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.459 (0.340)	Data 1.30e-04 (7.50e-04)	Tok/s 50593 (40541)	Loss/tok 3.4906 (3.2489)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.287 (0.340)	Data 1.58e-04 (7.41e-04)	Tok/s 36477 (40552)	Loss/tok 3.0182 (3.2485)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.206 (0.340)	Data 1.51e-04 (7.33e-04)	Tok/s 25238 (40513)	Loss/tok 2.5587 (3.2472)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.286 (0.339)	Data 1.30e-04 (7.25e-04)	Tok/s 35949 (40465)	Loss/tok 3.0742 (3.2458)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.460 (0.340)	Data 1.59e-04 (7.17e-04)	Tok/s 50381 (40522)	Loss/tok 3.4752 (3.2479)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][740/1938]	Time 0.363 (0.340)	Data 1.62e-04 (7.09e-04)	Tok/s 45972 (40567)	Loss/tok 3.2999 (3.2498)	LR 2.000e-03
0: TRAIN [2][750/1938]	Time 0.284 (0.340)	Data 1.37e-04 (7.01e-04)	Tok/s 35326 (40517)	Loss/tok 3.0861 (3.2484)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.572 (0.340)	Data 1.51e-04 (6.94e-04)	Tok/s 51716 (40523)	Loss/tok 3.6311 (3.2494)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.570 (0.341)	Data 1.48e-04 (6.87e-04)	Tok/s 51949 (40593)	Loss/tok 3.6921 (3.2522)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.287 (0.341)	Data 1.33e-04 (6.80e-04)	Tok/s 36271 (40563)	Loss/tok 3.0610 (3.2513)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.286 (0.340)	Data 1.28e-04 (6.73e-04)	Tok/s 35844 (40537)	Loss/tok 3.0107 (3.2498)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.283 (0.340)	Data 1.29e-04 (6.67e-04)	Tok/s 36794 (40517)	Loss/tok 3.0290 (3.2489)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.572 (0.340)	Data 1.32e-04 (6.60e-04)	Tok/s 51448 (40514)	Loss/tok 3.6678 (3.2492)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.375 (0.339)	Data 1.36e-04 (6.54e-04)	Tok/s 44828 (40482)	Loss/tok 3.3052 (3.2481)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.460 (0.340)	Data 1.35e-04 (6.48e-04)	Tok/s 50230 (40517)	Loss/tok 3.4978 (3.2492)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.369 (0.339)	Data 1.50e-04 (6.42e-04)	Tok/s 46063 (40464)	Loss/tok 3.1845 (3.2482)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.370 (0.339)	Data 1.46e-04 (6.36e-04)	Tok/s 45189 (40474)	Loss/tok 3.1831 (3.2477)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.459 (0.339)	Data 1.36e-04 (6.30e-04)	Tok/s 51363 (40497)	Loss/tok 3.3662 (3.2483)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.374 (0.339)	Data 1.57e-04 (6.25e-04)	Tok/s 44173 (40473)	Loss/tok 3.3155 (3.2480)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.284 (0.339)	Data 1.55e-04 (6.20e-04)	Tok/s 36532 (40408)	Loss/tok 3.0705 (3.2465)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.369 (0.339)	Data 1.47e-04 (6.14e-04)	Tok/s 45677 (40450)	Loss/tok 3.2993 (3.2479)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.285 (0.339)	Data 1.35e-04 (6.09e-04)	Tok/s 35842 (40475)	Loss/tok 3.0259 (3.2482)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.369 (0.339)	Data 1.33e-04 (6.04e-04)	Tok/s 45124 (40458)	Loss/tok 3.2192 (3.2489)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.206 (0.339)	Data 1.38e-04 (5.99e-04)	Tok/s 25854 (40420)	Loss/tok 2.6234 (3.2478)	LR 2.000e-03
0: TRAIN [2][930/1938]	Time 0.285 (0.339)	Data 1.45e-04 (5.94e-04)	Tok/s 35712 (40416)	Loss/tok 3.1093 (3.2473)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.285 (0.339)	Data 1.34e-04 (5.89e-04)	Tok/s 37336 (40463)	Loss/tok 3.0635 (3.2489)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.370 (0.339)	Data 1.31e-04 (5.84e-04)	Tok/s 44630 (40496)	Loss/tok 3.2804 (3.2488)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.288 (0.339)	Data 1.55e-04 (5.80e-04)	Tok/s 35848 (40497)	Loss/tok 3.0346 (3.2484)	LR 2.000e-03
0: TRAIN [2][970/1938]	Time 0.286 (0.339)	Data 1.29e-04 (5.75e-04)	Tok/s 35735 (40495)	Loss/tok 2.9830 (3.2483)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.285 (0.339)	Data 1.55e-04 (5.71e-04)	Tok/s 36519 (40465)	Loss/tok 2.9753 (3.2471)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.458 (0.339)	Data 1.39e-04 (5.66e-04)	Tok/s 51120 (40481)	Loss/tok 3.4209 (3.2478)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1000/1938]	Time 0.566 (0.339)	Data 1.78e-04 (5.62e-04)	Tok/s 52081 (40492)	Loss/tok 3.6894 (3.2491)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.288 (0.339)	Data 1.49e-04 (5.58e-04)	Tok/s 35880 (40489)	Loss/tok 3.0056 (3.2487)	LR 2.000e-03
0: TRAIN [2][1020/1938]	Time 0.286 (0.340)	Data 1.61e-04 (5.54e-04)	Tok/s 36082 (40526)	Loss/tok 3.0033 (3.2495)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.459 (0.340)	Data 1.52e-04 (5.50e-04)	Tok/s 50780 (40566)	Loss/tok 3.5379 (3.2517)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.287 (0.341)	Data 1.76e-04 (5.46e-04)	Tok/s 35493 (40568)	Loss/tok 3.0877 (3.2522)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.286 (0.340)	Data 1.33e-04 (5.42e-04)	Tok/s 35850 (40533)	Loss/tok 3.0501 (3.2511)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.458 (0.341)	Data 1.48e-04 (5.39e-04)	Tok/s 50171 (40575)	Loss/tok 3.4660 (3.2526)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.285 (0.341)	Data 1.51e-04 (5.35e-04)	Tok/s 36826 (40581)	Loss/tok 3.1362 (3.2525)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.284 (0.341)	Data 1.33e-04 (5.31e-04)	Tok/s 36999 (40573)	Loss/tok 3.0515 (3.2530)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.287 (0.341)	Data 1.29e-04 (5.28e-04)	Tok/s 36304 (40569)	Loss/tok 3.1242 (3.2529)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.460 (0.341)	Data 1.44e-04 (5.24e-04)	Tok/s 50445 (40570)	Loss/tok 3.5698 (3.2534)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.286 (0.341)	Data 1.31e-04 (5.21e-04)	Tok/s 36363 (40622)	Loss/tok 3.0832 (3.2549)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.376 (0.341)	Data 1.42e-04 (5.17e-04)	Tok/s 44422 (40602)	Loss/tok 3.2385 (3.2546)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.284 (0.341)	Data 1.51e-04 (5.14e-04)	Tok/s 36970 (40575)	Loss/tok 3.0388 (3.2536)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.287 (0.341)	Data 1.51e-04 (5.11e-04)	Tok/s 35601 (40560)	Loss/tok 2.9818 (3.2525)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.288 (0.341)	Data 1.52e-04 (5.08e-04)	Tok/s 35657 (40565)	Loss/tok 3.0270 (3.2526)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.571 (0.341)	Data 1.30e-04 (5.04e-04)	Tok/s 52620 (40601)	Loss/tok 3.6329 (3.2531)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.371 (0.341)	Data 1.29e-04 (5.01e-04)	Tok/s 45052 (40586)	Loss/tok 3.2283 (3.2521)	LR 2.000e-03
0: TRAIN [2][1180/1938]	Time 0.285 (0.341)	Data 1.29e-04 (4.98e-04)	Tok/s 37286 (40591)	Loss/tok 3.0208 (3.2518)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.373 (0.341)	Data 1.48e-04 (4.95e-04)	Tok/s 45217 (40636)	Loss/tok 3.2525 (3.2528)	LR 2.000e-03
0: TRAIN [2][1200/1938]	Time 0.372 (0.341)	Data 1.46e-04 (4.92e-04)	Tok/s 45913 (40628)	Loss/tok 3.1644 (3.2523)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.458 (0.341)	Data 1.33e-04 (4.90e-04)	Tok/s 51377 (40614)	Loss/tok 3.2204 (3.2517)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.371 (0.341)	Data 1.53e-04 (4.87e-04)	Tok/s 44949 (40643)	Loss/tok 3.3500 (3.2525)	LR 2.000e-03
0: TRAIN [2][1230/1938]	Time 0.202 (0.341)	Data 1.49e-04 (4.84e-04)	Tok/s 26290 (40629)	Loss/tok 2.7509 (3.2528)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.460 (0.341)	Data 1.42e-04 (4.81e-04)	Tok/s 51009 (40620)	Loss/tok 3.3917 (3.2520)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.571 (0.341)	Data 1.27e-04 (4.79e-04)	Tok/s 52403 (40613)	Loss/tok 3.6258 (3.2524)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1260/1938]	Time 0.285 (0.341)	Data 1.27e-04 (4.76e-04)	Tok/s 36666 (40584)	Loss/tok 3.1846 (3.2520)	LR 2.000e-03
0: TRAIN [2][1270/1938]	Time 0.286 (0.341)	Data 1.44e-04 (4.73e-04)	Tok/s 36177 (40580)	Loss/tok 3.1136 (3.2521)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.287 (0.340)	Data 1.27e-04 (4.71e-04)	Tok/s 36228 (40543)	Loss/tok 3.0808 (3.2510)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.370 (0.340)	Data 1.54e-04 (4.68e-04)	Tok/s 45303 (40572)	Loss/tok 3.2836 (3.2512)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.284 (0.340)	Data 1.33e-04 (4.66e-04)	Tok/s 36299 (40552)	Loss/tok 3.0811 (3.2510)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.572 (0.341)	Data 1.59e-04 (4.63e-04)	Tok/s 52697 (40575)	Loss/tok 3.4528 (3.2517)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.287 (0.341)	Data 1.51e-04 (4.61e-04)	Tok/s 36163 (40575)	Loss/tok 2.9760 (3.2518)	LR 2.000e-03
0: TRAIN [2][1330/1938]	Time 0.372 (0.341)	Data 1.31e-04 (4.58e-04)	Tok/s 45426 (40576)	Loss/tok 3.2023 (3.2514)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.375 (0.340)	Data 1.39e-04 (4.56e-04)	Tok/s 44755 (40539)	Loss/tok 3.2765 (3.2504)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.371 (0.340)	Data 1.41e-04 (4.54e-04)	Tok/s 44970 (40557)	Loss/tok 3.3069 (3.2511)	LR 2.000e-03
0: TRAIN [2][1360/1938]	Time 0.204 (0.341)	Data 1.62e-04 (4.51e-04)	Tok/s 26290 (40564)	Loss/tok 2.6634 (3.2511)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.571 (0.341)	Data 1.47e-04 (4.49e-04)	Tok/s 51926 (40581)	Loss/tok 3.5675 (3.2520)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.287 (0.341)	Data 1.48e-04 (4.47e-04)	Tok/s 36068 (40603)	Loss/tok 3.0637 (3.2521)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.287 (0.341)	Data 1.53e-04 (4.45e-04)	Tok/s 36284 (40609)	Loss/tok 3.0536 (3.2523)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.207 (0.341)	Data 1.49e-04 (4.43e-04)	Tok/s 25333 (40618)	Loss/tok 2.6606 (3.2523)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1410/1938]	Time 0.574 (0.342)	Data 1.31e-04 (4.41e-04)	Tok/s 51687 (40658)	Loss/tok 3.6038 (3.2540)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.288 (0.342)	Data 1.43e-04 (4.38e-04)	Tok/s 35979 (40654)	Loss/tok 2.9462 (3.2536)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.370 (0.342)	Data 1.37e-04 (4.36e-04)	Tok/s 45522 (40663)	Loss/tok 3.2941 (3.2539)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.370 (0.342)	Data 1.27e-04 (4.34e-04)	Tok/s 45546 (40654)	Loss/tok 3.2894 (3.2535)	LR 2.000e-03
0: TRAIN [2][1450/1938]	Time 0.458 (0.342)	Data 1.62e-04 (4.32e-04)	Tok/s 50844 (40688)	Loss/tok 3.4216 (3.2552)	LR 2.000e-03
0: TRAIN [2][1460/1938]	Time 0.371 (0.342)	Data 1.60e-04 (4.30e-04)	Tok/s 44170 (40693)	Loss/tok 3.2295 (3.2551)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.204 (0.342)	Data 1.36e-04 (4.29e-04)	Tok/s 26225 (40661)	Loss/tok 2.6090 (3.2543)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.285 (0.342)	Data 1.32e-04 (4.27e-04)	Tok/s 36511 (40653)	Loss/tok 2.9827 (3.2537)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.458 (0.342)	Data 1.31e-04 (4.25e-04)	Tok/s 50627 (40666)	Loss/tok 3.3648 (3.2542)	LR 2.000e-03
0: TRAIN [2][1500/1938]	Time 0.370 (0.342)	Data 1.31e-04 (4.23e-04)	Tok/s 45276 (40673)	Loss/tok 3.3538 (3.2544)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.286 (0.342)	Data 1.48e-04 (4.21e-04)	Tok/s 35880 (40695)	Loss/tok 3.1095 (3.2552)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.572 (0.342)	Data 1.51e-04 (4.19e-04)	Tok/s 51594 (40703)	Loss/tok 3.6044 (3.2557)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.287 (0.342)	Data 1.38e-04 (4.17e-04)	Tok/s 35666 (40702)	Loss/tok 3.0341 (3.2557)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1540/1938]	Time 0.370 (0.342)	Data 1.30e-04 (4.15e-04)	Tok/s 45429 (40701)	Loss/tok 3.2476 (3.2557)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.286 (0.342)	Data 1.37e-04 (4.14e-04)	Tok/s 36359 (40707)	Loss/tok 3.0501 (3.2555)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.374 (0.342)	Data 1.62e-04 (4.12e-04)	Tok/s 45209 (40699)	Loss/tok 3.1723 (3.2552)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.205 (0.342)	Data 1.39e-04 (4.10e-04)	Tok/s 24935 (40693)	Loss/tok 2.5767 (3.2554)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.287 (0.342)	Data 1.49e-04 (4.09e-04)	Tok/s 35158 (40706)	Loss/tok 2.9651 (3.2555)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.370 (0.342)	Data 1.53e-04 (4.07e-04)	Tok/s 45761 (40695)	Loss/tok 3.2138 (3.2549)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.285 (0.342)	Data 1.43e-04 (4.05e-04)	Tok/s 35944 (40699)	Loss/tok 2.9819 (3.2548)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.287 (0.342)	Data 1.33e-04 (4.04e-04)	Tok/s 35503 (40706)	Loss/tok 3.0115 (3.2546)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.287 (0.342)	Data 1.32e-04 (4.02e-04)	Tok/s 35770 (40695)	Loss/tok 3.1847 (3.2546)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.459 (0.342)	Data 1.31e-04 (4.00e-04)	Tok/s 51382 (40703)	Loss/tok 3.3876 (3.2545)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.286 (0.342)	Data 1.48e-04 (3.99e-04)	Tok/s 35585 (40712)	Loss/tok 3.0246 (3.2550)	LR 2.000e-03
0: TRAIN [2][1650/1938]	Time 0.458 (0.342)	Data 1.45e-04 (3.97e-04)	Tok/s 50797 (40713)	Loss/tok 3.4924 (3.2547)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.202 (0.342)	Data 3.37e-04 (3.96e-04)	Tok/s 25230 (40712)	Loss/tok 2.6541 (3.2553)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.370 (0.342)	Data 1.43e-04 (3.94e-04)	Tok/s 45304 (40722)	Loss/tok 3.2903 (3.2555)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.286 (0.342)	Data 1.49e-04 (3.93e-04)	Tok/s 36502 (40713)	Loss/tok 3.1303 (3.2551)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.373 (0.342)	Data 1.48e-04 (3.91e-04)	Tok/s 45729 (40734)	Loss/tok 3.1961 (3.2552)	LR 2.000e-03
0: TRAIN [2][1700/1938]	Time 0.458 (0.342)	Data 1.30e-04 (3.90e-04)	Tok/s 51150 (40729)	Loss/tok 3.4219 (3.2549)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.458 (0.343)	Data 2.14e-04 (3.89e-04)	Tok/s 51004 (40755)	Loss/tok 3.4342 (3.2551)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.287 (0.343)	Data 1.57e-04 (3.87e-04)	Tok/s 35168 (40744)	Loss/tok 3.0722 (3.2552)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1730/1938]	Time 0.285 (0.342)	Data 1.44e-04 (3.86e-04)	Tok/s 36253 (40732)	Loss/tok 3.0878 (3.2546)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.374 (0.343)	Data 1.58e-04 (3.84e-04)	Tok/s 44445 (40752)	Loss/tok 3.3790 (3.2550)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.201 (0.343)	Data 1.35e-04 (3.83e-04)	Tok/s 26386 (40761)	Loss/tok 2.6100 (3.2554)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.287 (0.343)	Data 1.57e-04 (3.82e-04)	Tok/s 36057 (40768)	Loss/tok 3.0138 (3.2558)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.287 (0.343)	Data 1.29e-04 (3.80e-04)	Tok/s 35906 (40763)	Loss/tok 3.0208 (3.2557)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.285 (0.343)	Data 1.29e-04 (3.79e-04)	Tok/s 35564 (40762)	Loss/tok 3.1399 (3.2552)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.370 (0.343)	Data 1.30e-04 (3.78e-04)	Tok/s 45508 (40783)	Loss/tok 3.2457 (3.2552)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.369 (0.343)	Data 1.48e-04 (3.76e-04)	Tok/s 45511 (40783)	Loss/tok 3.2243 (3.2549)	LR 2.000e-03
0: TRAIN [2][1810/1938]	Time 0.287 (0.343)	Data 1.47e-04 (3.75e-04)	Tok/s 35887 (40791)	Loss/tok 3.1063 (3.2549)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.284 (0.343)	Data 1.46e-04 (3.74e-04)	Tok/s 36345 (40792)	Loss/tok 3.0267 (3.2545)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.459 (0.343)	Data 1.93e-04 (3.73e-04)	Tok/s 51257 (40797)	Loss/tok 3.4060 (3.2547)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.287 (0.343)	Data 1.31e-04 (3.71e-04)	Tok/s 35968 (40792)	Loss/tok 2.9001 (3.2546)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.461 (0.343)	Data 1.31e-04 (3.70e-04)	Tok/s 50937 (40800)	Loss/tok 3.4456 (3.2547)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.287 (0.343)	Data 1.48e-04 (3.69e-04)	Tok/s 35862 (40808)	Loss/tok 3.0198 (3.2546)	LR 2.000e-03
0: TRAIN [2][1870/1938]	Time 0.369 (0.343)	Data 1.60e-04 (3.68e-04)	Tok/s 45253 (40821)	Loss/tok 3.1768 (3.2548)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.376 (0.343)	Data 1.29e-04 (3.66e-04)	Tok/s 44580 (40831)	Loss/tok 3.3764 (3.2549)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.370 (0.343)	Data 1.28e-04 (3.65e-04)	Tok/s 45329 (40808)	Loss/tok 3.2922 (3.2542)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.459 (0.343)	Data 1.55e-04 (3.64e-04)	Tok/s 51259 (40802)	Loss/tok 3.2321 (3.2540)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.371 (0.343)	Data 1.32e-04 (3.63e-04)	Tok/s 45634 (40803)	Loss/tok 3.2638 (3.2542)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.370 (0.343)	Data 1.32e-04 (3.62e-04)	Tok/s 45822 (40801)	Loss/tok 3.1288 (3.2540)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1930/1938]	Time 0.569 (0.343)	Data 1.44e-04 (3.61e-04)	Tok/s 52568 (40797)	Loss/tok 3.5359 (3.2540)	LR 2.000e-03
:::MLL 1572988680.681 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1572988680.682 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.795 (0.795)	Decoder iters 95.0 (95.0)	Tok/s 20486 (20486)
0: Running moses detokenizer
0: BLEU(score=23.19188938016978, counts=[36437, 17907, 10026, 5874], totals=[64967, 61964, 58961, 55961], precisions=[56.08539720165623, 28.89903815118456, 17.00446057563474, 10.496595843533889], bp=1.0, sys_len=64967, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1572988683.361 eval_accuracy: {"value": 23.19, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1572988683.362 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.2578	Test BLEU: 23.19
0: Performance: Epoch: 2	Training: 326343 Tok/s
0: Finished epoch 2
:::MLL 1572988683.363 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1572988683.363 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1572988683.363 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 651368740
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][0/1938]	Time 0.677 (0.677)	Data 3.68e-01 (3.68e-01)	Tok/s 15121 (15121)	Loss/tok 2.9332 (2.9332)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.457 (0.374)	Data 1.30e-04 (3.35e-02)	Tok/s 50571 (39551)	Loss/tok 3.3646 (3.1439)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.369 (0.365)	Data 1.26e-04 (1.76e-02)	Tok/s 45683 (41160)	Loss/tok 3.2211 (3.1597)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.572 (0.369)	Data 1.40e-04 (1.20e-02)	Tok/s 52009 (41669)	Loss/tok 3.4827 (3.1784)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.458 (0.362)	Data 1.47e-04 (9.10e-03)	Tok/s 50819 (41572)	Loss/tok 3.3535 (3.1603)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.284 (0.362)	Data 1.35e-04 (7.35e-03)	Tok/s 35972 (41803)	Loss/tok 3.0502 (3.1727)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.286 (0.357)	Data 1.68e-04 (6.17e-03)	Tok/s 36056 (41562)	Loss/tok 2.8767 (3.1668)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.284 (0.354)	Data 1.40e-04 (5.32e-03)	Tok/s 36607 (41381)	Loss/tok 2.9823 (3.1639)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.284 (0.353)	Data 1.36e-04 (4.68e-03)	Tok/s 36588 (41423)	Loss/tok 3.0639 (3.1630)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.371 (0.354)	Data 1.54e-04 (4.18e-03)	Tok/s 44654 (41463)	Loss/tok 3.2045 (3.1649)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.458 (0.353)	Data 1.32e-04 (3.78e-03)	Tok/s 50169 (41475)	Loss/tok 3.3796 (3.1647)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.459 (0.350)	Data 1.31e-04 (3.45e-03)	Tok/s 50736 (41276)	Loss/tok 3.3192 (3.1597)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.456 (0.353)	Data 1.48e-04 (3.18e-03)	Tok/s 50519 (41610)	Loss/tok 3.3699 (3.1694)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.372 (0.354)	Data 1.33e-04 (2.95e-03)	Tok/s 45070 (41720)	Loss/tok 3.1535 (3.1701)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.570 (0.353)	Data 1.48e-04 (2.75e-03)	Tok/s 52305 (41608)	Loss/tok 3.5469 (3.1702)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.457 (0.353)	Data 1.32e-04 (2.58e-03)	Tok/s 50860 (41709)	Loss/tok 3.4354 (3.1679)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.372 (0.355)	Data 1.41e-04 (2.42e-03)	Tok/s 45197 (41802)	Loss/tok 3.1158 (3.1723)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.572 (0.356)	Data 1.46e-04 (2.29e-03)	Tok/s 51738 (41792)	Loss/tok 3.5022 (3.1785)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.287 (0.355)	Data 1.41e-04 (2.17e-03)	Tok/s 36559 (41753)	Loss/tok 2.9593 (3.1766)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.286 (0.356)	Data 1.45e-04 (2.07e-03)	Tok/s 35477 (41835)	Loss/tok 3.0064 (3.1810)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.286 (0.355)	Data 1.54e-04 (1.97e-03)	Tok/s 36310 (41793)	Loss/tok 2.9566 (3.1770)	LR 2.000e-03
0: TRAIN [3][210/1938]	Time 0.284 (0.354)	Data 1.27e-04 (1.88e-03)	Tok/s 36756 (41669)	Loss/tok 2.9716 (3.1764)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.285 (0.352)	Data 1.30e-04 (1.81e-03)	Tok/s 36736 (41540)	Loss/tok 2.9621 (3.1721)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.287 (0.352)	Data 1.46e-04 (1.73e-03)	Tok/s 35878 (41541)	Loss/tok 3.0100 (3.1741)	LR 2.000e-03
0: TRAIN [3][240/1938]	Time 0.286 (0.352)	Data 1.51e-04 (1.67e-03)	Tok/s 36309 (41527)	Loss/tok 2.9799 (3.1755)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.286 (0.351)	Data 1.25e-04 (1.61e-03)	Tok/s 36360 (41424)	Loss/tok 3.0390 (3.1768)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.285 (0.349)	Data 1.31e-04 (1.55e-03)	Tok/s 36180 (41196)	Loss/tok 2.9747 (3.1741)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][270/1938]	Time 0.369 (0.351)	Data 1.29e-04 (1.50e-03)	Tok/s 45566 (41314)	Loss/tok 3.2237 (3.1801)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.288 (0.350)	Data 1.45e-04 (1.45e-03)	Tok/s 36374 (41260)	Loss/tok 2.9626 (3.1797)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.370 (0.350)	Data 1.42e-04 (1.40e-03)	Tok/s 44782 (41241)	Loss/tok 3.1826 (3.1786)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.370 (0.349)	Data 1.31e-04 (1.36e-03)	Tok/s 44870 (41233)	Loss/tok 3.1694 (3.1772)	LR 2.000e-03
0: TRAIN [3][310/1938]	Time 0.287 (0.347)	Data 1.29e-04 (1.32e-03)	Tok/s 35144 (41091)	Loss/tok 2.9112 (3.1739)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.286 (0.346)	Data 1.49e-04 (1.29e-03)	Tok/s 35621 (40986)	Loss/tok 2.9812 (3.1710)	LR 2.000e-03
0: TRAIN [3][330/1938]	Time 0.372 (0.346)	Data 2.02e-04 (1.25e-03)	Tok/s 45034 (40932)	Loss/tok 3.1215 (3.1703)	LR 2.000e-03
0: TRAIN [3][340/1938]	Time 0.204 (0.344)	Data 1.53e-04 (1.22e-03)	Tok/s 25785 (40795)	Loss/tok 2.5478 (3.1674)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.285 (0.344)	Data 1.34e-04 (1.19e-03)	Tok/s 35800 (40776)	Loss/tok 2.9063 (3.1695)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.370 (0.345)	Data 1.37e-04 (1.16e-03)	Tok/s 45665 (40876)	Loss/tok 3.1882 (3.1724)	LR 2.000e-03
0: TRAIN [3][370/1938]	Time 0.370 (0.346)	Data 1.63e-04 (1.13e-03)	Tok/s 45367 (40990)	Loss/tok 3.1030 (3.1734)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.287 (0.346)	Data 1.50e-04 (1.11e-03)	Tok/s 35547 (40912)	Loss/tok 2.9579 (3.1729)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.369 (0.345)	Data 1.56e-04 (1.08e-03)	Tok/s 45246 (40835)	Loss/tok 3.2477 (3.1704)	LR 2.000e-03
0: TRAIN [3][400/1938]	Time 0.369 (0.345)	Data 1.44e-04 (1.06e-03)	Tok/s 46322 (40896)	Loss/tok 3.0936 (3.1728)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.458 (0.345)	Data 1.54e-04 (1.04e-03)	Tok/s 51488 (40841)	Loss/tok 3.1933 (3.1706)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.369 (0.344)	Data 1.38e-04 (1.02e-03)	Tok/s 45839 (40839)	Loss/tok 3.3059 (3.1704)	LR 2.000e-03
0: TRAIN [3][430/1938]	Time 0.372 (0.344)	Data 1.52e-04 (9.96e-04)	Tok/s 44688 (40829)	Loss/tok 3.1464 (3.1691)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.206 (0.344)	Data 1.46e-04 (9.76e-04)	Tok/s 25303 (40827)	Loss/tok 2.6117 (3.1685)	LR 2.000e-03
0: TRAIN [3][450/1938]	Time 0.370 (0.344)	Data 1.35e-04 (9.58e-04)	Tok/s 44888 (40833)	Loss/tok 3.1471 (3.1689)	LR 2.000e-03
0: TRAIN [3][460/1938]	Time 0.285 (0.344)	Data 1.57e-04 (9.40e-04)	Tok/s 36817 (40857)	Loss/tok 2.9378 (3.1697)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.285 (0.344)	Data 1.30e-04 (9.24e-04)	Tok/s 36167 (40831)	Loss/tok 2.9592 (3.1687)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.285 (0.344)	Data 1.49e-04 (9.08e-04)	Tok/s 37146 (40838)	Loss/tok 2.9733 (3.1706)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.285 (0.344)	Data 1.28e-04 (8.92e-04)	Tok/s 35822 (40841)	Loss/tok 2.9751 (3.1715)	LR 2.000e-03
0: TRAIN [3][500/1938]	Time 0.286 (0.343)	Data 1.42e-04 (8.77e-04)	Tok/s 35948 (40777)	Loss/tok 2.8894 (3.1697)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.374 (0.344)	Data 1.33e-04 (8.62e-04)	Tok/s 45461 (40854)	Loss/tok 3.1847 (3.1715)	LR 2.000e-03
0: TRAIN [3][520/1938]	Time 0.286 (0.345)	Data 1.53e-04 (8.49e-04)	Tok/s 34984 (40945)	Loss/tok 3.1485 (3.1747)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][530/1938]	Time 0.371 (0.346)	Data 1.29e-04 (8.35e-04)	Tok/s 45156 (40993)	Loss/tok 3.1491 (3.1767)	LR 2.000e-03
0: TRAIN [3][540/1938]	Time 0.286 (0.346)	Data 1.28e-04 (8.23e-04)	Tok/s 35705 (40982)	Loss/tok 3.0031 (3.1766)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.460 (0.345)	Data 1.39e-04 (8.10e-04)	Tok/s 50510 (40945)	Loss/tok 3.3956 (3.1760)	LR 2.000e-03
0: TRAIN [3][560/1938]	Time 0.287 (0.345)	Data 1.59e-04 (7.98e-04)	Tok/s 35740 (40886)	Loss/tok 3.0301 (3.1742)	LR 2.000e-03
0: TRAIN [3][570/1938]	Time 0.572 (0.345)	Data 1.53e-04 (7.87e-04)	Tok/s 52369 (40929)	Loss/tok 3.4698 (3.1756)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.287 (0.345)	Data 1.34e-04 (7.76e-04)	Tok/s 36565 (40927)	Loss/tok 2.9571 (3.1746)	LR 2.000e-03
0: TRAIN [3][590/1938]	Time 0.460 (0.345)	Data 1.30e-04 (7.66e-04)	Tok/s 51159 (40941)	Loss/tok 3.3119 (3.1768)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.371 (0.346)	Data 1.46e-04 (7.55e-04)	Tok/s 45124 (40981)	Loss/tok 3.0815 (3.1768)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.370 (0.346)	Data 1.28e-04 (7.45e-04)	Tok/s 44910 (40984)	Loss/tok 3.1605 (3.1774)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.370 (0.346)	Data 1.51e-04 (7.35e-04)	Tok/s 46002 (41009)	Loss/tok 3.0130 (3.1779)	LR 2.000e-03
0: TRAIN [3][630/1938]	Time 0.457 (0.347)	Data 1.36e-04 (7.26e-04)	Tok/s 51187 (41057)	Loss/tok 3.3925 (3.1794)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.285 (0.347)	Data 1.43e-04 (7.17e-04)	Tok/s 36892 (41054)	Loss/tok 2.9381 (3.1796)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.572 (0.347)	Data 1.59e-04 (7.08e-04)	Tok/s 52003 (41048)	Loss/tok 3.5623 (3.1802)	LR 2.000e-03
0: TRAIN [3][660/1938]	Time 0.460 (0.346)	Data 1.43e-04 (6.99e-04)	Tok/s 50205 (41048)	Loss/tok 3.3934 (3.1802)	LR 2.000e-03
0: TRAIN [3][670/1938]	Time 0.287 (0.346)	Data 1.53e-04 (6.91e-04)	Tok/s 36246 (40972)	Loss/tok 3.0120 (3.1783)	LR 2.000e-03
0: TRAIN [3][680/1938]	Time 0.372 (0.346)	Data 1.30e-04 (6.83e-04)	Tok/s 44984 (40984)	Loss/tok 3.2908 (3.1789)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.284 (0.345)	Data 1.45e-04 (6.76e-04)	Tok/s 36842 (40963)	Loss/tok 3.0021 (3.1783)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.284 (0.345)	Data 1.34e-04 (6.68e-04)	Tok/s 36469 (40960)	Loss/tok 3.1145 (3.1777)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.459 (0.345)	Data 1.43e-04 (6.61e-04)	Tok/s 51137 (40926)	Loss/tok 3.2358 (3.1763)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.284 (0.344)	Data 1.46e-04 (6.54e-04)	Tok/s 36424 (40919)	Loss/tok 2.9446 (3.1754)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.458 (0.344)	Data 1.56e-04 (6.46e-04)	Tok/s 50507 (40920)	Loss/tok 3.3524 (3.1749)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.458 (0.345)	Data 1.50e-04 (6.40e-04)	Tok/s 49917 (40945)	Loss/tok 3.5347 (3.1757)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.571 (0.344)	Data 1.33e-04 (6.33e-04)	Tok/s 52436 (40922)	Loss/tok 3.5021 (3.1756)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.286 (0.344)	Data 1.32e-04 (6.26e-04)	Tok/s 35541 (40884)	Loss/tok 3.0459 (3.1751)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.573 (0.344)	Data 1.50e-04 (6.20e-04)	Tok/s 52109 (40832)	Loss/tok 3.5044 (3.1750)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.287 (0.343)	Data 1.50e-04 (6.14e-04)	Tok/s 36252 (40814)	Loss/tok 2.9948 (3.1743)	LR 1.000e-03
0: TRAIN [3][790/1938]	Time 0.461 (0.343)	Data 1.32e-04 (6.08e-04)	Tok/s 50767 (40830)	Loss/tok 3.2964 (3.1736)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.459 (0.343)	Data 1.56e-04 (6.02e-04)	Tok/s 51199 (40820)	Loss/tok 3.2891 (3.1732)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.369 (0.344)	Data 1.50e-04 (5.97e-04)	Tok/s 45766 (40844)	Loss/tok 3.0559 (3.1732)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][820/1938]	Time 0.369 (0.343)	Data 1.41e-04 (5.91e-04)	Tok/s 45553 (40804)	Loss/tok 3.2402 (3.1728)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.285 (0.343)	Data 1.39e-04 (5.86e-04)	Tok/s 36826 (40799)	Loss/tok 3.0090 (3.1723)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.288 (0.343)	Data 1.52e-04 (5.81e-04)	Tok/s 36133 (40762)	Loss/tok 2.9100 (3.1713)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.285 (0.343)	Data 1.52e-04 (5.75e-04)	Tok/s 35965 (40787)	Loss/tok 3.0061 (3.1716)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.371 (0.344)	Data 1.30e-04 (5.70e-04)	Tok/s 45308 (40834)	Loss/tok 3.0305 (3.1719)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.459 (0.344)	Data 1.54e-04 (5.66e-04)	Tok/s 50777 (40890)	Loss/tok 3.2946 (3.1733)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.286 (0.344)	Data 1.32e-04 (5.61e-04)	Tok/s 36142 (40873)	Loss/tok 2.9971 (3.1725)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.572 (0.344)	Data 1.58e-04 (5.56e-04)	Tok/s 52335 (40908)	Loss/tok 3.4554 (3.1731)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.370 (0.344)	Data 1.43e-04 (5.51e-04)	Tok/s 45125 (40921)	Loss/tok 3.1538 (3.1727)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.285 (0.344)	Data 1.30e-04 (5.47e-04)	Tok/s 35966 (40903)	Loss/tok 2.8431 (3.1716)	LR 1.000e-03
0: TRAIN [3][920/1938]	Time 0.284 (0.344)	Data 1.30e-04 (5.43e-04)	Tok/s 36084 (40923)	Loss/tok 2.9764 (3.1722)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.283 (0.344)	Data 1.44e-04 (5.38e-04)	Tok/s 36802 (40954)	Loss/tok 2.8763 (3.1718)	LR 1.000e-03
0: TRAIN [3][940/1938]	Time 0.285 (0.345)	Data 1.33e-04 (5.34e-04)	Tok/s 37210 (40959)	Loss/tok 2.9150 (3.1718)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][950/1938]	Time 0.285 (0.344)	Data 1.47e-04 (5.30e-04)	Tok/s 36999 (40947)	Loss/tok 2.7730 (3.1711)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.370 (0.344)	Data 1.46e-04 (5.26e-04)	Tok/s 45197 (40946)	Loss/tok 3.1489 (3.1710)	LR 1.000e-03
0: TRAIN [3][970/1938]	Time 0.203 (0.345)	Data 1.31e-04 (5.22e-04)	Tok/s 26071 (40963)	Loss/tok 2.5629 (3.1716)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.369 (0.345)	Data 1.34e-04 (5.18e-04)	Tok/s 45046 (40985)	Loss/tok 3.1963 (3.1719)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.460 (0.345)	Data 1.48e-04 (5.14e-04)	Tok/s 50731 (40993)	Loss/tok 3.3164 (3.1715)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.286 (0.345)	Data 1.50e-04 (5.11e-04)	Tok/s 36558 (40994)	Loss/tok 2.9857 (3.1713)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.285 (0.345)	Data 1.33e-04 (5.07e-04)	Tok/s 36713 (40989)	Loss/tok 2.9259 (3.1704)	LR 1.000e-03
0: TRAIN [3][1020/1938]	Time 0.285 (0.345)	Data 1.64e-04 (5.03e-04)	Tok/s 35801 (40963)	Loss/tok 2.9367 (3.1694)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.288 (0.345)	Data 1.27e-04 (5.00e-04)	Tok/s 35433 (40959)	Loss/tok 2.9153 (3.1700)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.373 (0.345)	Data 1.31e-04 (4.97e-04)	Tok/s 45318 (40965)	Loss/tok 3.1829 (3.1694)	LR 1.000e-03
0: TRAIN [3][1050/1938]	Time 0.370 (0.345)	Data 1.32e-04 (4.93e-04)	Tok/s 45241 (40995)	Loss/tok 3.1105 (3.1698)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.286 (0.345)	Data 1.25e-04 (4.90e-04)	Tok/s 35821 (41019)	Loss/tok 3.0173 (3.1708)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.287 (0.346)	Data 1.33e-04 (4.87e-04)	Tok/s 37089 (41035)	Loss/tok 2.9006 (3.1710)	LR 1.000e-03
0: TRAIN [3][1080/1938]	Time 0.284 (0.345)	Data 1.31e-04 (4.83e-04)	Tok/s 36599 (41017)	Loss/tok 2.9646 (3.1706)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.203 (0.345)	Data 1.33e-04 (4.80e-04)	Tok/s 26543 (40989)	Loss/tok 2.6594 (3.1697)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.203 (0.345)	Data 1.59e-04 (4.77e-04)	Tok/s 26009 (40949)	Loss/tok 2.5934 (3.1686)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.459 (0.345)	Data 1.56e-04 (4.74e-04)	Tok/s 50070 (40965)	Loss/tok 3.4238 (3.1690)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.284 (0.344)	Data 1.29e-04 (4.72e-04)	Tok/s 36352 (40953)	Loss/tok 2.8343 (3.1684)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.371 (0.345)	Data 1.46e-04 (4.69e-04)	Tok/s 45727 (40961)	Loss/tok 3.0730 (3.1686)	LR 1.000e-03
0: TRAIN [3][1140/1938]	Time 0.287 (0.345)	Data 1.44e-04 (4.66e-04)	Tok/s 36374 (40957)	Loss/tok 3.0795 (3.1690)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.374 (0.345)	Data 1.54e-04 (4.63e-04)	Tok/s 43871 (40961)	Loss/tok 3.1611 (3.1686)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1160/1938]	Time 0.573 (0.345)	Data 1.25e-04 (4.60e-04)	Tok/s 51823 (40958)	Loss/tok 3.4783 (3.1687)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.372 (0.344)	Data 1.29e-04 (4.57e-04)	Tok/s 45435 (40915)	Loss/tok 3.0721 (3.1674)	LR 1.000e-03
0: TRAIN [3][1180/1938]	Time 0.286 (0.344)	Data 1.28e-04 (4.55e-04)	Tok/s 35449 (40889)	Loss/tok 2.9712 (3.1666)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.285 (0.344)	Data 1.30e-04 (4.52e-04)	Tok/s 36175 (40881)	Loss/tok 3.0211 (3.1667)	LR 1.000e-03
0: TRAIN [3][1200/1938]	Time 0.203 (0.344)	Data 1.72e-04 (4.49e-04)	Tok/s 26011 (40863)	Loss/tok 2.5819 (3.1660)	LR 1.000e-03
0: TRAIN [3][1210/1938]	Time 0.458 (0.344)	Data 1.50e-04 (4.47e-04)	Tok/s 50967 (40916)	Loss/tok 3.2691 (3.1673)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.370 (0.345)	Data 1.34e-04 (4.44e-04)	Tok/s 44978 (40937)	Loss/tok 3.1210 (3.1676)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.376 (0.345)	Data 1.55e-04 (4.42e-04)	Tok/s 44893 (40956)	Loss/tok 3.1482 (3.1682)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.372 (0.345)	Data 1.31e-04 (4.40e-04)	Tok/s 45332 (40952)	Loss/tok 3.1192 (3.1683)	LR 1.000e-03
0: TRAIN [3][1250/1938]	Time 0.372 (0.345)	Data 1.46e-04 (4.37e-04)	Tok/s 45115 (40967)	Loss/tok 3.1003 (3.1682)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.369 (0.345)	Data 1.49e-04 (4.35e-04)	Tok/s 45975 (40991)	Loss/tok 3.2315 (3.1680)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.285 (0.345)	Data 1.41e-04 (4.33e-04)	Tok/s 36534 (40979)	Loss/tok 3.0193 (3.1673)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.376 (0.345)	Data 1.35e-04 (4.30e-04)	Tok/s 44867 (40984)	Loss/tok 3.1120 (3.1671)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.285 (0.345)	Data 1.46e-04 (4.28e-04)	Tok/s 36273 (40963)	Loss/tok 2.9102 (3.1667)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.204 (0.345)	Data 1.34e-04 (4.26e-04)	Tok/s 26121 (40973)	Loss/tok 2.5464 (3.1666)	LR 1.000e-03
0: TRAIN [3][1310/1938]	Time 0.202 (0.345)	Data 1.45e-04 (4.24e-04)	Tok/s 26073 (40971)	Loss/tok 2.5677 (3.1664)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.459 (0.345)	Data 1.63e-04 (4.22e-04)	Tok/s 50827 (40952)	Loss/tok 3.3154 (3.1655)	LR 1.000e-03
0: TRAIN [3][1330/1938]	Time 0.205 (0.345)	Data 1.31e-04 (4.19e-04)	Tok/s 25724 (40939)	Loss/tok 2.4444 (3.1650)	LR 1.000e-03
0: TRAIN [3][1340/1938]	Time 0.370 (0.345)	Data 1.45e-04 (4.17e-04)	Tok/s 45692 (40943)	Loss/tok 3.1465 (3.1647)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.286 (0.345)	Data 1.31e-04 (4.15e-04)	Tok/s 36121 (40943)	Loss/tok 2.8929 (3.1642)	LR 1.000e-03
0: TRAIN [3][1360/1938]	Time 0.459 (0.345)	Data 1.48e-04 (4.13e-04)	Tok/s 50243 (40953)	Loss/tok 3.2415 (3.1641)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.374 (0.345)	Data 1.63e-04 (4.11e-04)	Tok/s 46057 (40949)	Loss/tok 3.0855 (3.1634)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.373 (0.345)	Data 1.42e-04 (4.09e-04)	Tok/s 44369 (40945)	Loss/tok 3.3433 (3.1632)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.373 (0.345)	Data 1.41e-04 (4.08e-04)	Tok/s 44817 (40961)	Loss/tok 3.2405 (3.1630)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.459 (0.344)	Data 1.59e-04 (4.06e-04)	Tok/s 50728 (40950)	Loss/tok 3.3230 (3.1624)	LR 1.000e-03
0: TRAIN [3][1410/1938]	Time 0.459 (0.345)	Data 1.45e-04 (4.04e-04)	Tok/s 50280 (40973)	Loss/tok 3.3103 (3.1627)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.205 (0.345)	Data 1.46e-04 (4.02e-04)	Tok/s 25788 (40972)	Loss/tok 2.5820 (3.1627)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1430/1938]	Time 0.286 (0.345)	Data 1.45e-04 (4.00e-04)	Tok/s 36566 (40954)	Loss/tok 2.9806 (3.1622)	LR 1.000e-03
0: TRAIN [3][1440/1938]	Time 0.287 (0.345)	Data 1.33e-04 (3.98e-04)	Tok/s 36837 (40952)	Loss/tok 2.9964 (3.1620)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1450/1938]	Time 0.369 (0.345)	Data 1.42e-04 (3.97e-04)	Tok/s 45400 (40981)	Loss/tok 3.2451 (3.1624)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.206 (0.345)	Data 1.34e-04 (3.95e-04)	Tok/s 25543 (40965)	Loss/tok 2.6077 (3.1621)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.458 (0.345)	Data 1.51e-04 (3.93e-04)	Tok/s 50248 (40952)	Loss/tok 3.3314 (3.1619)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.285 (0.345)	Data 2.17e-04 (3.92e-04)	Tok/s 35900 (40952)	Loss/tok 2.9941 (3.1622)	LR 1.000e-03
0: TRAIN [3][1490/1938]	Time 0.285 (0.345)	Data 1.62e-04 (3.90e-04)	Tok/s 36057 (40965)	Loss/tok 2.9864 (3.1621)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.287 (0.345)	Data 1.47e-04 (3.88e-04)	Tok/s 36265 (40987)	Loss/tok 2.9059 (3.1626)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.373 (0.345)	Data 1.29e-04 (3.87e-04)	Tok/s 45013 (40972)	Loss/tok 3.1973 (3.1619)	LR 5.000e-04
0: TRAIN [3][1520/1938]	Time 0.374 (0.345)	Data 1.48e-04 (3.85e-04)	Tok/s 45106 (40986)	Loss/tok 3.1945 (3.1622)	LR 5.000e-04
0: TRAIN [3][1530/1938]	Time 0.205 (0.345)	Data 1.49e-04 (3.83e-04)	Tok/s 25815 (40967)	Loss/tok 2.5950 (3.1622)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.369 (0.345)	Data 1.66e-04 (3.82e-04)	Tok/s 44803 (40954)	Loss/tok 3.0691 (3.1616)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.370 (0.345)	Data 1.70e-04 (3.80e-04)	Tok/s 45577 (40969)	Loss/tok 3.0802 (3.1613)	LR 5.000e-04
0: TRAIN [3][1560/1938]	Time 0.286 (0.345)	Data 1.52e-04 (3.79e-04)	Tok/s 36261 (40975)	Loss/tok 2.9333 (3.1613)	LR 5.000e-04
0: TRAIN [3][1570/1938]	Time 0.205 (0.345)	Data 1.33e-04 (3.77e-04)	Tok/s 26000 (40982)	Loss/tok 2.5386 (3.1609)	LR 5.000e-04
0: TRAIN [3][1580/1938]	Time 0.369 (0.345)	Data 1.51e-04 (3.76e-04)	Tok/s 46010 (40967)	Loss/tok 3.1872 (3.1602)	LR 5.000e-04
0: TRAIN [3][1590/1938]	Time 0.370 (0.345)	Data 1.59e-04 (3.74e-04)	Tok/s 45304 (40994)	Loss/tok 3.2505 (3.1604)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.285 (0.345)	Data 1.35e-04 (3.73e-04)	Tok/s 36510 (40988)	Loss/tok 2.9428 (3.1603)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.374 (0.345)	Data 1.28e-04 (3.72e-04)	Tok/s 44734 (40984)	Loss/tok 3.1120 (3.1600)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.285 (0.345)	Data 1.50e-04 (3.70e-04)	Tok/s 35695 (41005)	Loss/tok 2.7891 (3.1603)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.288 (0.345)	Data 1.32e-04 (3.69e-04)	Tok/s 36518 (40985)	Loss/tok 2.8886 (3.1594)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.287 (0.344)	Data 1.54e-04 (3.67e-04)	Tok/s 36047 (40940)	Loss/tok 2.9554 (3.1584)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.460 (0.344)	Data 1.41e-04 (3.66e-04)	Tok/s 50475 (40929)	Loss/tok 3.3605 (3.1580)	LR 5.000e-04
0: TRAIN [3][1660/1938]	Time 0.374 (0.344)	Data 1.47e-04 (3.65e-04)	Tok/s 44876 (40940)	Loss/tok 3.1013 (3.1576)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.286 (0.345)	Data 1.47e-04 (3.63e-04)	Tok/s 36216 (40963)	Loss/tok 2.8873 (3.1582)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.287 (0.344)	Data 1.43e-04 (3.62e-04)	Tok/s 35633 (40928)	Loss/tok 2.9613 (3.1572)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1690/1938]	Time 0.287 (0.344)	Data 1.49e-04 (3.61e-04)	Tok/s 35715 (40924)	Loss/tok 2.8741 (3.1575)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.371 (0.345)	Data 1.25e-04 (3.59e-04)	Tok/s 45587 (40925)	Loss/tok 3.0854 (3.1572)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.204 (0.344)	Data 1.95e-04 (3.58e-04)	Tok/s 25599 (40915)	Loss/tok 2.5788 (3.1569)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.285 (0.344)	Data 1.55e-04 (3.57e-04)	Tok/s 36062 (40914)	Loss/tok 2.9735 (3.1564)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.286 (0.344)	Data 1.50e-04 (3.56e-04)	Tok/s 36551 (40890)	Loss/tok 2.8567 (3.1556)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.370 (0.344)	Data 1.45e-04 (3.55e-04)	Tok/s 45303 (40878)	Loss/tok 3.0884 (3.1551)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.370 (0.344)	Data 1.51e-04 (3.53e-04)	Tok/s 45783 (40872)	Loss/tok 3.0960 (3.1550)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.285 (0.344)	Data 1.36e-04 (3.52e-04)	Tok/s 36209 (40852)	Loss/tok 3.0156 (3.1544)	LR 5.000e-04
0: TRAIN [3][1770/1938]	Time 0.204 (0.344)	Data 1.65e-04 (3.51e-04)	Tok/s 25976 (40841)	Loss/tok 2.5573 (3.1539)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.287 (0.344)	Data 1.56e-04 (3.50e-04)	Tok/s 35801 (40840)	Loss/tok 2.9435 (3.1533)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.373 (0.344)	Data 1.53e-04 (3.49e-04)	Tok/s 44810 (40845)	Loss/tok 3.1117 (3.1535)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.287 (0.344)	Data 1.99e-04 (3.48e-04)	Tok/s 35530 (40853)	Loss/tok 2.9901 (3.1531)	LR 5.000e-04
0: TRAIN [3][1810/1938]	Time 0.458 (0.344)	Data 1.28e-04 (3.47e-04)	Tok/s 50415 (40857)	Loss/tok 3.3883 (3.1533)	LR 5.000e-04
0: TRAIN [3][1820/1938]	Time 0.289 (0.344)	Data 1.28e-04 (3.46e-04)	Tok/s 35572 (40838)	Loss/tok 2.8999 (3.1527)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.288 (0.343)	Data 1.42e-04 (3.44e-04)	Tok/s 35458 (40822)	Loss/tok 2.9065 (3.1522)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.287 (0.343)	Data 1.30e-04 (3.43e-04)	Tok/s 35849 (40815)	Loss/tok 2.8590 (3.1515)	LR 5.000e-04
0: TRAIN [3][1850/1938]	Time 0.285 (0.343)	Data 1.55e-04 (3.42e-04)	Tok/s 36305 (40815)	Loss/tok 2.8475 (3.1511)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.460 (0.343)	Data 1.46e-04 (3.41e-04)	Tok/s 50994 (40806)	Loss/tok 3.1311 (3.1508)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.371 (0.343)	Data 1.52e-04 (3.40e-04)	Tok/s 45157 (40814)	Loss/tok 3.1211 (3.1504)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.286 (0.343)	Data 1.47e-04 (3.39e-04)	Tok/s 36168 (40818)	Loss/tok 2.9421 (3.1501)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.370 (0.343)	Data 1.29e-04 (3.38e-04)	Tok/s 45864 (40822)	Loss/tok 3.0311 (3.1496)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.285 (0.343)	Data 1.49e-04 (3.37e-04)	Tok/s 36522 (40829)	Loss/tok 2.7700 (3.1496)	LR 5.000e-04
0: TRAIN [3][1910/1938]	Time 0.285 (0.343)	Data 1.60e-04 (3.36e-04)	Tok/s 35190 (40821)	Loss/tok 2.9629 (3.1491)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.571 (0.343)	Data 1.52e-04 (3.35e-04)	Tok/s 51959 (40812)	Loss/tok 3.4569 (3.1491)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.285 (0.343)	Data 1.67e-04 (3.34e-04)	Tok/s 35943 (40794)	Loss/tok 2.9407 (3.1484)	LR 5.000e-04
:::MLL 1572989349.165 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1572989349.165 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.826 (0.826)	Decoder iters 104.0 (104.0)	Tok/s 19836 (19836)
0: Running moses detokenizer
0: BLEU(score=24.21805796038004, counts=[37056, 18564, 10561, 6300], totals=[64992, 61989, 58986, 55988], precisions=[57.0162481536189, 29.947248705415475, 17.904248465737634, 11.252411230978067], bp=1.0, sys_len=64992, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1572989351.567 eval_accuracy: {"value": 24.22, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1572989351.568 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.1487	Test BLEU: 24.22
0: Performance: Epoch: 3	Training: 326264 Tok/s
0: Finished epoch 3
:::MLL 1572989351.569 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1572989351.569 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-11-05 09:29:18 PM
RESULT,RNN_TRANSLATOR,,2702,nvidia,2019-11-05 08:44:16 PM

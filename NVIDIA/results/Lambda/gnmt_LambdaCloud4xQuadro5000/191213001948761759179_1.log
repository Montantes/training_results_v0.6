Beginning trial 1 of 1
Gathering sys log on oop
:::MLL 1576196470.107 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1576196470.108 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1576196470.109 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1576196470.110 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1576196470.111 submission_platform: {"value": "1xStandard PC (Q35 + ICH9, 2009)", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1576196470.111 submission_entry: {"value": "{'hardware': 'Standard PC (Q35 + ICH9, 2009)', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': ' ', 'os': 'Ubuntu 18.04.3 LTS / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.4-1.0.0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '1x AMD EPYC Processor (with IBPB)', 'num_cores': '1', 'num_vcpus': '1', 'accelerator': 'Quadro RTX 5000', 'num_accelerators': '4', 'sys_mem_size': '31 GB', 'sys_storage_type': '<unknown bus> SSD', 'sys_storage_size': '2x 89.1M + 1x 500G + 1x 48G', 'cpu_accel_interconnect': 'QPI', 'network_card': '', 'num_network_cards': '0', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1576196470.112 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1576196470.112 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1576196475.234 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node oop
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=LambdaCloud4xQuadro5000 -e 'MULTI_NODE= --master_port=4264' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191213001948761759179 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191213001948761759179 ./run_and_time.sh
Run vars: id 191213001948761759179 gpus 4 mparams  --master_port=4264
STARTING TIMING RUN AT 2019-12-13 12:21:16 AM
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 1  --ncores_per_socket 10 --nproc_per_node 4  --master_port=4264'
+ echo 'running benchmark'
+ python -m bind_launch --nsockets_per_node 1 --ncores_per_socket 10 --nproc_per_node 4 --master_port=4264 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1576196477.928 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1576196477.928 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1576196477.929 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1576196477.937 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 3389550027
0: Worker 0 is using worker seed: 1099488963
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1576196491.608 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1576196493.633 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1576196493.633 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1576196493.634 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1576196493.972 global_batch_size: {"value": 1024, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1576196493.974 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1576196493.974 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1576196493.975 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1576196493.976 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1576196493.976 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1576196493.977 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1576196493.977 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1576196493.978 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1576196493.979 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 3658104662
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/3880]	Time 0.840 (0.840)	Data 4.34e-01 (4.34e-01)	Tok/s 20118 (20118)	Loss/tok 10.7591 (10.7591)	LR 2.000e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][10/3880]	Time 0.267 (0.322)	Data 1.50e-04 (3.96e-02)	Tok/s 38805 (36424)	Loss/tok 9.7579 (10.2630)	LR 2.518e-05
0: TRAIN [0][20/3880]	Time 0.425 (0.335)	Data 1.44e-04 (2.09e-02)	Tok/s 54615 (40318)	Loss/tok 9.5412 (9.9002)	LR 3.170e-05
0: TRAIN [0][30/3880]	Time 0.344 (0.321)	Data 1.56e-04 (1.42e-02)	Tok/s 49403 (40781)	Loss/tok 9.1135 (9.6831)	LR 3.991e-05
0: TRAIN [0][40/3880]	Time 0.346 (0.331)	Data 1.36e-04 (1.08e-02)	Tok/s 48544 (42173)	Loss/tok 8.9100 (9.4845)	LR 5.024e-05
0: TRAIN [0][50/3880]	Time 0.267 (0.333)	Data 1.75e-04 (8.69e-03)	Tok/s 38502 (42808)	Loss/tok 8.4929 (9.3218)	LR 6.325e-05
0: TRAIN [0][60/3880]	Time 0.348 (0.338)	Data 1.75e-04 (7.30e-03)	Tok/s 48636 (43452)	Loss/tok 8.3871 (9.1795)	LR 7.962e-05
0: TRAIN [0][70/3880]	Time 0.428 (0.330)	Data 1.66e-04 (6.29e-03)	Tok/s 54733 (42929)	Loss/tok 8.2776 (9.0796)	LR 1.002e-04
0: TRAIN [0][80/3880]	Time 0.343 (0.327)	Data 1.33e-04 (5.53e-03)	Tok/s 49076 (42755)	Loss/tok 8.2858 (8.9948)	LR 1.262e-04
0: TRAIN [0][90/3880]	Time 0.265 (0.325)	Data 1.70e-04 (4.95e-03)	Tok/s 39772 (42981)	Loss/tok 7.9701 (8.9021)	LR 1.589e-04
0: TRAIN [0][100/3880]	Time 0.266 (0.325)	Data 1.44e-04 (4.47e-03)	Tok/s 38113 (43125)	Loss/tok 7.8026 (8.8148)	LR 2.000e-04
0: TRAIN [0][110/3880]	Time 0.427 (0.326)	Data 1.75e-04 (4.08e-03)	Tok/s 54391 (43376)	Loss/tok 8.0680 (8.7335)	LR 2.518e-04
0: TRAIN [0][120/3880]	Time 0.265 (0.321)	Data 1.37e-04 (3.76e-03)	Tok/s 39324 (42895)	Loss/tok 7.7548 (8.6765)	LR 3.170e-04
0: TRAIN [0][130/3880]	Time 0.192 (0.321)	Data 1.49e-04 (3.48e-03)	Tok/s 28216 (43042)	Loss/tok 7.2106 (8.6140)	LR 3.991e-04
0: TRAIN [0][140/3880]	Time 0.429 (0.321)	Data 1.35e-04 (3.25e-03)	Tok/s 55155 (43141)	Loss/tok 8.0958 (8.5631)	LR 5.024e-04
0: TRAIN [0][150/3880]	Time 0.542 (0.324)	Data 1.39e-04 (3.04e-03)	Tok/s 54551 (43379)	Loss/tok 7.9625 (8.5093)	LR 6.325e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][160/3880]	Time 0.429 (0.324)	Data 1.22e-04 (2.86e-03)	Tok/s 54526 (43460)	Loss/tok 7.8861 (8.4615)	LR 7.781e-04
0: TRAIN [0][170/3880]	Time 0.427 (0.325)	Data 1.65e-04 (2.71e-03)	Tok/s 54934 (43539)	Loss/tok 7.7826 (8.4152)	LR 9.796e-04
0: TRAIN [0][180/3880]	Time 0.265 (0.327)	Data 1.83e-04 (2.56e-03)	Tok/s 38547 (43743)	Loss/tok 7.2264 (8.3603)	LR 1.233e-03
0: TRAIN [0][190/3880]	Time 0.266 (0.328)	Data 1.73e-04 (2.44e-03)	Tok/s 38830 (43872)	Loss/tok 7.0841 (8.3052)	LR 1.552e-03
0: TRAIN [0][200/3880]	Time 0.347 (0.329)	Data 1.31e-04 (2.33e-03)	Tok/s 48318 (43946)	Loss/tok 7.1784 (8.2493)	LR 1.954e-03
0: TRAIN [0][210/3880]	Time 0.346 (0.330)	Data 1.77e-04 (2.23e-03)	Tok/s 48700 (44026)	Loss/tok 6.9708 (8.1908)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][220/3880]	Time 0.428 (0.330)	Data 1.37e-04 (2.13e-03)	Tok/s 54701 (44112)	Loss/tok 7.2024 (8.1425)	LR 2.000e-03
0: TRAIN [0][230/3880]	Time 0.265 (0.328)	Data 1.30e-04 (2.05e-03)	Tok/s 38992 (43933)	Loss/tok 6.6277 (8.0966)	LR 2.000e-03
0: TRAIN [0][240/3880]	Time 0.264 (0.329)	Data 1.40e-04 (1.97e-03)	Tok/s 39521 (44042)	Loss/tok 6.5274 (8.0340)	LR 2.000e-03
0: TRAIN [0][250/3880]	Time 0.346 (0.328)	Data 1.42e-04 (1.90e-03)	Tok/s 49298 (43991)	Loss/tok 6.4995 (7.9812)	LR 2.000e-03
0: TRAIN [0][260/3880]	Time 0.266 (0.330)	Data 1.59e-04 (1.83e-03)	Tok/s 38283 (44240)	Loss/tok 6.1603 (7.9108)	LR 2.000e-03
0: TRAIN [0][270/3880]	Time 0.346 (0.329)	Data 1.39e-04 (1.77e-03)	Tok/s 48860 (44273)	Loss/tok 6.3383 (7.8559)	LR 2.000e-03
0: TRAIN [0][280/3880]	Time 0.265 (0.327)	Data 1.74e-04 (1.71e-03)	Tok/s 39029 (44110)	Loss/tok 6.0624 (7.8092)	LR 2.000e-03
0: TRAIN [0][290/3880]	Time 0.346 (0.329)	Data 1.24e-04 (1.66e-03)	Tok/s 48992 (44203)	Loss/tok 6.2846 (7.7479)	LR 2.000e-03
0: TRAIN [0][300/3880]	Time 0.265 (0.328)	Data 1.24e-04 (1.60e-03)	Tok/s 39449 (44182)	Loss/tok 5.7920 (7.6974)	LR 2.000e-03
0: TRAIN [0][310/3880]	Time 0.190 (0.326)	Data 1.93e-04 (1.56e-03)	Tok/s 28455 (44057)	Loss/tok 5.0546 (7.6507)	LR 2.000e-03
0: TRAIN [0][320/3880]	Time 0.270 (0.326)	Data 1.26e-04 (1.51e-03)	Tok/s 37529 (44012)	Loss/tok 5.8610 (7.6019)	LR 2.000e-03
0: TRAIN [0][330/3880]	Time 0.265 (0.326)	Data 1.57e-04 (1.47e-03)	Tok/s 38492 (44025)	Loss/tok 5.6258 (7.5527)	LR 2.000e-03
0: TRAIN [0][340/3880]	Time 0.346 (0.327)	Data 1.44e-04 (1.43e-03)	Tok/s 49183 (44081)	Loss/tok 5.7224 (7.4958)	LR 2.000e-03
0: TRAIN [0][350/3880]	Time 0.266 (0.326)	Data 1.59e-04 (1.40e-03)	Tok/s 38462 (44056)	Loss/tok 5.4480 (7.4496)	LR 2.000e-03
0: TRAIN [0][360/3880]	Time 0.265 (0.326)	Data 1.31e-04 (1.36e-03)	Tok/s 39325 (44069)	Loss/tok 5.4863 (7.4021)	LR 2.000e-03
0: TRAIN [0][370/3880]	Time 0.266 (0.325)	Data 1.27e-04 (1.33e-03)	Tok/s 39398 (44050)	Loss/tok 5.3646 (7.3554)	LR 2.000e-03
0: TRAIN [0][380/3880]	Time 0.347 (0.324)	Data 1.34e-04 (1.30e-03)	Tok/s 48506 (43981)	Loss/tok 5.4924 (7.3129)	LR 2.000e-03
0: TRAIN [0][390/3880]	Time 0.266 (0.324)	Data 1.29e-04 (1.27e-03)	Tok/s 39203 (43969)	Loss/tok 5.0398 (7.2658)	LR 2.000e-03
0: TRAIN [0][400/3880]	Time 0.543 (0.324)	Data 1.50e-04 (1.24e-03)	Tok/s 55019 (43906)	Loss/tok 5.8682 (7.2234)	LR 2.000e-03
0: TRAIN [0][410/3880]	Time 0.349 (0.324)	Data 1.27e-04 (1.21e-03)	Tok/s 48392 (43957)	Loss/tok 5.3628 (7.1739)	LR 2.000e-03
0: TRAIN [0][420/3880]	Time 0.265 (0.324)	Data 1.35e-04 (1.19e-03)	Tok/s 38881 (43897)	Loss/tok 4.8747 (7.1334)	LR 2.000e-03
0: TRAIN [0][430/3880]	Time 0.193 (0.323)	Data 1.28e-04 (1.17e-03)	Tok/s 27778 (43776)	Loss/tok 4.0786 (7.0969)	LR 2.000e-03
0: TRAIN [0][440/3880]	Time 0.346 (0.323)	Data 1.29e-04 (1.14e-03)	Tok/s 48022 (43768)	Loss/tok 5.2085 (7.0552)	LR 2.000e-03
0: TRAIN [0][450/3880]	Time 0.265 (0.322)	Data 1.23e-04 (1.12e-03)	Tok/s 39730 (43707)	Loss/tok 4.7089 (7.0146)	LR 2.000e-03
0: TRAIN [0][460/3880]	Time 0.266 (0.322)	Data 1.46e-04 (1.10e-03)	Tok/s 39178 (43699)	Loss/tok 4.7654 (6.9733)	LR 2.000e-03
0: TRAIN [0][470/3880]	Time 0.429 (0.321)	Data 2.55e-04 (1.08e-03)	Tok/s 54677 (43672)	Loss/tok 5.3182 (6.9339)	LR 2.000e-03
0: TRAIN [0][480/3880]	Time 0.265 (0.322)	Data 1.29e-04 (1.06e-03)	Tok/s 37827 (43704)	Loss/tok 4.6599 (6.8907)	LR 2.000e-03
0: TRAIN [0][490/3880]	Time 0.268 (0.322)	Data 1.58e-04 (1.04e-03)	Tok/s 38385 (43737)	Loss/tok 4.5651 (6.8479)	LR 2.000e-03
0: TRAIN [0][500/3880]	Time 0.265 (0.321)	Data 1.32e-04 (1.02e-03)	Tok/s 38757 (43647)	Loss/tok 4.6096 (6.8141)	LR 2.000e-03
0: TRAIN [0][510/3880]	Time 0.193 (0.321)	Data 1.33e-04 (1.01e-03)	Tok/s 27611 (43639)	Loss/tok 3.7614 (6.7745)	LR 2.000e-03
0: TRAIN [0][520/3880]	Time 0.265 (0.320)	Data 1.29e-04 (9.91e-04)	Tok/s 38879 (43565)	Loss/tok 4.7193 (6.7407)	LR 2.000e-03
0: TRAIN [0][530/3880]	Time 0.266 (0.319)	Data 1.33e-04 (9.75e-04)	Tok/s 39181 (43495)	Loss/tok 4.3020 (6.7080)	LR 2.000e-03
0: TRAIN [0][540/3880]	Time 0.266 (0.319)	Data 1.62e-04 (9.60e-04)	Tok/s 38697 (43513)	Loss/tok 4.4057 (6.6698)	LR 2.000e-03
0: TRAIN [0][550/3880]	Time 0.265 (0.318)	Data 1.32e-04 (9.45e-04)	Tok/s 38485 (43459)	Loss/tok 4.2876 (6.6375)	LR 2.000e-03
0: TRAIN [0][560/3880]	Time 0.266 (0.318)	Data 1.22e-04 (9.31e-04)	Tok/s 38980 (43455)	Loss/tok 4.2035 (6.6028)	LR 2.000e-03
0: TRAIN [0][570/3880]	Time 0.430 (0.318)	Data 1.36e-04 (9.18e-04)	Tok/s 54441 (43431)	Loss/tok 4.7728 (6.5697)	LR 2.000e-03
0: TRAIN [0][580/3880]	Time 0.265 (0.319)	Data 1.29e-04 (9.21e-04)	Tok/s 38825 (43537)	Loss/tok 4.3099 (6.5270)	LR 2.000e-03
0: TRAIN [0][590/3880]	Time 0.265 (0.320)	Data 1.29e-04 (9.08e-04)	Tok/s 39291 (43556)	Loss/tok 4.3302 (6.4927)	LR 2.000e-03
0: TRAIN [0][600/3880]	Time 0.346 (0.320)	Data 1.35e-04 (8.95e-04)	Tok/s 48722 (43613)	Loss/tok 4.5931 (6.4555)	LR 2.000e-03
0: TRAIN [0][610/3880]	Time 0.347 (0.320)	Data 1.71e-04 (8.83e-04)	Tok/s 48594 (43604)	Loss/tok 4.4670 (6.4237)	LR 2.000e-03
0: TRAIN [0][620/3880]	Time 0.265 (0.320)	Data 1.72e-04 (8.71e-04)	Tok/s 38737 (43572)	Loss/tok 4.1528 (6.3947)	LR 2.000e-03
0: TRAIN [0][630/3880]	Time 0.192 (0.319)	Data 1.38e-04 (8.60e-04)	Tok/s 27357 (43488)	Loss/tok 3.4203 (6.3691)	LR 2.000e-03
0: TRAIN [0][640/3880]	Time 0.345 (0.320)	Data 1.56e-04 (8.48e-04)	Tok/s 47704 (43561)	Loss/tok 4.3437 (6.3319)	LR 2.000e-03
0: TRAIN [0][650/3880]	Time 0.265 (0.320)	Data 1.88e-04 (8.37e-04)	Tok/s 38382 (43502)	Loss/tok 4.1411 (6.3053)	LR 2.000e-03
0: TRAIN [0][660/3880]	Time 0.348 (0.320)	Data 1.39e-04 (8.27e-04)	Tok/s 48410 (43483)	Loss/tok 4.5162 (6.2775)	LR 2.000e-03
0: TRAIN [0][670/3880]	Time 0.428 (0.320)	Data 1.30e-04 (8.17e-04)	Tok/s 54030 (43579)	Loss/tok 4.5047 (6.2422)	LR 2.000e-03
0: TRAIN [0][680/3880]	Time 0.266 (0.320)	Data 1.27e-04 (8.07e-04)	Tok/s 38013 (43580)	Loss/tok 3.9487 (6.2145)	LR 2.000e-03
0: TRAIN [0][690/3880]	Time 0.266 (0.320)	Data 1.28e-04 (7.97e-04)	Tok/s 39384 (43589)	Loss/tok 4.0659 (6.1874)	LR 2.000e-03
0: TRAIN [0][700/3880]	Time 0.192 (0.321)	Data 1.27e-04 (7.88e-04)	Tok/s 27858 (43609)	Loss/tok 3.3298 (6.1596)	LR 2.000e-03
0: TRAIN [0][710/3880]	Time 0.266 (0.321)	Data 1.42e-04 (7.79e-04)	Tok/s 38951 (43614)	Loss/tok 4.0935 (6.1337)	LR 2.000e-03
0: TRAIN [0][720/3880]	Time 0.265 (0.320)	Data 1.34e-04 (7.70e-04)	Tok/s 38137 (43595)	Loss/tok 4.0063 (6.1100)	LR 2.000e-03
0: TRAIN [0][730/3880]	Time 0.192 (0.321)	Data 1.41e-04 (7.61e-04)	Tok/s 28379 (43604)	Loss/tok 3.3289 (6.0837)	LR 2.000e-03
0: TRAIN [0][740/3880]	Time 0.266 (0.321)	Data 1.40e-04 (7.53e-04)	Tok/s 38971 (43621)	Loss/tok 4.0228 (6.0595)	LR 2.000e-03
0: TRAIN [0][750/3880]	Time 0.355 (0.321)	Data 1.40e-04 (7.45e-04)	Tok/s 47170 (43658)	Loss/tok 4.3134 (6.0332)	LR 2.000e-03
0: TRAIN [0][760/3880]	Time 0.347 (0.321)	Data 1.31e-04 (7.37e-04)	Tok/s 47805 (43633)	Loss/tok 4.2879 (6.0107)	LR 2.000e-03
0: TRAIN [0][770/3880]	Time 0.432 (0.321)	Data 1.33e-04 (7.29e-04)	Tok/s 53386 (43665)	Loss/tok 4.4801 (5.9854)	LR 2.000e-03
0: TRAIN [0][780/3880]	Time 0.429 (0.321)	Data 1.35e-04 (7.22e-04)	Tok/s 54168 (43702)	Loss/tok 4.5076 (5.9612)	LR 2.000e-03
0: TRAIN [0][790/3880]	Time 0.346 (0.321)	Data 1.30e-04 (7.15e-04)	Tok/s 47982 (43694)	Loss/tok 4.2542 (5.9400)	LR 2.000e-03
0: TRAIN [0][800/3880]	Time 0.430 (0.321)	Data 1.39e-04 (7.07e-04)	Tok/s 54676 (43692)	Loss/tok 4.3356 (5.9184)	LR 2.000e-03
0: TRAIN [0][810/3880]	Time 0.346 (0.322)	Data 1.50e-04 (7.00e-04)	Tok/s 47770 (43735)	Loss/tok 4.1694 (5.8947)	LR 2.000e-03
0: TRAIN [0][820/3880]	Time 0.266 (0.321)	Data 1.40e-04 (6.94e-04)	Tok/s 38975 (43665)	Loss/tok 3.7568 (5.8772)	LR 2.000e-03
0: TRAIN [0][830/3880]	Time 0.266 (0.321)	Data 1.27e-04 (6.87e-04)	Tok/s 39694 (43696)	Loss/tok 3.9908 (5.8547)	LR 2.000e-03
0: TRAIN [0][840/3880]	Time 0.191 (0.321)	Data 1.42e-04 (6.81e-04)	Tok/s 27254 (43682)	Loss/tok 3.2482 (5.8358)	LR 2.000e-03
0: TRAIN [0][850/3880]	Time 0.265 (0.321)	Data 3.13e-04 (6.75e-04)	Tok/s 38946 (43648)	Loss/tok 3.8638 (5.8178)	LR 2.000e-03
0: TRAIN [0][860/3880]	Time 0.191 (0.321)	Data 3.28e-04 (6.69e-04)	Tok/s 27331 (43639)	Loss/tok 3.2175 (5.7989)	LR 2.000e-03
0: TRAIN [0][870/3880]	Time 0.266 (0.321)	Data 1.34e-04 (6.63e-04)	Tok/s 39755 (43668)	Loss/tok 3.8523 (5.7780)	LR 2.000e-03
0: TRAIN [0][880/3880]	Time 0.429 (0.321)	Data 1.84e-04 (6.57e-04)	Tok/s 54201 (43635)	Loss/tok 4.4364 (5.7612)	LR 2.000e-03
0: TRAIN [0][890/3880]	Time 0.347 (0.321)	Data 1.33e-04 (6.51e-04)	Tok/s 48785 (43618)	Loss/tok 4.1369 (5.7436)	LR 2.000e-03
0: TRAIN [0][900/3880]	Time 0.347 (0.321)	Data 1.59e-04 (6.46e-04)	Tok/s 48411 (43621)	Loss/tok 4.1936 (5.7255)	LR 2.000e-03
0: TRAIN [0][910/3880]	Time 0.346 (0.321)	Data 1.39e-04 (6.41e-04)	Tok/s 49199 (43633)	Loss/tok 4.1392 (5.7071)	LR 2.000e-03
0: TRAIN [0][920/3880]	Time 0.347 (0.321)	Data 1.50e-04 (6.35e-04)	Tok/s 49543 (43607)	Loss/tok 4.0067 (5.6908)	LR 2.000e-03
0: TRAIN [0][930/3880]	Time 0.266 (0.321)	Data 1.32e-04 (6.30e-04)	Tok/s 39217 (43636)	Loss/tok 3.7546 (5.6717)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][940/3880]	Time 0.347 (0.321)	Data 1.34e-04 (6.25e-04)	Tok/s 48075 (43621)	Loss/tok 4.1180 (5.6556)	LR 2.000e-03
0: TRAIN [0][950/3880]	Time 0.430 (0.321)	Data 1.34e-04 (6.20e-04)	Tok/s 54533 (43623)	Loss/tok 4.2290 (5.6390)	LR 2.000e-03
0: TRAIN [0][960/3880]	Time 0.266 (0.321)	Data 1.37e-04 (6.15e-04)	Tok/s 38857 (43583)	Loss/tok 3.8283 (5.6247)	LR 2.000e-03
0: TRAIN [0][970/3880]	Time 0.266 (0.320)	Data 1.28e-04 (6.17e-04)	Tok/s 39200 (43537)	Loss/tok 3.7949 (5.6115)	LR 2.000e-03
0: TRAIN [0][980/3880]	Time 0.347 (0.320)	Data 1.82e-04 (6.13e-04)	Tok/s 47616 (43505)	Loss/tok 4.1790 (5.5972)	LR 2.000e-03
0: TRAIN [0][990/3880]	Time 0.431 (0.320)	Data 1.27e-04 (6.08e-04)	Tok/s 54478 (43535)	Loss/tok 4.3020 (5.5802)	LR 2.000e-03
0: TRAIN [0][1000/3880]	Time 0.430 (0.320)	Data 1.36e-04 (6.03e-04)	Tok/s 54001 (43527)	Loss/tok 4.2234 (5.5649)	LR 2.000e-03
0: TRAIN [0][1010/3880]	Time 0.266 (0.320)	Data 1.32e-04 (5.99e-04)	Tok/s 38660 (43507)	Loss/tok 3.7529 (5.5508)	LR 2.000e-03
0: TRAIN [0][1020/3880]	Time 0.345 (0.320)	Data 1.26e-04 (5.94e-04)	Tok/s 47634 (43513)	Loss/tok 4.0987 (5.5352)	LR 2.000e-03
0: TRAIN [0][1030/3880]	Time 0.266 (0.320)	Data 1.34e-04 (5.90e-04)	Tok/s 38229 (43516)	Loss/tok 3.8463 (5.5208)	LR 2.000e-03
0: TRAIN [0][1040/3880]	Time 0.346 (0.320)	Data 1.43e-04 (5.85e-04)	Tok/s 48333 (43523)	Loss/tok 4.1268 (5.5064)	LR 2.000e-03
0: TRAIN [0][1050/3880]	Time 0.544 (0.320)	Data 1.59e-04 (5.81e-04)	Tok/s 54299 (43559)	Loss/tok 4.4531 (5.4902)	LR 2.000e-03
0: TRAIN [0][1060/3880]	Time 0.266 (0.320)	Data 1.68e-04 (5.77e-04)	Tok/s 39153 (43554)	Loss/tok 3.7793 (5.4765)	LR 2.000e-03
0: TRAIN [0][1070/3880]	Time 0.346 (0.320)	Data 1.32e-04 (5.73e-04)	Tok/s 48818 (43530)	Loss/tok 4.0605 (5.4633)	LR 2.000e-03
0: TRAIN [0][1080/3880]	Time 0.429 (0.320)	Data 1.34e-04 (5.69e-04)	Tok/s 54699 (43557)	Loss/tok 4.1347 (5.4481)	LR 2.000e-03
0: TRAIN [0][1090/3880]	Time 0.195 (0.320)	Data 1.39e-04 (5.66e-04)	Tok/s 27408 (43564)	Loss/tok 3.0402 (5.4339)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1100/3880]	Time 0.336 (0.320)	Data 1.36e-04 (5.62e-04)	Tok/s 49800 (43551)	Loss/tok 3.9110 (5.4212)	LR 2.000e-03
0: TRAIN [0][1110/3880]	Time 0.266 (0.320)	Data 1.54e-04 (5.58e-04)	Tok/s 38168 (43557)	Loss/tok 3.7655 (5.4081)	LR 2.000e-03
0: TRAIN [0][1120/3880]	Time 0.266 (0.320)	Data 1.48e-04 (5.55e-04)	Tok/s 38157 (43561)	Loss/tok 3.6585 (5.3953)	LR 2.000e-03
0: TRAIN [0][1130/3880]	Time 0.432 (0.321)	Data 1.36e-04 (5.51e-04)	Tok/s 53558 (43587)	Loss/tok 4.2373 (5.3813)	LR 2.000e-03
0: TRAIN [0][1140/3880]	Time 0.194 (0.321)	Data 1.38e-04 (5.47e-04)	Tok/s 27273 (43579)	Loss/tok 3.1183 (5.3688)	LR 2.000e-03
0: TRAIN [0][1150/3880]	Time 0.265 (0.321)	Data 1.40e-04 (5.44e-04)	Tok/s 39011 (43598)	Loss/tok 3.5971 (5.3551)	LR 2.000e-03
0: TRAIN [0][1160/3880]	Time 0.345 (0.321)	Data 1.78e-04 (5.40e-04)	Tok/s 48501 (43589)	Loss/tok 3.9517 (5.3429)	LR 2.000e-03
0: TRAIN [0][1170/3880]	Time 0.432 (0.321)	Data 1.24e-04 (5.37e-04)	Tok/s 54126 (43565)	Loss/tok 4.1791 (5.3317)	LR 2.000e-03
0: TRAIN [0][1180/3880]	Time 0.355 (0.321)	Data 1.32e-04 (5.34e-04)	Tok/s 47764 (43548)	Loss/tok 3.8741 (5.3208)	LR 2.000e-03
0: TRAIN [0][1190/3880]	Time 0.429 (0.321)	Data 1.25e-04 (5.31e-04)	Tok/s 54373 (43560)	Loss/tok 4.1637 (5.3086)	LR 2.000e-03
0: TRAIN [0][1200/3880]	Time 0.429 (0.321)	Data 1.46e-04 (5.27e-04)	Tok/s 55368 (43593)	Loss/tok 4.1061 (5.2959)	LR 2.000e-03
0: TRAIN [0][1210/3880]	Time 0.266 (0.321)	Data 1.31e-04 (5.24e-04)	Tok/s 39219 (43586)	Loss/tok 3.6426 (5.2846)	LR 2.000e-03
0: TRAIN [0][1220/3880]	Time 0.347 (0.321)	Data 1.24e-04 (5.21e-04)	Tok/s 48899 (43593)	Loss/tok 3.8781 (5.2730)	LR 2.000e-03
0: TRAIN [0][1230/3880]	Time 0.192 (0.321)	Data 1.54e-04 (5.18e-04)	Tok/s 28090 (43571)	Loss/tok 3.1243 (5.2629)	LR 2.000e-03
0: TRAIN [0][1240/3880]	Time 0.194 (0.320)	Data 1.52e-04 (5.15e-04)	Tok/s 27355 (43536)	Loss/tok 3.1074 (5.2533)	LR 2.000e-03
0: TRAIN [0][1250/3880]	Time 0.265 (0.320)	Data 1.63e-04 (5.12e-04)	Tok/s 38811 (43536)	Loss/tok 3.6190 (5.2424)	LR 2.000e-03
0: TRAIN [0][1260/3880]	Time 0.429 (0.320)	Data 1.33e-04 (5.09e-04)	Tok/s 54019 (43536)	Loss/tok 4.2333 (5.2318)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1270/3880]	Time 0.345 (0.320)	Data 1.34e-04 (5.07e-04)	Tok/s 48680 (43553)	Loss/tok 3.8979 (5.2205)	LR 2.000e-03
0: TRAIN [0][1280/3880]	Time 0.268 (0.320)	Data 1.34e-04 (5.04e-04)	Tok/s 39208 (43533)	Loss/tok 3.6485 (5.2112)	LR 2.000e-03
0: TRAIN [0][1290/3880]	Time 0.266 (0.320)	Data 1.29e-04 (5.01e-04)	Tok/s 39420 (43541)	Loss/tok 3.5734 (5.2008)	LR 2.000e-03
0: TRAIN [0][1300/3880]	Time 0.266 (0.320)	Data 1.41e-04 (4.98e-04)	Tok/s 38374 (43562)	Loss/tok 3.5923 (5.1891)	LR 2.000e-03
0: TRAIN [0][1310/3880]	Time 0.265 (0.320)	Data 1.20e-04 (4.96e-04)	Tok/s 39111 (43567)	Loss/tok 3.5460 (5.1789)	LR 2.000e-03
0: TRAIN [0][1320/3880]	Time 0.428 (0.321)	Data 1.60e-04 (4.93e-04)	Tok/s 54127 (43589)	Loss/tok 4.2175 (5.1681)	LR 2.000e-03
0: TRAIN [0][1330/3880]	Time 0.192 (0.320)	Data 1.86e-04 (4.90e-04)	Tok/s 26818 (43570)	Loss/tok 2.9951 (5.1592)	LR 2.000e-03
0: TRAIN [0][1340/3880]	Time 0.430 (0.321)	Data 1.76e-04 (4.88e-04)	Tok/s 54321 (43575)	Loss/tok 4.1060 (5.1493)	LR 2.000e-03
0: TRAIN [0][1350/3880]	Time 0.347 (0.320)	Data 1.37e-04 (4.85e-04)	Tok/s 49136 (43571)	Loss/tok 3.8371 (5.1398)	LR 2.000e-03
0: TRAIN [0][1360/3880]	Time 0.347 (0.321)	Data 1.34e-04 (4.83e-04)	Tok/s 48113 (43604)	Loss/tok 3.9428 (5.1293)	LR 2.000e-03
0: TRAIN [0][1370/3880]	Time 0.265 (0.321)	Data 1.81e-04 (4.80e-04)	Tok/s 38648 (43599)	Loss/tok 3.6715 (5.1200)	LR 2.000e-03
0: TRAIN [0][1380/3880]	Time 0.346 (0.321)	Data 1.34e-04 (4.78e-04)	Tok/s 48590 (43595)	Loss/tok 3.7471 (5.1104)	LR 2.000e-03
0: TRAIN [0][1390/3880]	Time 0.346 (0.321)	Data 1.72e-04 (4.75e-04)	Tok/s 48742 (43583)	Loss/tok 3.9347 (5.1017)	LR 2.000e-03
0: TRAIN [0][1400/3880]	Time 0.429 (0.321)	Data 1.22e-04 (4.73e-04)	Tok/s 54120 (43592)	Loss/tok 4.0889 (5.0922)	LR 2.000e-03
0: TRAIN [0][1410/3880]	Time 0.347 (0.321)	Data 1.32e-04 (4.71e-04)	Tok/s 48475 (43571)	Loss/tok 3.9221 (5.0842)	LR 2.000e-03
0: TRAIN [0][1420/3880]	Time 0.346 (0.321)	Data 1.33e-04 (4.69e-04)	Tok/s 48199 (43569)	Loss/tok 3.8262 (5.0756)	LR 2.000e-03
0: TRAIN [0][1430/3880]	Time 0.352 (0.320)	Data 1.26e-04 (4.66e-04)	Tok/s 47142 (43554)	Loss/tok 3.8394 (5.0673)	LR 2.000e-03
0: TRAIN [0][1440/3880]	Time 0.431 (0.321)	Data 1.26e-04 (4.64e-04)	Tok/s 54652 (43576)	Loss/tok 3.9719 (5.0577)	LR 2.000e-03
0: TRAIN [0][1450/3880]	Time 0.542 (0.321)	Data 1.30e-04 (4.62e-04)	Tok/s 55706 (43594)	Loss/tok 4.1799 (5.0481)	LR 2.000e-03
0: TRAIN [0][1460/3880]	Time 0.268 (0.321)	Data 1.85e-04 (4.60e-04)	Tok/s 39269 (43574)	Loss/tok 3.6526 (5.0404)	LR 2.000e-03
0: TRAIN [0][1470/3880]	Time 0.193 (0.321)	Data 1.31e-04 (4.58e-04)	Tok/s 27372 (43571)	Loss/tok 3.0317 (5.0323)	LR 2.000e-03
0: TRAIN [0][1480/3880]	Time 0.265 (0.321)	Data 1.75e-04 (4.56e-04)	Tok/s 38835 (43612)	Loss/tok 3.7545 (5.0226)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1490/3880]	Time 0.265 (0.321)	Data 1.84e-04 (4.54e-04)	Tok/s 39005 (43622)	Loss/tok 3.4812 (5.0142)	LR 2.000e-03
0: TRAIN [0][1500/3880]	Time 0.348 (0.321)	Data 1.47e-04 (4.52e-04)	Tok/s 47696 (43648)	Loss/tok 3.7407 (5.0052)	LR 2.000e-03
0: TRAIN [0][1510/3880]	Time 0.543 (0.322)	Data 1.31e-04 (4.50e-04)	Tok/s 54595 (43657)	Loss/tok 4.2705 (4.9969)	LR 2.000e-03
0: TRAIN [0][1520/3880]	Time 0.192 (0.322)	Data 1.37e-04 (4.48e-04)	Tok/s 27778 (43667)	Loss/tok 3.0126 (4.9887)	LR 2.000e-03
0: TRAIN [0][1530/3880]	Time 0.431 (0.322)	Data 1.44e-04 (4.47e-04)	Tok/s 54591 (43688)	Loss/tok 3.9997 (4.9803)	LR 2.000e-03
0: TRAIN [0][1540/3880]	Time 0.266 (0.322)	Data 1.52e-04 (4.45e-04)	Tok/s 38644 (43665)	Loss/tok 3.5973 (4.9733)	LR 2.000e-03
0: TRAIN [0][1550/3880]	Time 0.351 (0.322)	Data 1.45e-04 (4.43e-04)	Tok/s 47784 (43676)	Loss/tok 3.8797 (4.9654)	LR 2.000e-03
0: TRAIN [0][1560/3880]	Time 0.266 (0.322)	Data 1.32e-04 (4.41e-04)	Tok/s 39181 (43697)	Loss/tok 3.4524 (4.9572)	LR 2.000e-03
0: TRAIN [0][1570/3880]	Time 0.265 (0.322)	Data 1.85e-04 (4.39e-04)	Tok/s 39046 (43691)	Loss/tok 3.5663 (4.9501)	LR 2.000e-03
0: TRAIN [0][1580/3880]	Time 0.354 (0.322)	Data 1.68e-04 (4.38e-04)	Tok/s 48093 (43680)	Loss/tok 3.7754 (4.9431)	LR 2.000e-03
0: TRAIN [0][1590/3880]	Time 0.195 (0.322)	Data 1.81e-04 (4.36e-04)	Tok/s 26817 (43654)	Loss/tok 2.9936 (4.9366)	LR 2.000e-03
0: TRAIN [0][1600/3880]	Time 0.543 (0.322)	Data 2.06e-04 (4.34e-04)	Tok/s 54685 (43654)	Loss/tok 4.1575 (4.9292)	LR 2.000e-03
0: TRAIN [0][1610/3880]	Time 0.349 (0.322)	Data 1.55e-04 (4.33e-04)	Tok/s 47798 (43668)	Loss/tok 3.8364 (4.9217)	LR 2.000e-03
0: TRAIN [0][1620/3880]	Time 0.545 (0.322)	Data 1.33e-04 (4.31e-04)	Tok/s 54994 (43684)	Loss/tok 4.0998 (4.9137)	LR 2.000e-03
0: TRAIN [0][1630/3880]	Time 0.542 (0.322)	Data 1.56e-04 (4.29e-04)	Tok/s 55545 (43709)	Loss/tok 4.0315 (4.9057)	LR 2.000e-03
0: TRAIN [0][1640/3880]	Time 0.266 (0.322)	Data 1.40e-04 (4.28e-04)	Tok/s 38753 (43708)	Loss/tok 3.6161 (4.8990)	LR 2.000e-03
0: TRAIN [0][1650/3880]	Time 0.191 (0.322)	Data 1.72e-04 (4.26e-04)	Tok/s 26908 (43697)	Loss/tok 2.8898 (4.8925)	LR 2.000e-03
0: TRAIN [0][1660/3880]	Time 0.348 (0.322)	Data 1.35e-04 (4.24e-04)	Tok/s 48554 (43704)	Loss/tok 3.7030 (4.8854)	LR 2.000e-03
0: TRAIN [0][1670/3880]	Time 0.267 (0.322)	Data 1.28e-04 (4.23e-04)	Tok/s 38184 (43695)	Loss/tok 3.4942 (4.8788)	LR 2.000e-03
0: TRAIN [0][1680/3880]	Time 0.432 (0.322)	Data 1.42e-04 (4.22e-04)	Tok/s 54962 (43690)	Loss/tok 3.8200 (4.8722)	LR 2.000e-03
0: TRAIN [0][1690/3880]	Time 0.266 (0.322)	Data 1.33e-04 (4.21e-04)	Tok/s 38393 (43649)	Loss/tok 3.5450 (4.8667)	LR 2.000e-03
0: TRAIN [0][1700/3880]	Time 0.265 (0.321)	Data 1.54e-04 (4.19e-04)	Tok/s 38663 (43631)	Loss/tok 3.4221 (4.8607)	LR 2.000e-03
0: TRAIN [0][1710/3880]	Time 0.265 (0.321)	Data 1.50e-04 (4.18e-04)	Tok/s 38799 (43625)	Loss/tok 3.4765 (4.8545)	LR 2.000e-03
0: TRAIN [0][1720/3880]	Time 0.265 (0.321)	Data 1.34e-04 (4.16e-04)	Tok/s 39471 (43599)	Loss/tok 3.4926 (4.8487)	LR 2.000e-03
0: TRAIN [0][1730/3880]	Time 0.266 (0.321)	Data 1.41e-04 (4.15e-04)	Tok/s 39461 (43591)	Loss/tok 3.4569 (4.8425)	LR 2.000e-03
0: TRAIN [0][1740/3880]	Time 0.265 (0.321)	Data 2.83e-04 (4.14e-04)	Tok/s 39539 (43623)	Loss/tok 3.4761 (4.8354)	LR 2.000e-03
0: TRAIN [0][1750/3880]	Time 0.265 (0.321)	Data 1.33e-04 (4.12e-04)	Tok/s 38470 (43614)	Loss/tok 3.4869 (4.8292)	LR 2.000e-03
0: TRAIN [0][1760/3880]	Time 0.430 (0.321)	Data 1.39e-04 (4.11e-04)	Tok/s 54796 (43614)	Loss/tok 3.8102 (4.8230)	LR 2.000e-03
0: TRAIN [0][1770/3880]	Time 0.431 (0.321)	Data 1.38e-04 (4.09e-04)	Tok/s 54222 (43616)	Loss/tok 3.8202 (4.8167)	LR 2.000e-03
0: TRAIN [0][1780/3880]	Time 0.346 (0.321)	Data 1.82e-04 (4.07e-04)	Tok/s 48530 (43610)	Loss/tok 3.7154 (4.8105)	LR 2.000e-03
0: TRAIN [0][1790/3880]	Time 0.266 (0.321)	Data 1.32e-04 (4.06e-04)	Tok/s 38772 (43608)	Loss/tok 3.6398 (4.8046)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1800/3880]	Time 0.346 (0.321)	Data 1.58e-04 (4.05e-04)	Tok/s 49136 (43611)	Loss/tok 3.7432 (4.7985)	LR 2.000e-03
0: TRAIN [0][1810/3880]	Time 0.346 (0.321)	Data 1.79e-04 (4.03e-04)	Tok/s 48178 (43602)	Loss/tok 3.7711 (4.7931)	LR 2.000e-03
0: TRAIN [0][1820/3880]	Time 0.346 (0.321)	Data 1.40e-04 (4.02e-04)	Tok/s 47542 (43611)	Loss/tok 3.6348 (4.7869)	LR 2.000e-03
0: TRAIN [0][1830/3880]	Time 0.191 (0.321)	Data 1.62e-04 (4.00e-04)	Tok/s 27806 (43606)	Loss/tok 2.9500 (4.7812)	LR 2.000e-03
0: TRAIN [0][1840/3880]	Time 0.266 (0.321)	Data 1.53e-04 (3.99e-04)	Tok/s 38654 (43596)	Loss/tok 3.5079 (4.7761)	LR 2.000e-03
0: TRAIN [0][1850/3880]	Time 0.194 (0.321)	Data 1.29e-04 (3.98e-04)	Tok/s 27027 (43593)	Loss/tok 2.9767 (4.7704)	LR 2.000e-03
0: TRAIN [0][1860/3880]	Time 0.266 (0.321)	Data 1.62e-04 (3.96e-04)	Tok/s 39421 (43591)	Loss/tok 3.4371 (4.7650)	LR 2.000e-03
0: TRAIN [0][1870/3880]	Time 0.265 (0.321)	Data 1.37e-04 (3.95e-04)	Tok/s 38968 (43607)	Loss/tok 3.4625 (4.7586)	LR 2.000e-03
0: TRAIN [0][1880/3880]	Time 0.431 (0.321)	Data 1.31e-04 (3.94e-04)	Tok/s 54317 (43621)	Loss/tok 3.9500 (4.7528)	LR 2.000e-03
0: TRAIN [0][1890/3880]	Time 0.266 (0.321)	Data 1.40e-04 (3.92e-04)	Tok/s 38206 (43637)	Loss/tok 3.3718 (4.7469)	LR 2.000e-03
0: TRAIN [0][1900/3880]	Time 0.192 (0.321)	Data 1.34e-04 (3.91e-04)	Tok/s 27762 (43617)	Loss/tok 2.9354 (4.7421)	LR 2.000e-03
0: TRAIN [0][1910/3880]	Time 0.192 (0.321)	Data 1.51e-04 (3.90e-04)	Tok/s 27599 (43625)	Loss/tok 3.0146 (4.7366)	LR 2.000e-03
0: TRAIN [0][1920/3880]	Time 0.265 (0.321)	Data 1.30e-04 (3.88e-04)	Tok/s 39016 (43627)	Loss/tok 3.5884 (4.7312)	LR 2.000e-03
0: TRAIN [0][1930/3880]	Time 0.267 (0.321)	Data 1.34e-04 (3.87e-04)	Tok/s 38195 (43621)	Loss/tok 3.3167 (4.7261)	LR 2.000e-03
0: TRAIN [0][1940/3880]	Time 0.345 (0.321)	Data 1.48e-04 (3.86e-04)	Tok/s 48112 (43633)	Loss/tok 3.6907 (4.7206)	LR 2.000e-03
0: TRAIN [0][1950/3880]	Time 0.266 (0.321)	Data 1.31e-04 (3.85e-04)	Tok/s 38759 (43628)	Loss/tok 3.5118 (4.7154)	LR 2.000e-03
0: TRAIN [0][1960/3880]	Time 0.268 (0.321)	Data 1.29e-04 (3.84e-04)	Tok/s 37706 (43623)	Loss/tok 3.4858 (4.7104)	LR 2.000e-03
0: TRAIN [0][1970/3880]	Time 0.191 (0.321)	Data 1.39e-04 (3.82e-04)	Tok/s 27344 (43591)	Loss/tok 2.8936 (4.7060)	LR 2.000e-03
0: TRAIN [0][1980/3880]	Time 0.264 (0.321)	Data 2.68e-04 (3.82e-04)	Tok/s 40218 (43578)	Loss/tok 3.4056 (4.7015)	LR 2.000e-03
0: TRAIN [0][1990/3880]	Time 0.265 (0.320)	Data 1.26e-04 (3.80e-04)	Tok/s 38286 (43569)	Loss/tok 3.4974 (4.6965)	LR 2.000e-03
0: TRAIN [0][2000/3880]	Time 0.266 (0.320)	Data 1.31e-04 (3.79e-04)	Tok/s 38314 (43567)	Loss/tok 3.4918 (4.6915)	LR 2.000e-03
0: TRAIN [0][2010/3880]	Time 0.266 (0.320)	Data 1.50e-04 (3.78e-04)	Tok/s 38789 (43570)	Loss/tok 3.3221 (4.6862)	LR 2.000e-03
0: TRAIN [0][2020/3880]	Time 0.265 (0.320)	Data 1.70e-04 (3.77e-04)	Tok/s 39210 (43571)	Loss/tok 3.5230 (4.6812)	LR 2.000e-03
0: TRAIN [0][2030/3880]	Time 0.347 (0.320)	Data 1.24e-04 (3.76e-04)	Tok/s 48282 (43578)	Loss/tok 3.6353 (4.6759)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2040/3880]	Time 0.346 (0.320)	Data 1.85e-04 (3.75e-04)	Tok/s 48704 (43569)	Loss/tok 3.8325 (4.6715)	LR 2.000e-03
0: TRAIN [0][2050/3880]	Time 0.542 (0.320)	Data 1.65e-04 (3.74e-04)	Tok/s 55618 (43579)	Loss/tok 4.0651 (4.6665)	LR 2.000e-03
0: TRAIN [0][2060/3880]	Time 0.351 (0.320)	Data 1.23e-04 (3.72e-04)	Tok/s 47174 (43579)	Loss/tok 3.7214 (4.6617)	LR 2.000e-03
0: TRAIN [0][2070/3880]	Time 0.347 (0.320)	Data 1.31e-04 (3.71e-04)	Tok/s 48804 (43579)	Loss/tok 3.6657 (4.6569)	LR 2.000e-03
0: TRAIN [0][2080/3880]	Time 0.265 (0.321)	Data 1.39e-04 (3.70e-04)	Tok/s 38914 (43586)	Loss/tok 3.4161 (4.6521)	LR 2.000e-03
0: TRAIN [0][2090/3880]	Time 0.431 (0.321)	Data 1.32e-04 (3.69e-04)	Tok/s 54273 (43582)	Loss/tok 3.8321 (4.6473)	LR 2.000e-03
0: TRAIN [0][2100/3880]	Time 0.346 (0.321)	Data 1.34e-04 (3.68e-04)	Tok/s 48828 (43581)	Loss/tok 3.6706 (4.6426)	LR 2.000e-03
0: TRAIN [0][2110/3880]	Time 0.265 (0.320)	Data 1.75e-04 (3.67e-04)	Tok/s 38869 (43574)	Loss/tok 3.4547 (4.6381)	LR 2.000e-03
0: TRAIN [0][2120/3880]	Time 0.346 (0.321)	Data 1.75e-04 (3.66e-04)	Tok/s 48605 (43592)	Loss/tok 3.5568 (4.6329)	LR 2.000e-03
0: TRAIN [0][2130/3880]	Time 0.346 (0.321)	Data 1.64e-04 (3.65e-04)	Tok/s 48679 (43585)	Loss/tok 3.5923 (4.6285)	LR 2.000e-03
0: TRAIN [0][2140/3880]	Time 0.265 (0.320)	Data 1.66e-04 (3.64e-04)	Tok/s 38736 (43561)	Loss/tok 3.4778 (4.6246)	LR 2.000e-03
0: TRAIN [0][2150/3880]	Time 0.267 (0.320)	Data 1.67e-04 (3.63e-04)	Tok/s 38302 (43529)	Loss/tok 3.4585 (4.6207)	LR 2.000e-03
0: TRAIN [0][2160/3880]	Time 0.265 (0.320)	Data 1.50e-04 (3.62e-04)	Tok/s 39098 (43525)	Loss/tok 3.3189 (4.6165)	LR 2.000e-03
0: TRAIN [0][2170/3880]	Time 0.432 (0.320)	Data 1.35e-04 (3.61e-04)	Tok/s 54195 (43550)	Loss/tok 3.7479 (4.6115)	LR 2.000e-03
0: TRAIN [0][2180/3880]	Time 0.265 (0.320)	Data 1.63e-04 (3.60e-04)	Tok/s 39046 (43539)	Loss/tok 3.4708 (4.6073)	LR 2.000e-03
0: TRAIN [0][2190/3880]	Time 0.266 (0.320)	Data 1.55e-04 (3.59e-04)	Tok/s 37718 (43541)	Loss/tok 3.2590 (4.6029)	LR 2.000e-03
0: TRAIN [0][2200/3880]	Time 0.266 (0.320)	Data 1.27e-04 (3.58e-04)	Tok/s 38764 (43523)	Loss/tok 3.4709 (4.5989)	LR 2.000e-03
0: TRAIN [0][2210/3880]	Time 0.265 (0.320)	Data 1.37e-04 (3.57e-04)	Tok/s 37549 (43534)	Loss/tok 3.4313 (4.5945)	LR 2.000e-03
0: TRAIN [0][2220/3880]	Time 0.192 (0.320)	Data 1.44e-04 (3.56e-04)	Tok/s 28093 (43522)	Loss/tok 2.9197 (4.5904)	LR 2.000e-03
0: TRAIN [0][2230/3880]	Time 0.191 (0.320)	Data 1.37e-04 (3.55e-04)	Tok/s 26977 (43511)	Loss/tok 2.9548 (4.5865)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2240/3880]	Time 0.346 (0.320)	Data 1.32e-04 (3.54e-04)	Tok/s 48040 (43516)	Loss/tok 3.6358 (4.5824)	LR 2.000e-03
0: TRAIN [0][2250/3880]	Time 0.346 (0.320)	Data 1.26e-04 (3.53e-04)	Tok/s 48747 (43519)	Loss/tok 3.6189 (4.5783)	LR 2.000e-03
0: TRAIN [0][2260/3880]	Time 0.266 (0.320)	Data 1.27e-04 (3.52e-04)	Tok/s 38127 (43498)	Loss/tok 3.3599 (4.5747)	LR 2.000e-03
0: TRAIN [0][2270/3880]	Time 0.265 (0.320)	Data 2.97e-04 (3.52e-04)	Tok/s 38879 (43492)	Loss/tok 3.4947 (4.5707)	LR 2.000e-03
0: TRAIN [0][2280/3880]	Time 0.267 (0.320)	Data 1.31e-04 (3.51e-04)	Tok/s 37708 (43496)	Loss/tok 3.2913 (4.5665)	LR 2.000e-03
0: TRAIN [0][2290/3880]	Time 0.266 (0.320)	Data 1.31e-04 (3.50e-04)	Tok/s 38508 (43490)	Loss/tok 3.4360 (4.5625)	LR 2.000e-03
0: TRAIN [0][2300/3880]	Time 0.429 (0.320)	Data 1.38e-04 (3.49e-04)	Tok/s 54209 (43494)	Loss/tok 3.7752 (4.5585)	LR 2.000e-03
0: TRAIN [0][2310/3880]	Time 0.429 (0.320)	Data 1.44e-04 (3.48e-04)	Tok/s 54904 (43498)	Loss/tok 3.9201 (4.5543)	LR 2.000e-03
0: TRAIN [0][2320/3880]	Time 0.347 (0.320)	Data 1.70e-04 (3.47e-04)	Tok/s 48720 (43496)	Loss/tok 3.6943 (4.5503)	LR 2.000e-03
0: TRAIN [0][2330/3880]	Time 0.194 (0.320)	Data 1.32e-04 (3.46e-04)	Tok/s 27323 (43501)	Loss/tok 2.8877 (4.5461)	LR 2.000e-03
0: TRAIN [0][2340/3880]	Time 0.347 (0.320)	Data 1.44e-04 (3.46e-04)	Tok/s 48195 (43506)	Loss/tok 3.6552 (4.5419)	LR 2.000e-03
0: TRAIN [0][2350/3880]	Time 0.266 (0.320)	Data 1.41e-04 (3.45e-04)	Tok/s 38192 (43505)	Loss/tok 3.5612 (4.5380)	LR 2.000e-03
0: TRAIN [0][2360/3880]	Time 0.192 (0.320)	Data 1.28e-04 (3.44e-04)	Tok/s 28043 (43489)	Loss/tok 2.8281 (4.5345)	LR 2.000e-03
0: TRAIN [0][2370/3880]	Time 0.351 (0.320)	Data 1.32e-04 (3.43e-04)	Tok/s 48217 (43483)	Loss/tok 3.5727 (4.5307)	LR 2.000e-03
0: TRAIN [0][2380/3880]	Time 0.194 (0.320)	Data 1.71e-04 (3.43e-04)	Tok/s 27662 (43491)	Loss/tok 2.7291 (4.5267)	LR 2.000e-03
0: TRAIN [0][2390/3880]	Time 0.544 (0.320)	Data 1.40e-04 (3.42e-04)	Tok/s 54830 (43488)	Loss/tok 4.0013 (4.5230)	LR 2.000e-03
0: TRAIN [0][2400/3880]	Time 0.191 (0.320)	Data 1.49e-04 (3.41e-04)	Tok/s 27311 (43477)	Loss/tok 2.8459 (4.5195)	LR 2.000e-03
0: TRAIN [0][2410/3880]	Time 0.346 (0.320)	Data 1.28e-04 (3.40e-04)	Tok/s 49096 (43485)	Loss/tok 3.6375 (4.5154)	LR 2.000e-03
0: TRAIN [0][2420/3880]	Time 0.193 (0.320)	Data 1.24e-04 (3.39e-04)	Tok/s 26773 (43477)	Loss/tok 2.8029 (4.5119)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][2430/3880]	Time 0.540 (0.320)	Data 1.31e-04 (3.39e-04)	Tok/s 55896 (43483)	Loss/tok 3.8886 (4.5082)	LR 2.000e-03
0: TRAIN [0][2440/3880]	Time 0.430 (0.320)	Data 1.88e-04 (3.38e-04)	Tok/s 54972 (43474)	Loss/tok 3.7317 (4.5048)	LR 2.000e-03
0: TRAIN [0][2450/3880]	Time 0.347 (0.320)	Data 1.70e-04 (3.37e-04)	Tok/s 48529 (43463)	Loss/tok 3.6662 (4.5014)	LR 2.000e-03
0: TRAIN [0][2460/3880]	Time 0.432 (0.320)	Data 1.43e-04 (3.36e-04)	Tok/s 54265 (43453)	Loss/tok 3.8516 (4.4979)	LR 2.000e-03
0: TRAIN [0][2470/3880]	Time 0.347 (0.320)	Data 1.32e-04 (3.36e-04)	Tok/s 48353 (43465)	Loss/tok 3.6750 (4.4941)	LR 2.000e-03
0: TRAIN [0][2480/3880]	Time 0.266 (0.320)	Data 1.22e-04 (3.35e-04)	Tok/s 38775 (43465)	Loss/tok 3.3032 (4.4905)	LR 2.000e-03
0: TRAIN [0][2490/3880]	Time 0.265 (0.320)	Data 1.55e-04 (3.34e-04)	Tok/s 37876 (43453)	Loss/tok 3.3727 (4.4871)	LR 2.000e-03
0: TRAIN [0][2500/3880]	Time 0.264 (0.320)	Data 1.68e-04 (3.33e-04)	Tok/s 38348 (43475)	Loss/tok 3.3993 (4.4830)	LR 2.000e-03
0: TRAIN [0][2510/3880]	Time 0.266 (0.320)	Data 1.68e-04 (3.33e-04)	Tok/s 38901 (43474)	Loss/tok 3.4725 (4.4795)	LR 2.000e-03
0: TRAIN [0][2520/3880]	Time 0.358 (0.320)	Data 1.31e-04 (3.32e-04)	Tok/s 47030 (43476)	Loss/tok 3.5640 (4.4760)	LR 2.000e-03
0: TRAIN [0][2530/3880]	Time 0.265 (0.320)	Data 1.71e-04 (3.31e-04)	Tok/s 39294 (43456)	Loss/tok 3.3289 (4.4732)	LR 2.000e-03
0: TRAIN [0][2540/3880]	Time 0.433 (0.320)	Data 1.44e-04 (3.31e-04)	Tok/s 53745 (43467)	Loss/tok 3.8030 (4.4698)	LR 2.000e-03
0: TRAIN [0][2550/3880]	Time 0.433 (0.320)	Data 1.50e-04 (3.30e-04)	Tok/s 54168 (43487)	Loss/tok 3.8976 (4.4658)	LR 2.000e-03
0: TRAIN [0][2560/3880]	Time 0.346 (0.320)	Data 1.53e-04 (3.29e-04)	Tok/s 48709 (43499)	Loss/tok 3.5640 (4.4621)	LR 2.000e-03
0: TRAIN [0][2570/3880]	Time 0.346 (0.320)	Data 1.60e-04 (3.28e-04)	Tok/s 48881 (43498)	Loss/tok 3.6794 (4.4591)	LR 2.000e-03
0: TRAIN [0][2580/3880]	Time 0.195 (0.320)	Data 1.29e-04 (3.28e-04)	Tok/s 26964 (43483)	Loss/tok 2.8097 (4.4561)	LR 2.000e-03
0: TRAIN [0][2590/3880]	Time 0.266 (0.320)	Data 1.81e-04 (3.27e-04)	Tok/s 38395 (43473)	Loss/tok 3.3202 (4.4529)	LR 2.000e-03
0: TRAIN [0][2600/3880]	Time 0.347 (0.320)	Data 1.42e-04 (3.26e-04)	Tok/s 47740 (43493)	Loss/tok 3.6667 (4.4491)	LR 2.000e-03
0: TRAIN [0][2610/3880]	Time 0.266 (0.320)	Data 1.31e-04 (3.26e-04)	Tok/s 39683 (43497)	Loss/tok 3.3716 (4.4457)	LR 2.000e-03
0: TRAIN [0][2620/3880]	Time 0.347 (0.320)	Data 1.32e-04 (3.25e-04)	Tok/s 47980 (43506)	Loss/tok 3.6597 (4.4425)	LR 2.000e-03
0: TRAIN [0][2630/3880]	Time 0.346 (0.320)	Data 1.38e-04 (3.24e-04)	Tok/s 48597 (43516)	Loss/tok 3.5783 (4.4390)	LR 2.000e-03
0: TRAIN [0][2640/3880]	Time 0.347 (0.320)	Data 1.73e-04 (3.24e-04)	Tok/s 48771 (43519)	Loss/tok 3.5252 (4.4356)	LR 2.000e-03
0: TRAIN [0][2650/3880]	Time 0.266 (0.320)	Data 1.40e-04 (3.23e-04)	Tok/s 38839 (43515)	Loss/tok 3.3635 (4.4323)	LR 2.000e-03
0: TRAIN [0][2660/3880]	Time 0.266 (0.320)	Data 1.50e-04 (3.22e-04)	Tok/s 39489 (43507)	Loss/tok 3.3934 (4.4293)	LR 2.000e-03
0: TRAIN [0][2670/3880]	Time 0.266 (0.320)	Data 1.30e-04 (3.22e-04)	Tok/s 38870 (43502)	Loss/tok 3.1791 (4.4262)	LR 2.000e-03
0: TRAIN [0][2680/3880]	Time 0.192 (0.320)	Data 1.63e-04 (3.21e-04)	Tok/s 27726 (43504)	Loss/tok 2.8289 (4.4228)	LR 2.000e-03
0: TRAIN [0][2690/3880]	Time 0.266 (0.320)	Data 1.29e-04 (3.21e-04)	Tok/s 38962 (43509)	Loss/tok 3.3076 (4.4195)	LR 2.000e-03
0: TRAIN [0][2700/3880]	Time 0.265 (0.320)	Data 1.64e-04 (3.20e-04)	Tok/s 39483 (43520)	Loss/tok 3.3333 (4.4162)	LR 2.000e-03
0: TRAIN [0][2710/3880]	Time 0.267 (0.320)	Data 1.34e-04 (3.19e-04)	Tok/s 38311 (43514)	Loss/tok 3.2874 (4.4132)	LR 2.000e-03
0: TRAIN [0][2720/3880]	Time 0.347 (0.320)	Data 1.27e-04 (3.19e-04)	Tok/s 49073 (43505)	Loss/tok 3.5686 (4.4102)	LR 2.000e-03
0: TRAIN [0][2730/3880]	Time 0.192 (0.320)	Data 1.33e-04 (3.18e-04)	Tok/s 27586 (43498)	Loss/tok 2.9188 (4.4073)	LR 2.000e-03
0: TRAIN [0][2740/3880]	Time 0.266 (0.320)	Data 1.35e-04 (3.17e-04)	Tok/s 39382 (43495)	Loss/tok 3.2769 (4.4043)	LR 2.000e-03
0: TRAIN [0][2750/3880]	Time 0.265 (0.320)	Data 1.78e-04 (3.17e-04)	Tok/s 38854 (43490)	Loss/tok 3.3148 (4.4012)	LR 2.000e-03
0: TRAIN [0][2760/3880]	Time 0.439 (0.320)	Data 1.40e-04 (3.16e-04)	Tok/s 53749 (43496)	Loss/tok 3.7273 (4.3982)	LR 2.000e-03
0: TRAIN [0][2770/3880]	Time 0.347 (0.320)	Data 1.46e-04 (3.16e-04)	Tok/s 47649 (43486)	Loss/tok 3.5173 (4.3953)	LR 2.000e-03
0: TRAIN [0][2780/3880]	Time 0.432 (0.320)	Data 1.35e-04 (3.15e-04)	Tok/s 53884 (43489)	Loss/tok 3.7622 (4.3922)	LR 2.000e-03
0: TRAIN [0][2790/3880]	Time 0.265 (0.320)	Data 2.54e-04 (3.14e-04)	Tok/s 39279 (43490)	Loss/tok 3.3005 (4.3892)	LR 2.000e-03
0: TRAIN [0][2800/3880]	Time 0.266 (0.320)	Data 1.54e-04 (3.14e-04)	Tok/s 39205 (43496)	Loss/tok 3.3595 (4.3862)	LR 2.000e-03
0: TRAIN [0][2810/3880]	Time 0.266 (0.320)	Data 1.29e-04 (3.13e-04)	Tok/s 38629 (43480)	Loss/tok 3.3076 (4.3835)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][2820/3880]	Time 0.192 (0.320)	Data 1.38e-04 (3.13e-04)	Tok/s 28304 (43484)	Loss/tok 2.8996 (4.3807)	LR 2.000e-03
0: TRAIN [0][2830/3880]	Time 0.266 (0.320)	Data 2.23e-04 (3.12e-04)	Tok/s 38214 (43491)	Loss/tok 3.3355 (4.3777)	LR 2.000e-03
0: TRAIN [0][2840/3880]	Time 0.347 (0.320)	Data 1.36e-04 (3.12e-04)	Tok/s 47588 (43505)	Loss/tok 3.4880 (4.3745)	LR 2.000e-03
0: TRAIN [0][2850/3880]	Time 0.266 (0.320)	Data 1.86e-04 (3.11e-04)	Tok/s 38542 (43505)	Loss/tok 3.2575 (4.3716)	LR 2.000e-03
0: TRAIN [0][2860/3880]	Time 0.346 (0.320)	Data 1.53e-04 (3.17e-04)	Tok/s 47548 (43510)	Loss/tok 3.5529 (4.3687)	LR 2.000e-03
0: TRAIN [0][2870/3880]	Time 0.268 (0.320)	Data 1.33e-04 (3.19e-04)	Tok/s 38271 (43517)	Loss/tok 3.3986 (4.3657)	LR 2.000e-03
0: TRAIN [0][2880/3880]	Time 0.346 (0.320)	Data 1.65e-04 (3.18e-04)	Tok/s 49347 (43535)	Loss/tok 3.5833 (4.3625)	LR 2.000e-03
0: TRAIN [0][2890/3880]	Time 0.349 (0.320)	Data 1.27e-04 (3.18e-04)	Tok/s 48966 (43536)	Loss/tok 3.5347 (4.3596)	LR 2.000e-03
0: TRAIN [0][2900/3880]	Time 0.347 (0.320)	Data 2.16e-04 (3.17e-04)	Tok/s 48490 (43538)	Loss/tok 3.5363 (4.3568)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2910/3880]	Time 0.336 (0.320)	Data 1.41e-04 (3.17e-04)	Tok/s 49805 (43537)	Loss/tok 3.6490 (4.3545)	LR 2.000e-03
0: TRAIN [0][2920/3880]	Time 0.267 (0.320)	Data 1.61e-04 (3.16e-04)	Tok/s 39084 (43534)	Loss/tok 3.4677 (4.3517)	LR 2.000e-03
0: TRAIN [0][2930/3880]	Time 0.348 (0.320)	Data 1.32e-04 (3.16e-04)	Tok/s 48651 (43543)	Loss/tok 3.5642 (4.3488)	LR 2.000e-03
0: TRAIN [0][2940/3880]	Time 0.347 (0.321)	Data 1.33e-04 (3.15e-04)	Tok/s 49012 (43552)	Loss/tok 3.5088 (4.3462)	LR 2.000e-03
0: TRAIN [0][2950/3880]	Time 0.348 (0.321)	Data 1.74e-04 (3.14e-04)	Tok/s 48866 (43553)	Loss/tok 3.5490 (4.3435)	LR 2.000e-03
0: TRAIN [0][2960/3880]	Time 0.266 (0.321)	Data 1.57e-04 (3.14e-04)	Tok/s 38940 (43557)	Loss/tok 3.2989 (4.3406)	LR 2.000e-03
0: TRAIN [0][2970/3880]	Time 0.346 (0.320)	Data 1.70e-04 (3.13e-04)	Tok/s 48393 (43554)	Loss/tok 3.5720 (4.3380)	LR 2.000e-03
0: TRAIN [0][2980/3880]	Time 0.349 (0.320)	Data 1.39e-04 (3.13e-04)	Tok/s 49069 (43547)	Loss/tok 3.6243 (4.3356)	LR 2.000e-03
0: TRAIN [0][2990/3880]	Time 0.194 (0.320)	Data 1.49e-04 (3.12e-04)	Tok/s 26964 (43544)	Loss/tok 2.7337 (4.3329)	LR 2.000e-03
0: TRAIN [0][3000/3880]	Time 0.347 (0.320)	Data 1.32e-04 (3.12e-04)	Tok/s 48257 (43545)	Loss/tok 3.6041 (4.3302)	LR 2.000e-03
0: TRAIN [0][3010/3880]	Time 0.357 (0.320)	Data 2.10e-04 (3.11e-04)	Tok/s 47717 (43551)	Loss/tok 3.4762 (4.3276)	LR 2.000e-03
0: TRAIN [0][3020/3880]	Time 0.544 (0.321)	Data 1.70e-04 (3.11e-04)	Tok/s 55016 (43555)	Loss/tok 3.8428 (4.3249)	LR 2.000e-03
0: TRAIN [0][3030/3880]	Time 0.346 (0.320)	Data 1.62e-04 (3.10e-04)	Tok/s 48786 (43545)	Loss/tok 3.5969 (4.3224)	LR 2.000e-03
0: TRAIN [0][3040/3880]	Time 0.266 (0.320)	Data 1.40e-04 (3.10e-04)	Tok/s 38504 (43540)	Loss/tok 3.3166 (4.3200)	LR 2.000e-03
0: TRAIN [0][3050/3880]	Time 0.430 (0.320)	Data 1.52e-04 (3.09e-04)	Tok/s 53921 (43538)	Loss/tok 3.7256 (4.3174)	LR 2.000e-03
0: TRAIN [0][3060/3880]	Time 0.265 (0.320)	Data 1.32e-04 (3.08e-04)	Tok/s 38644 (43539)	Loss/tok 3.3954 (4.3149)	LR 2.000e-03
0: TRAIN [0][3070/3880]	Time 0.266 (0.320)	Data 1.40e-04 (3.08e-04)	Tok/s 39092 (43541)	Loss/tok 3.4118 (4.3124)	LR 2.000e-03
0: TRAIN [0][3080/3880]	Time 0.195 (0.320)	Data 1.36e-04 (3.07e-04)	Tok/s 26747 (43530)	Loss/tok 2.9239 (4.3100)	LR 2.000e-03
0: TRAIN [0][3090/3880]	Time 0.346 (0.320)	Data 1.82e-04 (3.07e-04)	Tok/s 48113 (43530)	Loss/tok 3.5219 (4.3074)	LR 2.000e-03
0: TRAIN [0][3100/3880]	Time 0.354 (0.320)	Data 1.25e-04 (3.06e-04)	Tok/s 47151 (43534)	Loss/tok 3.5675 (4.3050)	LR 2.000e-03
0: TRAIN [0][3110/3880]	Time 0.194 (0.320)	Data 1.42e-04 (3.06e-04)	Tok/s 26865 (43528)	Loss/tok 2.8076 (4.3026)	LR 2.000e-03
0: TRAIN [0][3120/3880]	Time 0.266 (0.320)	Data 1.34e-04 (3.05e-04)	Tok/s 38035 (43518)	Loss/tok 3.2785 (4.3003)	LR 2.000e-03
0: TRAIN [0][3130/3880]	Time 0.349 (0.320)	Data 1.32e-04 (3.05e-04)	Tok/s 48332 (43533)	Loss/tok 3.5124 (4.2978)	LR 2.000e-03
0: TRAIN [0][3140/3880]	Time 0.348 (0.320)	Data 1.33e-04 (3.04e-04)	Tok/s 48520 (43530)	Loss/tok 3.5478 (4.2953)	LR 2.000e-03
0: TRAIN [0][3150/3880]	Time 0.269 (0.320)	Data 1.31e-04 (3.04e-04)	Tok/s 38693 (43520)	Loss/tok 3.3845 (4.2931)	LR 2.000e-03
0: TRAIN [0][3160/3880]	Time 0.347 (0.320)	Data 1.37e-04 (3.03e-04)	Tok/s 47636 (43531)	Loss/tok 3.6293 (4.2904)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3170/3880]	Time 0.431 (0.320)	Data 2.13e-04 (3.03e-04)	Tok/s 54250 (43543)	Loss/tok 3.7753 (4.2879)	LR 2.000e-03
0: TRAIN [0][3180/3880]	Time 0.346 (0.321)	Data 1.33e-04 (3.02e-04)	Tok/s 48281 (43562)	Loss/tok 3.5234 (4.2855)	LR 2.000e-03
0: TRAIN [0][3190/3880]	Time 0.192 (0.321)	Data 1.40e-04 (3.02e-04)	Tok/s 28039 (43560)	Loss/tok 2.8752 (4.2832)	LR 2.000e-03
0: TRAIN [0][3200/3880]	Time 0.266 (0.321)	Data 1.66e-04 (3.02e-04)	Tok/s 38358 (43562)	Loss/tok 3.3480 (4.2808)	LR 2.000e-03
0: TRAIN [0][3210/3880]	Time 0.265 (0.321)	Data 1.34e-04 (3.01e-04)	Tok/s 38827 (43557)	Loss/tok 3.2699 (4.2785)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3220/3880]	Time 0.192 (0.321)	Data 1.65e-04 (3.01e-04)	Tok/s 27473 (43549)	Loss/tok 2.9301 (4.2765)	LR 2.000e-03
0: TRAIN [0][3230/3880]	Time 0.348 (0.320)	Data 1.63e-04 (3.00e-04)	Tok/s 48525 (43540)	Loss/tok 3.4962 (4.2743)	LR 2.000e-03
0: TRAIN [0][3240/3880]	Time 0.347 (0.320)	Data 1.93e-04 (3.00e-04)	Tok/s 48305 (43528)	Loss/tok 3.6018 (4.2722)	LR 2.000e-03
0: TRAIN [0][3250/3880]	Time 0.266 (0.320)	Data 1.48e-04 (2.99e-04)	Tok/s 39468 (43544)	Loss/tok 3.3713 (4.2696)	LR 2.000e-03
0: TRAIN [0][3260/3880]	Time 0.543 (0.321)	Data 1.19e-04 (2.99e-04)	Tok/s 54787 (43544)	Loss/tok 3.9131 (4.2674)	LR 2.000e-03
0: TRAIN [0][3270/3880]	Time 0.265 (0.320)	Data 1.29e-04 (2.98e-04)	Tok/s 37559 (43540)	Loss/tok 3.2592 (4.2653)	LR 2.000e-03
0: TRAIN [0][3280/3880]	Time 0.265 (0.320)	Data 1.28e-04 (2.98e-04)	Tok/s 38826 (43533)	Loss/tok 3.3106 (4.2632)	LR 2.000e-03
0: TRAIN [0][3290/3880]	Time 0.346 (0.320)	Data 1.40e-04 (2.97e-04)	Tok/s 49517 (43528)	Loss/tok 3.4931 (4.2609)	LR 2.000e-03
0: TRAIN [0][3300/3880]	Time 0.265 (0.320)	Data 1.71e-04 (2.97e-04)	Tok/s 39993 (43516)	Loss/tok 3.3167 (4.2590)	LR 2.000e-03
0: TRAIN [0][3310/3880]	Time 0.193 (0.320)	Data 1.37e-04 (2.97e-04)	Tok/s 27513 (43512)	Loss/tok 2.7298 (4.2568)	LR 2.000e-03
0: TRAIN [0][3320/3880]	Time 0.543 (0.320)	Data 1.27e-04 (2.96e-04)	Tok/s 54623 (43532)	Loss/tok 3.7920 (4.2540)	LR 2.000e-03
0: TRAIN [0][3330/3880]	Time 0.544 (0.320)	Data 1.30e-04 (2.96e-04)	Tok/s 54617 (43530)	Loss/tok 3.8323 (4.2519)	LR 2.000e-03
0: TRAIN [0][3340/3880]	Time 0.347 (0.320)	Data 1.37e-04 (2.95e-04)	Tok/s 48102 (43532)	Loss/tok 3.4354 (4.2496)	LR 2.000e-03
0: TRAIN [0][3350/3880]	Time 0.266 (0.320)	Data 1.31e-04 (2.95e-04)	Tok/s 40086 (43526)	Loss/tok 3.2245 (4.2474)	LR 2.000e-03
0: TRAIN [0][3360/3880]	Time 0.270 (0.320)	Data 1.80e-04 (2.95e-04)	Tok/s 38137 (43527)	Loss/tok 3.3816 (4.2453)	LR 2.000e-03
0: TRAIN [0][3370/3880]	Time 0.266 (0.320)	Data 1.85e-04 (2.94e-04)	Tok/s 38635 (43533)	Loss/tok 3.3240 (4.2430)	LR 2.000e-03
0: TRAIN [0][3380/3880]	Time 0.347 (0.320)	Data 1.37e-04 (2.94e-04)	Tok/s 48462 (43528)	Loss/tok 3.4468 (4.2409)	LR 2.000e-03
0: TRAIN [0][3390/3880]	Time 0.265 (0.320)	Data 1.34e-04 (2.93e-04)	Tok/s 38460 (43521)	Loss/tok 3.3268 (4.2389)	LR 2.000e-03
0: TRAIN [0][3400/3880]	Time 0.346 (0.320)	Data 1.59e-04 (2.93e-04)	Tok/s 48613 (43529)	Loss/tok 3.5024 (4.2367)	LR 2.000e-03
0: TRAIN [0][3410/3880]	Time 0.266 (0.320)	Data 1.93e-04 (2.92e-04)	Tok/s 39198 (43522)	Loss/tok 3.1913 (4.2346)	LR 2.000e-03
0: TRAIN [0][3420/3880]	Time 0.349 (0.320)	Data 1.34e-04 (2.92e-04)	Tok/s 47844 (43516)	Loss/tok 3.3615 (4.2324)	LR 2.000e-03
0: TRAIN [0][3430/3880]	Time 0.430 (0.320)	Data 1.38e-04 (2.92e-04)	Tok/s 54484 (43517)	Loss/tok 3.5731 (4.2302)	LR 2.000e-03
0: TRAIN [0][3440/3880]	Time 0.542 (0.320)	Data 1.28e-04 (2.91e-04)	Tok/s 55002 (43530)	Loss/tok 3.8395 (4.2279)	LR 2.000e-03
0: TRAIN [0][3450/3880]	Time 0.347 (0.320)	Data 1.35e-04 (2.91e-04)	Tok/s 48757 (43538)	Loss/tok 3.5518 (4.2257)	LR 2.000e-03
0: TRAIN [0][3460/3880]	Time 0.347 (0.321)	Data 1.26e-04 (2.90e-04)	Tok/s 47610 (43540)	Loss/tok 3.5798 (4.2237)	LR 2.000e-03
0: TRAIN [0][3470/3880]	Time 0.347 (0.320)	Data 1.21e-04 (2.90e-04)	Tok/s 49056 (43542)	Loss/tok 3.5296 (4.2215)	LR 2.000e-03
0: TRAIN [0][3480/3880]	Time 0.348 (0.320)	Data 1.33e-04 (2.89e-04)	Tok/s 48850 (43532)	Loss/tok 3.4305 (4.2195)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3490/3880]	Time 0.192 (0.320)	Data 1.43e-04 (2.89e-04)	Tok/s 27650 (43537)	Loss/tok 2.7153 (4.2173)	LR 2.000e-03
0: TRAIN [0][3500/3880]	Time 0.435 (0.320)	Data 1.33e-04 (2.89e-04)	Tok/s 52841 (43538)	Loss/tok 3.6959 (4.2152)	LR 2.000e-03
0: TRAIN [0][3510/3880]	Time 0.346 (0.320)	Data 1.29e-04 (2.89e-04)	Tok/s 49523 (43545)	Loss/tok 3.5492 (4.2130)	LR 2.000e-03
0: TRAIN [0][3520/3880]	Time 0.346 (0.320)	Data 1.41e-04 (2.88e-04)	Tok/s 48553 (43546)	Loss/tok 3.5342 (4.2109)	LR 2.000e-03
0: TRAIN [0][3530/3880]	Time 0.265 (0.320)	Data 1.71e-04 (2.88e-04)	Tok/s 38850 (43545)	Loss/tok 3.3333 (4.2089)	LR 2.000e-03
0: TRAIN [0][3540/3880]	Time 0.194 (0.320)	Data 1.30e-04 (2.87e-04)	Tok/s 27658 (43532)	Loss/tok 2.7534 (4.2070)	LR 2.000e-03
0: TRAIN [0][3550/3880]	Time 0.430 (0.320)	Data 1.83e-04 (2.87e-04)	Tok/s 54504 (43534)	Loss/tok 3.6216 (4.2050)	LR 2.000e-03
0: TRAIN [0][3560/3880]	Time 0.432 (0.320)	Data 1.52e-04 (2.87e-04)	Tok/s 53833 (43544)	Loss/tok 3.6254 (4.2029)	LR 2.000e-03
0: TRAIN [0][3570/3880]	Time 0.348 (0.320)	Data 1.29e-04 (2.86e-04)	Tok/s 47967 (43546)	Loss/tok 3.4851 (4.2008)	LR 2.000e-03
0: TRAIN [0][3580/3880]	Time 0.266 (0.321)	Data 1.34e-04 (2.86e-04)	Tok/s 38432 (43548)	Loss/tok 3.2308 (4.1989)	LR 2.000e-03
0: TRAIN [0][3590/3880]	Time 0.544 (0.321)	Data 1.40e-04 (2.86e-04)	Tok/s 54238 (43546)	Loss/tok 3.8596 (4.1972)	LR 2.000e-03
0: TRAIN [0][3600/3880]	Time 0.268 (0.321)	Data 1.54e-04 (2.85e-04)	Tok/s 38451 (43556)	Loss/tok 3.3597 (4.1951)	LR 2.000e-03
0: TRAIN [0][3610/3880]	Time 0.545 (0.321)	Data 1.87e-04 (2.85e-04)	Tok/s 54745 (43564)	Loss/tok 3.8271 (4.1930)	LR 2.000e-03
0: TRAIN [0][3620/3880]	Time 0.347 (0.321)	Data 1.29e-04 (2.84e-04)	Tok/s 48884 (43556)	Loss/tok 3.4382 (4.1911)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3630/3880]	Time 0.347 (0.321)	Data 1.40e-04 (2.84e-04)	Tok/s 48179 (43567)	Loss/tok 3.4658 (4.1892)	LR 2.000e-03
0: TRAIN [0][3640/3880]	Time 0.347 (0.321)	Data 1.22e-04 (2.84e-04)	Tok/s 48849 (43574)	Loss/tok 3.4877 (4.1871)	LR 2.000e-03
0: TRAIN [0][3650/3880]	Time 0.266 (0.321)	Data 1.42e-04 (2.83e-04)	Tok/s 38475 (43581)	Loss/tok 3.2033 (4.1851)	LR 2.000e-03
0: TRAIN [0][3660/3880]	Time 0.265 (0.321)	Data 1.20e-04 (2.83e-04)	Tok/s 38148 (43578)	Loss/tok 3.2971 (4.1832)	LR 2.000e-03
0: TRAIN [0][3670/3880]	Time 0.267 (0.321)	Data 1.79e-04 (2.82e-04)	Tok/s 38707 (43571)	Loss/tok 3.2079 (4.1814)	LR 2.000e-03
0: TRAIN [0][3680/3880]	Time 0.265 (0.321)	Data 1.54e-04 (2.82e-04)	Tok/s 39162 (43565)	Loss/tok 3.1736 (4.1796)	LR 2.000e-03
0: TRAIN [0][3690/3880]	Time 0.348 (0.321)	Data 1.36e-04 (2.82e-04)	Tok/s 48899 (43568)	Loss/tok 3.4111 (4.1778)	LR 2.000e-03
0: TRAIN [0][3700/3880]	Time 0.347 (0.321)	Data 1.49e-04 (2.81e-04)	Tok/s 48767 (43571)	Loss/tok 3.5551 (4.1759)	LR 2.000e-03
0: TRAIN [0][3710/3880]	Time 0.544 (0.321)	Data 1.39e-04 (2.81e-04)	Tok/s 54241 (43566)	Loss/tok 3.9910 (4.1742)	LR 2.000e-03
0: TRAIN [0][3720/3880]	Time 0.430 (0.321)	Data 1.44e-04 (2.81e-04)	Tok/s 53842 (43575)	Loss/tok 3.6285 (4.1722)	LR 2.000e-03
0: TRAIN [0][3730/3880]	Time 0.347 (0.321)	Data 1.44e-04 (2.80e-04)	Tok/s 48498 (43572)	Loss/tok 3.5699 (4.1704)	LR 2.000e-03
0: TRAIN [0][3740/3880]	Time 0.265 (0.321)	Data 1.50e-04 (2.80e-04)	Tok/s 38420 (43566)	Loss/tok 3.1744 (4.1686)	LR 2.000e-03
0: TRAIN [0][3750/3880]	Time 0.265 (0.321)	Data 1.68e-04 (2.80e-04)	Tok/s 39621 (43568)	Loss/tok 3.1994 (4.1668)	LR 2.000e-03
0: TRAIN [0][3760/3880]	Time 0.430 (0.321)	Data 1.32e-04 (2.79e-04)	Tok/s 54446 (43578)	Loss/tok 3.6303 (4.1648)	LR 2.000e-03
0: TRAIN [0][3770/3880]	Time 0.346 (0.321)	Data 2.22e-04 (2.79e-04)	Tok/s 49263 (43583)	Loss/tok 3.4604 (4.1628)	LR 2.000e-03
0: TRAIN [0][3780/3880]	Time 0.348 (0.321)	Data 1.29e-04 (2.78e-04)	Tok/s 48207 (43582)	Loss/tok 3.4018 (4.1609)	LR 2.000e-03
0: TRAIN [0][3790/3880]	Time 0.348 (0.321)	Data 1.30e-04 (2.78e-04)	Tok/s 49272 (43586)	Loss/tok 3.4397 (4.1589)	LR 2.000e-03
0: TRAIN [0][3800/3880]	Time 0.347 (0.321)	Data 1.81e-04 (2.78e-04)	Tok/s 48185 (43585)	Loss/tok 3.4234 (4.1570)	LR 2.000e-03
0: TRAIN [0][3810/3880]	Time 0.346 (0.321)	Data 1.91e-04 (2.77e-04)	Tok/s 48703 (43574)	Loss/tok 3.5677 (4.1554)	LR 2.000e-03
0: TRAIN [0][3820/3880]	Time 0.347 (0.321)	Data 1.76e-04 (2.77e-04)	Tok/s 49068 (43588)	Loss/tok 3.3672 (4.1533)	LR 2.000e-03
0: TRAIN [0][3830/3880]	Time 0.346 (0.321)	Data 1.36e-04 (2.77e-04)	Tok/s 48593 (43593)	Loss/tok 3.4553 (4.1515)	LR 2.000e-03
0: TRAIN [0][3840/3880]	Time 0.266 (0.321)	Data 1.23e-04 (2.76e-04)	Tok/s 39474 (43592)	Loss/tok 3.2763 (4.1496)	LR 2.000e-03
0: TRAIN [0][3850/3880]	Time 0.431 (0.321)	Data 1.67e-04 (2.76e-04)	Tok/s 54578 (43592)	Loss/tok 3.6490 (4.1478)	LR 2.000e-03
0: TRAIN [0][3860/3880]	Time 0.543 (0.321)	Data 1.26e-04 (2.76e-04)	Tok/s 54370 (43590)	Loss/tok 3.9614 (4.1462)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3870/3880]	Time 0.266 (0.321)	Data 1.63e-04 (2.76e-04)	Tok/s 39250 (43591)	Loss/tok 3.1900 (4.1445)	LR 2.000e-03
:::MLL 1576197740.517 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1576197740.517 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/6]	Time 0.862 (0.862)	Decoder iters 113.0 (113.0)	Tok/s 18636 (18636)
0: Running moses detokenizer
0: BLEU(score=20.827501640601387, counts=[34580, 16229, 8801, 4927], totals=[62846, 59843, 56841, 53845], precisions=[55.02339051013589, 27.11929548986515, 15.48354180960926, 9.15033893583434], bp=0.9713010686650796, sys_len=62846, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1576197744.847 eval_accuracy: {"value": 20.83, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1576197744.847 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.1419	Test BLEU: 20.83
0: Performance: Epoch: 0	Training: 174358 Tok/s
0: Finished epoch 0
:::MLL 1576197744.848 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1576197744.848 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1576197744.848 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 3027650223
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][0/3880]	Time 0.697 (0.697)	Data 3.83e-01 (3.83e-01)	Tok/s 14926 (14926)	Loss/tok 3.1029 (3.1029)	LR 2.000e-03
0: TRAIN [1][10/3880]	Time 0.349 (0.370)	Data 1.47e-04 (3.49e-02)	Tok/s 48563 (42361)	Loss/tok 3.3934 (3.3984)	LR 2.000e-03
0: TRAIN [1][20/3880]	Time 0.194 (0.337)	Data 1.85e-04 (1.85e-02)	Tok/s 26627 (42127)	Loss/tok 2.7117 (3.3719)	LR 2.000e-03
0: TRAIN [1][30/3880]	Time 0.192 (0.314)	Data 1.35e-04 (1.26e-02)	Tok/s 27849 (40860)	Loss/tok 2.7655 (3.3409)	LR 2.000e-03
0: TRAIN [1][40/3880]	Time 0.348 (0.322)	Data 1.64e-04 (9.55e-03)	Tok/s 48598 (41968)	Loss/tok 3.4482 (3.3688)	LR 2.000e-03
0: TRAIN [1][50/3880]	Time 0.193 (0.323)	Data 1.37e-04 (7.71e-03)	Tok/s 27607 (42310)	Loss/tok 2.6578 (3.3797)	LR 2.000e-03
0: TRAIN [1][60/3880]	Time 0.266 (0.320)	Data 1.34e-04 (6.46e-03)	Tok/s 39030 (42487)	Loss/tok 3.2405 (3.3741)	LR 2.000e-03
0: TRAIN [1][70/3880]	Time 0.266 (0.317)	Data 1.61e-04 (5.58e-03)	Tok/s 37902 (42124)	Loss/tok 3.1458 (3.3732)	LR 2.000e-03
0: TRAIN [1][80/3880]	Time 0.191 (0.317)	Data 1.49e-04 (4.90e-03)	Tok/s 27449 (42125)	Loss/tok 2.8256 (3.3819)	LR 2.000e-03
0: TRAIN [1][90/3880]	Time 0.543 (0.321)	Data 1.83e-04 (4.38e-03)	Tok/s 55179 (42420)	Loss/tok 3.7214 (3.3976)	LR 2.000e-03
0: TRAIN [1][100/3880]	Time 0.346 (0.318)	Data 1.30e-04 (3.96e-03)	Tok/s 48305 (42245)	Loss/tok 3.4038 (3.3851)	LR 2.000e-03
0: TRAIN [1][110/3880]	Time 0.265 (0.315)	Data 1.65e-04 (3.62e-03)	Tok/s 39626 (42170)	Loss/tok 3.0817 (3.3772)	LR 2.000e-03
0: TRAIN [1][120/3880]	Time 0.345 (0.317)	Data 1.63e-04 (3.33e-03)	Tok/s 48847 (42395)	Loss/tok 3.3576 (3.3797)	LR 2.000e-03
0: TRAIN [1][130/3880]	Time 0.265 (0.313)	Data 1.43e-04 (3.09e-03)	Tok/s 38345 (42110)	Loss/tok 3.1057 (3.3684)	LR 2.000e-03
0: TRAIN [1][140/3880]	Time 0.432 (0.313)	Data 1.34e-04 (2.88e-03)	Tok/s 53492 (42121)	Loss/tok 3.6534 (3.3697)	LR 2.000e-03
0: TRAIN [1][150/3880]	Time 0.265 (0.312)	Data 1.32e-04 (2.70e-03)	Tok/s 38881 (42112)	Loss/tok 3.2158 (3.3693)	LR 2.000e-03
0: TRAIN [1][160/3880]	Time 0.265 (0.309)	Data 1.22e-04 (2.54e-03)	Tok/s 37998 (41746)	Loss/tok 3.2382 (3.3611)	LR 2.000e-03
0: TRAIN [1][170/3880]	Time 0.266 (0.309)	Data 1.71e-04 (2.40e-03)	Tok/s 38837 (41866)	Loss/tok 3.1008 (3.3632)	LR 2.000e-03
0: TRAIN [1][180/3880]	Time 0.269 (0.310)	Data 1.42e-04 (2.28e-03)	Tok/s 38456 (41933)	Loss/tok 3.1343 (3.3670)	LR 2.000e-03
0: TRAIN [1][190/3880]	Time 0.431 (0.312)	Data 1.62e-04 (2.17e-03)	Tok/s 53607 (42157)	Loss/tok 3.4424 (3.3735)	LR 2.000e-03
0: TRAIN [1][200/3880]	Time 0.346 (0.311)	Data 1.29e-04 (2.07e-03)	Tok/s 49218 (42014)	Loss/tok 3.4251 (3.3709)	LR 2.000e-03
0: TRAIN [1][210/3880]	Time 0.192 (0.312)	Data 1.31e-04 (1.98e-03)	Tok/s 27233 (42207)	Loss/tok 2.7792 (3.3747)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][220/3880]	Time 0.544 (0.313)	Data 1.39e-04 (1.89e-03)	Tok/s 54839 (42258)	Loss/tok 3.7329 (3.3779)	LR 2.000e-03
0: TRAIN [1][230/3880]	Time 0.347 (0.313)	Data 1.64e-04 (1.82e-03)	Tok/s 48221 (42325)	Loss/tok 3.3677 (3.3775)	LR 2.000e-03
0: TRAIN [1][240/3880]	Time 0.347 (0.312)	Data 1.60e-04 (1.75e-03)	Tok/s 48133 (42325)	Loss/tok 3.5404 (3.3751)	LR 2.000e-03
0: TRAIN [1][250/3880]	Time 0.266 (0.314)	Data 1.25e-04 (1.69e-03)	Tok/s 39311 (42516)	Loss/tok 3.2236 (3.3790)	LR 2.000e-03
0: TRAIN [1][260/3880]	Time 0.265 (0.315)	Data 1.77e-04 (1.63e-03)	Tok/s 38722 (42603)	Loss/tok 3.1149 (3.3822)	LR 2.000e-03
0: TRAIN [1][270/3880]	Time 0.266 (0.315)	Data 1.35e-04 (1.57e-03)	Tok/s 39183 (42606)	Loss/tok 3.2512 (3.3831)	LR 2.000e-03
0: TRAIN [1][280/3880]	Time 0.192 (0.316)	Data 1.40e-04 (1.52e-03)	Tok/s 28016 (42800)	Loss/tok 2.7504 (3.3852)	LR 2.000e-03
0: TRAIN [1][290/3880]	Time 0.265 (0.317)	Data 1.37e-04 (1.48e-03)	Tok/s 39149 (42849)	Loss/tok 3.1250 (3.3878)	LR 2.000e-03
0: TRAIN [1][300/3880]	Time 0.346 (0.317)	Data 1.70e-04 (1.43e-03)	Tok/s 47620 (42992)	Loss/tok 3.4762 (3.3884)	LR 2.000e-03
0: TRAIN [1][310/3880]	Time 0.430 (0.317)	Data 1.33e-04 (1.39e-03)	Tok/s 55163 (42897)	Loss/tok 3.6195 (3.3881)	LR 2.000e-03
0: TRAIN [1][320/3880]	Time 0.543 (0.318)	Data 1.58e-04 (1.35e-03)	Tok/s 55236 (43007)	Loss/tok 3.8717 (3.3914)	LR 2.000e-03
0: TRAIN [1][330/3880]	Time 0.265 (0.317)	Data 1.35e-04 (1.32e-03)	Tok/s 38896 (42920)	Loss/tok 3.2470 (3.3879)	LR 2.000e-03
0: TRAIN [1][340/3880]	Time 0.351 (0.316)	Data 1.37e-04 (1.28e-03)	Tok/s 47504 (42926)	Loss/tok 3.3764 (3.3872)	LR 2.000e-03
0: TRAIN [1][350/3880]	Time 0.266 (0.316)	Data 1.27e-04 (1.25e-03)	Tok/s 38772 (42908)	Loss/tok 3.2960 (3.3845)	LR 2.000e-03
0: TRAIN [1][360/3880]	Time 0.348 (0.316)	Data 1.31e-04 (1.22e-03)	Tok/s 48154 (42959)	Loss/tok 3.4658 (3.3863)	LR 2.000e-03
0: TRAIN [1][370/3880]	Time 0.344 (0.317)	Data 1.39e-04 (1.19e-03)	Tok/s 48312 (43020)	Loss/tok 3.3873 (3.3888)	LR 2.000e-03
0: TRAIN [1][380/3880]	Time 0.269 (0.318)	Data 1.43e-04 (1.16e-03)	Tok/s 38764 (43119)	Loss/tok 3.2027 (3.3912)	LR 2.000e-03
0: TRAIN [1][390/3880]	Time 0.267 (0.318)	Data 1.32e-04 (1.14e-03)	Tok/s 38008 (43090)	Loss/tok 3.1637 (3.3895)	LR 2.000e-03
0: TRAIN [1][400/3880]	Time 0.268 (0.318)	Data 1.71e-04 (1.11e-03)	Tok/s 38399 (43143)	Loss/tok 3.1851 (3.3936)	LR 2.000e-03
0: TRAIN [1][410/3880]	Time 0.347 (0.319)	Data 2.51e-04 (1.09e-03)	Tok/s 48213 (43196)	Loss/tok 3.3748 (3.3952)	LR 2.000e-03
0: TRAIN [1][420/3880]	Time 0.266 (0.319)	Data 1.82e-04 (1.07e-03)	Tok/s 38692 (43202)	Loss/tok 3.2249 (3.3952)	LR 2.000e-03
0: TRAIN [1][430/3880]	Time 0.347 (0.319)	Data 1.68e-04 (1.04e-03)	Tok/s 48142 (43200)	Loss/tok 3.4392 (3.3943)	LR 2.000e-03
0: TRAIN [1][440/3880]	Time 0.347 (0.318)	Data 1.33e-04 (1.02e-03)	Tok/s 48402 (43174)	Loss/tok 3.3731 (3.3940)	LR 2.000e-03
0: TRAIN [1][450/3880]	Time 0.266 (0.318)	Data 1.37e-04 (1.00e-03)	Tok/s 38922 (43102)	Loss/tok 3.2014 (3.3925)	LR 2.000e-03
0: TRAIN [1][460/3880]	Time 0.270 (0.318)	Data 1.41e-04 (9.87e-04)	Tok/s 38038 (43184)	Loss/tok 3.0748 (3.3942)	LR 2.000e-03
0: TRAIN [1][470/3880]	Time 0.347 (0.318)	Data 1.27e-04 (9.69e-04)	Tok/s 47302 (43174)	Loss/tok 3.4261 (3.3927)	LR 2.000e-03
0: TRAIN [1][480/3880]	Time 0.265 (0.317)	Data 1.45e-04 (9.52e-04)	Tok/s 39165 (43161)	Loss/tok 3.0762 (3.3910)	LR 2.000e-03
0: TRAIN [1][490/3880]	Time 0.266 (0.317)	Data 1.03e-03 (9.43e-04)	Tok/s 39065 (43159)	Loss/tok 3.0771 (3.3917)	LR 2.000e-03
0: TRAIN [1][500/3880]	Time 0.268 (0.317)	Data 1.56e-04 (9.27e-04)	Tok/s 38135 (43156)	Loss/tok 3.2708 (3.3909)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][510/3880]	Time 0.334 (0.318)	Data 1.94e-04 (9.12e-04)	Tok/s 50614 (43247)	Loss/tok 3.4052 (3.3945)	LR 2.000e-03
0: TRAIN [1][520/3880]	Time 0.431 (0.318)	Data 1.42e-04 (8.98e-04)	Tok/s 53401 (43238)	Loss/tok 3.6468 (3.3939)	LR 2.000e-03
0: TRAIN [1][530/3880]	Time 0.348 (0.319)	Data 1.35e-04 (8.84e-04)	Tok/s 48090 (43369)	Loss/tok 3.3489 (3.3962)	LR 2.000e-03
0: TRAIN [1][540/3880]	Time 0.266 (0.319)	Data 1.30e-04 (8.70e-04)	Tok/s 39851 (43315)	Loss/tok 3.1858 (3.3961)	LR 2.000e-03
0: TRAIN [1][550/3880]	Time 0.347 (0.318)	Data 1.30e-04 (8.57e-04)	Tok/s 47712 (43254)	Loss/tok 3.5115 (3.3949)	LR 2.000e-03
0: TRAIN [1][560/3880]	Time 0.266 (0.318)	Data 1.67e-04 (8.44e-04)	Tok/s 39604 (43257)	Loss/tok 3.2246 (3.3955)	LR 2.000e-03
0: TRAIN [1][570/3880]	Time 0.348 (0.317)	Data 1.34e-04 (8.32e-04)	Tok/s 47961 (43168)	Loss/tok 3.4819 (3.3934)	LR 2.000e-03
0: TRAIN [1][580/3880]	Time 0.266 (0.317)	Data 1.25e-04 (8.20e-04)	Tok/s 39175 (43178)	Loss/tok 3.1101 (3.3923)	LR 2.000e-03
0: TRAIN [1][590/3880]	Time 0.348 (0.318)	Data 1.37e-04 (8.09e-04)	Tok/s 47738 (43248)	Loss/tok 3.4425 (3.3946)	LR 2.000e-03
0: TRAIN [1][600/3880]	Time 0.347 (0.319)	Data 1.28e-04 (7.98e-04)	Tok/s 47406 (43313)	Loss/tok 3.4667 (3.3956)	LR 2.000e-03
0: TRAIN [1][610/3880]	Time 0.265 (0.319)	Data 1.77e-04 (7.87e-04)	Tok/s 39620 (43324)	Loss/tok 3.1189 (3.3950)	LR 2.000e-03
0: TRAIN [1][620/3880]	Time 0.192 (0.319)	Data 1.42e-04 (7.77e-04)	Tok/s 27382 (43309)	Loss/tok 2.6169 (3.3935)	LR 2.000e-03
0: TRAIN [1][630/3880]	Time 0.431 (0.319)	Data 1.50e-04 (7.67e-04)	Tok/s 54004 (43348)	Loss/tok 3.5122 (3.3946)	LR 2.000e-03
0: TRAIN [1][640/3880]	Time 0.267 (0.319)	Data 1.53e-04 (7.58e-04)	Tok/s 38789 (43368)	Loss/tok 3.2092 (3.3948)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][650/3880]	Time 0.432 (0.320)	Data 1.36e-04 (7.48e-04)	Tok/s 53636 (43419)	Loss/tok 3.6745 (3.3986)	LR 2.000e-03
0: TRAIN [1][660/3880]	Time 0.347 (0.320)	Data 1.32e-04 (7.40e-04)	Tok/s 47935 (43431)	Loss/tok 3.4310 (3.3979)	LR 2.000e-03
0: TRAIN [1][670/3880]	Time 0.265 (0.320)	Data 1.46e-04 (7.31e-04)	Tok/s 39182 (43460)	Loss/tok 3.2545 (3.3988)	LR 2.000e-03
0: TRAIN [1][680/3880]	Time 0.192 (0.320)	Data 1.18e-04 (7.22e-04)	Tok/s 27952 (43407)	Loss/tok 2.6921 (3.3985)	LR 2.000e-03
0: TRAIN [1][690/3880]	Time 0.193 (0.319)	Data 1.36e-04 (7.14e-04)	Tok/s 27592 (43318)	Loss/tok 2.7169 (3.3959)	LR 2.000e-03
0: TRAIN [1][700/3880]	Time 0.193 (0.319)	Data 1.63e-04 (7.06e-04)	Tok/s 26973 (43317)	Loss/tok 2.7172 (3.3962)	LR 2.000e-03
0: TRAIN [1][710/3880]	Time 0.191 (0.320)	Data 1.31e-04 (6.98e-04)	Tok/s 27746 (43338)	Loss/tok 2.6623 (3.3980)	LR 2.000e-03
0: TRAIN [1][720/3880]	Time 0.266 (0.320)	Data 1.28e-04 (6.91e-04)	Tok/s 38542 (43392)	Loss/tok 3.2300 (3.3995)	LR 2.000e-03
0: TRAIN [1][730/3880]	Time 0.348 (0.320)	Data 1.27e-04 (6.83e-04)	Tok/s 47968 (43396)	Loss/tok 3.3751 (3.3994)	LR 2.000e-03
0: TRAIN [1][740/3880]	Time 0.192 (0.320)	Data 1.55e-04 (6.76e-04)	Tok/s 27143 (43318)	Loss/tok 2.6856 (3.3980)	LR 2.000e-03
0: TRAIN [1][750/3880]	Time 0.191 (0.320)	Data 1.40e-04 (6.69e-04)	Tok/s 27753 (43372)	Loss/tok 2.7211 (3.3997)	LR 2.000e-03
0: TRAIN [1][760/3880]	Time 0.347 (0.320)	Data 1.83e-04 (6.62e-04)	Tok/s 48870 (43355)	Loss/tok 3.3155 (3.3983)	LR 2.000e-03
0: TRAIN [1][770/3880]	Time 0.347 (0.320)	Data 1.33e-04 (6.56e-04)	Tok/s 48301 (43343)	Loss/tok 3.3889 (3.3973)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][780/3880]	Time 0.419 (0.320)	Data 1.35e-04 (6.49e-04)	Tok/s 55666 (43362)	Loss/tok 3.6582 (3.4002)	LR 2.000e-03
0: TRAIN [1][790/3880]	Time 0.194 (0.320)	Data 1.31e-04 (6.43e-04)	Tok/s 26259 (43342)	Loss/tok 2.6453 (3.4011)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][800/3880]	Time 0.346 (0.320)	Data 1.67e-04 (6.37e-04)	Tok/s 48416 (43287)	Loss/tok 3.4004 (3.4001)	LR 2.000e-03
0: TRAIN [1][810/3880]	Time 0.346 (0.320)	Data 1.44e-04 (6.31e-04)	Tok/s 48305 (43307)	Loss/tok 3.4725 (3.4000)	LR 2.000e-03
0: TRAIN [1][820/3880]	Time 0.542 (0.321)	Data 1.31e-04 (6.25e-04)	Tok/s 55568 (43328)	Loss/tok 3.6852 (3.4020)	LR 2.000e-03
0: TRAIN [1][830/3880]	Time 0.356 (0.321)	Data 1.46e-04 (6.19e-04)	Tok/s 47190 (43350)	Loss/tok 3.4271 (3.4036)	LR 2.000e-03
0: TRAIN [1][840/3880]	Time 0.266 (0.321)	Data 1.39e-04 (6.14e-04)	Tok/s 38541 (43346)	Loss/tok 3.0949 (3.4024)	LR 2.000e-03
0: TRAIN [1][850/3880]	Time 0.266 (0.321)	Data 1.36e-04 (6.08e-04)	Tok/s 38335 (43373)	Loss/tok 3.1914 (3.4035)	LR 2.000e-03
0: TRAIN [1][860/3880]	Time 0.265 (0.321)	Data 1.32e-04 (6.03e-04)	Tok/s 38961 (43376)	Loss/tok 3.2972 (3.4033)	LR 2.000e-03
0: TRAIN [1][870/3880]	Time 0.266 (0.321)	Data 1.74e-04 (5.98e-04)	Tok/s 38171 (43366)	Loss/tok 3.2221 (3.4034)	LR 2.000e-03
0: TRAIN [1][880/3880]	Time 0.346 (0.322)	Data 1.34e-04 (5.93e-04)	Tok/s 48996 (43417)	Loss/tok 3.3842 (3.4049)	LR 2.000e-03
0: TRAIN [1][890/3880]	Time 0.347 (0.322)	Data 1.39e-04 (5.88e-04)	Tok/s 48154 (43480)	Loss/tok 3.3078 (3.4055)	LR 2.000e-03
0: TRAIN [1][900/3880]	Time 0.265 (0.322)	Data 1.29e-04 (5.83e-04)	Tok/s 38835 (43507)	Loss/tok 3.1384 (3.4056)	LR 2.000e-03
0: TRAIN [1][910/3880]	Time 0.545 (0.322)	Data 1.78e-04 (5.78e-04)	Tok/s 55525 (43500)	Loss/tok 3.6459 (3.4059)	LR 2.000e-03
0: TRAIN [1][920/3880]	Time 0.347 (0.322)	Data 1.34e-04 (5.73e-04)	Tok/s 48458 (43502)	Loss/tok 3.3636 (3.4058)	LR 2.000e-03
0: TRAIN [1][930/3880]	Time 0.346 (0.322)	Data 1.97e-04 (5.69e-04)	Tok/s 49069 (43509)	Loss/tok 3.4306 (3.4064)	LR 2.000e-03
0: TRAIN [1][940/3880]	Time 0.347 (0.322)	Data 1.35e-04 (5.64e-04)	Tok/s 49166 (43500)	Loss/tok 3.2580 (3.4055)	LR 2.000e-03
0: TRAIN [1][950/3880]	Time 0.265 (0.322)	Data 1.41e-04 (5.60e-04)	Tok/s 39422 (43480)	Loss/tok 3.1405 (3.4042)	LR 2.000e-03
0: TRAIN [1][960/3880]	Time 0.265 (0.322)	Data 1.35e-04 (5.56e-04)	Tok/s 38913 (43455)	Loss/tok 3.1249 (3.4034)	LR 2.000e-03
0: TRAIN [1][970/3880]	Time 0.265 (0.321)	Data 1.25e-04 (5.52e-04)	Tok/s 39096 (43410)	Loss/tok 3.1887 (3.4019)	LR 2.000e-03
0: TRAIN [1][980/3880]	Time 0.266 (0.321)	Data 1.91e-04 (5.47e-04)	Tok/s 38751 (43438)	Loss/tok 3.1204 (3.4020)	LR 2.000e-03
0: TRAIN [1][990/3880]	Time 0.266 (0.321)	Data 1.36e-04 (5.43e-04)	Tok/s 39336 (43425)	Loss/tok 3.1554 (3.4012)	LR 2.000e-03
0: TRAIN [1][1000/3880]	Time 0.193 (0.321)	Data 1.85e-04 (5.39e-04)	Tok/s 27309 (43424)	Loss/tok 2.7205 (3.4010)	LR 2.000e-03
0: TRAIN [1][1010/3880]	Time 0.266 (0.321)	Data 2.26e-04 (5.36e-04)	Tok/s 37570 (43437)	Loss/tok 3.1867 (3.4011)	LR 2.000e-03
0: TRAIN [1][1020/3880]	Time 0.431 (0.322)	Data 1.37e-04 (5.32e-04)	Tok/s 54305 (43484)	Loss/tok 3.5879 (3.4023)	LR 2.000e-03
0: TRAIN [1][1030/3880]	Time 0.428 (0.322)	Data 1.30e-04 (5.28e-04)	Tok/s 53921 (43484)	Loss/tok 3.5608 (3.4020)	LR 2.000e-03
0: TRAIN [1][1040/3880]	Time 0.265 (0.322)	Data 1.46e-04 (5.24e-04)	Tok/s 38707 (43485)	Loss/tok 3.2509 (3.4022)	LR 2.000e-03
0: TRAIN [1][1050/3880]	Time 0.545 (0.322)	Data 1.34e-04 (5.21e-04)	Tok/s 54283 (43481)	Loss/tok 3.7588 (3.4019)	LR 2.000e-03
0: TRAIN [1][1060/3880]	Time 0.347 (0.322)	Data 1.56e-04 (5.17e-04)	Tok/s 49023 (43490)	Loss/tok 3.4115 (3.4016)	LR 2.000e-03
0: TRAIN [1][1070/3880]	Time 0.267 (0.322)	Data 1.30e-04 (5.13e-04)	Tok/s 37600 (43472)	Loss/tok 3.1446 (3.4004)	LR 2.000e-03
0: TRAIN [1][1080/3880]	Time 0.266 (0.321)	Data 1.68e-04 (5.10e-04)	Tok/s 38050 (43468)	Loss/tok 3.1598 (3.3996)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1090/3880]	Time 0.256 (0.321)	Data 1.50e-04 (5.07e-04)	Tok/s 39600 (43461)	Loss/tok 3.1678 (3.3996)	LR 2.000e-03
0: TRAIN [1][1100/3880]	Time 0.266 (0.321)	Data 1.25e-04 (5.04e-04)	Tok/s 38553 (43433)	Loss/tok 3.1811 (3.3987)	LR 2.000e-03
0: TRAIN [1][1110/3880]	Time 0.346 (0.321)	Data 1.31e-04 (5.00e-04)	Tok/s 48652 (43421)	Loss/tok 3.3636 (3.3980)	LR 2.000e-03
0: TRAIN [1][1120/3880]	Time 0.265 (0.321)	Data 1.29e-04 (4.99e-04)	Tok/s 39191 (43423)	Loss/tok 3.1170 (3.3972)	LR 2.000e-03
0: TRAIN [1][1130/3880]	Time 0.266 (0.320)	Data 1.35e-04 (4.95e-04)	Tok/s 38497 (43377)	Loss/tok 3.2043 (3.3967)	LR 2.000e-03
0: TRAIN [1][1140/3880]	Time 0.198 (0.320)	Data 1.31e-04 (4.92e-04)	Tok/s 26210 (43324)	Loss/tok 2.6510 (3.3954)	LR 2.000e-03
0: TRAIN [1][1150/3880]	Time 0.346 (0.320)	Data 1.34e-04 (4.89e-04)	Tok/s 49192 (43340)	Loss/tok 3.5693 (3.3954)	LR 2.000e-03
0: TRAIN [1][1160/3880]	Time 0.266 (0.320)	Data 1.34e-04 (4.86e-04)	Tok/s 38958 (43370)	Loss/tok 3.2321 (3.3963)	LR 2.000e-03
0: TRAIN [1][1170/3880]	Time 0.346 (0.320)	Data 1.34e-04 (4.83e-04)	Tok/s 48148 (43360)	Loss/tok 3.2555 (3.3957)	LR 2.000e-03
0: TRAIN [1][1180/3880]	Time 0.266 (0.320)	Data 1.32e-04 (4.80e-04)	Tok/s 38700 (43358)	Loss/tok 3.1174 (3.3958)	LR 2.000e-03
0: TRAIN [1][1190/3880]	Time 0.266 (0.320)	Data 1.63e-04 (4.78e-04)	Tok/s 38762 (43379)	Loss/tok 3.2331 (3.3955)	LR 2.000e-03
0: TRAIN [1][1200/3880]	Time 0.258 (0.321)	Data 3.16e-04 (4.75e-04)	Tok/s 40093 (43401)	Loss/tok 3.0815 (3.3961)	LR 2.000e-03
0: TRAIN [1][1210/3880]	Time 0.266 (0.320)	Data 1.44e-04 (4.72e-04)	Tok/s 38955 (43394)	Loss/tok 3.1822 (3.3958)	LR 2.000e-03
0: TRAIN [1][1220/3880]	Time 0.266 (0.320)	Data 1.32e-04 (4.70e-04)	Tok/s 38277 (43355)	Loss/tok 3.1781 (3.3946)	LR 2.000e-03
0: TRAIN [1][1230/3880]	Time 0.429 (0.321)	Data 2.80e-04 (4.67e-04)	Tok/s 53815 (43401)	Loss/tok 3.5869 (3.3954)	LR 2.000e-03
0: TRAIN [1][1240/3880]	Time 0.431 (0.321)	Data 1.30e-04 (4.65e-04)	Tok/s 54386 (43434)	Loss/tok 3.6072 (3.3959)	LR 2.000e-03
0: TRAIN [1][1250/3880]	Time 0.351 (0.320)	Data 2.59e-04 (4.62e-04)	Tok/s 48336 (43401)	Loss/tok 3.3172 (3.3947)	LR 2.000e-03
0: TRAIN [1][1260/3880]	Time 0.265 (0.320)	Data 1.55e-04 (4.60e-04)	Tok/s 38212 (43390)	Loss/tok 3.1176 (3.3951)	LR 2.000e-03
0: TRAIN [1][1270/3880]	Time 0.268 (0.320)	Data 1.49e-04 (4.57e-04)	Tok/s 38778 (43380)	Loss/tok 3.2264 (3.3944)	LR 2.000e-03
0: TRAIN [1][1280/3880]	Time 0.268 (0.320)	Data 1.80e-04 (4.55e-04)	Tok/s 39319 (43372)	Loss/tok 3.0614 (3.3942)	LR 2.000e-03
0: TRAIN [1][1290/3880]	Time 0.346 (0.321)	Data 1.30e-04 (4.53e-04)	Tok/s 48594 (43414)	Loss/tok 3.3627 (3.3949)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1300/3880]	Time 0.347 (0.321)	Data 1.75e-04 (4.50e-04)	Tok/s 47963 (43441)	Loss/tok 3.2863 (3.3954)	LR 2.000e-03
0: TRAIN [1][1310/3880]	Time 0.193 (0.321)	Data 1.65e-04 (4.48e-04)	Tok/s 27607 (43443)	Loss/tok 2.7577 (3.3958)	LR 2.000e-03
0: TRAIN [1][1320/3880]	Time 0.544 (0.321)	Data 1.41e-04 (4.46e-04)	Tok/s 54912 (43478)	Loss/tok 3.7488 (3.3971)	LR 2.000e-03
0: TRAIN [1][1330/3880]	Time 0.545 (0.321)	Data 1.32e-04 (4.43e-04)	Tok/s 54284 (43458)	Loss/tok 3.7192 (3.3968)	LR 2.000e-03
0: TRAIN [1][1340/3880]	Time 0.266 (0.321)	Data 1.30e-04 (4.41e-04)	Tok/s 38302 (43442)	Loss/tok 3.2486 (3.3964)	LR 2.000e-03
0: TRAIN [1][1350/3880]	Time 0.265 (0.321)	Data 1.45e-04 (4.39e-04)	Tok/s 38704 (43440)	Loss/tok 3.0954 (3.3959)	LR 2.000e-03
0: TRAIN [1][1360/3880]	Time 0.348 (0.321)	Data 1.36e-04 (4.37e-04)	Tok/s 47975 (43415)	Loss/tok 3.4211 (3.3950)	LR 2.000e-03
0: TRAIN [1][1370/3880]	Time 0.266 (0.321)	Data 1.42e-04 (4.35e-04)	Tok/s 38655 (43416)	Loss/tok 3.1018 (3.3944)	LR 2.000e-03
0: TRAIN [1][1380/3880]	Time 0.431 (0.321)	Data 1.49e-04 (4.33e-04)	Tok/s 53559 (43454)	Loss/tok 3.6913 (3.3948)	LR 2.000e-03
0: TRAIN [1][1390/3880]	Time 0.346 (0.321)	Data 1.80e-04 (4.31e-04)	Tok/s 49311 (43483)	Loss/tok 3.3413 (3.3952)	LR 2.000e-03
0: TRAIN [1][1400/3880]	Time 0.429 (0.322)	Data 1.33e-04 (4.29e-04)	Tok/s 53612 (43524)	Loss/tok 3.6135 (3.3961)	LR 2.000e-03
0: TRAIN [1][1410/3880]	Time 0.544 (0.322)	Data 1.34e-04 (4.27e-04)	Tok/s 54411 (43543)	Loss/tok 3.8893 (3.3970)	LR 2.000e-03
0: TRAIN [1][1420/3880]	Time 0.266 (0.322)	Data 1.36e-04 (4.25e-04)	Tok/s 39328 (43531)	Loss/tok 3.0361 (3.3964)	LR 2.000e-03
0: TRAIN [1][1430/3880]	Time 0.267 (0.322)	Data 1.43e-04 (4.23e-04)	Tok/s 38638 (43537)	Loss/tok 3.1889 (3.3960)	LR 2.000e-03
0: TRAIN [1][1440/3880]	Time 0.265 (0.322)	Data 1.36e-04 (4.21e-04)	Tok/s 38974 (43540)	Loss/tok 3.1707 (3.3955)	LR 2.000e-03
0: TRAIN [1][1450/3880]	Time 0.347 (0.321)	Data 1.54e-04 (4.19e-04)	Tok/s 49435 (43539)	Loss/tok 3.3421 (3.3951)	LR 2.000e-03
0: TRAIN [1][1460/3880]	Time 0.266 (0.321)	Data 1.58e-04 (4.17e-04)	Tok/s 38442 (43528)	Loss/tok 3.2180 (3.3948)	LR 2.000e-03
0: TRAIN [1][1470/3880]	Time 0.266 (0.321)	Data 1.56e-04 (4.16e-04)	Tok/s 39159 (43529)	Loss/tok 3.1077 (3.3944)	LR 2.000e-03
0: TRAIN [1][1480/3880]	Time 0.267 (0.321)	Data 1.29e-04 (4.14e-04)	Tok/s 38200 (43521)	Loss/tok 3.1364 (3.3937)	LR 2.000e-03
0: TRAIN [1][1490/3880]	Time 0.192 (0.321)	Data 1.29e-04 (4.12e-04)	Tok/s 28203 (43513)	Loss/tok 2.7884 (3.3933)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1500/3880]	Time 0.430 (0.321)	Data 1.32e-04 (4.10e-04)	Tok/s 53338 (43540)	Loss/tok 3.5169 (3.3939)	LR 2.000e-03
0: TRAIN [1][1510/3880]	Time 0.348 (0.321)	Data 1.88e-04 (4.09e-04)	Tok/s 48436 (43549)	Loss/tok 3.4243 (3.3940)	LR 2.000e-03
0: TRAIN [1][1520/3880]	Time 0.346 (0.321)	Data 1.27e-04 (4.07e-04)	Tok/s 49104 (43531)	Loss/tok 3.3896 (3.3934)	LR 2.000e-03
0: TRAIN [1][1530/3880]	Time 0.544 (0.321)	Data 1.28e-04 (4.05e-04)	Tok/s 53915 (43553)	Loss/tok 3.7903 (3.3938)	LR 2.000e-03
0: TRAIN [1][1540/3880]	Time 0.430 (0.321)	Data 1.31e-04 (4.03e-04)	Tok/s 54061 (43536)	Loss/tok 3.5612 (3.3936)	LR 2.000e-03
0: TRAIN [1][1550/3880]	Time 0.266 (0.321)	Data 1.37e-04 (4.02e-04)	Tok/s 38586 (43545)	Loss/tok 3.2826 (3.3935)	LR 2.000e-03
0: TRAIN [1][1560/3880]	Time 0.346 (0.321)	Data 1.19e-04 (4.00e-04)	Tok/s 47655 (43557)	Loss/tok 3.4590 (3.3936)	LR 2.000e-03
0: TRAIN [1][1570/3880]	Time 0.347 (0.322)	Data 1.63e-04 (3.98e-04)	Tok/s 48163 (43559)	Loss/tok 3.3942 (3.3946)	LR 2.000e-03
0: TRAIN [1][1580/3880]	Time 0.346 (0.322)	Data 1.36e-04 (3.97e-04)	Tok/s 49182 (43570)	Loss/tok 3.2097 (3.3941)	LR 2.000e-03
0: TRAIN [1][1590/3880]	Time 0.268 (0.322)	Data 1.31e-04 (3.95e-04)	Tok/s 39086 (43577)	Loss/tok 3.2713 (3.3937)	LR 2.000e-03
0: TRAIN [1][1600/3880]	Time 0.191 (0.322)	Data 1.36e-04 (3.94e-04)	Tok/s 27528 (43588)	Loss/tok 2.7374 (3.3937)	LR 2.000e-03
0: TRAIN [1][1610/3880]	Time 0.266 (0.322)	Data 1.33e-04 (3.92e-04)	Tok/s 39672 (43574)	Loss/tok 3.0980 (3.3938)	LR 2.000e-03
0: TRAIN [1][1620/3880]	Time 0.265 (0.321)	Data 1.92e-04 (3.90e-04)	Tok/s 39324 (43543)	Loss/tok 3.1890 (3.3927)	LR 2.000e-03
0: TRAIN [1][1630/3880]	Time 0.195 (0.321)	Data 1.17e-04 (3.89e-04)	Tok/s 26676 (43527)	Loss/tok 2.6856 (3.3926)	LR 2.000e-03
0: TRAIN [1][1640/3880]	Time 0.545 (0.321)	Data 1.80e-04 (3.88e-04)	Tok/s 55229 (43542)	Loss/tok 3.6488 (3.3925)	LR 2.000e-03
0: TRAIN [1][1650/3880]	Time 0.347 (0.321)	Data 1.39e-04 (3.86e-04)	Tok/s 48549 (43542)	Loss/tok 3.3628 (3.3929)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1660/3880]	Time 0.347 (0.322)	Data 1.43e-04 (3.85e-04)	Tok/s 47661 (43578)	Loss/tok 3.3705 (3.3941)	LR 2.000e-03
0: TRAIN [1][1670/3880]	Time 0.265 (0.322)	Data 1.49e-04 (3.83e-04)	Tok/s 39236 (43580)	Loss/tok 3.1724 (3.3940)	LR 2.000e-03
0: TRAIN [1][1680/3880]	Time 0.266 (0.322)	Data 1.83e-04 (3.82e-04)	Tok/s 38567 (43568)	Loss/tok 3.1732 (3.3938)	LR 2.000e-03
0: TRAIN [1][1690/3880]	Time 0.432 (0.322)	Data 1.28e-04 (3.80e-04)	Tok/s 53864 (43564)	Loss/tok 3.6105 (3.3935)	LR 2.000e-03
0: TRAIN [1][1700/3880]	Time 0.348 (0.322)	Data 1.47e-04 (3.79e-04)	Tok/s 48776 (43583)	Loss/tok 3.4280 (3.3937)	LR 2.000e-03
0: TRAIN [1][1710/3880]	Time 0.192 (0.322)	Data 1.35e-04 (3.78e-04)	Tok/s 27729 (43585)	Loss/tok 2.6877 (3.3937)	LR 2.000e-03
0: TRAIN [1][1720/3880]	Time 0.192 (0.322)	Data 1.28e-04 (3.76e-04)	Tok/s 27947 (43584)	Loss/tok 2.7009 (3.3934)	LR 2.000e-03
0: TRAIN [1][1730/3880]	Time 0.348 (0.322)	Data 1.25e-04 (3.75e-04)	Tok/s 48329 (43590)	Loss/tok 3.3755 (3.3933)	LR 2.000e-03
0: TRAIN [1][1740/3880]	Time 0.266 (0.322)	Data 1.31e-04 (3.74e-04)	Tok/s 39171 (43572)	Loss/tok 3.2694 (3.3929)	LR 2.000e-03
0: TRAIN [1][1750/3880]	Time 0.347 (0.322)	Data 1.30e-04 (3.73e-04)	Tok/s 48794 (43573)	Loss/tok 3.2930 (3.3926)	LR 2.000e-03
0: TRAIN [1][1760/3880]	Time 0.347 (0.322)	Data 1.37e-04 (3.71e-04)	Tok/s 48301 (43573)	Loss/tok 3.3057 (3.3926)	LR 2.000e-03
0: TRAIN [1][1770/3880]	Time 0.266 (0.322)	Data 1.48e-04 (3.70e-04)	Tok/s 38739 (43570)	Loss/tok 3.0782 (3.3921)	LR 2.000e-03
0: TRAIN [1][1780/3880]	Time 0.348 (0.321)	Data 1.34e-04 (3.69e-04)	Tok/s 47410 (43562)	Loss/tok 3.4043 (3.3916)	LR 2.000e-03
0: TRAIN [1][1790/3880]	Time 0.266 (0.321)	Data 1.28e-04 (3.67e-04)	Tok/s 39394 (43540)	Loss/tok 3.1479 (3.3910)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1800/3880]	Time 0.266 (0.321)	Data 1.90e-04 (3.66e-04)	Tok/s 39595 (43535)	Loss/tok 3.1520 (3.3906)	LR 2.000e-03
0: TRAIN [1][1810/3880]	Time 0.268 (0.321)	Data 1.72e-04 (3.65e-04)	Tok/s 38984 (43530)	Loss/tok 3.1261 (3.3904)	LR 2.000e-03
0: TRAIN [1][1820/3880]	Time 0.266 (0.321)	Data 1.69e-04 (3.64e-04)	Tok/s 38838 (43522)	Loss/tok 3.2067 (3.3900)	LR 2.000e-03
0: TRAIN [1][1830/3880]	Time 0.265 (0.321)	Data 1.26e-04 (3.63e-04)	Tok/s 39103 (43534)	Loss/tok 3.1656 (3.3897)	LR 2.000e-03
0: TRAIN [1][1840/3880]	Time 0.268 (0.321)	Data 1.29e-04 (3.61e-04)	Tok/s 38312 (43539)	Loss/tok 3.1500 (3.3896)	LR 2.000e-03
0: TRAIN [1][1850/3880]	Time 0.348 (0.321)	Data 1.44e-04 (3.60e-04)	Tok/s 48344 (43557)	Loss/tok 3.3166 (3.3895)	LR 2.000e-03
0: TRAIN [1][1860/3880]	Time 0.545 (0.321)	Data 1.45e-04 (3.59e-04)	Tok/s 55012 (43563)	Loss/tok 3.7705 (3.3895)	LR 2.000e-03
0: TRAIN [1][1870/3880]	Time 0.347 (0.321)	Data 1.35e-04 (3.58e-04)	Tok/s 47805 (43592)	Loss/tok 3.4839 (3.3900)	LR 2.000e-03
0: TRAIN [1][1880/3880]	Time 0.348 (0.321)	Data 1.30e-04 (3.57e-04)	Tok/s 48126 (43598)	Loss/tok 3.2354 (3.3899)	LR 2.000e-03
0: TRAIN [1][1890/3880]	Time 0.195 (0.321)	Data 1.33e-04 (3.56e-04)	Tok/s 27212 (43579)	Loss/tok 2.6579 (3.3895)	LR 2.000e-03
0: TRAIN [1][1900/3880]	Time 0.192 (0.321)	Data 1.36e-04 (3.55e-04)	Tok/s 27684 (43566)	Loss/tok 2.7678 (3.3891)	LR 2.000e-03
0: TRAIN [1][1910/3880]	Time 0.347 (0.321)	Data 1.29e-04 (3.54e-04)	Tok/s 47474 (43565)	Loss/tok 3.4892 (3.3893)	LR 2.000e-03
0: TRAIN [1][1920/3880]	Time 0.265 (0.321)	Data 1.38e-04 (3.53e-04)	Tok/s 38669 (43561)	Loss/tok 3.1190 (3.3892)	LR 2.000e-03
0: TRAIN [1][1930/3880]	Time 0.266 (0.321)	Data 1.43e-04 (3.52e-04)	Tok/s 38839 (43560)	Loss/tok 3.2037 (3.3894)	LR 2.000e-03
0: TRAIN [1][1940/3880]	Time 0.265 (0.321)	Data 1.46e-04 (3.50e-04)	Tok/s 39774 (43553)	Loss/tok 3.1041 (3.3889)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1950/3880]	Time 0.350 (0.321)	Data 1.25e-04 (3.49e-04)	Tok/s 47797 (43558)	Loss/tok 3.4479 (3.3888)	LR 2.000e-03
0: TRAIN [1][1960/3880]	Time 0.544 (0.322)	Data 1.49e-04 (3.48e-04)	Tok/s 55687 (43590)	Loss/tok 3.6648 (3.3893)	LR 2.000e-03
0: TRAIN [1][1970/3880]	Time 0.266 (0.321)	Data 1.60e-04 (3.47e-04)	Tok/s 38099 (43580)	Loss/tok 3.0439 (3.3891)	LR 2.000e-03
0: TRAIN [1][1980/3880]	Time 0.431 (0.322)	Data 1.61e-04 (3.46e-04)	Tok/s 53735 (43588)	Loss/tok 3.5410 (3.3902)	LR 2.000e-03
0: TRAIN [1][1990/3880]	Time 0.265 (0.321)	Data 1.76e-04 (3.45e-04)	Tok/s 38877 (43578)	Loss/tok 3.2229 (3.3895)	LR 2.000e-03
0: TRAIN [1][2000/3880]	Time 0.347 (0.321)	Data 1.35e-04 (3.44e-04)	Tok/s 48438 (43572)	Loss/tok 3.2767 (3.3889)	LR 2.000e-03
0: TRAIN [1][2010/3880]	Time 0.266 (0.322)	Data 1.49e-04 (3.43e-04)	Tok/s 39249 (43575)	Loss/tok 3.1762 (3.3893)	LR 2.000e-03
0: TRAIN [1][2020/3880]	Time 0.347 (0.322)	Data 1.39e-04 (3.42e-04)	Tok/s 48380 (43584)	Loss/tok 3.3174 (3.3891)	LR 2.000e-03
0: TRAIN [1][2030/3880]	Time 0.266 (0.322)	Data 1.31e-04 (3.41e-04)	Tok/s 38237 (43594)	Loss/tok 3.0729 (3.3894)	LR 2.000e-03
0: TRAIN [1][2040/3880]	Time 0.266 (0.321)	Data 1.63e-04 (3.40e-04)	Tok/s 39027 (43577)	Loss/tok 3.1574 (3.3889)	LR 2.000e-03
0: TRAIN [1][2050/3880]	Time 0.347 (0.321)	Data 1.34e-04 (3.40e-04)	Tok/s 48873 (43564)	Loss/tok 3.3589 (3.3887)	LR 2.000e-03
0: TRAIN [1][2060/3880]	Time 0.543 (0.321)	Data 1.33e-04 (3.39e-04)	Tok/s 53519 (43542)	Loss/tok 3.7542 (3.3887)	LR 2.000e-03
0: TRAIN [1][2070/3880]	Time 0.346 (0.322)	Data 1.80e-04 (3.38e-04)	Tok/s 48736 (43558)	Loss/tok 3.4072 (3.3893)	LR 2.000e-03
0: TRAIN [1][2080/3880]	Time 0.266 (0.321)	Data 1.74e-04 (3.37e-04)	Tok/s 38063 (43554)	Loss/tok 3.1674 (3.3890)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2090/3880]	Time 0.346 (0.322)	Data 1.90e-04 (3.36e-04)	Tok/s 48656 (43574)	Loss/tok 3.4500 (3.3892)	LR 2.000e-03
0: TRAIN [1][2100/3880]	Time 0.347 (0.322)	Data 1.44e-04 (3.35e-04)	Tok/s 47867 (43576)	Loss/tok 3.3496 (3.3889)	LR 2.000e-03
0: TRAIN [1][2110/3880]	Time 0.347 (0.322)	Data 1.43e-04 (3.34e-04)	Tok/s 48322 (43577)	Loss/tok 3.3165 (3.3887)	LR 2.000e-03
0: TRAIN [1][2120/3880]	Time 0.266 (0.322)	Data 1.35e-04 (3.33e-04)	Tok/s 38697 (43587)	Loss/tok 3.1893 (3.3891)	LR 2.000e-03
0: TRAIN [1][2130/3880]	Time 0.201 (0.322)	Data 1.33e-04 (3.33e-04)	Tok/s 27074 (43577)	Loss/tok 2.7035 (3.3885)	LR 2.000e-03
0: TRAIN [1][2140/3880]	Time 0.266 (0.322)	Data 1.38e-04 (3.32e-04)	Tok/s 38513 (43589)	Loss/tok 3.1637 (3.3889)	LR 2.000e-03
0: TRAIN [1][2150/3880]	Time 0.430 (0.322)	Data 1.74e-04 (3.31e-04)	Tok/s 54445 (43588)	Loss/tok 3.5934 (3.3887)	LR 2.000e-03
0: TRAIN [1][2160/3880]	Time 0.544 (0.322)	Data 1.30e-04 (3.30e-04)	Tok/s 54247 (43596)	Loss/tok 3.6948 (3.3889)	LR 2.000e-03
0: TRAIN [1][2170/3880]	Time 0.348 (0.322)	Data 1.36e-04 (3.29e-04)	Tok/s 48401 (43586)	Loss/tok 3.3297 (3.3882)	LR 2.000e-03
0: TRAIN [1][2180/3880]	Time 0.266 (0.322)	Data 1.20e-04 (3.28e-04)	Tok/s 38699 (43578)	Loss/tok 3.2133 (3.3878)	LR 2.000e-03
0: TRAIN [1][2190/3880]	Time 0.266 (0.322)	Data 1.72e-04 (3.27e-04)	Tok/s 37919 (43582)	Loss/tok 3.2285 (3.3879)	LR 2.000e-03
0: TRAIN [1][2200/3880]	Time 0.265 (0.322)	Data 1.32e-04 (3.27e-04)	Tok/s 39370 (43591)	Loss/tok 3.1258 (3.3883)	LR 2.000e-03
0: TRAIN [1][2210/3880]	Time 0.266 (0.322)	Data 1.43e-04 (3.26e-04)	Tok/s 38658 (43588)	Loss/tok 3.1994 (3.3880)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2220/3880]	Time 0.544 (0.322)	Data 1.31e-04 (3.25e-04)	Tok/s 54906 (43595)	Loss/tok 3.6715 (3.3884)	LR 2.000e-03
0: TRAIN [1][2230/3880]	Time 0.266 (0.322)	Data 1.60e-04 (3.24e-04)	Tok/s 38666 (43608)	Loss/tok 3.0189 (3.3887)	LR 2.000e-03
0: TRAIN [1][2240/3880]	Time 0.266 (0.322)	Data 1.37e-04 (3.24e-04)	Tok/s 38982 (43608)	Loss/tok 3.1300 (3.3885)	LR 2.000e-03
0: TRAIN [1][2250/3880]	Time 0.267 (0.322)	Data 1.45e-04 (3.23e-04)	Tok/s 38855 (43616)	Loss/tok 3.0938 (3.3884)	LR 2.000e-03
0: TRAIN [1][2260/3880]	Time 0.266 (0.322)	Data 1.30e-04 (3.22e-04)	Tok/s 38574 (43642)	Loss/tok 3.1619 (3.3891)	LR 2.000e-03
0: TRAIN [1][2270/3880]	Time 0.192 (0.322)	Data 1.92e-04 (3.22e-04)	Tok/s 28050 (43632)	Loss/tok 2.8079 (3.3890)	LR 2.000e-03
0: TRAIN [1][2280/3880]	Time 0.192 (0.322)	Data 1.74e-04 (3.21e-04)	Tok/s 27422 (43624)	Loss/tok 2.7215 (3.3885)	LR 2.000e-03
0: TRAIN [1][2290/3880]	Time 0.192 (0.322)	Data 1.54e-04 (3.20e-04)	Tok/s 27651 (43618)	Loss/tok 2.7815 (3.3880)	LR 2.000e-03
0: TRAIN [1][2300/3880]	Time 0.265 (0.322)	Data 1.30e-04 (3.19e-04)	Tok/s 38905 (43609)	Loss/tok 3.0767 (3.3875)	LR 2.000e-03
0: TRAIN [1][2310/3880]	Time 0.266 (0.322)	Data 1.42e-04 (3.19e-04)	Tok/s 39071 (43592)	Loss/tok 3.1720 (3.3871)	LR 2.000e-03
0: TRAIN [1][2320/3880]	Time 0.430 (0.322)	Data 1.59e-04 (3.18e-04)	Tok/s 54439 (43603)	Loss/tok 3.4877 (3.3869)	LR 2.000e-03
0: TRAIN [1][2330/3880]	Time 0.265 (0.322)	Data 1.46e-04 (3.17e-04)	Tok/s 39002 (43610)	Loss/tok 3.1127 (3.3871)	LR 2.000e-03
0: TRAIN [1][2340/3880]	Time 0.432 (0.322)	Data 1.95e-04 (3.17e-04)	Tok/s 54286 (43618)	Loss/tok 3.4429 (3.3871)	LR 2.000e-03
0: TRAIN [1][2350/3880]	Time 0.266 (0.322)	Data 1.36e-04 (3.16e-04)	Tok/s 39081 (43627)	Loss/tok 3.1988 (3.3875)	LR 2.000e-03
0: TRAIN [1][2360/3880]	Time 0.347 (0.322)	Data 1.92e-04 (3.15e-04)	Tok/s 47981 (43617)	Loss/tok 3.3950 (3.3870)	LR 2.000e-03
0: TRAIN [1][2370/3880]	Time 0.346 (0.322)	Data 1.33e-04 (3.15e-04)	Tok/s 48272 (43621)	Loss/tok 3.3579 (3.3869)	LR 2.000e-03
0: TRAIN [1][2380/3880]	Time 0.266 (0.322)	Data 1.54e-04 (3.14e-04)	Tok/s 39591 (43595)	Loss/tok 3.0731 (3.3863)	LR 2.000e-03
0: TRAIN [1][2390/3880]	Time 0.347 (0.322)	Data 1.33e-04 (3.13e-04)	Tok/s 48067 (43584)	Loss/tok 3.3047 (3.3857)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2400/3880]	Time 0.347 (0.322)	Data 1.30e-04 (3.12e-04)	Tok/s 49218 (43594)	Loss/tok 3.2918 (3.3856)	LR 2.000e-03
0: TRAIN [1][2410/3880]	Time 0.432 (0.322)	Data 1.50e-04 (3.12e-04)	Tok/s 54135 (43595)	Loss/tok 3.5779 (3.3856)	LR 2.000e-03
0: TRAIN [1][2420/3880]	Time 0.266 (0.322)	Data 1.88e-04 (3.11e-04)	Tok/s 38232 (43586)	Loss/tok 3.1757 (3.3853)	LR 2.000e-03
0: TRAIN [1][2430/3880]	Time 0.266 (0.322)	Data 1.88e-04 (3.10e-04)	Tok/s 38887 (43597)	Loss/tok 3.0965 (3.3849)	LR 2.000e-03
0: TRAIN [1][2440/3880]	Time 0.348 (0.322)	Data 1.30e-04 (3.10e-04)	Tok/s 48249 (43598)	Loss/tok 3.3185 (3.3843)	LR 2.000e-03
0: TRAIN [1][2450/3880]	Time 0.346 (0.322)	Data 1.48e-04 (3.09e-04)	Tok/s 48238 (43600)	Loss/tok 3.3317 (3.3839)	LR 2.000e-03
0: TRAIN [1][2460/3880]	Time 0.435 (0.322)	Data 1.25e-04 (3.08e-04)	Tok/s 53642 (43602)	Loss/tok 3.5410 (3.3838)	LR 2.000e-03
0: TRAIN [1][2470/3880]	Time 0.266 (0.322)	Data 1.26e-04 (3.08e-04)	Tok/s 39050 (43594)	Loss/tok 3.2453 (3.3836)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][2480/3880]	Time 0.347 (0.322)	Data 1.37e-04 (3.08e-04)	Tok/s 48402 (43591)	Loss/tok 3.2665 (3.3835)	LR 2.000e-03
0: TRAIN [1][2490/3880]	Time 0.347 (0.322)	Data 1.36e-04 (3.08e-04)	Tok/s 48791 (43597)	Loss/tok 3.3136 (3.3836)	LR 2.000e-03
0: TRAIN [1][2500/3880]	Time 0.266 (0.322)	Data 1.34e-04 (3.07e-04)	Tok/s 39141 (43596)	Loss/tok 3.1328 (3.3834)	LR 2.000e-03
0: TRAIN [1][2510/3880]	Time 0.346 (0.321)	Data 1.29e-04 (3.06e-04)	Tok/s 48160 (43578)	Loss/tok 3.3236 (3.3829)	LR 2.000e-03
0: TRAIN [1][2520/3880]	Time 0.346 (0.321)	Data 1.79e-04 (3.06e-04)	Tok/s 48425 (43571)	Loss/tok 3.4088 (3.3826)	LR 2.000e-03
0: TRAIN [1][2530/3880]	Time 0.266 (0.321)	Data 1.44e-04 (3.05e-04)	Tok/s 38988 (43555)	Loss/tok 3.1259 (3.3819)	LR 2.000e-03
0: TRAIN [1][2540/3880]	Time 0.193 (0.321)	Data 1.38e-04 (3.04e-04)	Tok/s 26876 (43568)	Loss/tok 2.6728 (3.3820)	LR 2.000e-03
0: TRAIN [1][2550/3880]	Time 0.347 (0.321)	Data 1.70e-04 (3.04e-04)	Tok/s 47827 (43566)	Loss/tok 3.3558 (3.3817)	LR 2.000e-03
0: TRAIN [1][2560/3880]	Time 0.346 (0.321)	Data 1.45e-04 (3.03e-04)	Tok/s 47918 (43561)	Loss/tok 3.3893 (3.3813)	LR 2.000e-03
0: TRAIN [1][2570/3880]	Time 0.279 (0.321)	Data 1.32e-04 (3.03e-04)	Tok/s 37527 (43554)	Loss/tok 3.2380 (3.3809)	LR 2.000e-03
0: TRAIN [1][2580/3880]	Time 0.265 (0.321)	Data 1.69e-04 (3.02e-04)	Tok/s 38841 (43548)	Loss/tok 3.1110 (3.3809)	LR 2.000e-03
0: TRAIN [1][2590/3880]	Time 0.359 (0.321)	Data 1.43e-04 (3.01e-04)	Tok/s 47286 (43537)	Loss/tok 3.2374 (3.3802)	LR 2.000e-03
0: TRAIN [1][2600/3880]	Time 0.265 (0.321)	Data 1.31e-04 (3.01e-04)	Tok/s 38737 (43533)	Loss/tok 3.1366 (3.3800)	LR 2.000e-03
0: TRAIN [1][2610/3880]	Time 0.428 (0.321)	Data 1.94e-04 (3.00e-04)	Tok/s 54291 (43550)	Loss/tok 3.4811 (3.3802)	LR 1.000e-03
0: TRAIN [1][2620/3880]	Time 0.191 (0.321)	Data 1.41e-04 (3.00e-04)	Tok/s 28509 (43545)	Loss/tok 2.7056 (3.3799)	LR 1.000e-03
0: TRAIN [1][2630/3880]	Time 0.347 (0.321)	Data 3.11e-04 (2.99e-04)	Tok/s 48290 (43541)	Loss/tok 3.3511 (3.3794)	LR 1.000e-03
0: TRAIN [1][2640/3880]	Time 0.266 (0.321)	Data 1.57e-04 (2.99e-04)	Tok/s 39120 (43525)	Loss/tok 3.0693 (3.3788)	LR 1.000e-03
0: TRAIN [1][2650/3880]	Time 0.266 (0.321)	Data 1.98e-04 (2.98e-04)	Tok/s 38808 (43531)	Loss/tok 3.1425 (3.3787)	LR 1.000e-03
0: TRAIN [1][2660/3880]	Time 0.430 (0.321)	Data 1.51e-04 (2.98e-04)	Tok/s 54527 (43524)	Loss/tok 3.4410 (3.3781)	LR 1.000e-03
0: TRAIN [1][2670/3880]	Time 0.269 (0.321)	Data 1.70e-04 (2.97e-04)	Tok/s 37830 (43515)	Loss/tok 3.0228 (3.3776)	LR 1.000e-03
0: TRAIN [1][2680/3880]	Time 0.266 (0.321)	Data 1.38e-04 (2.97e-04)	Tok/s 38316 (43509)	Loss/tok 3.1966 (3.3774)	LR 1.000e-03
0: TRAIN [1][2690/3880]	Time 0.193 (0.321)	Data 1.32e-04 (2.96e-04)	Tok/s 27589 (43504)	Loss/tok 2.7373 (3.3769)	LR 1.000e-03
0: TRAIN [1][2700/3880]	Time 0.266 (0.321)	Data 1.45e-04 (2.96e-04)	Tok/s 38906 (43510)	Loss/tok 3.1301 (3.3767)	LR 1.000e-03
0: TRAIN [1][2710/3880]	Time 0.193 (0.320)	Data 1.41e-04 (2.95e-04)	Tok/s 27958 (43498)	Loss/tok 2.7030 (3.3764)	LR 1.000e-03
0: TRAIN [1][2720/3880]	Time 0.347 (0.320)	Data 1.54e-04 (2.94e-04)	Tok/s 48725 (43494)	Loss/tok 3.3756 (3.3762)	LR 1.000e-03
0: TRAIN [1][2730/3880]	Time 0.430 (0.320)	Data 1.75e-04 (2.94e-04)	Tok/s 54127 (43497)	Loss/tok 3.4311 (3.3758)	LR 1.000e-03
0: TRAIN [1][2740/3880]	Time 0.431 (0.321)	Data 1.42e-04 (2.93e-04)	Tok/s 53722 (43503)	Loss/tok 3.4903 (3.3756)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2750/3880]	Time 0.543 (0.320)	Data 1.48e-04 (2.93e-04)	Tok/s 54523 (43490)	Loss/tok 3.5716 (3.3751)	LR 1.000e-03
0: TRAIN [1][2760/3880]	Time 0.430 (0.320)	Data 1.29e-04 (2.92e-04)	Tok/s 54116 (43495)	Loss/tok 3.4983 (3.3749)	LR 1.000e-03
0: TRAIN [1][2770/3880]	Time 0.347 (0.320)	Data 1.35e-04 (2.92e-04)	Tok/s 49085 (43495)	Loss/tok 3.2708 (3.3746)	LR 1.000e-03
0: TRAIN [1][2780/3880]	Time 0.266 (0.320)	Data 1.29e-04 (2.91e-04)	Tok/s 38646 (43493)	Loss/tok 3.0325 (3.3742)	LR 1.000e-03
0: TRAIN [1][2790/3880]	Time 0.266 (0.320)	Data 1.43e-04 (2.91e-04)	Tok/s 39708 (43481)	Loss/tok 3.0768 (3.3737)	LR 1.000e-03
0: TRAIN [1][2800/3880]	Time 0.542 (0.320)	Data 1.78e-04 (2.90e-04)	Tok/s 55020 (43490)	Loss/tok 3.5677 (3.3735)	LR 1.000e-03
0: TRAIN [1][2810/3880]	Time 0.430 (0.320)	Data 1.29e-04 (2.90e-04)	Tok/s 53926 (43484)	Loss/tok 3.4594 (3.3732)	LR 1.000e-03
0: TRAIN [1][2820/3880]	Time 0.265 (0.320)	Data 1.34e-04 (2.89e-04)	Tok/s 38967 (43492)	Loss/tok 3.1561 (3.3731)	LR 1.000e-03
0: TRAIN [1][2830/3880]	Time 0.543 (0.320)	Data 1.34e-04 (2.89e-04)	Tok/s 55185 (43496)	Loss/tok 3.5795 (3.3730)	LR 1.000e-03
0: TRAIN [1][2840/3880]	Time 0.265 (0.320)	Data 2.53e-04 (2.88e-04)	Tok/s 39502 (43493)	Loss/tok 3.1195 (3.3730)	LR 1.000e-03
0: TRAIN [1][2850/3880]	Time 0.193 (0.320)	Data 1.84e-04 (2.88e-04)	Tok/s 26923 (43487)	Loss/tok 2.7183 (3.3729)	LR 1.000e-03
0: TRAIN [1][2860/3880]	Time 0.352 (0.320)	Data 1.40e-04 (2.87e-04)	Tok/s 47693 (43487)	Loss/tok 3.3398 (3.3723)	LR 1.000e-03
0: TRAIN [1][2870/3880]	Time 0.346 (0.320)	Data 1.63e-04 (2.87e-04)	Tok/s 48623 (43477)	Loss/tok 3.2725 (3.3718)	LR 1.000e-03
0: TRAIN [1][2880/3880]	Time 0.194 (0.320)	Data 1.33e-04 (2.86e-04)	Tok/s 27208 (43475)	Loss/tok 2.6877 (3.3719)	LR 1.000e-03
0: TRAIN [1][2890/3880]	Time 0.430 (0.320)	Data 1.73e-04 (2.86e-04)	Tok/s 54097 (43483)	Loss/tok 3.5679 (3.3717)	LR 1.000e-03
0: TRAIN [1][2900/3880]	Time 0.347 (0.320)	Data 1.51e-04 (2.85e-04)	Tok/s 48803 (43487)	Loss/tok 3.2109 (3.3716)	LR 1.000e-03
0: TRAIN [1][2910/3880]	Time 0.430 (0.321)	Data 1.37e-04 (2.85e-04)	Tok/s 55076 (43502)	Loss/tok 3.4058 (3.3719)	LR 1.000e-03
0: TRAIN [1][2920/3880]	Time 0.193 (0.321)	Data 1.28e-04 (2.84e-04)	Tok/s 27917 (43497)	Loss/tok 2.7562 (3.3714)	LR 1.000e-03
0: TRAIN [1][2930/3880]	Time 0.265 (0.320)	Data 1.30e-04 (2.84e-04)	Tok/s 37923 (43490)	Loss/tok 3.1120 (3.3708)	LR 1.000e-03
0: TRAIN [1][2940/3880]	Time 0.348 (0.321)	Data 1.42e-04 (2.84e-04)	Tok/s 48847 (43501)	Loss/tok 3.3368 (3.3707)	LR 1.000e-03
0: TRAIN [1][2950/3880]	Time 0.265 (0.320)	Data 1.36e-04 (2.83e-04)	Tok/s 39358 (43498)	Loss/tok 2.9896 (3.3701)	LR 1.000e-03
0: TRAIN [1][2960/3880]	Time 0.265 (0.320)	Data 1.32e-04 (2.83e-04)	Tok/s 38612 (43495)	Loss/tok 3.1807 (3.3698)	LR 1.000e-03
0: TRAIN [1][2970/3880]	Time 0.269 (0.320)	Data 1.44e-04 (2.82e-04)	Tok/s 38487 (43483)	Loss/tok 3.1377 (3.3694)	LR 1.000e-03
0: TRAIN [1][2980/3880]	Time 0.192 (0.320)	Data 1.77e-04 (2.82e-04)	Tok/s 26462 (43483)	Loss/tok 2.6985 (3.3694)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2990/3880]	Time 0.265 (0.321)	Data 1.80e-04 (2.81e-04)	Tok/s 38572 (43491)	Loss/tok 3.1110 (3.3695)	LR 1.000e-03
0: TRAIN [1][3000/3880]	Time 0.193 (0.320)	Data 1.36e-04 (2.81e-04)	Tok/s 26473 (43486)	Loss/tok 2.7493 (3.3691)	LR 1.000e-03
0: TRAIN [1][3010/3880]	Time 0.430 (0.320)	Data 1.43e-04 (2.80e-04)	Tok/s 54615 (43481)	Loss/tok 3.4892 (3.3687)	LR 1.000e-03
0: TRAIN [1][3020/3880]	Time 0.266 (0.320)	Data 1.34e-04 (2.80e-04)	Tok/s 38220 (43471)	Loss/tok 2.8919 (3.3681)	LR 1.000e-03
0: TRAIN [1][3030/3880]	Time 0.265 (0.320)	Data 1.58e-04 (2.80e-04)	Tok/s 38814 (43470)	Loss/tok 3.1128 (3.3678)	LR 1.000e-03
0: TRAIN [1][3040/3880]	Time 0.266 (0.320)	Data 1.60e-04 (2.79e-04)	Tok/s 39727 (43463)	Loss/tok 3.0979 (3.3675)	LR 1.000e-03
0: TRAIN [1][3050/3880]	Time 0.348 (0.320)	Data 1.64e-04 (2.79e-04)	Tok/s 48506 (43467)	Loss/tok 3.3340 (3.3673)	LR 1.000e-03
0: TRAIN [1][3060/3880]	Time 0.347 (0.320)	Data 1.36e-04 (2.78e-04)	Tok/s 49057 (43470)	Loss/tok 3.1649 (3.3669)	LR 1.000e-03
0: TRAIN [1][3070/3880]	Time 0.544 (0.320)	Data 1.28e-04 (2.78e-04)	Tok/s 54813 (43478)	Loss/tok 3.6579 (3.3671)	LR 1.000e-03
0: TRAIN [1][3080/3880]	Time 0.430 (0.320)	Data 1.06e-03 (2.78e-04)	Tok/s 54493 (43480)	Loss/tok 3.4099 (3.3667)	LR 1.000e-03
0: TRAIN [1][3090/3880]	Time 0.346 (0.321)	Data 1.45e-04 (2.77e-04)	Tok/s 47645 (43496)	Loss/tok 3.2724 (3.3670)	LR 1.000e-03
0: TRAIN [1][3100/3880]	Time 0.266 (0.320)	Data 1.60e-04 (2.77e-04)	Tok/s 38851 (43489)	Loss/tok 3.0429 (3.3665)	LR 1.000e-03
0: TRAIN [1][3110/3880]	Time 0.431 (0.320)	Data 1.67e-04 (2.76e-04)	Tok/s 54795 (43488)	Loss/tok 3.3875 (3.3660)	LR 1.000e-03
0: TRAIN [1][3120/3880]	Time 0.347 (0.321)	Data 1.80e-04 (2.76e-04)	Tok/s 48434 (43499)	Loss/tok 3.3045 (3.3661)	LR 1.000e-03
0: TRAIN [1][3130/3880]	Time 0.266 (0.321)	Data 1.29e-04 (2.76e-04)	Tok/s 39114 (43500)	Loss/tok 3.0494 (3.3661)	LR 1.000e-03
0: TRAIN [1][3140/3880]	Time 0.348 (0.321)	Data 2.64e-04 (2.75e-04)	Tok/s 48881 (43496)	Loss/tok 3.1840 (3.3657)	LR 1.000e-03
0: TRAIN [1][3150/3880]	Time 0.430 (0.320)	Data 1.63e-04 (2.75e-04)	Tok/s 55023 (43485)	Loss/tok 3.3662 (3.3651)	LR 1.000e-03
0: TRAIN [1][3160/3880]	Time 0.346 (0.320)	Data 1.77e-04 (2.74e-04)	Tok/s 48294 (43483)	Loss/tok 3.3116 (3.3646)	LR 1.000e-03
0: TRAIN [1][3170/3880]	Time 0.272 (0.320)	Data 1.44e-04 (2.74e-04)	Tok/s 37630 (43463)	Loss/tok 3.1534 (3.3640)	LR 1.000e-03
0: TRAIN [1][3180/3880]	Time 0.430 (0.320)	Data 1.36e-04 (2.74e-04)	Tok/s 53787 (43465)	Loss/tok 3.5905 (3.3639)	LR 1.000e-03
0: TRAIN [1][3190/3880]	Time 0.266 (0.320)	Data 1.45e-04 (2.73e-04)	Tok/s 38722 (43473)	Loss/tok 3.1810 (3.3637)	LR 1.000e-03
0: TRAIN [1][3200/3880]	Time 0.268 (0.320)	Data 1.71e-04 (2.73e-04)	Tok/s 38244 (43472)	Loss/tok 3.0698 (3.3635)	LR 1.000e-03
0: TRAIN [1][3210/3880]	Time 0.266 (0.320)	Data 1.44e-04 (2.73e-04)	Tok/s 37857 (43454)	Loss/tok 3.1249 (3.3629)	LR 1.000e-03
0: TRAIN [1][3220/3880]	Time 0.347 (0.320)	Data 1.70e-04 (2.72e-04)	Tok/s 47663 (43463)	Loss/tok 3.3203 (3.3628)	LR 1.000e-03
0: TRAIN [1][3230/3880]	Time 0.347 (0.320)	Data 1.40e-04 (2.72e-04)	Tok/s 48604 (43461)	Loss/tok 3.3663 (3.3625)	LR 1.000e-03
0: TRAIN [1][3240/3880]	Time 0.265 (0.320)	Data 1.34e-04 (2.71e-04)	Tok/s 39447 (43465)	Loss/tok 3.2113 (3.3623)	LR 1.000e-03
0: TRAIN [1][3250/3880]	Time 0.265 (0.320)	Data 1.27e-04 (2.71e-04)	Tok/s 38886 (43472)	Loss/tok 3.1494 (3.3624)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][3260/3880]	Time 0.270 (0.320)	Data 1.29e-04 (2.71e-04)	Tok/s 38520 (43469)	Loss/tok 3.1235 (3.3622)	LR 1.000e-03
0: TRAIN [1][3270/3880]	Time 0.192 (0.320)	Data 1.28e-04 (2.70e-04)	Tok/s 27300 (43473)	Loss/tok 2.5653 (3.3619)	LR 1.000e-03
0: TRAIN [1][3280/3880]	Time 0.347 (0.320)	Data 1.46e-04 (2.70e-04)	Tok/s 49059 (43469)	Loss/tok 3.2588 (3.3615)	LR 1.000e-03
0: TRAIN [1][3290/3880]	Time 0.352 (0.320)	Data 1.37e-04 (2.69e-04)	Tok/s 48418 (43461)	Loss/tok 3.2633 (3.3611)	LR 1.000e-03
0: TRAIN [1][3300/3880]	Time 0.265 (0.320)	Data 2.00e-04 (2.69e-04)	Tok/s 39213 (43480)	Loss/tok 2.9593 (3.3614)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3310/3880]	Time 0.254 (0.320)	Data 1.35e-04 (2.69e-04)	Tok/s 40547 (43498)	Loss/tok 2.9530 (3.3614)	LR 1.000e-03
0: TRAIN [1][3320/3880]	Time 0.353 (0.320)	Data 1.38e-04 (2.68e-04)	Tok/s 46688 (43492)	Loss/tok 3.3318 (3.3613)	LR 1.000e-03
0: TRAIN [1][3330/3880]	Time 0.265 (0.320)	Data 1.66e-04 (2.68e-04)	Tok/s 38488 (43493)	Loss/tok 3.1750 (3.3611)	LR 1.000e-03
0: TRAIN [1][3340/3880]	Time 0.193 (0.320)	Data 1.28e-04 (2.68e-04)	Tok/s 26492 (43483)	Loss/tok 2.6122 (3.3606)	LR 1.000e-03
0: TRAIN [1][3350/3880]	Time 0.432 (0.320)	Data 1.35e-04 (2.67e-04)	Tok/s 53759 (43497)	Loss/tok 3.5226 (3.3606)	LR 1.000e-03
0: TRAIN [1][3360/3880]	Time 0.267 (0.321)	Data 5.01e-04 (2.67e-04)	Tok/s 37659 (43504)	Loss/tok 3.0289 (3.3606)	LR 1.000e-03
0: TRAIN [1][3370/3880]	Time 0.265 (0.320)	Data 1.44e-04 (2.67e-04)	Tok/s 39018 (43495)	Loss/tok 3.2210 (3.3602)	LR 1.000e-03
0: TRAIN [1][3380/3880]	Time 0.271 (0.320)	Data 1.75e-04 (2.66e-04)	Tok/s 37623 (43494)	Loss/tok 3.1821 (3.3597)	LR 1.000e-03
0: TRAIN [1][3390/3880]	Time 0.346 (0.321)	Data 1.21e-04 (2.66e-04)	Tok/s 49066 (43499)	Loss/tok 3.2081 (3.3596)	LR 1.000e-03
0: TRAIN [1][3400/3880]	Time 0.268 (0.320)	Data 1.24e-04 (2.66e-04)	Tok/s 38540 (43494)	Loss/tok 3.0047 (3.3592)	LR 1.000e-03
0: TRAIN [1][3410/3880]	Time 0.266 (0.320)	Data 1.34e-04 (2.65e-04)	Tok/s 38649 (43495)	Loss/tok 3.1238 (3.3589)	LR 1.000e-03
0: TRAIN [1][3420/3880]	Time 0.430 (0.320)	Data 1.29e-04 (2.65e-04)	Tok/s 54258 (43498)	Loss/tok 3.3965 (3.3586)	LR 5.000e-04
0: TRAIN [1][3430/3880]	Time 0.266 (0.320)	Data 1.49e-04 (2.64e-04)	Tok/s 38861 (43497)	Loss/tok 2.9918 (3.3583)	LR 5.000e-04
0: TRAIN [1][3440/3880]	Time 0.266 (0.320)	Data 1.55e-04 (2.64e-04)	Tok/s 39227 (43504)	Loss/tok 3.1431 (3.3581)	LR 5.000e-04
0: TRAIN [1][3450/3880]	Time 0.543 (0.321)	Data 1.34e-04 (2.64e-04)	Tok/s 54698 (43511)	Loss/tok 3.7025 (3.3582)	LR 5.000e-04
0: TRAIN [1][3460/3880]	Time 0.266 (0.321)	Data 1.37e-04 (2.63e-04)	Tok/s 39433 (43513)	Loss/tok 2.9482 (3.3580)	LR 5.000e-04
0: TRAIN [1][3470/3880]	Time 0.430 (0.321)	Data 1.62e-04 (2.63e-04)	Tok/s 53804 (43518)	Loss/tok 3.4616 (3.3577)	LR 5.000e-04
0: TRAIN [1][3480/3880]	Time 0.265 (0.321)	Data 1.66e-04 (2.63e-04)	Tok/s 39221 (43520)	Loss/tok 3.0017 (3.3575)	LR 5.000e-04
0: TRAIN [1][3490/3880]	Time 0.192 (0.321)	Data 1.34e-04 (2.62e-04)	Tok/s 27075 (43520)	Loss/tok 2.5283 (3.3571)	LR 5.000e-04
0: TRAIN [1][3500/3880]	Time 0.281 (0.321)	Data 1.70e-04 (2.62e-04)	Tok/s 36665 (43514)	Loss/tok 2.9965 (3.3565)	LR 5.000e-04
0: TRAIN [1][3510/3880]	Time 0.267 (0.321)	Data 1.60e-04 (2.62e-04)	Tok/s 38156 (43521)	Loss/tok 3.0580 (3.3566)	LR 5.000e-04
0: TRAIN [1][3520/3880]	Time 0.268 (0.321)	Data 1.30e-04 (2.61e-04)	Tok/s 39005 (43520)	Loss/tok 3.0165 (3.3563)	LR 5.000e-04
0: TRAIN [1][3530/3880]	Time 0.431 (0.321)	Data 1.30e-04 (2.61e-04)	Tok/s 54206 (43520)	Loss/tok 3.3923 (3.3561)	LR 5.000e-04
0: TRAIN [1][3540/3880]	Time 0.347 (0.321)	Data 1.45e-04 (2.61e-04)	Tok/s 48676 (43521)	Loss/tok 3.3135 (3.3559)	LR 5.000e-04
0: TRAIN [1][3550/3880]	Time 0.192 (0.321)	Data 1.73e-04 (2.61e-04)	Tok/s 27262 (43517)	Loss/tok 2.6335 (3.3557)	LR 5.000e-04
0: TRAIN [1][3560/3880]	Time 0.346 (0.321)	Data 1.37e-04 (2.60e-04)	Tok/s 48564 (43515)	Loss/tok 3.3377 (3.3553)	LR 5.000e-04
0: TRAIN [1][3570/3880]	Time 0.196 (0.321)	Data 1.34e-04 (2.60e-04)	Tok/s 27184 (43525)	Loss/tok 2.6235 (3.3551)	LR 5.000e-04
0: TRAIN [1][3580/3880]	Time 0.430 (0.321)	Data 1.35e-04 (2.60e-04)	Tok/s 53963 (43518)	Loss/tok 3.3970 (3.3548)	LR 5.000e-04
0: TRAIN [1][3590/3880]	Time 0.346 (0.321)	Data 1.22e-04 (2.60e-04)	Tok/s 47967 (43516)	Loss/tok 3.3270 (3.3546)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][3600/3880]	Time 0.545 (0.321)	Data 1.54e-04 (2.59e-04)	Tok/s 54335 (43521)	Loss/tok 3.6164 (3.3544)	LR 5.000e-04
0: TRAIN [1][3610/3880]	Time 0.346 (0.321)	Data 1.61e-04 (2.59e-04)	Tok/s 47876 (43514)	Loss/tok 3.2913 (3.3540)	LR 5.000e-04
0: TRAIN [1][3620/3880]	Time 0.266 (0.321)	Data 1.37e-04 (2.59e-04)	Tok/s 38155 (43512)	Loss/tok 2.9495 (3.3539)	LR 5.000e-04
0: TRAIN [1][3630/3880]	Time 0.191 (0.321)	Data 1.36e-04 (2.58e-04)	Tok/s 27047 (43510)	Loss/tok 2.5558 (3.3535)	LR 5.000e-04
0: TRAIN [1][3640/3880]	Time 0.349 (0.321)	Data 1.82e-04 (2.58e-04)	Tok/s 49080 (43508)	Loss/tok 3.3450 (3.3534)	LR 5.000e-04
0: TRAIN [1][3650/3880]	Time 0.191 (0.321)	Data 1.44e-04 (2.58e-04)	Tok/s 27522 (43510)	Loss/tok 2.7339 (3.3532)	LR 5.000e-04
0: TRAIN [1][3660/3880]	Time 0.267 (0.321)	Data 1.33e-04 (2.57e-04)	Tok/s 38889 (43509)	Loss/tok 3.0782 (3.3527)	LR 5.000e-04
0: TRAIN [1][3670/3880]	Time 0.265 (0.321)	Data 1.39e-04 (2.57e-04)	Tok/s 39068 (43502)	Loss/tok 3.0076 (3.3524)	LR 5.000e-04
0: TRAIN [1][3680/3880]	Time 0.266 (0.321)	Data 1.30e-04 (2.57e-04)	Tok/s 39130 (43507)	Loss/tok 2.9481 (3.3522)	LR 5.000e-04
0: TRAIN [1][3690/3880]	Time 0.431 (0.321)	Data 1.71e-04 (2.57e-04)	Tok/s 54489 (43516)	Loss/tok 3.3259 (3.3518)	LR 5.000e-04
0: TRAIN [1][3700/3880]	Time 0.266 (0.321)	Data 1.67e-04 (2.56e-04)	Tok/s 38354 (43520)	Loss/tok 2.9860 (3.3516)	LR 5.000e-04
0: TRAIN [1][3710/3880]	Time 0.431 (0.321)	Data 1.65e-04 (2.56e-04)	Tok/s 54565 (43521)	Loss/tok 3.4163 (3.3516)	LR 5.000e-04
0: TRAIN [1][3720/3880]	Time 0.266 (0.321)	Data 1.39e-04 (2.56e-04)	Tok/s 39038 (43525)	Loss/tok 3.1028 (3.3515)	LR 5.000e-04
0: TRAIN [1][3730/3880]	Time 0.347 (0.321)	Data 1.48e-04 (2.55e-04)	Tok/s 49201 (43536)	Loss/tok 3.1451 (3.3515)	LR 5.000e-04
0: TRAIN [1][3740/3880]	Time 0.348 (0.321)	Data 1.33e-04 (2.55e-04)	Tok/s 48746 (43532)	Loss/tok 3.2425 (3.3511)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][3750/3880]	Time 0.551 (0.321)	Data 1.29e-04 (2.55e-04)	Tok/s 54683 (43535)	Loss/tok 3.5987 (3.3510)	LR 5.000e-04
0: TRAIN [1][3760/3880]	Time 0.431 (0.321)	Data 1.48e-04 (2.55e-04)	Tok/s 54168 (43555)	Loss/tok 3.4337 (3.3510)	LR 5.000e-04
0: TRAIN [1][3770/3880]	Time 0.266 (0.321)	Data 1.50e-04 (2.54e-04)	Tok/s 38711 (43551)	Loss/tok 3.0654 (3.3507)	LR 5.000e-04
0: TRAIN [1][3780/3880]	Time 0.347 (0.321)	Data 1.35e-04 (2.54e-04)	Tok/s 49140 (43550)	Loss/tok 3.1774 (3.3503)	LR 5.000e-04
0: TRAIN [1][3790/3880]	Time 0.431 (0.321)	Data 1.52e-04 (2.54e-04)	Tok/s 54377 (43555)	Loss/tok 3.4539 (3.3500)	LR 5.000e-04
0: TRAIN [1][3800/3880]	Time 0.355 (0.321)	Data 1.86e-04 (2.54e-04)	Tok/s 47341 (43560)	Loss/tok 3.2991 (3.3498)	LR 5.000e-04
0: TRAIN [1][3810/3880]	Time 0.348 (0.321)	Data 1.32e-04 (2.53e-04)	Tok/s 48096 (43561)	Loss/tok 3.1983 (3.3498)	LR 5.000e-04
0: TRAIN [1][3820/3880]	Time 0.267 (0.321)	Data 1.27e-04 (2.53e-04)	Tok/s 39252 (43564)	Loss/tok 2.9709 (3.3496)	LR 5.000e-04
0: TRAIN [1][3830/3880]	Time 0.543 (0.321)	Data 1.39e-04 (2.53e-04)	Tok/s 54775 (43572)	Loss/tok 3.6050 (3.3496)	LR 5.000e-04
0: TRAIN [1][3840/3880]	Time 0.265 (0.321)	Data 1.47e-04 (2.53e-04)	Tok/s 38486 (43576)	Loss/tok 3.1326 (3.3494)	LR 5.000e-04
0: TRAIN [1][3850/3880]	Time 0.266 (0.321)	Data 1.29e-04 (2.52e-04)	Tok/s 38642 (43576)	Loss/tok 3.0907 (3.3490)	LR 5.000e-04
0: TRAIN [1][3860/3880]	Time 0.266 (0.321)	Data 1.30e-04 (2.52e-04)	Tok/s 38090 (43568)	Loss/tok 3.1273 (3.3486)	LR 5.000e-04
0: TRAIN [1][3870/3880]	Time 0.348 (0.321)	Data 1.82e-04 (2.52e-04)	Tok/s 48373 (43561)	Loss/tok 3.3080 (3.3482)	LR 5.000e-04
:::MLL 1576198992.136 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1576198992.137 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/6]	Time 0.858 (0.858)	Decoder iters 109.0 (109.0)	Tok/s 19291 (19291)
0: Running moses detokenizer
0: BLEU(score=23.014055064696173, counts=[36703, 18045, 10102, 5901], totals=[65845, 62842, 59839, 56841], precisions=[55.74151416204723, 28.71487221921645, 16.881966610404586, 10.381590753153533], bp=1.0, sys_len=65845, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1576198996.361 eval_accuracy: {"value": 23.01, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1576198996.361 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.3474	Test BLEU: 23.01
0: Performance: Epoch: 1	Training: 174228 Tok/s
0: Finished epoch 1
:::MLL 1576198996.362 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1576198996.362 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1576198996.363 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 1236925988
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][0/3880]	Time 0.718 (0.718)	Data 4.55e-01 (4.55e-01)	Tok/s 14296 (14296)	Loss/tok 2.9968 (2.9968)	LR 5.000e-04
0: TRAIN [2][10/3880]	Time 0.347 (0.370)	Data 1.90e-04 (4.15e-02)	Tok/s 48374 (42279)	Loss/tok 3.0686 (3.1439)	LR 5.000e-04
0: TRAIN [2][20/3880]	Time 0.266 (0.350)	Data 1.35e-04 (2.18e-02)	Tok/s 39357 (43268)	Loss/tok 2.9136 (3.1495)	LR 5.000e-04
0: TRAIN [2][30/3880]	Time 0.265 (0.353)	Data 1.63e-04 (1.48e-02)	Tok/s 39395 (44522)	Loss/tok 2.9697 (3.1866)	LR 5.000e-04
0: TRAIN [2][40/3880]	Time 0.266 (0.345)	Data 1.46e-04 (1.13e-02)	Tok/s 38877 (44032)	Loss/tok 2.9248 (3.1826)	LR 5.000e-04
0: TRAIN [2][50/3880]	Time 0.265 (0.336)	Data 2.62e-04 (9.08e-03)	Tok/s 38593 (43600)	Loss/tok 2.9487 (3.1670)	LR 5.000e-04
0: TRAIN [2][60/3880]	Time 0.266 (0.332)	Data 1.77e-04 (7.61e-03)	Tok/s 39697 (43576)	Loss/tok 2.9394 (3.1515)	LR 5.000e-04
0: TRAIN [2][70/3880]	Time 0.431 (0.328)	Data 1.49e-04 (6.56e-03)	Tok/s 54028 (43528)	Loss/tok 3.4227 (3.1429)	LR 5.000e-04
0: TRAIN [2][80/3880]	Time 0.347 (0.331)	Data 1.30e-04 (5.77e-03)	Tok/s 48134 (43921)	Loss/tok 3.1402 (3.1485)	LR 5.000e-04
0: TRAIN [2][90/3880]	Time 0.270 (0.330)	Data 1.55e-04 (5.15e-03)	Tok/s 38029 (43872)	Loss/tok 2.9293 (3.1459)	LR 5.000e-04
0: TRAIN [2][100/3880]	Time 0.266 (0.333)	Data 1.33e-04 (4.65e-03)	Tok/s 38309 (44162)	Loss/tok 3.0289 (3.1558)	LR 5.000e-04
0: TRAIN [2][110/3880]	Time 0.265 (0.331)	Data 1.66e-04 (4.25e-03)	Tok/s 38769 (43999)	Loss/tok 2.8759 (3.1509)	LR 5.000e-04
0: TRAIN [2][120/3880]	Time 0.266 (0.331)	Data 1.36e-04 (3.91e-03)	Tok/s 37595 (44007)	Loss/tok 3.0094 (3.1592)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][130/3880]	Time 0.335 (0.330)	Data 2.04e-04 (3.62e-03)	Tok/s 50272 (43862)	Loss/tok 3.1869 (3.1646)	LR 5.000e-04
0: TRAIN [2][140/3880]	Time 0.431 (0.328)	Data 1.59e-04 (3.38e-03)	Tok/s 54342 (43743)	Loss/tok 3.3189 (3.1595)	LR 5.000e-04
0: TRAIN [2][150/3880]	Time 0.347 (0.327)	Data 1.24e-04 (3.16e-03)	Tok/s 48238 (43609)	Loss/tok 3.1910 (3.1604)	LR 5.000e-04
0: TRAIN [2][160/3880]	Time 0.348 (0.325)	Data 1.25e-04 (2.98e-03)	Tok/s 48577 (43524)	Loss/tok 3.2102 (3.1574)	LR 5.000e-04
0: TRAIN [2][170/3880]	Time 0.267 (0.326)	Data 1.78e-04 (2.81e-03)	Tok/s 39178 (43599)	Loss/tok 2.9237 (3.1615)	LR 5.000e-04
0: TRAIN [2][180/3880]	Time 0.265 (0.326)	Data 4.17e-04 (2.67e-03)	Tok/s 37984 (43668)	Loss/tok 2.9826 (3.1609)	LR 5.000e-04
0: TRAIN [2][190/3880]	Time 0.265 (0.327)	Data 2.84e-04 (2.53e-03)	Tok/s 38687 (43868)	Loss/tok 3.1339 (3.1641)	LR 5.000e-04
0: TRAIN [2][200/3880]	Time 0.267 (0.327)	Data 1.33e-04 (2.42e-03)	Tok/s 37185 (43891)	Loss/tok 2.9019 (3.1630)	LR 5.000e-04
0: TRAIN [2][210/3880]	Time 0.431 (0.328)	Data 1.32e-04 (2.31e-03)	Tok/s 53985 (44008)	Loss/tok 3.4343 (3.1674)	LR 5.000e-04
0: TRAIN [2][220/3880]	Time 0.545 (0.327)	Data 1.41e-04 (2.21e-03)	Tok/s 54630 (43884)	Loss/tok 3.6153 (3.1649)	LR 5.000e-04
0: TRAIN [2][230/3880]	Time 0.346 (0.327)	Data 1.60e-04 (2.12e-03)	Tok/s 48620 (43889)	Loss/tok 3.1590 (3.1639)	LR 5.000e-04
0: TRAIN [2][240/3880]	Time 0.347 (0.325)	Data 1.24e-04 (2.04e-03)	Tok/s 48200 (43756)	Loss/tok 3.2001 (3.1603)	LR 5.000e-04
0: TRAIN [2][250/3880]	Time 0.347 (0.325)	Data 1.27e-04 (1.96e-03)	Tok/s 48155 (43751)	Loss/tok 3.1797 (3.1622)	LR 5.000e-04
0: TRAIN [2][260/3880]	Time 0.268 (0.324)	Data 1.28e-04 (1.89e-03)	Tok/s 38679 (43645)	Loss/tok 2.9730 (3.1583)	LR 5.000e-04
0: TRAIN [2][270/3880]	Time 0.265 (0.324)	Data 1.37e-04 (1.83e-03)	Tok/s 39016 (43650)	Loss/tok 2.9032 (3.1568)	LR 5.000e-04
0: TRAIN [2][280/3880]	Time 0.348 (0.324)	Data 1.34e-04 (1.77e-03)	Tok/s 48035 (43725)	Loss/tok 3.1464 (3.1551)	LR 5.000e-04
0: TRAIN [2][290/3880]	Time 0.266 (0.323)	Data 1.25e-04 (1.72e-03)	Tok/s 39891 (43651)	Loss/tok 2.9941 (3.1538)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][300/3880]	Time 0.255 (0.324)	Data 1.51e-04 (1.66e-03)	Tok/s 39401 (43718)	Loss/tok 2.9141 (3.1575)	LR 5.000e-04
0: TRAIN [2][310/3880]	Time 0.265 (0.324)	Data 1.49e-04 (1.62e-03)	Tok/s 38815 (43693)	Loss/tok 2.9364 (3.1588)	LR 5.000e-04
0: TRAIN [2][320/3880]	Time 0.266 (0.324)	Data 1.37e-04 (1.57e-03)	Tok/s 38756 (43736)	Loss/tok 2.9422 (3.1593)	LR 5.000e-04
0: TRAIN [2][330/3880]	Time 0.266 (0.324)	Data 1.30e-04 (1.53e-03)	Tok/s 39247 (43723)	Loss/tok 3.0566 (3.1593)	LR 5.000e-04
0: TRAIN [2][340/3880]	Time 0.553 (0.325)	Data 1.41e-04 (1.49e-03)	Tok/s 53641 (43806)	Loss/tok 3.4199 (3.1610)	LR 5.000e-04
0: TRAIN [2][350/3880]	Time 0.348 (0.325)	Data 1.46e-04 (1.45e-03)	Tok/s 48179 (43863)	Loss/tok 3.2416 (3.1621)	LR 5.000e-04
0: TRAIN [2][360/3880]	Time 0.544 (0.325)	Data 1.27e-04 (1.41e-03)	Tok/s 55199 (43860)	Loss/tok 3.4597 (3.1617)	LR 2.500e-04
0: TRAIN [2][370/3880]	Time 0.192 (0.324)	Data 1.31e-04 (1.38e-03)	Tok/s 27567 (43710)	Loss/tok 2.5950 (3.1587)	LR 2.500e-04
0: TRAIN [2][380/3880]	Time 0.430 (0.325)	Data 1.42e-04 (1.35e-03)	Tok/s 53893 (43760)	Loss/tok 3.2696 (3.1620)	LR 2.500e-04
0: TRAIN [2][390/3880]	Time 0.265 (0.324)	Data 1.40e-04 (1.32e-03)	Tok/s 39452 (43766)	Loss/tok 2.9957 (3.1616)	LR 2.500e-04
0: TRAIN [2][400/3880]	Time 0.266 (0.324)	Data 1.62e-04 (1.29e-03)	Tok/s 39275 (43726)	Loss/tok 2.8821 (3.1604)	LR 2.500e-04
0: TRAIN [2][410/3880]	Time 0.267 (0.324)	Data 1.46e-04 (1.26e-03)	Tok/s 39374 (43741)	Loss/tok 2.9289 (3.1624)	LR 2.500e-04
0: TRAIN [2][420/3880]	Time 0.266 (0.325)	Data 1.44e-04 (1.23e-03)	Tok/s 38972 (43786)	Loss/tok 2.9156 (3.1633)	LR 2.500e-04
0: TRAIN [2][430/3880]	Time 0.266 (0.324)	Data 1.44e-04 (1.21e-03)	Tok/s 39017 (43695)	Loss/tok 2.9904 (3.1623)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][440/3880]	Time 0.430 (0.324)	Data 1.31e-04 (1.18e-03)	Tok/s 53714 (43726)	Loss/tok 3.3417 (3.1637)	LR 2.500e-04
0: TRAIN [2][450/3880]	Time 0.349 (0.325)	Data 1.44e-04 (1.16e-03)	Tok/s 47249 (43813)	Loss/tok 3.0354 (3.1645)	LR 2.500e-04
0: TRAIN [2][460/3880]	Time 0.191 (0.326)	Data 1.39e-04 (1.14e-03)	Tok/s 28279 (43915)	Loss/tok 2.6408 (3.1665)	LR 2.500e-04
0: TRAIN [2][470/3880]	Time 0.347 (0.326)	Data 1.82e-04 (1.12e-03)	Tok/s 48704 (43928)	Loss/tok 3.2334 (3.1654)	LR 2.500e-04
0: TRAIN [2][480/3880]	Time 0.347 (0.325)	Data 1.40e-04 (1.10e-03)	Tok/s 49451 (43861)	Loss/tok 3.1614 (3.1630)	LR 2.500e-04
0: TRAIN [2][490/3880]	Time 0.266 (0.325)	Data 1.45e-04 (1.08e-03)	Tok/s 38371 (43902)	Loss/tok 3.0166 (3.1626)	LR 2.500e-04
0: TRAIN [2][500/3880]	Time 0.346 (0.325)	Data 1.37e-04 (1.06e-03)	Tok/s 48575 (43884)	Loss/tok 3.0824 (3.1611)	LR 2.500e-04
0: TRAIN [2][510/3880]	Time 0.267 (0.324)	Data 1.75e-04 (1.04e-03)	Tok/s 39054 (43826)	Loss/tok 2.9372 (3.1596)	LR 2.500e-04
0: TRAIN [2][520/3880]	Time 0.347 (0.324)	Data 1.68e-04 (1.03e-03)	Tok/s 48821 (43781)	Loss/tok 3.1305 (3.1586)	LR 2.500e-04
0: TRAIN [2][530/3880]	Time 0.191 (0.323)	Data 1.31e-04 (1.01e-03)	Tok/s 27653 (43659)	Loss/tok 2.5548 (3.1560)	LR 2.500e-04
0: TRAIN [2][540/3880]	Time 0.192 (0.322)	Data 1.44e-04 (9.98e-04)	Tok/s 27691 (43613)	Loss/tok 2.6503 (3.1568)	LR 2.500e-04
0: TRAIN [2][550/3880]	Time 0.266 (0.322)	Data 1.56e-04 (9.83e-04)	Tok/s 37981 (43601)	Loss/tok 2.9160 (3.1565)	LR 2.500e-04
0: TRAIN [2][560/3880]	Time 0.270 (0.322)	Data 1.85e-04 (9.68e-04)	Tok/s 38662 (43605)	Loss/tok 2.9417 (3.1575)	LR 2.500e-04
0: TRAIN [2][570/3880]	Time 0.265 (0.322)	Data 1.33e-04 (9.54e-04)	Tok/s 38710 (43588)	Loss/tok 2.9206 (3.1571)	LR 2.500e-04
0: TRAIN [2][580/3880]	Time 0.431 (0.323)	Data 1.35e-04 (9.40e-04)	Tok/s 53741 (43638)	Loss/tok 3.3194 (3.1586)	LR 2.500e-04
0: TRAIN [2][590/3880]	Time 0.265 (0.322)	Data 1.30e-04 (9.27e-04)	Tok/s 38677 (43596)	Loss/tok 2.9109 (3.1575)	LR 2.500e-04
0: TRAIN [2][600/3880]	Time 0.346 (0.323)	Data 1.41e-04 (9.14e-04)	Tok/s 48973 (43639)	Loss/tok 3.0698 (3.1571)	LR 2.500e-04
0: TRAIN [2][610/3880]	Time 0.431 (0.323)	Data 1.44e-04 (9.01e-04)	Tok/s 54448 (43642)	Loss/tok 3.2670 (3.1567)	LR 2.500e-04
0: TRAIN [2][620/3880]	Time 0.267 (0.323)	Data 1.65e-04 (8.89e-04)	Tok/s 38848 (43674)	Loss/tok 2.9136 (3.1572)	LR 2.500e-04
0: TRAIN [2][630/3880]	Time 0.347 (0.323)	Data 1.42e-04 (8.78e-04)	Tok/s 47892 (43720)	Loss/tok 3.1536 (3.1575)	LR 2.500e-04
0: TRAIN [2][640/3880]	Time 0.430 (0.323)	Data 1.27e-04 (8.66e-04)	Tok/s 54015 (43685)	Loss/tok 3.3847 (3.1578)	LR 2.500e-04
0: TRAIN [2][650/3880]	Time 0.266 (0.323)	Data 1.41e-04 (8.55e-04)	Tok/s 38809 (43678)	Loss/tok 2.9514 (3.1575)	LR 2.500e-04
0: TRAIN [2][660/3880]	Time 0.353 (0.323)	Data 1.30e-04 (8.44e-04)	Tok/s 48061 (43683)	Loss/tok 3.0739 (3.1580)	LR 2.500e-04
0: TRAIN [2][670/3880]	Time 0.544 (0.324)	Data 1.44e-04 (8.34e-04)	Tok/s 54886 (43693)	Loss/tok 3.4364 (3.1585)	LR 2.500e-04
0: TRAIN [2][680/3880]	Time 0.429 (0.324)	Data 1.84e-04 (8.24e-04)	Tok/s 54791 (43739)	Loss/tok 3.2499 (3.1596)	LR 2.500e-04
0: TRAIN [2][690/3880]	Time 0.430 (0.324)	Data 1.30e-04 (8.14e-04)	Tok/s 53697 (43744)	Loss/tok 3.4029 (3.1601)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][700/3880]	Time 0.266 (0.324)	Data 1.50e-04 (8.04e-04)	Tok/s 38648 (43720)	Loss/tok 2.8844 (3.1587)	LR 2.500e-04
0: TRAIN [2][710/3880]	Time 0.192 (0.323)	Data 1.40e-04 (7.95e-04)	Tok/s 27393 (43702)	Loss/tok 2.5583 (3.1575)	LR 2.500e-04
0: TRAIN [2][720/3880]	Time 0.430 (0.324)	Data 1.34e-04 (7.86e-04)	Tok/s 53822 (43756)	Loss/tok 3.4201 (3.1583)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][730/3880]	Time 0.543 (0.323)	Data 1.67e-04 (7.78e-04)	Tok/s 55264 (43693)	Loss/tok 3.4477 (3.1575)	LR 2.500e-04
0: TRAIN [2][740/3880]	Time 0.347 (0.323)	Data 1.69e-04 (7.69e-04)	Tok/s 48805 (43700)	Loss/tok 3.2370 (3.1572)	LR 2.500e-04
0: TRAIN [2][750/3880]	Time 0.265 (0.323)	Data 1.70e-04 (7.61e-04)	Tok/s 38829 (43670)	Loss/tok 2.8899 (3.1567)	LR 2.500e-04
0: TRAIN [2][760/3880]	Time 0.348 (0.323)	Data 1.72e-04 (7.53e-04)	Tok/s 48816 (43705)	Loss/tok 3.0070 (3.1568)	LR 2.500e-04
0: TRAIN [2][770/3880]	Time 0.431 (0.324)	Data 1.35e-04 (7.45e-04)	Tok/s 54590 (43777)	Loss/tok 3.3102 (3.1578)	LR 2.500e-04
0: TRAIN [2][780/3880]	Time 0.192 (0.324)	Data 1.65e-04 (7.38e-04)	Tok/s 27973 (43776)	Loss/tok 2.5021 (3.1586)	LR 2.500e-04
0: TRAIN [2][790/3880]	Time 0.194 (0.324)	Data 1.40e-04 (7.30e-04)	Tok/s 27634 (43800)	Loss/tok 2.4899 (3.1584)	LR 2.500e-04
0: TRAIN [2][800/3880]	Time 0.346 (0.324)	Data 1.73e-04 (7.23e-04)	Tok/s 48598 (43836)	Loss/tok 3.1328 (3.1584)	LR 2.500e-04
0: TRAIN [2][810/3880]	Time 0.347 (0.324)	Data 1.34e-04 (7.16e-04)	Tok/s 48698 (43840)	Loss/tok 3.1814 (3.1578)	LR 2.500e-04
0: TRAIN [2][820/3880]	Time 0.346 (0.324)	Data 2.19e-04 (7.09e-04)	Tok/s 49536 (43843)	Loss/tok 3.1215 (3.1577)	LR 2.500e-04
0: TRAIN [2][830/3880]	Time 0.430 (0.324)	Data 1.49e-04 (7.02e-04)	Tok/s 54399 (43817)	Loss/tok 3.2815 (3.1570)	LR 2.500e-04
0: TRAIN [2][840/3880]	Time 0.347 (0.324)	Data 1.32e-04 (6.96e-04)	Tok/s 48664 (43782)	Loss/tok 3.2079 (3.1564)	LR 2.500e-04
0: TRAIN [2][850/3880]	Time 0.266 (0.324)	Data 1.31e-04 (6.89e-04)	Tok/s 38533 (43818)	Loss/tok 2.9493 (3.1568)	LR 2.500e-04
0: TRAIN [2][860/3880]	Time 0.346 (0.324)	Data 1.32e-04 (6.83e-04)	Tok/s 48443 (43875)	Loss/tok 3.1167 (3.1569)	LR 2.500e-04
0: TRAIN [2][870/3880]	Time 0.266 (0.324)	Data 1.68e-04 (6.77e-04)	Tok/s 38755 (43844)	Loss/tok 2.9125 (3.1559)	LR 2.500e-04
0: TRAIN [2][880/3880]	Time 0.431 (0.324)	Data 1.31e-04 (6.71e-04)	Tok/s 54502 (43884)	Loss/tok 3.3494 (3.1565)	LR 2.500e-04
0: TRAIN [2][890/3880]	Time 0.265 (0.324)	Data 1.82e-04 (6.65e-04)	Tok/s 39678 (43896)	Loss/tok 2.8785 (3.1562)	LR 2.500e-04
0: TRAIN [2][900/3880]	Time 0.266 (0.324)	Data 1.51e-04 (6.59e-04)	Tok/s 38401 (43888)	Loss/tok 3.0171 (3.1561)	LR 2.500e-04
0: TRAIN [2][910/3880]	Time 0.193 (0.324)	Data 1.73e-04 (6.54e-04)	Tok/s 26688 (43872)	Loss/tok 2.6885 (3.1555)	LR 2.500e-04
0: TRAIN [2][920/3880]	Time 0.265 (0.324)	Data 1.33e-04 (6.48e-04)	Tok/s 38693 (43851)	Loss/tok 3.0315 (3.1556)	LR 2.500e-04
0: TRAIN [2][930/3880]	Time 0.348 (0.324)	Data 1.66e-04 (6.43e-04)	Tok/s 48476 (43878)	Loss/tok 3.0909 (3.1576)	LR 2.500e-04
0: TRAIN [2][940/3880]	Time 0.266 (0.324)	Data 1.32e-04 (6.37e-04)	Tok/s 39139 (43848)	Loss/tok 3.0345 (3.1571)	LR 2.500e-04
0: TRAIN [2][950/3880]	Time 0.347 (0.324)	Data 1.82e-04 (6.32e-04)	Tok/s 49048 (43873)	Loss/tok 3.1295 (3.1575)	LR 2.500e-04
0: TRAIN [2][960/3880]	Time 0.191 (0.325)	Data 1.50e-04 (6.27e-04)	Tok/s 28041 (43884)	Loss/tok 2.6205 (3.1581)	LR 2.500e-04
0: TRAIN [2][970/3880]	Time 0.432 (0.325)	Data 1.23e-04 (6.22e-04)	Tok/s 53989 (43909)	Loss/tok 3.3832 (3.1585)	LR 2.500e-04
0: TRAIN [2][980/3880]	Time 0.266 (0.324)	Data 1.26e-04 (6.18e-04)	Tok/s 38312 (43881)	Loss/tok 2.9157 (3.1582)	LR 2.500e-04
0: TRAIN [2][990/3880]	Time 0.193 (0.324)	Data 1.70e-04 (6.13e-04)	Tok/s 27128 (43849)	Loss/tok 2.6213 (3.1574)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1000/3880]	Time 0.266 (0.324)	Data 1.27e-04 (6.08e-04)	Tok/s 38874 (43840)	Loss/tok 2.9255 (3.1567)	LR 2.500e-04
0: TRAIN [2][1010/3880]	Time 0.347 (0.324)	Data 1.27e-04 (6.03e-04)	Tok/s 48590 (43847)	Loss/tok 3.1704 (3.1558)	LR 2.500e-04
0: TRAIN [2][1020/3880]	Time 0.266 (0.324)	Data 1.31e-04 (5.99e-04)	Tok/s 38462 (43836)	Loss/tok 3.0611 (3.1561)	LR 2.500e-04
0: TRAIN [2][1030/3880]	Time 0.266 (0.324)	Data 2.56e-04 (5.94e-04)	Tok/s 39278 (43820)	Loss/tok 2.8878 (3.1557)	LR 2.500e-04
0: TRAIN [2][1040/3880]	Time 0.349 (0.324)	Data 1.65e-04 (5.90e-04)	Tok/s 47986 (43846)	Loss/tok 3.0891 (3.1555)	LR 2.500e-04
0: TRAIN [2][1050/3880]	Time 0.275 (0.323)	Data 1.54e-04 (5.86e-04)	Tok/s 38086 (43791)	Loss/tok 3.0423 (3.1546)	LR 2.500e-04
0: TRAIN [2][1060/3880]	Time 0.348 (0.324)	Data 1.38e-04 (5.90e-04)	Tok/s 47763 (43810)	Loss/tok 3.2418 (3.1552)	LR 2.500e-04
0: TRAIN [2][1070/3880]	Time 0.195 (0.324)	Data 1.50e-04 (5.86e-04)	Tok/s 26866 (43814)	Loss/tok 2.6446 (3.1562)	LR 2.500e-04
0: TRAIN [2][1080/3880]	Time 0.267 (0.323)	Data 1.60e-04 (5.82e-04)	Tok/s 38621 (43791)	Loss/tok 2.8951 (3.1554)	LR 2.500e-04
0: TRAIN [2][1090/3880]	Time 0.194 (0.323)	Data 1.64e-04 (5.78e-04)	Tok/s 27296 (43781)	Loss/tok 2.4623 (3.1552)	LR 2.500e-04
0: TRAIN [2][1100/3880]	Time 0.348 (0.323)	Data 1.68e-04 (5.74e-04)	Tok/s 49197 (43792)	Loss/tok 3.0680 (3.1554)	LR 2.500e-04
0: TRAIN [2][1110/3880]	Time 0.267 (0.323)	Data 1.33e-04 (5.70e-04)	Tok/s 38484 (43797)	Loss/tok 2.9264 (3.1554)	LR 2.500e-04
0: TRAIN [2][1120/3880]	Time 0.547 (0.324)	Data 1.36e-04 (5.66e-04)	Tok/s 54740 (43789)	Loss/tok 3.4360 (3.1562)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1130/3880]	Time 0.420 (0.324)	Data 1.69e-04 (5.63e-04)	Tok/s 54883 (43813)	Loss/tok 3.4384 (3.1562)	LR 2.500e-04
0: TRAIN [2][1140/3880]	Time 0.347 (0.323)	Data 1.32e-04 (5.59e-04)	Tok/s 48825 (43760)	Loss/tok 3.0882 (3.1550)	LR 2.500e-04
0: TRAIN [2][1150/3880]	Time 0.193 (0.323)	Data 1.29e-04 (5.55e-04)	Tok/s 27443 (43731)	Loss/tok 2.5226 (3.1540)	LR 2.500e-04
0: TRAIN [2][1160/3880]	Time 0.266 (0.323)	Data 1.34e-04 (5.52e-04)	Tok/s 38112 (43724)	Loss/tok 2.9181 (3.1534)	LR 2.500e-04
0: TRAIN [2][1170/3880]	Time 0.431 (0.323)	Data 1.47e-04 (5.48e-04)	Tok/s 53509 (43753)	Loss/tok 3.3631 (3.1538)	LR 1.250e-04
0: TRAIN [2][1180/3880]	Time 0.347 (0.323)	Data 1.30e-04 (5.45e-04)	Tok/s 48299 (43751)	Loss/tok 3.0927 (3.1530)	LR 1.250e-04
0: TRAIN [2][1190/3880]	Time 0.348 (0.323)	Data 1.30e-04 (5.42e-04)	Tok/s 48371 (43743)	Loss/tok 3.2832 (3.1530)	LR 1.250e-04
0: TRAIN [2][1200/3880]	Time 0.266 (0.323)	Data 1.33e-04 (5.38e-04)	Tok/s 38412 (43732)	Loss/tok 2.9665 (3.1525)	LR 1.250e-04
0: TRAIN [2][1210/3880]	Time 0.266 (0.322)	Data 1.30e-04 (5.35e-04)	Tok/s 38647 (43687)	Loss/tok 2.9754 (3.1523)	LR 1.250e-04
0: TRAIN [2][1220/3880]	Time 0.544 (0.323)	Data 1.33e-04 (5.32e-04)	Tok/s 55148 (43693)	Loss/tok 3.4310 (3.1533)	LR 1.250e-04
0: TRAIN [2][1230/3880]	Time 0.266 (0.323)	Data 1.43e-04 (5.29e-04)	Tok/s 38980 (43688)	Loss/tok 2.8660 (3.1534)	LR 1.250e-04
0: TRAIN [2][1240/3880]	Time 0.195 (0.323)	Data 1.35e-04 (5.25e-04)	Tok/s 27096 (43704)	Loss/tok 2.5139 (3.1534)	LR 1.250e-04
0: TRAIN [2][1250/3880]	Time 0.348 (0.323)	Data 1.37e-04 (5.23e-04)	Tok/s 48233 (43702)	Loss/tok 3.1718 (3.1527)	LR 1.250e-04
0: TRAIN [2][1260/3880]	Time 0.349 (0.323)	Data 1.40e-04 (5.20e-04)	Tok/s 47989 (43725)	Loss/tok 3.1762 (3.1525)	LR 1.250e-04
0: TRAIN [2][1270/3880]	Time 0.266 (0.323)	Data 1.33e-04 (5.17e-04)	Tok/s 39206 (43705)	Loss/tok 3.0059 (3.1521)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1280/3880]	Time 0.192 (0.323)	Data 1.33e-04 (5.14e-04)	Tok/s 27730 (43698)	Loss/tok 2.5794 (3.1525)	LR 1.250e-04
0: TRAIN [2][1290/3880]	Time 0.543 (0.323)	Data 1.39e-04 (5.11e-04)	Tok/s 55307 (43674)	Loss/tok 3.4547 (3.1521)	LR 1.250e-04
0: TRAIN [2][1300/3880]	Time 0.265 (0.322)	Data 1.28e-04 (5.08e-04)	Tok/s 38780 (43663)	Loss/tok 2.9135 (3.1516)	LR 1.250e-04
0: TRAIN [2][1310/3880]	Time 0.277 (0.322)	Data 1.25e-04 (5.06e-04)	Tok/s 37312 (43646)	Loss/tok 2.9434 (3.1509)	LR 1.250e-04
0: TRAIN [2][1320/3880]	Time 0.347 (0.323)	Data 1.78e-04 (5.03e-04)	Tok/s 49083 (43695)	Loss/tok 3.1624 (3.1527)	LR 1.250e-04
0: TRAIN [2][1330/3880]	Time 0.346 (0.323)	Data 1.27e-04 (5.00e-04)	Tok/s 48869 (43685)	Loss/tok 3.1713 (3.1525)	LR 1.250e-04
0: TRAIN [2][1340/3880]	Time 0.347 (0.323)	Data 1.79e-04 (4.98e-04)	Tok/s 48296 (43684)	Loss/tok 3.1575 (3.1524)	LR 1.250e-04
0: TRAIN [2][1350/3880]	Time 0.266 (0.323)	Data 1.50e-04 (4.95e-04)	Tok/s 38802 (43687)	Loss/tok 2.8772 (3.1522)	LR 1.250e-04
0: TRAIN [2][1360/3880]	Time 0.268 (0.323)	Data 1.65e-04 (4.92e-04)	Tok/s 38547 (43700)	Loss/tok 2.9625 (3.1527)	LR 1.250e-04
0: TRAIN [2][1370/3880]	Time 0.347 (0.323)	Data 1.41e-04 (4.90e-04)	Tok/s 48363 (43692)	Loss/tok 3.1480 (3.1520)	LR 1.250e-04
0: TRAIN [2][1380/3880]	Time 0.266 (0.323)	Data 1.33e-04 (4.87e-04)	Tok/s 38845 (43702)	Loss/tok 2.9743 (3.1524)	LR 1.250e-04
0: TRAIN [2][1390/3880]	Time 0.201 (0.322)	Data 1.25e-04 (4.85e-04)	Tok/s 25880 (43662)	Loss/tok 2.6385 (3.1515)	LR 1.250e-04
0: TRAIN [2][1400/3880]	Time 0.265 (0.323)	Data 1.95e-04 (4.83e-04)	Tok/s 38505 (43669)	Loss/tok 2.9816 (3.1523)	LR 1.250e-04
0: TRAIN [2][1410/3880]	Time 0.265 (0.323)	Data 1.86e-04 (4.80e-04)	Tok/s 38737 (43674)	Loss/tok 3.0358 (3.1530)	LR 1.250e-04
0: TRAIN [2][1420/3880]	Time 0.266 (0.323)	Data 1.39e-04 (4.78e-04)	Tok/s 38820 (43665)	Loss/tok 2.9981 (3.1526)	LR 1.250e-04
0: TRAIN [2][1430/3880]	Time 0.266 (0.322)	Data 1.33e-04 (4.76e-04)	Tok/s 39183 (43614)	Loss/tok 3.0053 (3.1516)	LR 1.250e-04
0: TRAIN [2][1440/3880]	Time 0.347 (0.322)	Data 1.39e-04 (4.74e-04)	Tok/s 48323 (43609)	Loss/tok 3.1329 (3.1512)	LR 1.250e-04
0: TRAIN [2][1450/3880]	Time 0.266 (0.322)	Data 1.38e-04 (4.71e-04)	Tok/s 39146 (43608)	Loss/tok 2.9085 (3.1509)	LR 1.250e-04
0: TRAIN [2][1460/3880]	Time 0.267 (0.322)	Data 2.86e-04 (4.69e-04)	Tok/s 38314 (43579)	Loss/tok 3.0149 (3.1505)	LR 1.250e-04
0: TRAIN [2][1470/3880]	Time 0.348 (0.322)	Data 1.51e-04 (4.67e-04)	Tok/s 48728 (43602)	Loss/tok 3.0996 (3.1509)	LR 1.250e-04
0: TRAIN [2][1480/3880]	Time 0.347 (0.322)	Data 1.50e-04 (4.65e-04)	Tok/s 48316 (43636)	Loss/tok 3.1678 (3.1514)	LR 1.250e-04
0: TRAIN [2][1490/3880]	Time 0.432 (0.322)	Data 1.49e-04 (4.63e-04)	Tok/s 54183 (43631)	Loss/tok 3.2442 (3.1509)	LR 1.250e-04
0: TRAIN [2][1500/3880]	Time 0.353 (0.322)	Data 2.03e-04 (4.61e-04)	Tok/s 47234 (43605)	Loss/tok 3.0968 (3.1505)	LR 1.250e-04
0: TRAIN [2][1510/3880]	Time 0.266 (0.322)	Data 1.34e-04 (4.58e-04)	Tok/s 39539 (43620)	Loss/tok 2.8925 (3.1506)	LR 1.250e-04
0: TRAIN [2][1520/3880]	Time 0.266 (0.322)	Data 1.73e-04 (4.56e-04)	Tok/s 38606 (43598)	Loss/tok 2.9970 (3.1499)	LR 1.250e-04
0: TRAIN [2][1530/3880]	Time 0.266 (0.322)	Data 1.41e-04 (4.54e-04)	Tok/s 38638 (43586)	Loss/tok 2.8907 (3.1496)	LR 1.250e-04
0: TRAIN [2][1540/3880]	Time 0.349 (0.322)	Data 1.57e-04 (4.53e-04)	Tok/s 48251 (43578)	Loss/tok 3.0510 (3.1493)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1550/3880]	Time 0.346 (0.322)	Data 1.36e-04 (4.59e-04)	Tok/s 48738 (43589)	Loss/tok 3.1238 (3.1493)	LR 1.250e-04
0: TRAIN [2][1560/3880]	Time 0.266 (0.322)	Data 1.34e-04 (4.57e-04)	Tok/s 38162 (43599)	Loss/tok 3.0581 (3.1493)	LR 1.250e-04
0: TRAIN [2][1570/3880]	Time 0.546 (0.322)	Data 1.34e-04 (4.55e-04)	Tok/s 54710 (43625)	Loss/tok 3.5071 (3.1504)	LR 1.250e-04
0: TRAIN [2][1580/3880]	Time 0.266 (0.322)	Data 1.34e-04 (4.53e-04)	Tok/s 37896 (43633)	Loss/tok 2.9536 (3.1504)	LR 1.250e-04
0: TRAIN [2][1590/3880]	Time 0.348 (0.323)	Data 1.56e-04 (4.51e-04)	Tok/s 49218 (43644)	Loss/tok 3.3052 (3.1504)	LR 1.250e-04
0: TRAIN [2][1600/3880]	Time 0.346 (0.323)	Data 1.50e-04 (4.49e-04)	Tok/s 48269 (43659)	Loss/tok 3.1195 (3.1503)	LR 1.250e-04
0: TRAIN [2][1610/3880]	Time 0.192 (0.323)	Data 1.28e-04 (4.48e-04)	Tok/s 27456 (43668)	Loss/tok 2.5726 (3.1503)	LR 1.250e-04
0: TRAIN [2][1620/3880]	Time 0.347 (0.323)	Data 1.28e-04 (4.46e-04)	Tok/s 48768 (43662)	Loss/tok 3.0153 (3.1496)	LR 1.250e-04
0: TRAIN [2][1630/3880]	Time 0.192 (0.322)	Data 1.24e-04 (4.44e-04)	Tok/s 26901 (43657)	Loss/tok 2.4930 (3.1496)	LR 1.250e-04
0: TRAIN [2][1640/3880]	Time 0.192 (0.322)	Data 1.30e-04 (4.42e-04)	Tok/s 26895 (43641)	Loss/tok 2.5595 (3.1501)	LR 1.250e-04
0: TRAIN [2][1650/3880]	Time 0.346 (0.323)	Data 1.31e-04 (4.40e-04)	Tok/s 48309 (43662)	Loss/tok 3.1280 (3.1505)	LR 1.250e-04
0: TRAIN [2][1660/3880]	Time 0.266 (0.323)	Data 1.48e-04 (4.39e-04)	Tok/s 38519 (43646)	Loss/tok 2.9812 (3.1504)	LR 1.250e-04
0: TRAIN [2][1670/3880]	Time 0.347 (0.322)	Data 2.54e-04 (4.38e-04)	Tok/s 48772 (43625)	Loss/tok 3.0575 (3.1495)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1680/3880]	Time 0.430 (0.322)	Data 1.39e-04 (4.36e-04)	Tok/s 54687 (43625)	Loss/tok 3.3435 (3.1501)	LR 1.250e-04
0: TRAIN [2][1690/3880]	Time 0.347 (0.322)	Data 1.82e-04 (4.34e-04)	Tok/s 49013 (43633)	Loss/tok 3.0519 (3.1497)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1700/3880]	Time 0.265 (0.322)	Data 1.42e-04 (4.33e-04)	Tok/s 38659 (43622)	Loss/tok 2.8935 (3.1493)	LR 1.250e-04
0: TRAIN [2][1710/3880]	Time 0.348 (0.322)	Data 1.64e-04 (4.31e-04)	Tok/s 48575 (43618)	Loss/tok 3.2319 (3.1491)	LR 1.250e-04
0: TRAIN [2][1720/3880]	Time 0.266 (0.322)	Data 1.65e-04 (4.30e-04)	Tok/s 38464 (43629)	Loss/tok 2.9945 (3.1491)	LR 1.250e-04
0: TRAIN [2][1730/3880]	Time 0.266 (0.322)	Data 1.31e-04 (4.28e-04)	Tok/s 38033 (43630)	Loss/tok 3.0806 (3.1494)	LR 1.250e-04
0: TRAIN [2][1740/3880]	Time 0.268 (0.322)	Data 1.75e-04 (4.26e-04)	Tok/s 39059 (43629)	Loss/tok 3.0712 (3.1492)	LR 1.250e-04
0: TRAIN [2][1750/3880]	Time 0.431 (0.323)	Data 1.34e-04 (4.25e-04)	Tok/s 53956 (43655)	Loss/tok 3.2881 (3.1498)	LR 1.250e-04
0: TRAIN [2][1760/3880]	Time 0.431 (0.323)	Data 1.49e-04 (4.23e-04)	Tok/s 53624 (43666)	Loss/tok 3.3568 (3.1501)	LR 1.250e-04
0: TRAIN [2][1770/3880]	Time 0.543 (0.323)	Data 1.39e-04 (4.22e-04)	Tok/s 54729 (43663)	Loss/tok 3.4827 (3.1508)	LR 1.250e-04
0: TRAIN [2][1780/3880]	Time 0.265 (0.323)	Data 1.36e-04 (4.20e-04)	Tok/s 38448 (43660)	Loss/tok 2.9980 (3.1506)	LR 1.250e-04
0: TRAIN [2][1790/3880]	Time 0.265 (0.323)	Data 1.84e-04 (4.19e-04)	Tok/s 39531 (43662)	Loss/tok 2.9054 (3.1505)	LR 1.250e-04
0: TRAIN [2][1800/3880]	Time 0.266 (0.323)	Data 1.30e-04 (4.17e-04)	Tok/s 39837 (43663)	Loss/tok 2.9298 (3.1505)	LR 1.250e-04
0: TRAIN [2][1810/3880]	Time 0.348 (0.323)	Data 1.28e-04 (4.15e-04)	Tok/s 47165 (43676)	Loss/tok 3.2342 (3.1510)	LR 1.250e-04
0: TRAIN [2][1820/3880]	Time 0.266 (0.323)	Data 1.61e-04 (4.14e-04)	Tok/s 39318 (43691)	Loss/tok 3.0382 (3.1509)	LR 1.250e-04
0: TRAIN [2][1830/3880]	Time 0.547 (0.323)	Data 3.70e-04 (4.13e-04)	Tok/s 54532 (43709)	Loss/tok 3.5499 (3.1514)	LR 1.250e-04
0: TRAIN [2][1840/3880]	Time 0.546 (0.324)	Data 1.29e-04 (4.11e-04)	Tok/s 53752 (43720)	Loss/tok 3.5335 (3.1517)	LR 1.250e-04
0: TRAIN [2][1850/3880]	Time 0.266 (0.323)	Data 1.47e-04 (4.10e-04)	Tok/s 39222 (43704)	Loss/tok 2.9651 (3.1510)	LR 1.250e-04
0: TRAIN [2][1860/3880]	Time 0.430 (0.323)	Data 1.29e-04 (4.08e-04)	Tok/s 54244 (43695)	Loss/tok 3.4419 (3.1507)	LR 1.250e-04
0: TRAIN [2][1870/3880]	Time 0.267 (0.323)	Data 1.30e-04 (4.07e-04)	Tok/s 38242 (43694)	Loss/tok 2.9891 (3.1505)	LR 1.250e-04
0: TRAIN [2][1880/3880]	Time 0.280 (0.323)	Data 1.62e-04 (4.06e-04)	Tok/s 37505 (43690)	Loss/tok 2.9336 (3.1503)	LR 1.250e-04
0: TRAIN [2][1890/3880]	Time 0.266 (0.323)	Data 1.30e-04 (4.04e-04)	Tok/s 38964 (43697)	Loss/tok 3.0420 (3.1505)	LR 1.250e-04
0: TRAIN [2][1900/3880]	Time 0.429 (0.323)	Data 1.98e-04 (4.03e-04)	Tok/s 54773 (43691)	Loss/tok 3.2427 (3.1502)	LR 1.250e-04
0: TRAIN [2][1910/3880]	Time 0.267 (0.323)	Data 1.60e-04 (4.01e-04)	Tok/s 39358 (43687)	Loss/tok 2.9210 (3.1504)	LR 1.250e-04
0: TRAIN [2][1920/3880]	Time 0.432 (0.323)	Data 1.26e-04 (4.00e-04)	Tok/s 53796 (43701)	Loss/tok 3.2249 (3.1507)	LR 1.250e-04
0: TRAIN [2][1930/3880]	Time 0.266 (0.323)	Data 1.91e-04 (3.99e-04)	Tok/s 38795 (43698)	Loss/tok 2.9803 (3.1507)	LR 1.250e-04
0: TRAIN [2][1940/3880]	Time 0.348 (0.323)	Data 1.30e-04 (3.98e-04)	Tok/s 48081 (43720)	Loss/tok 3.1037 (3.1514)	LR 1.250e-04
0: TRAIN [2][1950/3880]	Time 0.430 (0.323)	Data 1.51e-04 (3.96e-04)	Tok/s 53673 (43720)	Loss/tok 3.4310 (3.1514)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1960/3880]	Time 0.266 (0.323)	Data 1.34e-04 (3.95e-04)	Tok/s 38555 (43703)	Loss/tok 3.0156 (3.1508)	LR 1.250e-04
0: TRAIN [2][1970/3880]	Time 0.267 (0.323)	Data 1.35e-04 (3.94e-04)	Tok/s 38624 (43708)	Loss/tok 3.0311 (3.1508)	LR 1.250e-04
0: TRAIN [2][1980/3880]	Time 0.346 (0.323)	Data 1.34e-04 (3.93e-04)	Tok/s 49430 (43704)	Loss/tok 3.1965 (3.1504)	LR 1.250e-04
0: TRAIN [2][1990/3880]	Time 0.266 (0.323)	Data 1.30e-04 (3.91e-04)	Tok/s 38946 (43687)	Loss/tok 2.9681 (3.1500)	LR 1.250e-04
0: TRAIN [2][2000/3880]	Time 0.348 (0.323)	Data 1.69e-04 (3.90e-04)	Tok/s 47598 (43686)	Loss/tok 3.1469 (3.1499)	LR 1.250e-04
0: TRAIN [2][2010/3880]	Time 0.194 (0.322)	Data 1.58e-04 (3.89e-04)	Tok/s 27640 (43644)	Loss/tok 2.5962 (3.1492)	LR 1.250e-04
0: TRAIN [2][2020/3880]	Time 0.349 (0.322)	Data 1.32e-04 (3.88e-04)	Tok/s 47829 (43651)	Loss/tok 3.1266 (3.1492)	LR 1.250e-04
0: TRAIN [2][2030/3880]	Time 0.193 (0.322)	Data 1.29e-04 (3.87e-04)	Tok/s 27704 (43636)	Loss/tok 2.5946 (3.1487)	LR 1.250e-04
0: TRAIN [2][2040/3880]	Time 0.266 (0.322)	Data 2.07e-04 (3.86e-04)	Tok/s 39042 (43624)	Loss/tok 2.8389 (3.1482)	LR 1.250e-04
0: TRAIN [2][2050/3880]	Time 0.347 (0.322)	Data 1.36e-04 (3.84e-04)	Tok/s 47856 (43623)	Loss/tok 3.1803 (3.1480)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2060/3880]	Time 0.542 (0.322)	Data 1.65e-04 (3.83e-04)	Tok/s 54699 (43654)	Loss/tok 3.3844 (3.1485)	LR 1.250e-04
0: TRAIN [2][2070/3880]	Time 0.347 (0.322)	Data 1.32e-04 (3.82e-04)	Tok/s 47702 (43667)	Loss/tok 3.1756 (3.1484)	LR 1.250e-04
0: TRAIN [2][2080/3880]	Time 0.192 (0.323)	Data 1.24e-04 (3.81e-04)	Tok/s 27432 (43670)	Loss/tok 2.5504 (3.1485)	LR 1.250e-04
0: TRAIN [2][2090/3880]	Time 0.431 (0.323)	Data 1.47e-04 (3.80e-04)	Tok/s 53891 (43680)	Loss/tok 3.2892 (3.1489)	LR 1.250e-04
0: TRAIN [2][2100/3880]	Time 0.347 (0.323)	Data 1.44e-04 (3.79e-04)	Tok/s 48375 (43684)	Loss/tok 3.2307 (3.1497)	LR 1.250e-04
0: TRAIN [2][2110/3880]	Time 0.431 (0.323)	Data 1.30e-04 (3.78e-04)	Tok/s 53437 (43684)	Loss/tok 3.3194 (3.1498)	LR 1.250e-04
0: TRAIN [2][2120/3880]	Time 0.348 (0.323)	Data 1.32e-04 (3.77e-04)	Tok/s 48060 (43680)	Loss/tok 3.2809 (3.1496)	LR 1.250e-04
0: TRAIN [2][2130/3880]	Time 0.265 (0.323)	Data 1.34e-04 (3.76e-04)	Tok/s 38963 (43676)	Loss/tok 2.9694 (3.1496)	LR 1.250e-04
0: TRAIN [2][2140/3880]	Time 0.346 (0.323)	Data 1.42e-04 (3.75e-04)	Tok/s 48363 (43689)	Loss/tok 3.1252 (3.1496)	LR 1.250e-04
0: TRAIN [2][2150/3880]	Time 0.266 (0.323)	Data 1.53e-04 (3.74e-04)	Tok/s 38457 (43676)	Loss/tok 2.9251 (3.1491)	LR 1.250e-04
0: TRAIN [2][2160/3880]	Time 0.191 (0.323)	Data 1.26e-04 (3.73e-04)	Tok/s 27502 (43694)	Loss/tok 2.4761 (3.1497)	LR 1.250e-04
0: TRAIN [2][2170/3880]	Time 0.266 (0.323)	Data 1.47e-04 (3.72e-04)	Tok/s 38036 (43712)	Loss/tok 2.9951 (3.1501)	LR 1.250e-04
0: TRAIN [2][2180/3880]	Time 0.265 (0.323)	Data 1.62e-04 (3.71e-04)	Tok/s 38438 (43696)	Loss/tok 2.9072 (3.1496)	LR 1.250e-04
0: TRAIN [2][2190/3880]	Time 0.349 (0.323)	Data 1.31e-04 (3.70e-04)	Tok/s 48160 (43689)	Loss/tok 3.1460 (3.1494)	LR 1.250e-04
0: TRAIN [2][2200/3880]	Time 0.192 (0.323)	Data 1.34e-04 (3.69e-04)	Tok/s 28714 (43681)	Loss/tok 2.6737 (3.1490)	LR 1.250e-04
0: TRAIN [2][2210/3880]	Time 0.266 (0.323)	Data 1.33e-04 (3.68e-04)	Tok/s 39799 (43696)	Loss/tok 2.9618 (3.1496)	LR 1.250e-04
0: TRAIN [2][2220/3880]	Time 0.269 (0.323)	Data 1.35e-04 (3.67e-04)	Tok/s 38016 (43693)	Loss/tok 3.0094 (3.1496)	LR 1.250e-04
0: TRAIN [2][2230/3880]	Time 0.348 (0.323)	Data 1.25e-04 (3.66e-04)	Tok/s 48236 (43696)	Loss/tok 3.1452 (3.1493)	LR 1.250e-04
0: TRAIN [2][2240/3880]	Time 0.267 (0.323)	Data 1.65e-04 (3.65e-04)	Tok/s 38213 (43677)	Loss/tok 2.8572 (3.1488)	LR 1.250e-04
0: TRAIN [2][2250/3880]	Time 0.268 (0.322)	Data 1.27e-04 (3.64e-04)	Tok/s 39214 (43662)	Loss/tok 2.9262 (3.1484)	LR 1.250e-04
0: TRAIN [2][2260/3880]	Time 0.544 (0.322)	Data 1.94e-04 (3.63e-04)	Tok/s 55581 (43663)	Loss/tok 3.4393 (3.1484)	LR 1.250e-04
0: TRAIN [2][2270/3880]	Time 0.431 (0.323)	Data 1.77e-04 (3.62e-04)	Tok/s 54201 (43675)	Loss/tok 3.2715 (3.1484)	LR 1.250e-04
0: TRAIN [2][2280/3880]	Time 0.347 (0.322)	Data 1.28e-04 (3.61e-04)	Tok/s 48144 (43675)	Loss/tok 3.1355 (3.1481)	LR 1.250e-04
0: TRAIN [2][2290/3880]	Time 0.266 (0.322)	Data 1.30e-04 (3.61e-04)	Tok/s 38108 (43668)	Loss/tok 2.9911 (3.1480)	LR 1.250e-04
0: TRAIN [2][2300/3880]	Time 0.266 (0.323)	Data 1.32e-04 (3.60e-04)	Tok/s 38592 (43679)	Loss/tok 2.9440 (3.1486)	LR 1.250e-04
0: TRAIN [2][2310/3880]	Time 0.193 (0.323)	Data 1.51e-04 (3.59e-04)	Tok/s 26807 (43688)	Loss/tok 2.5538 (3.1488)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][2320/3880]	Time 0.347 (0.323)	Data 1.66e-04 (3.58e-04)	Tok/s 48045 (43698)	Loss/tok 3.1732 (3.1490)	LR 1.250e-04
0: TRAIN [2][2330/3880]	Time 0.265 (0.323)	Data 1.31e-04 (3.57e-04)	Tok/s 39539 (43694)	Loss/tok 2.9927 (3.1488)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2340/3880]	Time 0.347 (0.323)	Data 1.76e-04 (3.56e-04)	Tok/s 49611 (43699)	Loss/tok 3.0775 (3.1489)	LR 1.250e-04
0: TRAIN [2][2350/3880]	Time 0.543 (0.323)	Data 1.46e-04 (3.55e-04)	Tok/s 54580 (43701)	Loss/tok 3.5070 (3.1492)	LR 1.250e-04
0: TRAIN [2][2360/3880]	Time 0.347 (0.323)	Data 1.26e-04 (3.54e-04)	Tok/s 48571 (43703)	Loss/tok 3.1483 (3.1494)	LR 1.250e-04
0: TRAIN [2][2370/3880]	Time 0.198 (0.323)	Data 1.24e-04 (3.54e-04)	Tok/s 26202 (43698)	Loss/tok 2.5383 (3.1494)	LR 1.250e-04
0: TRAIN [2][2380/3880]	Time 0.266 (0.323)	Data 1.25e-04 (3.53e-04)	Tok/s 38760 (43696)	Loss/tok 3.0498 (3.1492)	LR 1.250e-04
0: TRAIN [2][2390/3880]	Time 0.544 (0.323)	Data 2.07e-04 (3.52e-04)	Tok/s 54372 (43688)	Loss/tok 3.5337 (3.1494)	LR 1.250e-04
0: TRAIN [2][2400/3880]	Time 0.265 (0.323)	Data 1.48e-04 (3.51e-04)	Tok/s 38675 (43692)	Loss/tok 2.9297 (3.1491)	LR 1.250e-04
0: TRAIN [2][2410/3880]	Time 0.193 (0.323)	Data 1.28e-04 (3.50e-04)	Tok/s 27855 (43681)	Loss/tok 2.5217 (3.1487)	LR 1.250e-04
0: TRAIN [2][2420/3880]	Time 0.348 (0.323)	Data 1.29e-04 (3.49e-04)	Tok/s 48563 (43680)	Loss/tok 3.1086 (3.1485)	LR 1.250e-04
0: TRAIN [2][2430/3880]	Time 0.266 (0.323)	Data 1.29e-04 (3.49e-04)	Tok/s 38986 (43685)	Loss/tok 2.7981 (3.1482)	LR 1.250e-04
0: TRAIN [2][2440/3880]	Time 0.267 (0.323)	Data 1.29e-04 (3.48e-04)	Tok/s 39224 (43676)	Loss/tok 2.8949 (3.1478)	LR 1.250e-04
0: TRAIN [2][2450/3880]	Time 0.348 (0.322)	Data 1.41e-04 (3.47e-04)	Tok/s 48726 (43671)	Loss/tok 3.0199 (3.1475)	LR 1.250e-04
0: TRAIN [2][2460/3880]	Time 0.266 (0.323)	Data 1.34e-04 (3.46e-04)	Tok/s 39099 (43679)	Loss/tok 3.0052 (3.1475)	LR 1.250e-04
0: TRAIN [2][2470/3880]	Time 0.265 (0.323)	Data 1.77e-04 (3.45e-04)	Tok/s 38004 (43676)	Loss/tok 2.9397 (3.1476)	LR 1.250e-04
0: TRAIN [2][2480/3880]	Time 0.266 (0.323)	Data 1.35e-04 (3.45e-04)	Tok/s 39789 (43675)	Loss/tok 2.9772 (3.1477)	LR 1.250e-04
0: TRAIN [2][2490/3880]	Time 0.265 (0.323)	Data 1.91e-04 (3.44e-04)	Tok/s 38418 (43681)	Loss/tok 2.9319 (3.1478)	LR 1.250e-04
0: TRAIN [2][2500/3880]	Time 0.346 (0.323)	Data 1.49e-04 (3.43e-04)	Tok/s 47844 (43685)	Loss/tok 3.1059 (3.1476)	LR 1.250e-04
0: TRAIN [2][2510/3880]	Time 0.265 (0.323)	Data 1.31e-04 (3.42e-04)	Tok/s 39108 (43683)	Loss/tok 2.9712 (3.1474)	LR 1.250e-04
0: TRAIN [2][2520/3880]	Time 0.347 (0.323)	Data 1.37e-04 (3.41e-04)	Tok/s 48356 (43694)	Loss/tok 3.1700 (3.1475)	LR 1.250e-04
0: TRAIN [2][2530/3880]	Time 0.430 (0.323)	Data 1.98e-04 (3.41e-04)	Tok/s 55179 (43691)	Loss/tok 3.1767 (3.1471)	LR 1.250e-04
0: TRAIN [2][2540/3880]	Time 0.266 (0.323)	Data 1.38e-04 (3.40e-04)	Tok/s 39716 (43681)	Loss/tok 2.9704 (3.1468)	LR 1.250e-04
0: TRAIN [2][2550/3880]	Time 0.347 (0.322)	Data 1.34e-04 (3.39e-04)	Tok/s 47833 (43673)	Loss/tok 3.1750 (3.1467)	LR 1.250e-04
0: TRAIN [2][2560/3880]	Time 0.348 (0.322)	Data 1.29e-04 (3.38e-04)	Tok/s 47751 (43658)	Loss/tok 3.1387 (3.1466)	LR 1.250e-04
0: TRAIN [2][2570/3880]	Time 0.266 (0.322)	Data 1.28e-04 (3.38e-04)	Tok/s 38581 (43655)	Loss/tok 2.8675 (3.1465)	LR 1.250e-04
0: TRAIN [2][2580/3880]	Time 0.266 (0.322)	Data 1.32e-04 (3.37e-04)	Tok/s 39438 (43649)	Loss/tok 2.9255 (3.1461)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][2590/3880]	Time 0.253 (0.322)	Data 1.44e-04 (3.37e-04)	Tok/s 40646 (43649)	Loss/tok 3.0197 (3.1461)	LR 1.250e-04
0: TRAIN [2][2600/3880]	Time 0.267 (0.322)	Data 1.29e-04 (3.36e-04)	Tok/s 38115 (43646)	Loss/tok 2.9517 (3.1464)	LR 1.250e-04
0: TRAIN [2][2610/3880]	Time 0.267 (0.322)	Data 1.57e-04 (3.35e-04)	Tok/s 38059 (43637)	Loss/tok 2.9844 (3.1463)	LR 1.250e-04
0: TRAIN [2][2620/3880]	Time 0.346 (0.322)	Data 1.75e-04 (3.35e-04)	Tok/s 48736 (43632)	Loss/tok 3.1361 (3.1461)	LR 1.250e-04
0: TRAIN [2][2630/3880]	Time 0.347 (0.322)	Data 1.83e-04 (3.34e-04)	Tok/s 48170 (43637)	Loss/tok 3.0881 (3.1463)	LR 1.250e-04
0: TRAIN [2][2640/3880]	Time 0.349 (0.322)	Data 1.33e-04 (3.34e-04)	Tok/s 47452 (43641)	Loss/tok 3.2029 (3.1465)	LR 1.250e-04
0: TRAIN [2][2650/3880]	Time 0.347 (0.322)	Data 1.34e-04 (3.33e-04)	Tok/s 47960 (43636)	Loss/tok 3.2102 (3.1464)	LR 1.250e-04
0: TRAIN [2][2660/3880]	Time 0.348 (0.322)	Data 1.39e-04 (3.32e-04)	Tok/s 49847 (43632)	Loss/tok 3.0765 (3.1460)	LR 1.250e-04
0: TRAIN [2][2670/3880]	Time 0.347 (0.322)	Data 1.59e-04 (3.32e-04)	Tok/s 48211 (43630)	Loss/tok 3.0937 (3.1461)	LR 1.250e-04
0: TRAIN [2][2680/3880]	Time 0.266 (0.322)	Data 1.48e-04 (3.31e-04)	Tok/s 38468 (43628)	Loss/tok 2.9749 (3.1460)	LR 1.250e-04
0: TRAIN [2][2690/3880]	Time 0.192 (0.322)	Data 1.38e-04 (3.31e-04)	Tok/s 27924 (43612)	Loss/tok 2.4971 (3.1456)	LR 1.250e-04
0: TRAIN [2][2700/3880]	Time 0.266 (0.322)	Data 1.49e-04 (3.30e-04)	Tok/s 38656 (43610)	Loss/tok 2.8713 (3.1456)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2710/3880]	Time 0.545 (0.322)	Data 1.51e-04 (3.29e-04)	Tok/s 54169 (43612)	Loss/tok 3.4490 (3.1455)	LR 1.250e-04
0: TRAIN [2][2720/3880]	Time 0.347 (0.322)	Data 1.83e-04 (3.29e-04)	Tok/s 47816 (43609)	Loss/tok 3.1643 (3.1452)	LR 1.250e-04
0: TRAIN [2][2730/3880]	Time 0.346 (0.322)	Data 1.28e-04 (3.28e-04)	Tok/s 48714 (43606)	Loss/tok 3.1092 (3.1450)	LR 1.250e-04
0: TRAIN [2][2740/3880]	Time 0.348 (0.322)	Data 1.26e-04 (3.28e-04)	Tok/s 48380 (43609)	Loss/tok 3.0720 (3.1449)	LR 1.250e-04
0: TRAIN [2][2750/3880]	Time 0.347 (0.322)	Data 1.23e-04 (3.27e-04)	Tok/s 48369 (43607)	Loss/tok 3.1973 (3.1448)	LR 1.250e-04
0: TRAIN [2][2760/3880]	Time 0.265 (0.322)	Data 1.47e-04 (3.26e-04)	Tok/s 39029 (43587)	Loss/tok 3.0218 (3.1448)	LR 1.250e-04
0: TRAIN [2][2770/3880]	Time 0.277 (0.322)	Data 1.46e-04 (3.26e-04)	Tok/s 37893 (43594)	Loss/tok 2.9478 (3.1450)	LR 1.250e-04
0: TRAIN [2][2780/3880]	Time 0.346 (0.322)	Data 1.50e-04 (3.25e-04)	Tok/s 48801 (43587)	Loss/tok 3.2548 (3.1447)	LR 1.250e-04
0: TRAIN [2][2790/3880]	Time 0.266 (0.322)	Data 1.27e-04 (3.24e-04)	Tok/s 38378 (43588)	Loss/tok 2.9535 (3.1446)	LR 1.250e-04
0: TRAIN [2][2800/3880]	Time 0.195 (0.322)	Data 1.63e-04 (3.24e-04)	Tok/s 27607 (43575)	Loss/tok 2.5037 (3.1445)	LR 1.250e-04
0: TRAIN [2][2810/3880]	Time 0.266 (0.322)	Data 1.67e-04 (3.23e-04)	Tok/s 39788 (43583)	Loss/tok 3.0163 (3.1446)	LR 1.250e-04
0: TRAIN [2][2820/3880]	Time 0.266 (0.322)	Data 1.30e-04 (3.22e-04)	Tok/s 39085 (43586)	Loss/tok 2.9322 (3.1445)	LR 1.250e-04
0: TRAIN [2][2830/3880]	Time 0.432 (0.322)	Data 1.49e-04 (3.22e-04)	Tok/s 54850 (43590)	Loss/tok 3.2521 (3.1447)	LR 1.250e-04
0: TRAIN [2][2840/3880]	Time 0.266 (0.322)	Data 2.35e-04 (3.21e-04)	Tok/s 38404 (43585)	Loss/tok 2.9570 (3.1447)	LR 1.250e-04
0: TRAIN [2][2850/3880]	Time 0.347 (0.322)	Data 1.38e-04 (3.21e-04)	Tok/s 48822 (43586)	Loss/tok 3.0436 (3.1448)	LR 1.250e-04
0: TRAIN [2][2860/3880]	Time 0.281 (0.322)	Data 1.64e-04 (3.20e-04)	Tok/s 37070 (43584)	Loss/tok 2.9341 (3.1445)	LR 1.250e-04
0: TRAIN [2][2870/3880]	Time 0.353 (0.322)	Data 1.31e-04 (3.19e-04)	Tok/s 47600 (43577)	Loss/tok 3.1967 (3.1444)	LR 1.250e-04
0: TRAIN [2][2880/3880]	Time 0.266 (0.322)	Data 1.36e-04 (3.19e-04)	Tok/s 38751 (43586)	Loss/tok 2.9671 (3.1444)	LR 1.250e-04
0: TRAIN [2][2890/3880]	Time 0.345 (0.322)	Data 2.67e-04 (3.18e-04)	Tok/s 49090 (43584)	Loss/tok 3.0915 (3.1443)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2900/3880]	Time 0.348 (0.322)	Data 1.30e-04 (3.18e-04)	Tok/s 48940 (43585)	Loss/tok 3.0118 (3.1442)	LR 1.250e-04
0: TRAIN [2][2910/3880]	Time 0.265 (0.322)	Data 1.69e-04 (3.17e-04)	Tok/s 39361 (43588)	Loss/tok 3.0273 (3.1440)	LR 1.250e-04
0: TRAIN [2][2920/3880]	Time 0.193 (0.322)	Data 1.27e-04 (3.16e-04)	Tok/s 27021 (43580)	Loss/tok 2.5676 (3.1443)	LR 1.250e-04
0: TRAIN [2][2930/3880]	Time 0.348 (0.322)	Data 1.37e-04 (3.16e-04)	Tok/s 47664 (43590)	Loss/tok 3.2353 (3.1446)	LR 1.250e-04
0: TRAIN [2][2940/3880]	Time 0.266 (0.322)	Data 1.63e-04 (3.15e-04)	Tok/s 38073 (43576)	Loss/tok 2.8901 (3.1442)	LR 1.250e-04
0: TRAIN [2][2950/3880]	Time 0.265 (0.322)	Data 1.82e-04 (3.15e-04)	Tok/s 38872 (43570)	Loss/tok 2.9158 (3.1443)	LR 1.250e-04
0: TRAIN [2][2960/3880]	Time 0.347 (0.322)	Data 1.75e-04 (3.14e-04)	Tok/s 48368 (43576)	Loss/tok 3.1406 (3.1444)	LR 1.250e-04
0: TRAIN [2][2970/3880]	Time 0.266 (0.322)	Data 1.33e-04 (3.14e-04)	Tok/s 38851 (43578)	Loss/tok 2.9794 (3.1447)	LR 1.250e-04
0: TRAIN [2][2980/3880]	Time 0.544 (0.322)	Data 1.31e-04 (3.13e-04)	Tok/s 54125 (43581)	Loss/tok 3.4101 (3.1448)	LR 1.250e-04
0: TRAIN [2][2990/3880]	Time 0.266 (0.322)	Data 1.34e-04 (3.12e-04)	Tok/s 37651 (43577)	Loss/tok 2.9609 (3.1445)	LR 1.250e-04
0: TRAIN [2][3000/3880]	Time 0.349 (0.322)	Data 1.41e-04 (3.12e-04)	Tok/s 48596 (43582)	Loss/tok 3.1802 (3.1445)	LR 1.250e-04
0: TRAIN [2][3010/3880]	Time 0.347 (0.322)	Data 1.34e-04 (3.11e-04)	Tok/s 48518 (43576)	Loss/tok 3.0978 (3.1444)	LR 1.250e-04
0: TRAIN [2][3020/3880]	Time 0.266 (0.322)	Data 1.40e-04 (3.11e-04)	Tok/s 38801 (43581)	Loss/tok 2.9308 (3.1443)	LR 1.250e-04
0: TRAIN [2][3030/3880]	Time 0.347 (0.322)	Data 1.52e-04 (3.10e-04)	Tok/s 48767 (43580)	Loss/tok 3.1716 (3.1443)	LR 1.250e-04
0: TRAIN [2][3040/3880]	Time 0.347 (0.322)	Data 1.35e-04 (3.10e-04)	Tok/s 48021 (43601)	Loss/tok 2.9726 (3.1444)	LR 1.250e-04
0: TRAIN [2][3050/3880]	Time 0.265 (0.322)	Data 1.32e-04 (3.09e-04)	Tok/s 38781 (43596)	Loss/tok 2.9156 (3.1441)	LR 1.250e-04
0: TRAIN [2][3060/3880]	Time 0.267 (0.322)	Data 1.35e-04 (3.09e-04)	Tok/s 38433 (43601)	Loss/tok 2.9381 (3.1441)	LR 1.250e-04
0: TRAIN [2][3070/3880]	Time 0.544 (0.322)	Data 1.69e-04 (3.08e-04)	Tok/s 54905 (43612)	Loss/tok 3.4543 (3.1447)	LR 1.250e-04
0: TRAIN [2][3080/3880]	Time 0.430 (0.322)	Data 1.32e-04 (3.08e-04)	Tok/s 53866 (43612)	Loss/tok 3.5304 (3.1449)	LR 1.250e-04
0: TRAIN [2][3090/3880]	Time 0.350 (0.322)	Data 1.32e-04 (3.07e-04)	Tok/s 47976 (43616)	Loss/tok 3.0922 (3.1451)	LR 1.250e-04
0: TRAIN [2][3100/3880]	Time 0.266 (0.322)	Data 1.37e-04 (3.07e-04)	Tok/s 38377 (43613)	Loss/tok 2.9544 (3.1449)	LR 1.250e-04
0: TRAIN [2][3110/3880]	Time 0.266 (0.322)	Data 1.34e-04 (3.06e-04)	Tok/s 37627 (43615)	Loss/tok 3.0220 (3.1449)	LR 1.250e-04
0: TRAIN [2][3120/3880]	Time 0.266 (0.322)	Data 1.69e-04 (3.06e-04)	Tok/s 38375 (43624)	Loss/tok 3.0893 (3.1450)	LR 1.250e-04
0: TRAIN [2][3130/3880]	Time 0.266 (0.322)	Data 1.34e-04 (3.05e-04)	Tok/s 38047 (43610)	Loss/tok 2.8526 (3.1447)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3140/3880]	Time 0.194 (0.322)	Data 1.30e-04 (3.05e-04)	Tok/s 27265 (43605)	Loss/tok 2.5542 (3.1447)	LR 1.250e-04
0: TRAIN [2][3150/3880]	Time 0.266 (0.322)	Data 1.34e-04 (3.04e-04)	Tok/s 38889 (43597)	Loss/tok 2.9464 (3.1446)	LR 1.250e-04
0: TRAIN [2][3160/3880]	Time 0.267 (0.322)	Data 1.60e-04 (3.04e-04)	Tok/s 39219 (43597)	Loss/tok 2.8966 (3.1446)	LR 1.250e-04
0: TRAIN [2][3170/3880]	Time 0.347 (0.322)	Data 1.31e-04 (3.03e-04)	Tok/s 48236 (43592)	Loss/tok 3.1279 (3.1444)	LR 1.250e-04
0: TRAIN [2][3180/3880]	Time 0.348 (0.322)	Data 1.29e-04 (3.03e-04)	Tok/s 48122 (43591)	Loss/tok 3.0892 (3.1444)	LR 1.250e-04
0: TRAIN [2][3190/3880]	Time 0.347 (0.322)	Data 1.32e-04 (3.02e-04)	Tok/s 48231 (43593)	Loss/tok 3.0374 (3.1443)	LR 1.250e-04
0: TRAIN [2][3200/3880]	Time 0.430 (0.322)	Data 1.32e-04 (3.02e-04)	Tok/s 53618 (43597)	Loss/tok 3.3883 (3.1442)	LR 1.250e-04
0: TRAIN [2][3210/3880]	Time 0.265 (0.322)	Data 1.29e-04 (3.02e-04)	Tok/s 38754 (43597)	Loss/tok 3.1627 (3.1444)	LR 1.250e-04
0: TRAIN [2][3220/3880]	Time 0.192 (0.322)	Data 1.74e-04 (3.01e-04)	Tok/s 28056 (43592)	Loss/tok 2.4951 (3.1442)	LR 1.250e-04
0: TRAIN [2][3230/3880]	Time 0.348 (0.322)	Data 1.64e-04 (3.01e-04)	Tok/s 48835 (43580)	Loss/tok 3.1772 (3.1439)	LR 1.250e-04
0: TRAIN [2][3240/3880]	Time 0.195 (0.322)	Data 1.21e-04 (3.00e-04)	Tok/s 26843 (43581)	Loss/tok 2.5094 (3.1442)	LR 1.250e-04
0: TRAIN [2][3250/3880]	Time 0.348 (0.322)	Data 1.75e-04 (3.00e-04)	Tok/s 48698 (43575)	Loss/tok 3.0525 (3.1440)	LR 1.250e-04
0: TRAIN [2][3260/3880]	Time 0.266 (0.322)	Data 1.85e-04 (2.99e-04)	Tok/s 39091 (43582)	Loss/tok 2.9930 (3.1444)	LR 1.250e-04
0: TRAIN [2][3270/3880]	Time 0.347 (0.322)	Data 2.04e-04 (2.99e-04)	Tok/s 48208 (43597)	Loss/tok 3.1735 (3.1447)	LR 1.250e-04
0: TRAIN [2][3280/3880]	Time 0.266 (0.322)	Data 1.28e-04 (2.99e-04)	Tok/s 39928 (43595)	Loss/tok 2.8396 (3.1445)	LR 1.250e-04
0: TRAIN [2][3290/3880]	Time 0.432 (0.322)	Data 1.40e-04 (2.98e-04)	Tok/s 53622 (43586)	Loss/tok 3.3648 (3.1444)	LR 1.250e-04
0: TRAIN [2][3300/3880]	Time 0.266 (0.322)	Data 1.33e-04 (2.98e-04)	Tok/s 38765 (43567)	Loss/tok 2.9220 (3.1439)	LR 1.250e-04
0: TRAIN [2][3310/3880]	Time 0.430 (0.322)	Data 1.81e-04 (3.01e-04)	Tok/s 54628 (43558)	Loss/tok 3.3582 (3.1437)	LR 1.250e-04
0: TRAIN [2][3320/3880]	Time 0.348 (0.322)	Data 1.56e-04 (3.00e-04)	Tok/s 47536 (43563)	Loss/tok 3.1775 (3.1438)	LR 1.250e-04
0: TRAIN [2][3330/3880]	Time 0.347 (0.322)	Data 1.34e-04 (3.00e-04)	Tok/s 48424 (43571)	Loss/tok 3.0837 (3.1440)	LR 1.250e-04
0: TRAIN [2][3340/3880]	Time 0.430 (0.322)	Data 1.66e-04 (2.99e-04)	Tok/s 54120 (43573)	Loss/tok 3.3021 (3.1440)	LR 1.250e-04
0: TRAIN [2][3350/3880]	Time 0.352 (0.322)	Data 1.43e-04 (2.99e-04)	Tok/s 48166 (43576)	Loss/tok 3.1013 (3.1440)	LR 1.250e-04
0: TRAIN [2][3360/3880]	Time 0.266 (0.322)	Data 1.39e-04 (2.98e-04)	Tok/s 39223 (43569)	Loss/tok 2.9323 (3.1438)	LR 1.250e-04
0: TRAIN [2][3370/3880]	Time 0.430 (0.322)	Data 1.25e-04 (2.98e-04)	Tok/s 53829 (43568)	Loss/tok 3.3167 (3.1436)	LR 1.250e-04
0: TRAIN [2][3380/3880]	Time 0.266 (0.322)	Data 1.50e-04 (2.98e-04)	Tok/s 38935 (43570)	Loss/tok 2.9360 (3.1437)	LR 1.250e-04
0: TRAIN [2][3390/3880]	Time 0.266 (0.322)	Data 1.32e-04 (2.97e-04)	Tok/s 37903 (43564)	Loss/tok 2.9027 (3.1435)	LR 1.250e-04
0: TRAIN [2][3400/3880]	Time 0.266 (0.322)	Data 1.38e-04 (2.97e-04)	Tok/s 38689 (43574)	Loss/tok 2.8255 (3.1436)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3410/3880]	Time 0.266 (0.322)	Data 1.33e-04 (2.96e-04)	Tok/s 38323 (43583)	Loss/tok 3.0994 (3.1437)	LR 1.250e-04
0: TRAIN [2][3420/3880]	Time 0.431 (0.322)	Data 1.36e-04 (2.96e-04)	Tok/s 53431 (43581)	Loss/tok 3.3895 (3.1436)	LR 1.250e-04
0: TRAIN [2][3430/3880]	Time 0.195 (0.322)	Data 1.58e-04 (2.96e-04)	Tok/s 27029 (43577)	Loss/tok 2.5121 (3.1435)	LR 1.250e-04
0: TRAIN [2][3440/3880]	Time 0.347 (0.322)	Data 1.25e-04 (2.96e-04)	Tok/s 47783 (43574)	Loss/tok 3.1658 (3.1432)	LR 1.250e-04
0: TRAIN [2][3450/3880]	Time 0.431 (0.322)	Data 1.20e-04 (2.96e-04)	Tok/s 54407 (43571)	Loss/tok 3.3239 (3.1430)	LR 1.250e-04
0: TRAIN [2][3460/3880]	Time 0.347 (0.322)	Data 1.34e-04 (2.95e-04)	Tok/s 48242 (43572)	Loss/tok 3.0554 (3.1429)	LR 1.250e-04
0: TRAIN [2][3470/3880]	Time 0.544 (0.322)	Data 1.97e-04 (2.95e-04)	Tok/s 54991 (43573)	Loss/tok 3.4192 (3.1430)	LR 1.250e-04
0: TRAIN [2][3480/3880]	Time 0.431 (0.322)	Data 1.32e-04 (2.94e-04)	Tok/s 54733 (43569)	Loss/tok 3.2874 (3.1430)	LR 1.250e-04
0: TRAIN [2][3490/3880]	Time 0.267 (0.322)	Data 1.31e-04 (2.94e-04)	Tok/s 38522 (43568)	Loss/tok 2.9080 (3.1430)	LR 1.250e-04
0: TRAIN [2][3500/3880]	Time 0.265 (0.322)	Data 1.51e-04 (2.93e-04)	Tok/s 40680 (43573)	Loss/tok 2.9461 (3.1431)	LR 1.250e-04
0: TRAIN [2][3510/3880]	Time 0.347 (0.322)	Data 1.43e-04 (2.93e-04)	Tok/s 48386 (43568)	Loss/tok 3.2345 (3.1430)	LR 1.250e-04
0: TRAIN [2][3520/3880]	Time 0.347 (0.321)	Data 1.73e-04 (2.93e-04)	Tok/s 48702 (43563)	Loss/tok 3.1613 (3.1427)	LR 1.250e-04
0: TRAIN [2][3530/3880]	Time 0.347 (0.321)	Data 1.42e-04 (2.92e-04)	Tok/s 49048 (43558)	Loss/tok 3.0937 (3.1424)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3540/3880]	Time 0.347 (0.321)	Data 1.75e-04 (2.92e-04)	Tok/s 48504 (43556)	Loss/tok 3.1670 (3.1422)	LR 1.250e-04
0: TRAIN [2][3550/3880]	Time 0.429 (0.321)	Data 1.68e-04 (2.91e-04)	Tok/s 54141 (43562)	Loss/tok 3.3879 (3.1426)	LR 1.250e-04
0: TRAIN [2][3560/3880]	Time 0.544 (0.322)	Data 1.33e-04 (2.91e-04)	Tok/s 55003 (43567)	Loss/tok 3.5268 (3.1428)	LR 1.250e-04
0: TRAIN [2][3570/3880]	Time 0.431 (0.322)	Data 1.62e-04 (2.91e-04)	Tok/s 54181 (43581)	Loss/tok 3.3329 (3.1429)	LR 1.250e-04
0: TRAIN [2][3580/3880]	Time 0.347 (0.322)	Data 1.34e-04 (2.90e-04)	Tok/s 47265 (43578)	Loss/tok 3.1745 (3.1429)	LR 1.250e-04
0: TRAIN [2][3590/3880]	Time 0.346 (0.322)	Data 1.49e-04 (2.90e-04)	Tok/s 48234 (43578)	Loss/tok 3.1550 (3.1428)	LR 1.250e-04
0: TRAIN [2][3600/3880]	Time 0.543 (0.322)	Data 1.34e-04 (2.89e-04)	Tok/s 54497 (43585)	Loss/tok 3.5076 (3.1432)	LR 1.250e-04
0: TRAIN [2][3610/3880]	Time 0.267 (0.322)	Data 1.36e-04 (2.89e-04)	Tok/s 38873 (43578)	Loss/tok 2.8272 (3.1430)	LR 1.250e-04
0: TRAIN [2][3620/3880]	Time 0.266 (0.322)	Data 1.32e-04 (2.89e-04)	Tok/s 38345 (43584)	Loss/tok 2.9322 (3.1429)	LR 1.250e-04
0: TRAIN [2][3630/3880]	Time 0.266 (0.322)	Data 1.35e-04 (2.88e-04)	Tok/s 39162 (43583)	Loss/tok 2.8287 (3.1427)	LR 1.250e-04
0: TRAIN [2][3640/3880]	Time 0.266 (0.322)	Data 2.16e-04 (2.88e-04)	Tok/s 38678 (43570)	Loss/tok 2.8961 (3.1425)	LR 1.250e-04
0: TRAIN [2][3650/3880]	Time 0.348 (0.321)	Data 1.29e-04 (2.87e-04)	Tok/s 48133 (43567)	Loss/tok 3.1918 (3.1423)	LR 1.250e-04
0: TRAIN [2][3660/3880]	Time 0.266 (0.321)	Data 1.28e-04 (2.87e-04)	Tok/s 39167 (43554)	Loss/tok 2.8606 (3.1420)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3670/3880]	Time 0.544 (0.321)	Data 1.51e-04 (2.87e-04)	Tok/s 54435 (43547)	Loss/tok 3.4831 (3.1419)	LR 1.250e-04
0: TRAIN [2][3680/3880]	Time 0.266 (0.321)	Data 1.44e-04 (2.86e-04)	Tok/s 39216 (43552)	Loss/tok 2.9565 (3.1418)	LR 1.250e-04
0: TRAIN [2][3690/3880]	Time 0.545 (0.321)	Data 1.71e-04 (2.86e-04)	Tok/s 53673 (43552)	Loss/tok 3.4885 (3.1418)	LR 1.250e-04
0: TRAIN [2][3700/3880]	Time 0.266 (0.321)	Data 1.35e-04 (2.86e-04)	Tok/s 38557 (43545)	Loss/tok 2.9101 (3.1417)	LR 1.250e-04
0: TRAIN [2][3710/3880]	Time 0.432 (0.321)	Data 1.51e-04 (2.85e-04)	Tok/s 54146 (43548)	Loss/tok 3.3527 (3.1422)	LR 1.250e-04
0: TRAIN [2][3720/3880]	Time 0.432 (0.321)	Data 1.67e-04 (2.86e-04)	Tok/s 53975 (43548)	Loss/tok 3.3492 (3.1422)	LR 1.250e-04
0: TRAIN [2][3730/3880]	Time 0.543 (0.321)	Data 1.65e-04 (2.86e-04)	Tok/s 55134 (43549)	Loss/tok 3.3387 (3.1423)	LR 1.250e-04
0: TRAIN [2][3740/3880]	Time 0.347 (0.321)	Data 1.32e-04 (2.86e-04)	Tok/s 48566 (43542)	Loss/tok 3.0718 (3.1421)	LR 1.250e-04
0: TRAIN [2][3750/3880]	Time 0.347 (0.321)	Data 1.24e-04 (2.85e-04)	Tok/s 48603 (43548)	Loss/tok 3.1030 (3.1421)	LR 1.250e-04
0: TRAIN [2][3760/3880]	Time 0.265 (0.321)	Data 1.90e-04 (2.85e-04)	Tok/s 39656 (43555)	Loss/tok 2.9812 (3.1424)	LR 1.250e-04
0: TRAIN [2][3770/3880]	Time 0.543 (0.321)	Data 1.39e-04 (2.84e-04)	Tok/s 55648 (43554)	Loss/tok 3.4210 (3.1422)	LR 1.250e-04
0: TRAIN [2][3780/3880]	Time 0.347 (0.321)	Data 1.41e-04 (2.84e-04)	Tok/s 48351 (43547)	Loss/tok 3.1461 (3.1421)	LR 1.250e-04
0: TRAIN [2][3790/3880]	Time 0.348 (0.321)	Data 1.30e-04 (2.84e-04)	Tok/s 48271 (43548)	Loss/tok 3.0644 (3.1420)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3800/3880]	Time 0.430 (0.322)	Data 1.88e-04 (2.83e-04)	Tok/s 53617 (43566)	Loss/tok 3.2167 (3.1424)	LR 1.250e-04
0: TRAIN [2][3810/3880]	Time 0.194 (0.321)	Data 1.35e-04 (2.83e-04)	Tok/s 27499 (43551)	Loss/tok 2.5648 (3.1421)	LR 1.250e-04
0: TRAIN [2][3820/3880]	Time 0.433 (0.322)	Data 1.25e-04 (2.83e-04)	Tok/s 53738 (43560)	Loss/tok 3.2559 (3.1425)	LR 1.250e-04
0: TRAIN [2][3830/3880]	Time 0.431 (0.322)	Data 1.53e-04 (2.82e-04)	Tok/s 54325 (43564)	Loss/tok 3.3926 (3.1424)	LR 1.250e-04
0: TRAIN [2][3840/3880]	Time 0.266 (0.321)	Data 1.81e-04 (2.82e-04)	Tok/s 38565 (43553)	Loss/tok 2.8491 (3.1422)	LR 1.250e-04
0: TRAIN [2][3850/3880]	Time 0.346 (0.321)	Data 1.37e-04 (2.82e-04)	Tok/s 48001 (43549)	Loss/tok 3.1173 (3.1420)	LR 1.250e-04
0: TRAIN [2][3860/3880]	Time 0.430 (0.321)	Data 1.37e-04 (2.81e-04)	Tok/s 53962 (43544)	Loss/tok 3.3445 (3.1419)	LR 1.250e-04
0: TRAIN [2][3870/3880]	Time 0.430 (0.321)	Data 1.37e-04 (2.81e-04)	Tok/s 53092 (43548)	Loss/tok 3.3830 (3.1419)	LR 1.250e-04
:::MLL 1576200244.363 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1576200244.364 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/6]	Time 0.996 (0.996)	Decoder iters 111.0 (111.0)	Tok/s 16563 (16563)
0: Running moses detokenizer
0: BLEU(score=23.710367123942433, counts=[37018, 18486, 10470, 6197], totals=[65817, 62814, 59811, 56814], precisions=[56.24382758253947, 29.429744961314356, 17.505141194763507, 10.907522793677614], bp=1.0, sys_len=65817, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1576200248.658 eval_accuracy: {"value": 23.71, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1576200248.659 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.1417	Test BLEU: 23.71
0: Performance: Epoch: 2	Training: 174138 Tok/s
0: Finished epoch 2
:::MLL 1576200248.659 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1576200248.659 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1576200248.660 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 4162075815
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][0/3880]	Time 0.818 (0.818)	Data 3.34e-01 (3.34e-01)	Tok/s 20811 (20811)	Loss/tok 3.1086 (3.1086)	LR 1.250e-04
0: TRAIN [3][10/3880]	Time 0.267 (0.303)	Data 1.32e-04 (3.05e-02)	Tok/s 38020 (34732)	Loss/tok 2.9103 (2.9022)	LR 1.250e-04
0: TRAIN [3][20/3880]	Time 0.266 (0.294)	Data 1.30e-04 (1.60e-02)	Tok/s 39741 (37226)	Loss/tok 3.0080 (2.9636)	LR 1.250e-04
0: TRAIN [3][30/3880]	Time 0.265 (0.293)	Data 1.43e-04 (1.09e-02)	Tok/s 38941 (38551)	Loss/tok 2.9149 (2.9811)	LR 1.250e-04
0: TRAIN [3][40/3880]	Time 0.545 (0.303)	Data 1.36e-04 (8.28e-03)	Tok/s 54326 (39480)	Loss/tok 3.4852 (3.0436)	LR 1.250e-04
0: TRAIN [3][50/3880]	Time 0.265 (0.299)	Data 1.50e-04 (6.68e-03)	Tok/s 39218 (39637)	Loss/tok 3.0031 (3.0320)	LR 1.250e-04
0: TRAIN [3][60/3880]	Time 0.347 (0.302)	Data 1.21e-04 (5.61e-03)	Tok/s 48629 (40265)	Loss/tok 3.0842 (3.0410)	LR 1.250e-04
0: TRAIN [3][70/3880]	Time 0.351 (0.302)	Data 1.40e-04 (4.84e-03)	Tok/s 48256 (40409)	Loss/tok 3.0130 (3.0438)	LR 1.250e-04
0: TRAIN [3][80/3880]	Time 0.267 (0.304)	Data 1.33e-04 (4.26e-03)	Tok/s 39233 (40743)	Loss/tok 2.8789 (3.0504)	LR 1.250e-04
0: TRAIN [3][90/3880]	Time 0.269 (0.303)	Data 1.29e-04 (3.81e-03)	Tok/s 38078 (40832)	Loss/tok 2.9625 (3.0463)	LR 1.250e-04
0: TRAIN [3][100/3880]	Time 0.265 (0.305)	Data 1.23e-04 (3.44e-03)	Tok/s 39153 (41231)	Loss/tok 2.8617 (3.0511)	LR 1.250e-04
0: TRAIN [3][110/3880]	Time 0.348 (0.304)	Data 1.39e-04 (3.15e-03)	Tok/s 47893 (41300)	Loss/tok 3.1381 (3.0544)	LR 1.250e-04
0: TRAIN [3][120/3880]	Time 0.266 (0.304)	Data 1.37e-04 (2.91e-03)	Tok/s 38238 (41346)	Loss/tok 2.8854 (3.0536)	LR 1.250e-04
0: TRAIN [3][130/3880]	Time 0.193 (0.305)	Data 1.49e-04 (2.70e-03)	Tok/s 26955 (41540)	Loss/tok 2.5658 (3.0557)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][140/3880]	Time 0.428 (0.309)	Data 1.56e-04 (2.52e-03)	Tok/s 54916 (41932)	Loss/tok 3.2159 (3.0671)	LR 1.250e-04
0: TRAIN [3][150/3880]	Time 0.348 (0.309)	Data 1.30e-04 (2.36e-03)	Tok/s 47867 (42001)	Loss/tok 3.1236 (3.0685)	LR 1.250e-04
0: TRAIN [3][160/3880]	Time 0.265 (0.309)	Data 1.55e-04 (2.22e-03)	Tok/s 39448 (42063)	Loss/tok 2.8939 (3.0677)	LR 1.250e-04
0: TRAIN [3][170/3880]	Time 0.349 (0.314)	Data 1.51e-04 (2.10e-03)	Tok/s 49099 (42328)	Loss/tok 3.0632 (3.0835)	LR 1.250e-04
0: TRAIN [3][180/3880]	Time 0.545 (0.316)	Data 1.29e-04 (1.99e-03)	Tok/s 54537 (42506)	Loss/tok 3.4275 (3.0894)	LR 1.250e-04
0: TRAIN [3][190/3880]	Time 0.192 (0.314)	Data 1.62e-04 (1.90e-03)	Tok/s 28718 (42382)	Loss/tok 2.5281 (3.0837)	LR 1.250e-04
0: TRAIN [3][200/3880]	Time 0.347 (0.315)	Data 1.38e-04 (1.81e-03)	Tok/s 47819 (42523)	Loss/tok 3.1534 (3.0845)	LR 1.250e-04
0: TRAIN [3][210/3880]	Time 0.267 (0.315)	Data 1.35e-04 (1.73e-03)	Tok/s 38407 (42557)	Loss/tok 2.8789 (3.0897)	LR 1.250e-04
0: TRAIN [3][220/3880]	Time 0.266 (0.315)	Data 1.50e-04 (1.66e-03)	Tok/s 39395 (42522)	Loss/tok 3.0198 (3.0875)	LR 1.250e-04
0: TRAIN [3][230/3880]	Time 0.267 (0.315)	Data 1.30e-04 (1.59e-03)	Tok/s 38638 (42594)	Loss/tok 2.9013 (3.0898)	LR 1.250e-04
0: TRAIN [3][240/3880]	Time 0.348 (0.315)	Data 1.35e-04 (1.53e-03)	Tok/s 48318 (42662)	Loss/tok 2.9651 (3.0875)	LR 1.250e-04
0: TRAIN [3][250/3880]	Time 0.265 (0.316)	Data 1.70e-04 (1.48e-03)	Tok/s 39060 (42719)	Loss/tok 2.8179 (3.0884)	LR 1.250e-04
0: TRAIN [3][260/3880]	Time 0.431 (0.317)	Data 1.30e-04 (1.43e-03)	Tok/s 54034 (42737)	Loss/tok 3.3873 (3.0919)	LR 1.250e-04
0: TRAIN [3][270/3880]	Time 0.270 (0.317)	Data 1.32e-04 (1.38e-03)	Tok/s 37930 (42809)	Loss/tok 2.9201 (3.0921)	LR 1.250e-04
0: TRAIN [3][280/3880]	Time 0.347 (0.318)	Data 1.39e-04 (1.34e-03)	Tok/s 48276 (42834)	Loss/tok 3.1894 (3.0928)	LR 1.250e-04
0: TRAIN [3][290/3880]	Time 0.431 (0.318)	Data 1.70e-04 (1.29e-03)	Tok/s 54253 (42862)	Loss/tok 3.2924 (3.0937)	LR 1.250e-04
0: TRAIN [3][300/3880]	Time 0.265 (0.318)	Data 1.75e-04 (1.26e-03)	Tok/s 38908 (42938)	Loss/tok 3.0264 (3.0950)	LR 1.250e-04
0: TRAIN [3][310/3880]	Time 0.431 (0.320)	Data 1.57e-04 (1.22e-03)	Tok/s 53808 (43094)	Loss/tok 3.3035 (3.0990)	LR 1.250e-04
0: TRAIN [3][320/3880]	Time 0.347 (0.321)	Data 1.29e-04 (1.19e-03)	Tok/s 48014 (43134)	Loss/tok 3.0990 (3.1028)	LR 1.250e-04
0: TRAIN [3][330/3880]	Time 0.430 (0.321)	Data 1.32e-04 (1.16e-03)	Tok/s 53869 (43107)	Loss/tok 3.3465 (3.1057)	LR 1.250e-04
0: TRAIN [3][340/3880]	Time 0.266 (0.321)	Data 1.29e-04 (1.13e-03)	Tok/s 39800 (43129)	Loss/tok 2.9006 (3.1061)	LR 1.250e-04
0: TRAIN [3][350/3880]	Time 0.265 (0.321)	Data 1.27e-04 (1.10e-03)	Tok/s 39651 (43116)	Loss/tok 2.9406 (3.1034)	LR 1.250e-04
0: TRAIN [3][360/3880]	Time 0.431 (0.322)	Data 1.36e-04 (1.07e-03)	Tok/s 54678 (43223)	Loss/tok 3.1646 (3.1077)	LR 1.250e-04
0: TRAIN [3][370/3880]	Time 0.346 (0.322)	Data 1.27e-04 (1.05e-03)	Tok/s 47999 (43227)	Loss/tok 3.0565 (3.1049)	LR 1.250e-04
0: TRAIN [3][380/3880]	Time 0.265 (0.322)	Data 1.82e-04 (1.03e-03)	Tok/s 38430 (43207)	Loss/tok 2.9874 (3.1052)	LR 1.250e-04
0: TRAIN [3][390/3880]	Time 0.430 (0.322)	Data 1.34e-04 (1.00e-03)	Tok/s 54357 (43277)	Loss/tok 3.3302 (3.1059)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][400/3880]	Time 0.431 (0.322)	Data 1.32e-04 (9.82e-04)	Tok/s 54862 (43374)	Loss/tok 3.3053 (3.1074)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][410/3880]	Time 0.266 (0.322)	Data 1.35e-04 (9.62e-04)	Tok/s 38496 (43332)	Loss/tok 2.9718 (3.1077)	LR 1.250e-04
0: TRAIN [3][420/3880]	Time 0.348 (0.322)	Data 1.45e-04 (9.43e-04)	Tok/s 48806 (43343)	Loss/tok 3.1030 (3.1072)	LR 1.250e-04
0: TRAIN [3][430/3880]	Time 0.347 (0.323)	Data 1.29e-04 (9.24e-04)	Tok/s 48538 (43427)	Loss/tok 3.1087 (3.1109)	LR 1.250e-04
0: TRAIN [3][440/3880]	Time 0.542 (0.324)	Data 2.79e-04 (9.07e-04)	Tok/s 55058 (43458)	Loss/tok 3.3323 (3.1111)	LR 1.250e-04
0: TRAIN [3][450/3880]	Time 0.348 (0.324)	Data 1.29e-04 (8.90e-04)	Tok/s 48491 (43548)	Loss/tok 3.0488 (3.1110)	LR 1.250e-04
0: TRAIN [3][460/3880]	Time 0.266 (0.324)	Data 1.72e-04 (8.74e-04)	Tok/s 39327 (43514)	Loss/tok 3.0099 (3.1105)	LR 1.250e-04
0: TRAIN [3][470/3880]	Time 0.542 (0.324)	Data 1.19e-04 (8.58e-04)	Tok/s 55375 (43497)	Loss/tok 3.4221 (3.1110)	LR 1.250e-04
0: TRAIN [3][480/3880]	Time 0.347 (0.325)	Data 1.73e-04 (8.43e-04)	Tok/s 48441 (43583)	Loss/tok 3.0942 (3.1117)	LR 1.250e-04
0: TRAIN [3][490/3880]	Time 0.197 (0.324)	Data 1.66e-04 (8.30e-04)	Tok/s 26816 (43573)	Loss/tok 2.5608 (3.1114)	LR 1.250e-04
0: TRAIN [3][500/3880]	Time 0.267 (0.325)	Data 1.86e-04 (8.16e-04)	Tok/s 39497 (43621)	Loss/tok 3.0362 (3.1139)	LR 1.250e-04
0: TRAIN [3][510/3880]	Time 0.544 (0.327)	Data 1.42e-04 (8.03e-04)	Tok/s 55068 (43735)	Loss/tok 3.3964 (3.1184)	LR 1.250e-04
0: TRAIN [3][520/3880]	Time 0.193 (0.326)	Data 2.17e-04 (7.91e-04)	Tok/s 27214 (43741)	Loss/tok 2.6025 (3.1179)	LR 1.250e-04
0: TRAIN [3][530/3880]	Time 0.347 (0.326)	Data 1.31e-04 (7.78e-04)	Tok/s 48149 (43744)	Loss/tok 3.1566 (3.1170)	LR 1.250e-04
0: TRAIN [3][540/3880]	Time 0.430 (0.325)	Data 1.44e-04 (7.67e-04)	Tok/s 53596 (43656)	Loss/tok 3.3191 (3.1157)	LR 1.250e-04
0: TRAIN [3][550/3880]	Time 0.432 (0.325)	Data 1.37e-04 (7.55e-04)	Tok/s 52805 (43698)	Loss/tok 3.3770 (3.1155)	LR 1.250e-04
0: TRAIN [3][560/3880]	Time 0.265 (0.325)	Data 1.28e-04 (7.45e-04)	Tok/s 39081 (43683)	Loss/tok 3.0161 (3.1144)	LR 1.250e-04
0: TRAIN [3][570/3880]	Time 0.544 (0.326)	Data 1.30e-04 (7.34e-04)	Tok/s 53675 (43781)	Loss/tok 3.4743 (3.1180)	LR 1.250e-04
0: TRAIN [3][580/3880]	Time 0.544 (0.327)	Data 1.41e-04 (7.24e-04)	Tok/s 54753 (43793)	Loss/tok 3.4261 (3.1198)	LR 1.250e-04
0: TRAIN [3][590/3880]	Time 0.266 (0.327)	Data 1.35e-04 (7.15e-04)	Tok/s 38261 (43778)	Loss/tok 2.9560 (3.1189)	LR 1.250e-04
0: TRAIN [3][600/3880]	Time 0.266 (0.326)	Data 1.52e-04 (7.05e-04)	Tok/s 38865 (43734)	Loss/tok 2.8602 (3.1175)	LR 1.250e-04
0: TRAIN [3][610/3880]	Time 0.266 (0.326)	Data 1.50e-04 (6.96e-04)	Tok/s 39050 (43727)	Loss/tok 2.8391 (3.1170)	LR 1.250e-04
0: TRAIN [3][620/3880]	Time 0.347 (0.326)	Data 1.43e-04 (6.87e-04)	Tok/s 48883 (43766)	Loss/tok 3.0216 (3.1167)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][630/3880]	Time 0.347 (0.326)	Data 1.72e-04 (6.79e-04)	Tok/s 47473 (43766)	Loss/tok 3.1215 (3.1167)	LR 1.250e-04
0: TRAIN [3][640/3880]	Time 0.265 (0.326)	Data 1.51e-04 (6.70e-04)	Tok/s 39859 (43791)	Loss/tok 2.7454 (3.1173)	LR 1.250e-04
0: TRAIN [3][650/3880]	Time 0.192 (0.325)	Data 1.31e-04 (6.62e-04)	Tok/s 27848 (43708)	Loss/tok 2.4756 (3.1160)	LR 1.250e-04
0: TRAIN [3][660/3880]	Time 0.267 (0.325)	Data 1.34e-04 (6.55e-04)	Tok/s 38377 (43612)	Loss/tok 2.8778 (3.1151)	LR 1.250e-04
0: TRAIN [3][670/3880]	Time 0.347 (0.324)	Data 1.71e-04 (6.47e-04)	Tok/s 47976 (43571)	Loss/tok 3.1309 (3.1140)	LR 1.250e-04
0: TRAIN [3][680/3880]	Time 0.268 (0.323)	Data 1.35e-04 (6.40e-04)	Tok/s 38057 (43543)	Loss/tok 2.8573 (3.1128)	LR 1.250e-04
0: TRAIN [3][690/3880]	Time 0.196 (0.323)	Data 1.66e-04 (6.33e-04)	Tok/s 26694 (43482)	Loss/tok 2.6369 (3.1115)	LR 1.250e-04
0: TRAIN [3][700/3880]	Time 0.266 (0.322)	Data 1.32e-04 (6.26e-04)	Tok/s 38767 (43473)	Loss/tok 2.9566 (3.1105)	LR 1.250e-04
0: TRAIN [3][710/3880]	Time 0.266 (0.322)	Data 1.33e-04 (6.19e-04)	Tok/s 38627 (43417)	Loss/tok 2.9316 (3.1095)	LR 1.250e-04
0: TRAIN [3][720/3880]	Time 0.265 (0.321)	Data 1.33e-04 (6.12e-04)	Tok/s 38510 (43375)	Loss/tok 2.8458 (3.1079)	LR 1.250e-04
0: TRAIN [3][730/3880]	Time 0.345 (0.321)	Data 1.44e-04 (6.07e-04)	Tok/s 48694 (43384)	Loss/tok 3.0028 (3.1076)	LR 1.250e-04
0: TRAIN [3][740/3880]	Time 0.433 (0.321)	Data 1.77e-04 (6.01e-04)	Tok/s 53543 (43356)	Loss/tok 3.2708 (3.1078)	LR 1.250e-04
0: TRAIN [3][750/3880]	Time 0.348 (0.322)	Data 1.48e-04 (5.95e-04)	Tok/s 48730 (43421)	Loss/tok 3.1001 (3.1085)	LR 1.250e-04
0: TRAIN [3][760/3880]	Time 0.194 (0.322)	Data 1.30e-04 (5.89e-04)	Tok/s 27070 (43433)	Loss/tok 2.6055 (3.1094)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][770/3880]	Time 0.431 (0.322)	Data 1.96e-04 (5.83e-04)	Tok/s 53275 (43471)	Loss/tok 3.2828 (3.1103)	LR 1.250e-04
0: TRAIN [3][780/3880]	Time 0.347 (0.323)	Data 1.78e-04 (5.78e-04)	Tok/s 48667 (43512)	Loss/tok 3.2096 (3.1112)	LR 1.250e-04
0: TRAIN [3][790/3880]	Time 0.267 (0.323)	Data 1.66e-04 (5.73e-04)	Tok/s 39145 (43528)	Loss/tok 2.8428 (3.1109)	LR 1.250e-04
0: TRAIN [3][800/3880]	Time 0.193 (0.323)	Data 1.28e-04 (5.67e-04)	Tok/s 27498 (43538)	Loss/tok 2.5413 (3.1115)	LR 1.250e-04
0: TRAIN [3][810/3880]	Time 0.348 (0.323)	Data 1.29e-04 (5.62e-04)	Tok/s 48553 (43542)	Loss/tok 3.2240 (3.1110)	LR 1.250e-04
0: TRAIN [3][820/3880]	Time 0.266 (0.323)	Data 1.34e-04 (5.57e-04)	Tok/s 39246 (43498)	Loss/tok 2.8836 (3.1097)	LR 1.250e-04
0: TRAIN [3][830/3880]	Time 0.266 (0.322)	Data 1.43e-04 (5.52e-04)	Tok/s 39024 (43496)	Loss/tok 2.8864 (3.1089)	LR 1.250e-04
0: TRAIN [3][840/3880]	Time 0.266 (0.322)	Data 1.48e-04 (5.47e-04)	Tok/s 37789 (43489)	Loss/tok 2.9513 (3.1084)	LR 1.250e-04
0: TRAIN [3][850/3880]	Time 0.431 (0.323)	Data 1.37e-04 (5.43e-04)	Tok/s 54319 (43549)	Loss/tok 3.2094 (3.1090)	LR 1.250e-04
0: TRAIN [3][860/3880]	Time 0.347 (0.323)	Data 1.29e-04 (5.38e-04)	Tok/s 48272 (43574)	Loss/tok 3.1400 (3.1108)	LR 1.250e-04
0: TRAIN [3][870/3880]	Time 0.347 (0.323)	Data 1.32e-04 (5.33e-04)	Tok/s 48618 (43580)	Loss/tok 3.1297 (3.1103)	LR 1.250e-04
0: TRAIN [3][880/3880]	Time 0.266 (0.323)	Data 1.30e-04 (5.29e-04)	Tok/s 38712 (43565)	Loss/tok 2.9321 (3.1096)	LR 1.250e-04
0: TRAIN [3][890/3880]	Time 0.266 (0.323)	Data 1.42e-04 (5.25e-04)	Tok/s 38964 (43556)	Loss/tok 3.0341 (3.1106)	LR 1.250e-04
0: TRAIN [3][900/3880]	Time 0.266 (0.323)	Data 1.32e-04 (5.20e-04)	Tok/s 38161 (43583)	Loss/tok 2.9185 (3.1114)	LR 1.250e-04
0: TRAIN [3][910/3880]	Time 0.265 (0.323)	Data 1.92e-04 (5.16e-04)	Tok/s 39003 (43591)	Loss/tok 2.9599 (3.1113)	LR 1.250e-04
0: TRAIN [3][920/3880]	Time 0.544 (0.324)	Data 1.39e-04 (5.13e-04)	Tok/s 54438 (43649)	Loss/tok 3.5423 (3.1133)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][930/3880]	Time 0.266 (0.324)	Data 1.87e-04 (5.09e-04)	Tok/s 38742 (43692)	Loss/tok 2.8536 (3.1141)	LR 1.250e-04
0: TRAIN [3][940/3880]	Time 0.348 (0.324)	Data 1.39e-04 (5.06e-04)	Tok/s 48837 (43648)	Loss/tok 3.0734 (3.1133)	LR 1.250e-04
0: TRAIN [3][950/3880]	Time 0.276 (0.324)	Data 1.33e-04 (5.02e-04)	Tok/s 37809 (43628)	Loss/tok 2.9234 (3.1132)	LR 1.250e-04
0: TRAIN [3][960/3880]	Time 0.552 (0.324)	Data 1.28e-04 (4.98e-04)	Tok/s 54612 (43629)	Loss/tok 3.5208 (3.1134)	LR 1.250e-04
0: TRAIN [3][970/3880]	Time 0.348 (0.324)	Data 1.27e-04 (4.95e-04)	Tok/s 47771 (43628)	Loss/tok 3.1592 (3.1127)	LR 1.250e-04
0: TRAIN [3][980/3880]	Time 0.265 (0.323)	Data 1.78e-04 (4.92e-04)	Tok/s 38841 (43612)	Loss/tok 2.8842 (3.1121)	LR 1.250e-04
0: TRAIN [3][990/3880]	Time 0.267 (0.323)	Data 1.39e-04 (4.88e-04)	Tok/s 38425 (43580)	Loss/tok 2.9220 (3.1119)	LR 1.250e-04
0: TRAIN [3][1000/3880]	Time 0.347 (0.323)	Data 2.06e-04 (4.85e-04)	Tok/s 48389 (43579)	Loss/tok 3.1091 (3.1113)	LR 1.250e-04
0: TRAIN [3][1010/3880]	Time 0.192 (0.323)	Data 1.39e-04 (4.82e-04)	Tok/s 27152 (43570)	Loss/tok 2.6459 (3.1113)	LR 1.250e-04
0: TRAIN [3][1020/3880]	Time 0.545 (0.323)	Data 1.34e-04 (4.78e-04)	Tok/s 54441 (43550)	Loss/tok 3.5477 (3.1117)	LR 1.250e-04
0: TRAIN [3][1030/3880]	Time 0.431 (0.323)	Data 1.37e-04 (4.75e-04)	Tok/s 54532 (43576)	Loss/tok 3.1560 (3.1120)	LR 1.250e-04
0: TRAIN [3][1040/3880]	Time 0.266 (0.323)	Data 1.36e-04 (4.72e-04)	Tok/s 38853 (43572)	Loss/tok 2.8689 (3.1119)	LR 1.250e-04
0: TRAIN [3][1050/3880]	Time 0.545 (0.323)	Data 1.45e-04 (4.69e-04)	Tok/s 55320 (43579)	Loss/tok 3.4664 (3.1127)	LR 1.250e-04
0: TRAIN [3][1060/3880]	Time 0.266 (0.323)	Data 1.34e-04 (4.66e-04)	Tok/s 38261 (43604)	Loss/tok 2.8812 (3.1122)	LR 1.250e-04
0: TRAIN [3][1070/3880]	Time 0.266 (0.323)	Data 1.40e-04 (4.63e-04)	Tok/s 38903 (43604)	Loss/tok 2.9978 (3.1115)	LR 1.250e-04
0: TRAIN [3][1080/3880]	Time 0.265 (0.323)	Data 3.10e-04 (4.60e-04)	Tok/s 39554 (43611)	Loss/tok 2.9469 (3.1109)	LR 1.250e-04
0: TRAIN [3][1090/3880]	Time 0.347 (0.323)	Data 1.32e-04 (4.57e-04)	Tok/s 48059 (43628)	Loss/tok 3.0350 (3.1112)	LR 1.250e-04
0: TRAIN [3][1100/3880]	Time 0.192 (0.323)	Data 1.49e-04 (4.54e-04)	Tok/s 27254 (43633)	Loss/tok 2.5775 (3.1115)	LR 1.250e-04
0: TRAIN [3][1110/3880]	Time 0.266 (0.323)	Data 1.41e-04 (4.51e-04)	Tok/s 38648 (43604)	Loss/tok 2.8584 (3.1103)	LR 1.250e-04
0: TRAIN [3][1120/3880]	Time 0.432 (0.323)	Data 1.31e-04 (4.50e-04)	Tok/s 54313 (43597)	Loss/tok 3.3808 (3.1104)	LR 1.250e-04
0: TRAIN [3][1130/3880]	Time 0.267 (0.323)	Data 1.38e-04 (4.47e-04)	Tok/s 38865 (43618)	Loss/tok 2.8967 (3.1115)	LR 1.250e-04
0: TRAIN [3][1140/3880]	Time 0.347 (0.323)	Data 1.70e-04 (4.44e-04)	Tok/s 48430 (43620)	Loss/tok 3.1130 (3.1117)	LR 1.250e-04
0: TRAIN [3][1150/3880]	Time 0.431 (0.323)	Data 1.35e-04 (4.42e-04)	Tok/s 54522 (43643)	Loss/tok 3.2911 (3.1122)	LR 1.250e-04
0: TRAIN [3][1160/3880]	Time 0.348 (0.323)	Data 2.23e-04 (4.39e-04)	Tok/s 48601 (43664)	Loss/tok 3.0803 (3.1122)	LR 1.250e-04
0: TRAIN [3][1170/3880]	Time 0.198 (0.323)	Data 1.33e-04 (4.37e-04)	Tok/s 27036 (43641)	Loss/tok 2.5056 (3.1116)	LR 1.250e-04
0: TRAIN [3][1180/3880]	Time 0.266 (0.323)	Data 1.25e-04 (4.34e-04)	Tok/s 38478 (43659)	Loss/tok 2.9605 (3.1122)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][1190/3880]	Time 0.348 (0.324)	Data 1.89e-04 (4.32e-04)	Tok/s 47442 (43698)	Loss/tok 3.0298 (3.1125)	LR 1.250e-04
0: TRAIN [3][1200/3880]	Time 0.431 (0.324)	Data 1.43e-04 (4.29e-04)	Tok/s 53876 (43708)	Loss/tok 3.3633 (3.1124)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1210/3880]	Time 0.347 (0.324)	Data 1.75e-04 (4.27e-04)	Tok/s 48662 (43740)	Loss/tok 3.0467 (3.1134)	LR 1.250e-04
0: TRAIN [3][1220/3880]	Time 0.269 (0.324)	Data 1.77e-04 (4.25e-04)	Tok/s 38562 (43737)	Loss/tok 2.8943 (3.1132)	LR 1.250e-04
0: TRAIN [3][1230/3880]	Time 0.348 (0.324)	Data 1.68e-04 (4.23e-04)	Tok/s 48457 (43730)	Loss/tok 3.1736 (3.1134)	LR 1.250e-04
0: TRAIN [3][1240/3880]	Time 0.355 (0.324)	Data 2.57e-04 (4.21e-04)	Tok/s 47710 (43716)	Loss/tok 3.0052 (3.1131)	LR 1.250e-04
0: TRAIN [3][1250/3880]	Time 0.431 (0.324)	Data 1.45e-04 (4.18e-04)	Tok/s 54178 (43726)	Loss/tok 3.2221 (3.1133)	LR 1.250e-04
0: TRAIN [3][1260/3880]	Time 0.195 (0.324)	Data 1.67e-04 (4.16e-04)	Tok/s 26723 (43715)	Loss/tok 2.4669 (3.1129)	LR 1.250e-04
0: TRAIN [3][1270/3880]	Time 0.194 (0.324)	Data 1.71e-04 (4.14e-04)	Tok/s 27180 (43706)	Loss/tok 2.6275 (3.1131)	LR 1.250e-04
0: TRAIN [3][1280/3880]	Time 0.348 (0.324)	Data 1.97e-04 (4.12e-04)	Tok/s 49012 (43715)	Loss/tok 3.1351 (3.1133)	LR 1.250e-04
0: TRAIN [3][1290/3880]	Time 0.266 (0.324)	Data 1.63e-04 (4.10e-04)	Tok/s 38684 (43697)	Loss/tok 2.9663 (3.1124)	LR 1.250e-04
0: TRAIN [3][1300/3880]	Time 0.195 (0.324)	Data 1.37e-04 (4.08e-04)	Tok/s 26629 (43680)	Loss/tok 2.5650 (3.1122)	LR 1.250e-04
0: TRAIN [3][1310/3880]	Time 0.545 (0.323)	Data 7.91e-03 (4.12e-04)	Tok/s 54529 (43664)	Loss/tok 3.4995 (3.1123)	LR 1.250e-04
0: TRAIN [3][1320/3880]	Time 0.265 (0.324)	Data 1.37e-04 (4.10e-04)	Tok/s 39173 (43668)	Loss/tok 2.9506 (3.1129)	LR 1.250e-04
0: TRAIN [3][1330/3880]	Time 0.269 (0.323)	Data 1.26e-04 (4.08e-04)	Tok/s 38799 (43672)	Loss/tok 2.8086 (3.1125)	LR 1.250e-04
0: TRAIN [3][1340/3880]	Time 0.346 (0.323)	Data 1.43e-04 (4.06e-04)	Tok/s 48367 (43658)	Loss/tok 2.9598 (3.1125)	LR 1.250e-04
0: TRAIN [3][1350/3880]	Time 0.431 (0.323)	Data 1.50e-04 (4.04e-04)	Tok/s 53628 (43651)	Loss/tok 3.4172 (3.1123)	LR 1.250e-04
0: TRAIN [3][1360/3880]	Time 0.267 (0.323)	Data 1.50e-04 (4.02e-04)	Tok/s 39091 (43665)	Loss/tok 2.9101 (3.1123)	LR 1.250e-04
0: TRAIN [3][1370/3880]	Time 0.352 (0.323)	Data 1.32e-04 (4.01e-04)	Tok/s 48060 (43673)	Loss/tok 3.0021 (3.1122)	LR 1.250e-04
0: TRAIN [3][1380/3880]	Time 0.431 (0.323)	Data 1.50e-04 (3.99e-04)	Tok/s 54402 (43644)	Loss/tok 3.2836 (3.1117)	LR 1.250e-04
0: TRAIN [3][1390/3880]	Time 0.431 (0.323)	Data 1.35e-04 (3.97e-04)	Tok/s 55104 (43634)	Loss/tok 3.3020 (3.1113)	LR 1.250e-04
0: TRAIN [3][1400/3880]	Time 0.266 (0.323)	Data 1.93e-04 (3.95e-04)	Tok/s 39212 (43663)	Loss/tok 2.9529 (3.1120)	LR 1.250e-04
0: TRAIN [3][1410/3880]	Time 0.266 (0.323)	Data 1.34e-04 (3.93e-04)	Tok/s 38942 (43673)	Loss/tok 2.9091 (3.1119)	LR 1.250e-04
0: TRAIN [3][1420/3880]	Time 0.347 (0.323)	Data 1.45e-04 (3.92e-04)	Tok/s 48520 (43686)	Loss/tok 3.0310 (3.1124)	LR 1.250e-04
0: TRAIN [3][1430/3880]	Time 0.193 (0.323)	Data 1.38e-04 (3.90e-04)	Tok/s 27681 (43665)	Loss/tok 2.5201 (3.1116)	LR 1.250e-04
0: TRAIN [3][1440/3880]	Time 0.347 (0.323)	Data 2.53e-04 (3.89e-04)	Tok/s 47483 (43691)	Loss/tok 3.1264 (3.1118)	LR 1.250e-04
0: TRAIN [3][1450/3880]	Time 0.266 (0.323)	Data 1.36e-04 (3.87e-04)	Tok/s 38595 (43679)	Loss/tok 2.9085 (3.1116)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1460/3880]	Time 0.431 (0.323)	Data 1.44e-04 (3.85e-04)	Tok/s 54354 (43673)	Loss/tok 3.2550 (3.1116)	LR 1.250e-04
0: TRAIN [3][1470/3880]	Time 0.265 (0.323)	Data 1.70e-04 (3.84e-04)	Tok/s 38853 (43656)	Loss/tok 2.8624 (3.1111)	LR 1.250e-04
0: TRAIN [3][1480/3880]	Time 0.347 (0.323)	Data 1.35e-04 (3.82e-04)	Tok/s 48128 (43668)	Loss/tok 3.1595 (3.1111)	LR 1.250e-04
0: TRAIN [3][1490/3880]	Time 0.435 (0.323)	Data 1.37e-04 (3.81e-04)	Tok/s 53925 (43669)	Loss/tok 3.3070 (3.1113)	LR 1.250e-04
0: TRAIN [3][1500/3880]	Time 0.266 (0.323)	Data 1.33e-04 (3.79e-04)	Tok/s 38172 (43681)	Loss/tok 2.9365 (3.1113)	LR 1.250e-04
0: TRAIN [3][1510/3880]	Time 0.351 (0.323)	Data 1.24e-04 (3.77e-04)	Tok/s 47736 (43669)	Loss/tok 3.1554 (3.1107)	LR 1.250e-04
0: TRAIN [3][1520/3880]	Time 0.431 (0.323)	Data 1.33e-04 (3.76e-04)	Tok/s 54548 (43675)	Loss/tok 3.1727 (3.1109)	LR 1.250e-04
0: TRAIN [3][1530/3880]	Time 0.548 (0.323)	Data 1.89e-04 (3.74e-04)	Tok/s 55823 (43698)	Loss/tok 3.2953 (3.1112)	LR 1.250e-04
0: TRAIN [3][1540/3880]	Time 0.266 (0.323)	Data 1.38e-04 (3.73e-04)	Tok/s 39428 (43675)	Loss/tok 2.9520 (3.1104)	LR 1.250e-04
0: TRAIN [3][1550/3880]	Time 0.347 (0.324)	Data 1.82e-04 (3.71e-04)	Tok/s 48725 (43701)	Loss/tok 3.1191 (3.1110)	LR 1.250e-04
0: TRAIN [3][1560/3880]	Time 0.270 (0.323)	Data 1.37e-04 (3.70e-04)	Tok/s 39159 (43688)	Loss/tok 2.9286 (3.1103)	LR 1.250e-04
0: TRAIN [3][1570/3880]	Time 0.266 (0.323)	Data 1.38e-04 (3.69e-04)	Tok/s 38559 (43690)	Loss/tok 2.9127 (3.1101)	LR 1.250e-04
0: TRAIN [3][1580/3880]	Time 0.272 (0.323)	Data 1.35e-04 (3.67e-04)	Tok/s 37629 (43683)	Loss/tok 2.8386 (3.1098)	LR 1.250e-04
0: TRAIN [3][1590/3880]	Time 0.266 (0.323)	Data 1.81e-04 (3.66e-04)	Tok/s 39199 (43679)	Loss/tok 2.9558 (3.1095)	LR 1.250e-04
0: TRAIN [3][1600/3880]	Time 0.431 (0.323)	Data 1.50e-04 (3.65e-04)	Tok/s 53935 (43691)	Loss/tok 3.2798 (3.1097)	LR 1.250e-04
0: TRAIN [3][1610/3880]	Time 0.346 (0.323)	Data 1.72e-04 (3.63e-04)	Tok/s 48605 (43671)	Loss/tok 3.0463 (3.1091)	LR 1.250e-04
0: TRAIN [3][1620/3880]	Time 0.266 (0.323)	Data 1.33e-04 (3.62e-04)	Tok/s 39659 (43667)	Loss/tok 3.0058 (3.1092)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1630/3880]	Time 0.418 (0.323)	Data 1.54e-04 (3.61e-04)	Tok/s 55823 (43689)	Loss/tok 3.2238 (3.1098)	LR 1.250e-04
0: TRAIN [3][1640/3880]	Time 0.266 (0.323)	Data 1.33e-04 (3.60e-04)	Tok/s 39011 (43698)	Loss/tok 2.9404 (3.1095)	LR 1.250e-04
0: TRAIN [3][1650/3880]	Time 0.348 (0.323)	Data 1.32e-04 (3.58e-04)	Tok/s 48039 (43698)	Loss/tok 3.2425 (3.1094)	LR 1.250e-04
0: TRAIN [3][1660/3880]	Time 0.348 (0.323)	Data 1.38e-04 (3.57e-04)	Tok/s 48030 (43706)	Loss/tok 3.1581 (3.1096)	LR 1.250e-04
0: TRAIN [3][1670/3880]	Time 0.192 (0.323)	Data 1.29e-04 (3.56e-04)	Tok/s 27837 (43704)	Loss/tok 2.5210 (3.1093)	LR 1.250e-04
0: TRAIN [3][1680/3880]	Time 0.266 (0.323)	Data 1.30e-04 (3.54e-04)	Tok/s 38672 (43700)	Loss/tok 2.9128 (3.1093)	LR 1.250e-04
0: TRAIN [3][1690/3880]	Time 0.431 (0.323)	Data 1.38e-04 (3.53e-04)	Tok/s 54719 (43677)	Loss/tok 3.2349 (3.1088)	LR 1.250e-04
0: TRAIN [3][1700/3880]	Time 0.432 (0.323)	Data 1.72e-04 (3.52e-04)	Tok/s 54504 (43684)	Loss/tok 3.2500 (3.1091)	LR 1.250e-04
0: TRAIN [3][1710/3880]	Time 0.348 (0.323)	Data 1.29e-04 (3.51e-04)	Tok/s 48875 (43657)	Loss/tok 3.0769 (3.1085)	LR 1.250e-04
0: TRAIN [3][1720/3880]	Time 0.270 (0.322)	Data 1.59e-04 (3.49e-04)	Tok/s 37640 (43626)	Loss/tok 2.8297 (3.1078)	LR 1.250e-04
0: TRAIN [3][1730/3880]	Time 0.347 (0.322)	Data 1.63e-04 (3.48e-04)	Tok/s 48589 (43635)	Loss/tok 3.1714 (3.1076)	LR 1.250e-04
0: TRAIN [3][1740/3880]	Time 0.269 (0.322)	Data 1.39e-04 (3.47e-04)	Tok/s 37863 (43621)	Loss/tok 2.9793 (3.1072)	LR 1.250e-04
0: TRAIN [3][1750/3880]	Time 0.266 (0.322)	Data 1.86e-04 (3.46e-04)	Tok/s 38884 (43617)	Loss/tok 2.9538 (3.1073)	LR 1.250e-04
0: TRAIN [3][1760/3880]	Time 0.266 (0.322)	Data 1.77e-04 (3.45e-04)	Tok/s 39090 (43623)	Loss/tok 2.9507 (3.1075)	LR 1.250e-04
0: TRAIN [3][1770/3880]	Time 0.267 (0.322)	Data 1.40e-04 (3.44e-04)	Tok/s 38430 (43605)	Loss/tok 2.9176 (3.1070)	LR 1.250e-04
0: TRAIN [3][1780/3880]	Time 0.265 (0.322)	Data 1.28e-04 (3.43e-04)	Tok/s 38762 (43605)	Loss/tok 2.8505 (3.1067)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1790/3880]	Time 0.348 (0.322)	Data 1.72e-04 (3.42e-04)	Tok/s 48152 (43597)	Loss/tok 3.0613 (3.1066)	LR 1.250e-04
0: TRAIN [3][1800/3880]	Time 0.267 (0.322)	Data 1.34e-04 (3.41e-04)	Tok/s 38977 (43591)	Loss/tok 2.9775 (3.1065)	LR 1.250e-04
0: TRAIN [3][1810/3880]	Time 0.266 (0.322)	Data 1.29e-04 (3.40e-04)	Tok/s 39065 (43597)	Loss/tok 2.8607 (3.1061)	LR 1.250e-04
0: TRAIN [3][1820/3880]	Time 0.347 (0.322)	Data 1.59e-04 (3.39e-04)	Tok/s 48335 (43597)	Loss/tok 3.0521 (3.1057)	LR 1.250e-04
0: TRAIN [3][1830/3880]	Time 0.346 (0.322)	Data 1.88e-04 (3.38e-04)	Tok/s 47716 (43582)	Loss/tok 3.1009 (3.1054)	LR 1.250e-04
0: TRAIN [3][1840/3880]	Time 0.195 (0.321)	Data 1.30e-04 (3.37e-04)	Tok/s 27050 (43558)	Loss/tok 2.4951 (3.1048)	LR 1.250e-04
0: TRAIN [3][1850/3880]	Time 0.266 (0.322)	Data 1.32e-04 (3.36e-04)	Tok/s 38708 (43571)	Loss/tok 2.9342 (3.1051)	LR 1.250e-04
0: TRAIN [3][1860/3880]	Time 0.348 (0.321)	Data 1.26e-04 (3.35e-04)	Tok/s 47758 (43556)	Loss/tok 3.1446 (3.1046)	LR 1.250e-04
0: TRAIN [3][1870/3880]	Time 0.267 (0.321)	Data 2.31e-04 (3.34e-04)	Tok/s 38707 (43542)	Loss/tok 2.9708 (3.1041)	LR 1.250e-04
0: TRAIN [3][1880/3880]	Time 0.192 (0.321)	Data 1.95e-04 (3.33e-04)	Tok/s 27607 (43558)	Loss/tok 2.5606 (3.1052)	LR 1.250e-04
0: TRAIN [3][1890/3880]	Time 0.265 (0.321)	Data 1.51e-04 (3.32e-04)	Tok/s 38536 (43541)	Loss/tok 2.8584 (3.1050)	LR 1.250e-04
0: TRAIN [3][1900/3880]	Time 0.193 (0.321)	Data 1.62e-04 (3.31e-04)	Tok/s 27216 (43504)	Loss/tok 2.5418 (3.1045)	LR 1.250e-04
0: TRAIN [3][1910/3880]	Time 0.266 (0.321)	Data 1.32e-04 (3.30e-04)	Tok/s 39188 (43506)	Loss/tok 2.8244 (3.1046)	LR 1.250e-04
0: TRAIN [3][1920/3880]	Time 0.266 (0.321)	Data 1.77e-04 (3.30e-04)	Tok/s 39403 (43496)	Loss/tok 2.8101 (3.1047)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1930/3880]	Time 0.185 (0.321)	Data 1.31e-04 (3.29e-04)	Tok/s 28868 (43490)	Loss/tok 2.6974 (3.1047)	LR 1.250e-04
0: TRAIN [3][1940/3880]	Time 0.271 (0.321)	Data 1.44e-04 (3.28e-04)	Tok/s 37820 (43484)	Loss/tok 2.8523 (3.1045)	LR 1.250e-04
0: TRAIN [3][1950/3880]	Time 0.270 (0.321)	Data 1.29e-04 (3.27e-04)	Tok/s 38462 (43477)	Loss/tok 2.8539 (3.1052)	LR 1.250e-04
0: TRAIN [3][1960/3880]	Time 0.267 (0.321)	Data 1.59e-04 (3.26e-04)	Tok/s 38286 (43486)	Loss/tok 2.9569 (3.1054)	LR 1.250e-04
0: TRAIN [3][1970/3880]	Time 0.346 (0.321)	Data 1.54e-04 (3.31e-04)	Tok/s 48458 (43487)	Loss/tok 3.0898 (3.1052)	LR 1.250e-04
0: TRAIN [3][1980/3880]	Time 0.194 (0.321)	Data 1.50e-04 (3.30e-04)	Tok/s 27334 (43467)	Loss/tok 2.5793 (3.1049)	LR 1.250e-04
0: TRAIN [3][1990/3880]	Time 0.266 (0.321)	Data 1.28e-04 (3.29e-04)	Tok/s 39006 (43473)	Loss/tok 2.9040 (3.1052)	LR 1.250e-04
0: TRAIN [3][2000/3880]	Time 0.265 (0.321)	Data 1.46e-04 (3.28e-04)	Tok/s 39122 (43468)	Loss/tok 2.9396 (3.1048)	LR 1.250e-04
0: TRAIN [3][2010/3880]	Time 0.267 (0.321)	Data 1.83e-04 (3.27e-04)	Tok/s 38118 (43467)	Loss/tok 3.0009 (3.1044)	LR 1.250e-04
0: TRAIN [3][2020/3880]	Time 0.348 (0.321)	Data 1.34e-04 (3.26e-04)	Tok/s 48385 (43449)	Loss/tok 3.1041 (3.1044)	LR 1.250e-04
0: TRAIN [3][2030/3880]	Time 0.266 (0.321)	Data 1.40e-04 (3.25e-04)	Tok/s 39383 (43451)	Loss/tok 2.9269 (3.1044)	LR 1.250e-04
0: TRAIN [3][2040/3880]	Time 0.194 (0.321)	Data 1.28e-04 (3.24e-04)	Tok/s 27443 (43455)	Loss/tok 2.5333 (3.1052)	LR 1.250e-04
0: TRAIN [3][2050/3880]	Time 0.347 (0.321)	Data 1.36e-04 (3.23e-04)	Tok/s 48086 (43439)	Loss/tok 3.0815 (3.1049)	LR 1.250e-04
0: TRAIN [3][2060/3880]	Time 0.348 (0.321)	Data 3.11e-04 (3.23e-04)	Tok/s 48201 (43448)	Loss/tok 3.1515 (3.1046)	LR 1.250e-04
0: TRAIN [3][2070/3880]	Time 0.430 (0.321)	Data 1.34e-04 (3.22e-04)	Tok/s 54534 (43444)	Loss/tok 3.3578 (3.1047)	LR 1.250e-04
0: TRAIN [3][2080/3880]	Time 0.431 (0.321)	Data 1.69e-04 (3.21e-04)	Tok/s 54737 (43440)	Loss/tok 3.1503 (3.1044)	LR 1.250e-04
0: TRAIN [3][2090/3880]	Time 0.349 (0.321)	Data 1.40e-04 (3.20e-04)	Tok/s 48035 (43445)	Loss/tok 3.0392 (3.1043)	LR 1.250e-04
0: TRAIN [3][2100/3880]	Time 0.265 (0.321)	Data 1.38e-04 (3.19e-04)	Tok/s 38846 (43449)	Loss/tok 2.8411 (3.1041)	LR 1.250e-04
0: TRAIN [3][2110/3880]	Time 0.194 (0.320)	Data 1.77e-04 (3.19e-04)	Tok/s 27232 (43437)	Loss/tok 2.6270 (3.1040)	LR 1.250e-04
0: TRAIN [3][2120/3880]	Time 0.266 (0.320)	Data 1.29e-04 (3.18e-04)	Tok/s 39359 (43409)	Loss/tok 3.0028 (3.1035)	LR 1.250e-04
0: TRAIN [3][2130/3880]	Time 0.271 (0.320)	Data 1.33e-04 (3.17e-04)	Tok/s 38573 (43409)	Loss/tok 2.9295 (3.1035)	LR 1.250e-04
0: TRAIN [3][2140/3880]	Time 0.348 (0.320)	Data 1.54e-04 (3.16e-04)	Tok/s 48484 (43400)	Loss/tok 3.1237 (3.1035)	LR 1.250e-04
0: TRAIN [3][2150/3880]	Time 0.348 (0.320)	Data 1.70e-04 (3.15e-04)	Tok/s 48215 (43406)	Loss/tok 3.1393 (3.1036)	LR 1.250e-04
0: TRAIN [3][2160/3880]	Time 0.266 (0.320)	Data 1.37e-04 (3.15e-04)	Tok/s 38618 (43402)	Loss/tok 2.8934 (3.1032)	LR 1.250e-04
0: TRAIN [3][2170/3880]	Time 0.347 (0.320)	Data 1.58e-04 (3.14e-04)	Tok/s 47992 (43403)	Loss/tok 3.1187 (3.1033)	LR 1.250e-04
0: TRAIN [3][2180/3880]	Time 0.348 (0.320)	Data 1.30e-04 (3.13e-04)	Tok/s 48725 (43403)	Loss/tok 3.0582 (3.1032)	LR 1.250e-04
0: TRAIN [3][2190/3880]	Time 0.346 (0.320)	Data 1.71e-04 (3.13e-04)	Tok/s 49016 (43399)	Loss/tok 3.1061 (3.1029)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][2200/3880]	Time 0.267 (0.320)	Data 1.28e-04 (3.12e-04)	Tok/s 38912 (43393)	Loss/tok 2.9503 (3.1032)	LR 1.250e-04
0: TRAIN [3][2210/3880]	Time 0.270 (0.320)	Data 1.67e-04 (3.11e-04)	Tok/s 38865 (43407)	Loss/tok 2.8290 (3.1036)	LR 1.250e-04
0: TRAIN [3][2220/3880]	Time 0.429 (0.320)	Data 1.48e-04 (3.11e-04)	Tok/s 54623 (43398)	Loss/tok 3.3017 (3.1035)	LR 1.250e-04
0: TRAIN [3][2230/3880]	Time 0.266 (0.320)	Data 1.34e-04 (3.10e-04)	Tok/s 39010 (43401)	Loss/tok 2.8767 (3.1034)	LR 1.250e-04
0: TRAIN [3][2240/3880]	Time 0.266 (0.320)	Data 1.32e-04 (3.09e-04)	Tok/s 38910 (43408)	Loss/tok 2.8540 (3.1031)	LR 1.250e-04
0: TRAIN [3][2250/3880]	Time 0.347 (0.320)	Data 1.49e-04 (3.09e-04)	Tok/s 48255 (43424)	Loss/tok 3.0225 (3.1030)	LR 1.250e-04
0: TRAIN [3][2260/3880]	Time 0.347 (0.320)	Data 1.42e-04 (3.08e-04)	Tok/s 48405 (43432)	Loss/tok 3.1575 (3.1033)	LR 1.250e-04
0: TRAIN [3][2270/3880]	Time 0.266 (0.320)	Data 1.32e-04 (3.07e-04)	Tok/s 39354 (43434)	Loss/tok 2.9502 (3.1031)	LR 1.250e-04
0: TRAIN [3][2280/3880]	Time 0.347 (0.320)	Data 1.81e-04 (3.07e-04)	Tok/s 47780 (43448)	Loss/tok 3.1035 (3.1030)	LR 1.250e-04
0: TRAIN [3][2290/3880]	Time 0.346 (0.320)	Data 1.32e-04 (3.06e-04)	Tok/s 48593 (43455)	Loss/tok 3.0888 (3.1032)	LR 1.250e-04
0: TRAIN [3][2300/3880]	Time 0.191 (0.320)	Data 1.53e-04 (3.05e-04)	Tok/s 27547 (43442)	Loss/tok 2.6982 (3.1028)	LR 1.250e-04
0: TRAIN [3][2310/3880]	Time 0.347 (0.320)	Data 1.81e-04 (3.05e-04)	Tok/s 48777 (43446)	Loss/tok 3.0927 (3.1026)	LR 1.250e-04
0: TRAIN [3][2320/3880]	Time 0.266 (0.320)	Data 1.27e-04 (3.04e-04)	Tok/s 38603 (43443)	Loss/tok 2.8492 (3.1027)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][2330/3880]	Time 0.266 (0.320)	Data 1.31e-04 (3.03e-04)	Tok/s 39322 (43445)	Loss/tok 3.0534 (3.1027)	LR 1.250e-04
0: TRAIN [3][2340/3880]	Time 0.194 (0.320)	Data 1.34e-04 (3.02e-04)	Tok/s 27228 (43446)	Loss/tok 2.5722 (3.1027)	LR 1.250e-04
0: TRAIN [3][2350/3880]	Time 0.266 (0.320)	Data 2.13e-04 (3.02e-04)	Tok/s 38976 (43441)	Loss/tok 2.9095 (3.1023)	LR 1.250e-04
0: TRAIN [3][2360/3880]	Time 0.347 (0.320)	Data 1.68e-04 (3.01e-04)	Tok/s 47892 (43448)	Loss/tok 3.2016 (3.1024)	LR 1.250e-04
0: TRAIN [3][2370/3880]	Time 0.432 (0.320)	Data 1.49e-04 (3.01e-04)	Tok/s 53933 (43456)	Loss/tok 3.2362 (3.1028)	LR 1.250e-04
0: TRAIN [3][2380/3880]	Time 0.429 (0.321)	Data 1.39e-04 (3.00e-04)	Tok/s 54390 (43475)	Loss/tok 3.2149 (3.1034)	LR 1.250e-04
0: TRAIN [3][2390/3880]	Time 0.192 (0.321)	Data 1.31e-04 (2.99e-04)	Tok/s 27389 (43472)	Loss/tok 2.5545 (3.1036)	LR 1.250e-04
0: TRAIN [3][2400/3880]	Time 0.347 (0.321)	Data 1.35e-04 (2.99e-04)	Tok/s 49009 (43473)	Loss/tok 3.0044 (3.1042)	LR 1.250e-04
0: TRAIN [3][2410/3880]	Time 0.266 (0.321)	Data 1.29e-04 (2.98e-04)	Tok/s 39242 (43477)	Loss/tok 2.9335 (3.1038)	LR 1.250e-04
0: TRAIN [3][2420/3880]	Time 0.347 (0.321)	Data 1.39e-04 (2.97e-04)	Tok/s 48747 (43468)	Loss/tok 3.0967 (3.1034)	LR 1.250e-04
0: TRAIN [3][2430/3880]	Time 0.195 (0.320)	Data 1.50e-04 (2.97e-04)	Tok/s 26911 (43456)	Loss/tok 2.4266 (3.1031)	LR 1.250e-04
0: TRAIN [3][2440/3880]	Time 0.430 (0.321)	Data 1.35e-04 (2.96e-04)	Tok/s 54749 (43463)	Loss/tok 3.3287 (3.1032)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][2450/3880]	Time 0.348 (0.320)	Data 1.44e-04 (2.96e-04)	Tok/s 47972 (43459)	Loss/tok 3.1042 (3.1030)	LR 1.250e-04
0: TRAIN [3][2460/3880]	Time 0.266 (0.321)	Data 1.33e-04 (2.95e-04)	Tok/s 39154 (43464)	Loss/tok 2.8832 (3.1032)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2470/3880]	Time 0.193 (0.320)	Data 1.31e-04 (2.94e-04)	Tok/s 26449 (43463)	Loss/tok 2.5167 (3.1033)	LR 1.250e-04
0: TRAIN [3][2480/3880]	Time 0.348 (0.321)	Data 1.68e-04 (2.94e-04)	Tok/s 48387 (43467)	Loss/tok 3.0862 (3.1031)	LR 1.250e-04
0: TRAIN [3][2490/3880]	Time 0.266 (0.321)	Data 1.29e-04 (2.93e-04)	Tok/s 38320 (43463)	Loss/tok 2.8808 (3.1032)	LR 1.250e-04
0: TRAIN [3][2500/3880]	Time 0.347 (0.320)	Data 1.30e-04 (2.93e-04)	Tok/s 48321 (43462)	Loss/tok 3.1610 (3.1030)	LR 1.250e-04
0: TRAIN [3][2510/3880]	Time 0.348 (0.320)	Data 1.25e-04 (2.92e-04)	Tok/s 48853 (43469)	Loss/tok 3.2276 (3.1030)	LR 1.250e-04
0: TRAIN [3][2520/3880]	Time 0.266 (0.320)	Data 1.50e-04 (2.92e-04)	Tok/s 39037 (43472)	Loss/tok 2.9571 (3.1028)	LR 1.250e-04
0: TRAIN [3][2530/3880]	Time 0.265 (0.320)	Data 1.81e-04 (2.91e-04)	Tok/s 39033 (43466)	Loss/tok 2.9247 (3.1025)	LR 1.250e-04
0: TRAIN [3][2540/3880]	Time 0.430 (0.320)	Data 1.34e-04 (2.90e-04)	Tok/s 54139 (43464)	Loss/tok 3.2604 (3.1023)	LR 1.250e-04
0: TRAIN [3][2550/3880]	Time 0.347 (0.320)	Data 1.33e-04 (2.90e-04)	Tok/s 48475 (43452)	Loss/tok 3.1371 (3.1020)	LR 1.250e-04
0: TRAIN [3][2560/3880]	Time 0.346 (0.320)	Data 1.84e-04 (2.89e-04)	Tok/s 48224 (43449)	Loss/tok 3.2878 (3.1019)	LR 1.250e-04
0: TRAIN [3][2570/3880]	Time 0.347 (0.320)	Data 1.40e-04 (2.89e-04)	Tok/s 49076 (43434)	Loss/tok 2.9912 (3.1019)	LR 1.250e-04
0: TRAIN [3][2580/3880]	Time 0.266 (0.320)	Data 1.41e-04 (2.88e-04)	Tok/s 39153 (43441)	Loss/tok 2.9724 (3.1023)	LR 1.250e-04
0: TRAIN [3][2590/3880]	Time 0.431 (0.320)	Data 1.30e-04 (2.88e-04)	Tok/s 53683 (43444)	Loss/tok 3.3563 (3.1024)	LR 1.250e-04
0: TRAIN [3][2600/3880]	Time 0.347 (0.320)	Data 1.31e-04 (2.87e-04)	Tok/s 48251 (43445)	Loss/tok 3.1483 (3.1023)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2610/3880]	Time 0.348 (0.320)	Data 1.35e-04 (2.87e-04)	Tok/s 48888 (43452)	Loss/tok 3.1220 (3.1026)	LR 1.250e-04
0: TRAIN [3][2620/3880]	Time 0.349 (0.320)	Data 1.35e-04 (2.86e-04)	Tok/s 48694 (43476)	Loss/tok 3.0221 (3.1028)	LR 1.250e-04
0: TRAIN [3][2630/3880]	Time 0.267 (0.320)	Data 1.44e-04 (2.85e-04)	Tok/s 39135 (43468)	Loss/tok 2.9280 (3.1024)	LR 1.250e-04
0: TRAIN [3][2640/3880]	Time 0.266 (0.320)	Data 1.69e-04 (2.85e-04)	Tok/s 38069 (43467)	Loss/tok 2.9015 (3.1022)	LR 1.250e-04
0: TRAIN [3][2650/3880]	Time 0.430 (0.320)	Data 1.39e-04 (2.84e-04)	Tok/s 53835 (43467)	Loss/tok 3.2583 (3.1022)	LR 1.250e-04
0: TRAIN [3][2660/3880]	Time 0.265 (0.320)	Data 1.28e-04 (2.84e-04)	Tok/s 39520 (43469)	Loss/tok 2.8558 (3.1024)	LR 1.250e-04
0: TRAIN [3][2670/3880]	Time 0.350 (0.320)	Data 1.45e-04 (2.83e-04)	Tok/s 47389 (43478)	Loss/tok 3.0918 (3.1026)	LR 1.250e-04
0: TRAIN [3][2680/3880]	Time 0.348 (0.320)	Data 1.70e-04 (2.83e-04)	Tok/s 48228 (43486)	Loss/tok 3.1675 (3.1025)	LR 1.250e-04
0: TRAIN [3][2690/3880]	Time 0.267 (0.320)	Data 1.46e-04 (2.82e-04)	Tok/s 39090 (43474)	Loss/tok 2.8700 (3.1022)	LR 1.250e-04
0: TRAIN [3][2700/3880]	Time 0.267 (0.320)	Data 1.41e-04 (2.82e-04)	Tok/s 38373 (43478)	Loss/tok 2.9115 (3.1024)	LR 1.250e-04
0: TRAIN [3][2710/3880]	Time 0.195 (0.320)	Data 1.34e-04 (2.81e-04)	Tok/s 27379 (43476)	Loss/tok 2.5894 (3.1024)	LR 1.250e-04
0: TRAIN [3][2720/3880]	Time 0.545 (0.321)	Data 1.77e-04 (2.81e-04)	Tok/s 54786 (43493)	Loss/tok 3.3625 (3.1029)	LR 1.250e-04
0: TRAIN [3][2730/3880]	Time 0.265 (0.321)	Data 1.57e-04 (2.80e-04)	Tok/s 39139 (43485)	Loss/tok 2.8409 (3.1026)	LR 1.250e-04
0: TRAIN [3][2740/3880]	Time 0.267 (0.321)	Data 1.79e-04 (2.80e-04)	Tok/s 37584 (43489)	Loss/tok 2.9834 (3.1033)	LR 1.250e-04
0: TRAIN [3][2750/3880]	Time 0.193 (0.321)	Data 1.46e-04 (2.80e-04)	Tok/s 27483 (43473)	Loss/tok 2.5488 (3.1031)	LR 1.250e-04
0: TRAIN [3][2760/3880]	Time 0.194 (0.320)	Data 1.54e-04 (2.79e-04)	Tok/s 27218 (43453)	Loss/tok 2.6448 (3.1026)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2770/3880]	Time 0.348 (0.320)	Data 1.07e-03 (2.79e-04)	Tok/s 48522 (43450)	Loss/tok 3.0301 (3.1026)	LR 1.250e-04
0: TRAIN [3][2780/3880]	Time 0.266 (0.320)	Data 1.34e-04 (2.78e-04)	Tok/s 39079 (43438)	Loss/tok 2.9031 (3.1024)	LR 1.250e-04
0: TRAIN [3][2790/3880]	Time 0.265 (0.320)	Data 2.58e-04 (2.78e-04)	Tok/s 38742 (43431)	Loss/tok 3.0141 (3.1020)	LR 1.250e-04
0: TRAIN [3][2800/3880]	Time 0.266 (0.320)	Data 1.33e-04 (2.78e-04)	Tok/s 38939 (43410)	Loss/tok 2.8963 (3.1014)	LR 1.250e-04
0: TRAIN [3][2810/3880]	Time 0.348 (0.320)	Data 1.34e-04 (2.77e-04)	Tok/s 48140 (43414)	Loss/tok 3.0195 (3.1016)	LR 1.250e-04
0: TRAIN [3][2820/3880]	Time 0.266 (0.320)	Data 1.31e-04 (2.77e-04)	Tok/s 39395 (43425)	Loss/tok 2.9481 (3.1022)	LR 1.250e-04
0: TRAIN [3][2830/3880]	Time 0.265 (0.320)	Data 1.33e-04 (2.76e-04)	Tok/s 38660 (43426)	Loss/tok 2.9000 (3.1022)	LR 1.250e-04
0: TRAIN [3][2840/3880]	Time 0.267 (0.320)	Data 1.34e-04 (2.76e-04)	Tok/s 38445 (43439)	Loss/tok 2.8379 (3.1025)	LR 1.250e-04
0: TRAIN [3][2850/3880]	Time 0.349 (0.320)	Data 1.81e-04 (2.75e-04)	Tok/s 48045 (43435)	Loss/tok 3.1273 (3.1026)	LR 1.250e-04
0: TRAIN [3][2860/3880]	Time 0.265 (0.320)	Data 1.39e-04 (2.75e-04)	Tok/s 38858 (43446)	Loss/tok 2.9431 (3.1028)	LR 1.250e-04
0: TRAIN [3][2870/3880]	Time 0.347 (0.321)	Data 1.39e-04 (2.74e-04)	Tok/s 47838 (43458)	Loss/tok 3.1789 (3.1032)	LR 1.250e-04
0: TRAIN [3][2880/3880]	Time 0.431 (0.321)	Data 1.34e-04 (2.74e-04)	Tok/s 53661 (43455)	Loss/tok 3.3058 (3.1033)	LR 1.250e-04
0: TRAIN [3][2890/3880]	Time 0.267 (0.321)	Data 1.38e-04 (2.74e-04)	Tok/s 38814 (43454)	Loss/tok 2.9451 (3.1031)	LR 1.250e-04
0: TRAIN [3][2900/3880]	Time 0.346 (0.321)	Data 1.42e-04 (2.73e-04)	Tok/s 48657 (43452)	Loss/tok 3.0892 (3.1034)	LR 1.250e-04
0: TRAIN [3][2910/3880]	Time 0.193 (0.320)	Data 1.80e-04 (2.73e-04)	Tok/s 27235 (43443)	Loss/tok 2.5481 (3.1031)	LR 1.250e-04
0: TRAIN [3][2920/3880]	Time 0.265 (0.320)	Data 1.44e-04 (2.72e-04)	Tok/s 38802 (43437)	Loss/tok 2.9013 (3.1030)	LR 1.250e-04
0: TRAIN [3][2930/3880]	Time 0.431 (0.320)	Data 1.31e-04 (2.72e-04)	Tok/s 53908 (43446)	Loss/tok 3.2388 (3.1029)	LR 1.250e-04
0: TRAIN [3][2940/3880]	Time 0.266 (0.320)	Data 1.39e-04 (2.71e-04)	Tok/s 38063 (43440)	Loss/tok 2.9992 (3.1029)	LR 1.250e-04
0: TRAIN [3][2950/3880]	Time 0.431 (0.320)	Data 1.27e-04 (2.71e-04)	Tok/s 53876 (43429)	Loss/tok 3.1526 (3.1029)	LR 1.250e-04
0: TRAIN [3][2960/3880]	Time 0.431 (0.320)	Data 2.54e-04 (2.71e-04)	Tok/s 54130 (43435)	Loss/tok 3.2417 (3.1029)	LR 1.250e-04
0: TRAIN [3][2970/3880]	Time 0.348 (0.320)	Data 1.34e-04 (2.70e-04)	Tok/s 48750 (43428)	Loss/tok 3.1179 (3.1027)	LR 1.250e-04
0: TRAIN [3][2980/3880]	Time 0.265 (0.320)	Data 1.34e-04 (2.70e-04)	Tok/s 38906 (43425)	Loss/tok 2.9889 (3.1026)	LR 1.250e-04
0: TRAIN [3][2990/3880]	Time 0.346 (0.320)	Data 1.35e-04 (2.70e-04)	Tok/s 47617 (43444)	Loss/tok 3.1077 (3.1030)	LR 1.250e-04
0: TRAIN [3][3000/3880]	Time 0.432 (0.321)	Data 1.50e-04 (2.69e-04)	Tok/s 54346 (43457)	Loss/tok 3.1424 (3.1034)	LR 1.250e-04
0: TRAIN [3][3010/3880]	Time 0.266 (0.321)	Data 1.29e-04 (2.69e-04)	Tok/s 38936 (43467)	Loss/tok 2.9745 (3.1035)	LR 1.250e-04
0: TRAIN [3][3020/3880]	Time 0.194 (0.321)	Data 1.47e-04 (2.68e-04)	Tok/s 26138 (43465)	Loss/tok 2.5379 (3.1033)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][3030/3880]	Time 0.257 (0.321)	Data 1.39e-04 (2.68e-04)	Tok/s 40429 (43466)	Loss/tok 2.9298 (3.1034)	LR 1.250e-04
0: TRAIN [3][3040/3880]	Time 0.266 (0.321)	Data 1.35e-04 (2.68e-04)	Tok/s 39018 (43464)	Loss/tok 2.9311 (3.1033)	LR 1.250e-04
0: TRAIN [3][3050/3880]	Time 0.347 (0.321)	Data 1.32e-04 (2.67e-04)	Tok/s 48634 (43457)	Loss/tok 3.2476 (3.1032)	LR 1.250e-04
0: TRAIN [3][3060/3880]	Time 0.346 (0.321)	Data 1.30e-04 (2.67e-04)	Tok/s 49242 (43461)	Loss/tok 3.0871 (3.1032)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3070/3880]	Time 0.430 (0.321)	Data 1.29e-04 (2.66e-04)	Tok/s 55278 (43482)	Loss/tok 3.2423 (3.1035)	LR 1.250e-04
0: TRAIN [3][3080/3880]	Time 0.347 (0.321)	Data 1.49e-04 (2.66e-04)	Tok/s 48213 (43494)	Loss/tok 3.2190 (3.1039)	LR 1.250e-04
0: TRAIN [3][3090/3880]	Time 0.193 (0.321)	Data 1.29e-04 (2.66e-04)	Tok/s 27187 (43482)	Loss/tok 2.5438 (3.1038)	LR 1.250e-04
0: TRAIN [3][3100/3880]	Time 0.194 (0.321)	Data 1.32e-04 (2.65e-04)	Tok/s 27263 (43481)	Loss/tok 2.6216 (3.1041)	LR 1.250e-04
0: TRAIN [3][3110/3880]	Time 0.267 (0.321)	Data 1.32e-04 (2.65e-04)	Tok/s 39501 (43483)	Loss/tok 2.8884 (3.1041)	LR 1.250e-04
0: TRAIN [3][3120/3880]	Time 0.265 (0.321)	Data 1.47e-04 (2.65e-04)	Tok/s 38638 (43485)	Loss/tok 2.9361 (3.1041)	LR 1.250e-04
0: TRAIN [3][3130/3880]	Time 0.266 (0.321)	Data 1.39e-04 (2.64e-04)	Tok/s 38202 (43477)	Loss/tok 2.8738 (3.1038)	LR 1.250e-04
0: TRAIN [3][3140/3880]	Time 0.544 (0.321)	Data 1.50e-04 (2.64e-04)	Tok/s 54917 (43483)	Loss/tok 3.4641 (3.1040)	LR 1.250e-04
0: TRAIN [3][3150/3880]	Time 0.431 (0.321)	Data 1.81e-04 (2.64e-04)	Tok/s 54283 (43485)	Loss/tok 3.2942 (3.1040)	LR 1.250e-04
0: TRAIN [3][3160/3880]	Time 0.266 (0.321)	Data 1.49e-04 (2.63e-04)	Tok/s 38428 (43490)	Loss/tok 2.8886 (3.1040)	LR 1.250e-04
0: TRAIN [3][3170/3880]	Time 0.266 (0.321)	Data 1.43e-04 (2.63e-04)	Tok/s 38765 (43493)	Loss/tok 2.9618 (3.1041)	LR 1.250e-04
0: TRAIN [3][3180/3880]	Time 0.266 (0.321)	Data 1.37e-04 (2.62e-04)	Tok/s 38722 (43505)	Loss/tok 2.8407 (3.1042)	LR 1.250e-04
0: TRAIN [3][3190/3880]	Time 0.266 (0.321)	Data 1.41e-04 (2.62e-04)	Tok/s 39296 (43497)	Loss/tok 2.9112 (3.1040)	LR 1.250e-04
0: TRAIN [3][3200/3880]	Time 0.195 (0.321)	Data 1.60e-04 (2.62e-04)	Tok/s 27675 (43489)	Loss/tok 2.5257 (3.1037)	LR 1.250e-04
0: TRAIN [3][3210/3880]	Time 0.266 (0.321)	Data 1.66e-04 (2.61e-04)	Tok/s 38995 (43493)	Loss/tok 2.9248 (3.1037)	LR 1.250e-04
0: TRAIN [3][3220/3880]	Time 0.268 (0.321)	Data 1.57e-04 (2.61e-04)	Tok/s 37827 (43494)	Loss/tok 2.8360 (3.1036)	LR 1.250e-04
0: TRAIN [3][3230/3880]	Time 0.348 (0.321)	Data 1.46e-04 (2.61e-04)	Tok/s 48242 (43499)	Loss/tok 3.2280 (3.1037)	LR 1.250e-04
0: TRAIN [3][3240/3880]	Time 0.265 (0.321)	Data 1.22e-04 (2.60e-04)	Tok/s 39637 (43496)	Loss/tok 2.9216 (3.1035)	LR 1.250e-04
0: TRAIN [3][3250/3880]	Time 0.266 (0.321)	Data 1.35e-04 (2.60e-04)	Tok/s 38964 (43494)	Loss/tok 3.0276 (3.1035)	LR 1.250e-04
0: TRAIN [3][3260/3880]	Time 0.545 (0.321)	Data 1.64e-04 (2.60e-04)	Tok/s 54667 (43500)	Loss/tok 3.3670 (3.1038)	LR 1.250e-04
0: TRAIN [3][3270/3880]	Time 0.266 (0.321)	Data 1.76e-04 (2.59e-04)	Tok/s 38670 (43498)	Loss/tok 2.8462 (3.1036)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3280/3880]	Time 0.348 (0.321)	Data 1.49e-04 (2.59e-04)	Tok/s 49066 (43497)	Loss/tok 2.9980 (3.1039)	LR 1.250e-04
0: TRAIN [3][3290/3880]	Time 0.544 (0.321)	Data 1.52e-04 (2.59e-04)	Tok/s 54523 (43510)	Loss/tok 3.4534 (3.1046)	LR 1.250e-04
0: TRAIN [3][3300/3880]	Time 0.432 (0.321)	Data 1.47e-04 (2.58e-04)	Tok/s 54303 (43513)	Loss/tok 3.2129 (3.1044)	LR 1.250e-04
0: TRAIN [3][3310/3880]	Time 0.431 (0.321)	Data 1.32e-04 (2.58e-04)	Tok/s 53996 (43525)	Loss/tok 3.2306 (3.1045)	LR 1.250e-04
0: TRAIN [3][3320/3880]	Time 0.266 (0.321)	Data 1.23e-04 (2.58e-04)	Tok/s 39219 (43529)	Loss/tok 3.0070 (3.1046)	LR 1.250e-04
0: TRAIN [3][3330/3880]	Time 0.432 (0.321)	Data 1.36e-04 (2.57e-04)	Tok/s 53412 (43539)	Loss/tok 3.3458 (3.1048)	LR 1.250e-04
0: TRAIN [3][3340/3880]	Time 0.193 (0.321)	Data 1.32e-04 (2.57e-04)	Tok/s 27276 (43528)	Loss/tok 2.5356 (3.1047)	LR 1.250e-04
0: TRAIN [3][3350/3880]	Time 0.192 (0.321)	Data 1.62e-04 (2.57e-04)	Tok/s 26706 (43531)	Loss/tok 2.4262 (3.1047)	LR 1.250e-04
0: TRAIN [3][3360/3880]	Time 0.347 (0.321)	Data 1.29e-04 (2.56e-04)	Tok/s 48333 (43525)	Loss/tok 3.0377 (3.1045)	LR 1.250e-04
0: TRAIN [3][3370/3880]	Time 0.266 (0.321)	Data 1.42e-04 (2.56e-04)	Tok/s 38045 (43525)	Loss/tok 2.8941 (3.1047)	LR 1.250e-04
0: TRAIN [3][3380/3880]	Time 0.347 (0.321)	Data 1.43e-04 (2.56e-04)	Tok/s 49203 (43533)	Loss/tok 3.0139 (3.1048)	LR 1.250e-04
0: TRAIN [3][3390/3880]	Time 0.430 (0.321)	Data 1.35e-04 (2.55e-04)	Tok/s 53946 (43532)	Loss/tok 3.3414 (3.1049)	LR 1.250e-04
0: TRAIN [3][3400/3880]	Time 0.347 (0.321)	Data 1.47e-04 (2.55e-04)	Tok/s 48372 (43538)	Loss/tok 3.0527 (3.1050)	LR 1.250e-04
0: TRAIN [3][3410/3880]	Time 0.201 (0.321)	Data 1.79e-04 (2.55e-04)	Tok/s 26638 (43541)	Loss/tok 2.4842 (3.1053)	LR 1.250e-04
0: TRAIN [3][3420/3880]	Time 0.265 (0.321)	Data 1.31e-04 (2.55e-04)	Tok/s 38850 (43530)	Loss/tok 2.9359 (3.1050)	LR 1.250e-04
0: TRAIN [3][3430/3880]	Time 0.433 (0.321)	Data 1.31e-04 (2.54e-04)	Tok/s 53924 (43526)	Loss/tok 3.3196 (3.1049)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3440/3880]	Time 0.266 (0.321)	Data 2.06e-04 (2.54e-04)	Tok/s 38412 (43525)	Loss/tok 2.8545 (3.1050)	LR 1.250e-04
0: TRAIN [3][3450/3880]	Time 0.347 (0.321)	Data 1.78e-04 (2.54e-04)	Tok/s 48596 (43512)	Loss/tok 3.1795 (3.1049)	LR 1.250e-04
0: TRAIN [3][3460/3880]	Time 0.346 (0.321)	Data 1.34e-04 (2.54e-04)	Tok/s 48883 (43507)	Loss/tok 3.0724 (3.1047)	LR 1.250e-04
0: TRAIN [3][3470/3880]	Time 0.265 (0.321)	Data 1.25e-04 (2.53e-04)	Tok/s 39897 (43514)	Loss/tok 2.7303 (3.1050)	LR 1.250e-04
0: TRAIN [3][3480/3880]	Time 0.431 (0.321)	Data 3.07e-04 (2.53e-04)	Tok/s 53877 (43508)	Loss/tok 3.2907 (3.1049)	LR 1.250e-04
0: TRAIN [3][3490/3880]	Time 0.433 (0.321)	Data 1.41e-04 (2.53e-04)	Tok/s 54460 (43511)	Loss/tok 3.2862 (3.1048)	LR 1.250e-04
0: TRAIN [3][3500/3880]	Time 0.192 (0.321)	Data 1.89e-04 (2.52e-04)	Tok/s 26670 (43498)	Loss/tok 2.5526 (3.1046)	LR 1.250e-04
0: TRAIN [3][3510/3880]	Time 0.265 (0.321)	Data 1.53e-04 (2.52e-04)	Tok/s 39636 (43508)	Loss/tok 2.8207 (3.1050)	LR 1.250e-04
0: TRAIN [3][3520/3880]	Time 0.265 (0.321)	Data 1.21e-04 (2.52e-04)	Tok/s 39746 (43515)	Loss/tok 2.9014 (3.1050)	LR 1.250e-04
0: TRAIN [3][3530/3880]	Time 0.268 (0.321)	Data 1.44e-04 (2.52e-04)	Tok/s 38734 (43511)	Loss/tok 2.8561 (3.1049)	LR 1.250e-04
0: TRAIN [3][3540/3880]	Time 0.347 (0.321)	Data 1.83e-04 (2.51e-04)	Tok/s 48777 (43511)	Loss/tok 3.1143 (3.1050)	LR 1.250e-04
0: TRAIN [3][3550/3880]	Time 0.193 (0.321)	Data 3.27e-04 (2.51e-04)	Tok/s 27682 (43509)	Loss/tok 2.6339 (3.1049)	LR 1.250e-04
0: TRAIN [3][3560/3880]	Time 0.266 (0.321)	Data 1.25e-04 (2.51e-04)	Tok/s 38943 (43501)	Loss/tok 2.8929 (3.1047)	LR 1.250e-04
0: TRAIN [3][3570/3880]	Time 0.265 (0.321)	Data 1.89e-04 (2.50e-04)	Tok/s 38442 (43501)	Loss/tok 3.0480 (3.1046)	LR 1.250e-04
0: TRAIN [3][3580/3880]	Time 0.433 (0.321)	Data 1.44e-04 (2.50e-04)	Tok/s 54590 (43498)	Loss/tok 3.2460 (3.1046)	LR 1.250e-04
0: TRAIN [3][3590/3880]	Time 0.347 (0.321)	Data 2.60e-04 (2.50e-04)	Tok/s 48687 (43507)	Loss/tok 3.0845 (3.1047)	LR 1.250e-04
0: TRAIN [3][3600/3880]	Time 0.266 (0.321)	Data 1.42e-04 (2.50e-04)	Tok/s 38112 (43516)	Loss/tok 2.9158 (3.1051)	LR 1.250e-04
0: TRAIN [3][3610/3880]	Time 0.347 (0.321)	Data 1.68e-04 (2.49e-04)	Tok/s 48723 (43514)	Loss/tok 3.1480 (3.1052)	LR 1.250e-04
0: TRAIN [3][3620/3880]	Time 0.266 (0.321)	Data 1.26e-04 (2.49e-04)	Tok/s 38792 (43512)	Loss/tok 2.9250 (3.1051)	LR 1.250e-04
0: TRAIN [3][3630/3880]	Time 0.431 (0.321)	Data 1.46e-04 (2.49e-04)	Tok/s 54395 (43519)	Loss/tok 3.2653 (3.1053)	LR 1.250e-04
0: TRAIN [3][3640/3880]	Time 0.266 (0.321)	Data 1.28e-04 (2.49e-04)	Tok/s 38167 (43519)	Loss/tok 2.9352 (3.1053)	LR 1.250e-04
0: TRAIN [3][3650/3880]	Time 0.268 (0.321)	Data 1.42e-04 (2.49e-04)	Tok/s 38276 (43518)	Loss/tok 2.7899 (3.1053)	LR 1.250e-04
0: TRAIN [3][3660/3880]	Time 0.431 (0.321)	Data 1.25e-04 (2.48e-04)	Tok/s 54254 (43519)	Loss/tok 3.2601 (3.1053)	LR 1.250e-04
0: TRAIN [3][3670/3880]	Time 0.266 (0.321)	Data 1.73e-04 (2.48e-04)	Tok/s 38481 (43522)	Loss/tok 2.8680 (3.1053)	LR 1.250e-04
0: TRAIN [3][3680/3880]	Time 0.267 (0.321)	Data 1.37e-04 (2.48e-04)	Tok/s 38903 (43524)	Loss/tok 2.9050 (3.1055)	LR 1.250e-04
0: TRAIN [3][3690/3880]	Time 0.265 (0.321)	Data 1.62e-04 (2.48e-04)	Tok/s 38926 (43523)	Loss/tok 2.7891 (3.1053)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][3700/3880]	Time 0.266 (0.321)	Data 1.67e-04 (2.47e-04)	Tok/s 39095 (43523)	Loss/tok 2.8972 (3.1054)	LR 1.250e-04
0: TRAIN [3][3710/3880]	Time 0.270 (0.321)	Data 1.46e-04 (2.47e-04)	Tok/s 37844 (43529)	Loss/tok 2.8382 (3.1057)	LR 1.250e-04
0: TRAIN [3][3720/3880]	Time 0.545 (0.322)	Data 1.45e-04 (2.47e-04)	Tok/s 54682 (43534)	Loss/tok 3.4587 (3.1058)	LR 1.250e-04
0: TRAIN [3][3730/3880]	Time 0.348 (0.321)	Data 1.89e-04 (2.47e-04)	Tok/s 47987 (43531)	Loss/tok 3.0498 (3.1056)	LR 1.250e-04
0: TRAIN [3][3740/3880]	Time 0.431 (0.321)	Data 1.50e-04 (2.47e-04)	Tok/s 54295 (43535)	Loss/tok 3.2987 (3.1057)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3750/3880]	Time 0.195 (0.321)	Data 1.29e-04 (2.46e-04)	Tok/s 27740 (43533)	Loss/tok 2.5446 (3.1057)	LR 1.250e-04
0: TRAIN [3][3760/3880]	Time 0.357 (0.322)	Data 1.33e-04 (2.46e-04)	Tok/s 46895 (43529)	Loss/tok 3.0986 (3.1058)	LR 1.250e-04
0: TRAIN [3][3770/3880]	Time 0.266 (0.321)	Data 1.40e-04 (2.46e-04)	Tok/s 39608 (43527)	Loss/tok 2.8681 (3.1057)	LR 1.250e-04
0: TRAIN [3][3780/3880]	Time 0.545 (0.321)	Data 1.33e-04 (2.46e-04)	Tok/s 54203 (43524)	Loss/tok 3.5203 (3.1058)	LR 1.250e-04
0: TRAIN [3][3790/3880]	Time 0.266 (0.322)	Data 1.41e-04 (2.45e-04)	Tok/s 38057 (43525)	Loss/tok 2.8753 (3.1060)	LR 1.250e-04
0: TRAIN [3][3800/3880]	Time 0.266 (0.321)	Data 1.46e-04 (2.45e-04)	Tok/s 39056 (43521)	Loss/tok 2.9641 (3.1058)	LR 1.250e-04
0: TRAIN [3][3810/3880]	Time 0.347 (0.322)	Data 1.32e-04 (2.45e-04)	Tok/s 48507 (43529)	Loss/tok 3.0452 (3.1059)	LR 1.250e-04
0: TRAIN [3][3820/3880]	Time 0.348 (0.321)	Data 1.31e-04 (2.45e-04)	Tok/s 48110 (43528)	Loss/tok 3.0667 (3.1058)	LR 1.250e-04
0: TRAIN [3][3830/3880]	Time 0.266 (0.321)	Data 1.38e-04 (2.44e-04)	Tok/s 38573 (43524)	Loss/tok 2.9467 (3.1056)	LR 1.250e-04
0: TRAIN [3][3840/3880]	Time 0.346 (0.321)	Data 1.37e-04 (2.44e-04)	Tok/s 48924 (43524)	Loss/tok 3.1120 (3.1055)	LR 1.250e-04
0: TRAIN [3][3850/3880]	Time 0.430 (0.321)	Data 1.52e-04 (2.44e-04)	Tok/s 53770 (43536)	Loss/tok 3.2988 (3.1056)	LR 1.250e-04
0: TRAIN [3][3860/3880]	Time 0.266 (0.321)	Data 1.50e-04 (2.43e-04)	Tok/s 38871 (43524)	Loss/tok 3.0085 (3.1055)	LR 1.250e-04
0: TRAIN [3][3870/3880]	Time 0.265 (0.321)	Data 1.39e-04 (2.43e-04)	Tok/s 38889 (43519)	Loss/tok 2.9616 (3.1053)	LR 1.250e-04
:::MLL 1576201496.774 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1576201496.775 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/6]	Time 0.916 (0.916)	Decoder iters 108.0 (108.0)	Tok/s 18061 (18061)
0: Running moses detokenizer
0: BLEU(score=23.70570089979571, counts=[37039, 18521, 10484, 6182], totals=[65850, 62847, 59844, 56847], precisions=[56.24753227031131, 29.469982656292267, 17.51888242764521, 10.874804299259416], bp=1.0, sys_len=65850, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1576201501.022 eval_accuracy: {"value": 23.71, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1576201501.023 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.1056	Test BLEU: 23.71
0: Performance: Epoch: 3	Training: 174100 Tok/s
0: Finished epoch 3
:::MLL 1576201501.023 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
:::MLL 1576201501.024 block_start: {"value": null, "metadata": {"first_epoch_num": 5, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1576201501.024 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 514}}
0: Starting epoch 4
0: Executing preallocation
0: Sampler for epoch 4 uses seed 3134422949
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][0/3880]	Time 1.004 (1.004)	Data 3.61e-01 (3.61e-01)	Tok/s 29456 (29456)	Loss/tok 3.3414 (3.3414)	LR 1.250e-04
0: TRAIN [4][10/3880]	Time 0.265 (0.378)	Data 1.57e-04 (3.30e-02)	Tok/s 38766 (41477)	Loss/tok 2.7769 (3.0436)	LR 1.250e-04
0: TRAIN [4][20/3880]	Time 0.194 (0.367)	Data 1.67e-04 (1.74e-02)	Tok/s 27474 (43391)	Loss/tok 2.3842 (3.1086)	LR 1.250e-04
0: TRAIN [4][30/3880]	Time 0.432 (0.360)	Data 1.30e-04 (1.18e-02)	Tok/s 53834 (43952)	Loss/tok 3.2354 (3.1155)	LR 1.250e-04
0: TRAIN [4][40/3880]	Time 0.193 (0.350)	Data 1.56e-04 (8.97e-03)	Tok/s 28196 (43668)	Loss/tok 2.5746 (3.1115)	LR 1.250e-04
0: TRAIN [4][50/3880]	Time 0.191 (0.344)	Data 2.14e-04 (7.24e-03)	Tok/s 27815 (43620)	Loss/tok 2.5026 (3.1020)	LR 1.250e-04
0: TRAIN [4][60/3880]	Time 0.349 (0.351)	Data 1.29e-04 (6.08e-03)	Tok/s 48967 (44572)	Loss/tok 3.1035 (3.1208)	LR 1.250e-04
0: TRAIN [4][70/3880]	Time 0.266 (0.344)	Data 1.81e-04 (5.24e-03)	Tok/s 38562 (44110)	Loss/tok 2.8619 (3.1096)	LR 1.250e-04
0: TRAIN [4][80/3880]	Time 0.348 (0.340)	Data 2.10e-04 (4.62e-03)	Tok/s 48138 (43821)	Loss/tok 3.1565 (3.1114)	LR 1.250e-04
0: TRAIN [4][90/3880]	Time 0.544 (0.338)	Data 1.48e-04 (4.12e-03)	Tok/s 54448 (43713)	Loss/tok 3.5411 (3.1110)	LR 1.250e-04
0: TRAIN [4][100/3880]	Time 0.433 (0.336)	Data 1.40e-04 (3.73e-03)	Tok/s 53990 (43837)	Loss/tok 3.2346 (3.1052)	LR 1.250e-04
0: TRAIN [4][110/3880]	Time 0.194 (0.330)	Data 1.71e-04 (3.41e-03)	Tok/s 27118 (43306)	Loss/tok 2.4963 (3.0924)	LR 1.250e-04
0: TRAIN [4][120/3880]	Time 0.266 (0.330)	Data 1.49e-04 (3.14e-03)	Tok/s 38605 (43403)	Loss/tok 2.7939 (3.0885)	LR 1.250e-04
0: TRAIN [4][130/3880]	Time 0.348 (0.330)	Data 1.44e-04 (2.91e-03)	Tok/s 48193 (43443)	Loss/tok 3.1616 (3.0940)	LR 1.250e-04
0: TRAIN [4][140/3880]	Time 0.356 (0.331)	Data 1.43e-04 (2.71e-03)	Tok/s 47607 (43505)	Loss/tok 3.0194 (3.0954)	LR 1.250e-04
0: TRAIN [4][150/3880]	Time 0.272 (0.329)	Data 2.58e-04 (2.55e-03)	Tok/s 38517 (43408)	Loss/tok 2.8669 (3.0905)	LR 1.250e-04
0: TRAIN [4][160/3880]	Time 0.542 (0.331)	Data 1.84e-04 (2.40e-03)	Tok/s 55116 (43694)	Loss/tok 3.4372 (3.0987)	LR 1.250e-04
0: TRAIN [4][170/3880]	Time 0.266 (0.333)	Data 1.88e-04 (2.27e-03)	Tok/s 38814 (43909)	Loss/tok 2.9575 (3.1016)	LR 1.250e-04
0: TRAIN [4][180/3880]	Time 0.348 (0.332)	Data 1.29e-04 (2.15e-03)	Tok/s 48039 (43973)	Loss/tok 3.1080 (3.0980)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][190/3880]	Time 0.266 (0.333)	Data 1.30e-04 (2.05e-03)	Tok/s 38602 (43974)	Loss/tok 2.7303 (3.0992)	LR 1.250e-04
0: TRAIN [4][200/3880]	Time 0.347 (0.332)	Data 1.73e-04 (1.95e-03)	Tok/s 48370 (44008)	Loss/tok 3.0332 (3.0968)	LR 1.250e-04
0: TRAIN [4][210/3880]	Time 0.347 (0.332)	Data 1.77e-04 (1.87e-03)	Tok/s 48246 (43992)	Loss/tok 3.0802 (3.0944)	LR 1.250e-04
0: TRAIN [4][220/3880]	Time 0.267 (0.331)	Data 1.35e-04 (1.79e-03)	Tok/s 38622 (44002)	Loss/tok 2.9857 (3.0943)	LR 1.250e-04
0: TRAIN [4][230/3880]	Time 0.433 (0.332)	Data 1.27e-04 (1.72e-03)	Tok/s 53435 (44010)	Loss/tok 3.3242 (3.0993)	LR 1.250e-04
0: TRAIN [4][240/3880]	Time 0.266 (0.332)	Data 1.26e-04 (1.65e-03)	Tok/s 38243 (44035)	Loss/tok 2.8253 (3.0986)	LR 1.250e-04
0: TRAIN [4][250/3880]	Time 0.266 (0.330)	Data 1.46e-04 (1.59e-03)	Tok/s 38407 (43931)	Loss/tok 2.9261 (3.0952)	LR 1.250e-04
0: TRAIN [4][260/3880]	Time 0.345 (0.330)	Data 1.30e-04 (1.54e-03)	Tok/s 48649 (43925)	Loss/tok 3.1436 (3.0979)	LR 1.250e-04
0: TRAIN [4][270/3880]	Time 0.269 (0.328)	Data 1.25e-04 (1.49e-03)	Tok/s 38261 (43672)	Loss/tok 2.8040 (3.0924)	LR 1.250e-04
0: TRAIN [4][280/3880]	Time 0.267 (0.327)	Data 1.41e-04 (1.44e-03)	Tok/s 39276 (43635)	Loss/tok 2.8838 (3.0909)	LR 1.250e-04
0: TRAIN [4][290/3880]	Time 0.430 (0.328)	Data 1.79e-04 (1.40e-03)	Tok/s 54381 (43763)	Loss/tok 3.2870 (3.0927)	LR 1.250e-04
0: TRAIN [4][300/3880]	Time 0.266 (0.327)	Data 1.21e-04 (1.36e-03)	Tok/s 38046 (43743)	Loss/tok 2.8403 (3.0907)	LR 1.250e-04
0: TRAIN [4][310/3880]	Time 0.346 (0.326)	Data 1.35e-04 (1.32e-03)	Tok/s 48288 (43658)	Loss/tok 3.0758 (3.0891)	LR 1.250e-04
0: TRAIN [4][320/3880]	Time 0.266 (0.325)	Data 1.34e-04 (1.28e-03)	Tok/s 39102 (43634)	Loss/tok 2.9343 (3.0874)	LR 1.250e-04
0: TRAIN [4][330/3880]	Time 0.266 (0.326)	Data 1.69e-04 (1.25e-03)	Tok/s 39208 (43681)	Loss/tok 2.8878 (3.0887)	LR 1.250e-04
0: TRAIN [4][340/3880]	Time 0.347 (0.325)	Data 1.29e-04 (1.21e-03)	Tok/s 48101 (43652)	Loss/tok 3.1075 (3.0869)	LR 1.250e-04
0: TRAIN [4][350/3880]	Time 0.433 (0.325)	Data 1.30e-04 (1.18e-03)	Tok/s 53908 (43627)	Loss/tok 3.2139 (3.0861)	LR 1.250e-04
0: TRAIN [4][360/3880]	Time 0.265 (0.324)	Data 1.34e-04 (1.15e-03)	Tok/s 39534 (43577)	Loss/tok 2.8545 (3.0846)	LR 1.250e-04
0: TRAIN [4][370/3880]	Time 0.193 (0.324)	Data 1.38e-04 (1.13e-03)	Tok/s 27161 (43518)	Loss/tok 2.4994 (3.0844)	LR 1.250e-04
0: TRAIN [4][380/3880]	Time 0.432 (0.323)	Data 1.25e-04 (1.10e-03)	Tok/s 54227 (43451)	Loss/tok 3.3221 (3.0846)	LR 1.250e-04
0: TRAIN [4][390/3880]	Time 0.266 (0.323)	Data 1.34e-04 (1.08e-03)	Tok/s 39383 (43424)	Loss/tok 3.0562 (3.0835)	LR 1.250e-04
0: TRAIN [4][400/3880]	Time 0.346 (0.322)	Data 1.44e-04 (1.05e-03)	Tok/s 47850 (43364)	Loss/tok 3.1711 (3.0837)	LR 1.250e-04
0: TRAIN [4][410/3880]	Time 0.543 (0.323)	Data 1.47e-04 (1.03e-03)	Tok/s 54814 (43414)	Loss/tok 3.5146 (3.0877)	LR 1.250e-04
0: TRAIN [4][420/3880]	Time 0.346 (0.323)	Data 1.31e-04 (1.01e-03)	Tok/s 48028 (43416)	Loss/tok 3.0792 (3.0866)	LR 1.250e-04
0: TRAIN [4][430/3880]	Time 0.346 (0.323)	Data 1.31e-04 (9.90e-04)	Tok/s 48269 (43448)	Loss/tok 3.0532 (3.0858)	LR 1.250e-04
0: TRAIN [4][440/3880]	Time 0.431 (0.323)	Data 1.32e-04 (9.71e-04)	Tok/s 54126 (43499)	Loss/tok 3.3607 (3.0860)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][450/3880]	Time 0.271 (0.323)	Data 1.24e-04 (9.53e-04)	Tok/s 37963 (43517)	Loss/tok 2.9203 (3.0855)	LR 1.250e-04
0: TRAIN [4][460/3880]	Time 0.431 (0.323)	Data 1.35e-04 (9.35e-04)	Tok/s 53968 (43538)	Loss/tok 3.2130 (3.0873)	LR 1.250e-04
0: TRAIN [4][470/3880]	Time 0.267 (0.323)	Data 1.80e-04 (9.19e-04)	Tok/s 38193 (43557)	Loss/tok 2.9351 (3.0871)	LR 1.250e-04
0: TRAIN [4][480/3880]	Time 0.266 (0.323)	Data 1.60e-04 (9.03e-04)	Tok/s 38355 (43512)	Loss/tok 2.8758 (3.0868)	LR 1.250e-04
0: TRAIN [4][490/3880]	Time 0.347 (0.322)	Data 1.44e-04 (8.87e-04)	Tok/s 48540 (43478)	Loss/tok 3.0888 (3.0856)	LR 1.250e-04
0: TRAIN [4][500/3880]	Time 0.267 (0.322)	Data 1.32e-04 (8.73e-04)	Tok/s 38510 (43470)	Loss/tok 2.8547 (3.0851)	LR 1.250e-04
0: TRAIN [4][510/3880]	Time 0.347 (0.322)	Data 1.79e-04 (8.58e-04)	Tok/s 47660 (43430)	Loss/tok 3.0811 (3.0842)	LR 1.250e-04
0: TRAIN [4][520/3880]	Time 0.191 (0.321)	Data 1.80e-04 (8.45e-04)	Tok/s 27336 (43381)	Loss/tok 2.5597 (3.0829)	LR 1.250e-04
0: TRAIN [4][530/3880]	Time 0.429 (0.321)	Data 1.67e-04 (8.32e-04)	Tok/s 54631 (43422)	Loss/tok 3.3009 (3.0833)	LR 1.250e-04
0: TRAIN [4][540/3880]	Time 0.544 (0.321)	Data 1.36e-04 (8.20e-04)	Tok/s 53829 (43370)	Loss/tok 3.5175 (3.0832)	LR 1.250e-04
0: TRAIN [4][550/3880]	Time 0.191 (0.321)	Data 1.60e-04 (8.08e-04)	Tok/s 27501 (43356)	Loss/tok 2.5439 (3.0827)	LR 1.250e-04
0: TRAIN [4][560/3880]	Time 0.347 (0.321)	Data 1.30e-04 (7.96e-04)	Tok/s 49201 (43429)	Loss/tok 3.0072 (3.0830)	LR 1.250e-04
0: TRAIN [4][570/3880]	Time 0.193 (0.321)	Data 1.76e-04 (7.84e-04)	Tok/s 26988 (43410)	Loss/tok 2.6090 (3.0830)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][580/3880]	Time 0.347 (0.321)	Data 1.33e-04 (7.73e-04)	Tok/s 48557 (43415)	Loss/tok 3.0397 (3.0824)	LR 1.250e-04
0: TRAIN [4][590/3880]	Time 0.347 (0.321)	Data 1.41e-04 (7.63e-04)	Tok/s 48543 (43417)	Loss/tok 3.0306 (3.0812)	LR 1.250e-04
0: TRAIN [4][600/3880]	Time 0.545 (0.321)	Data 1.30e-04 (7.53e-04)	Tok/s 54320 (43446)	Loss/tok 3.4586 (3.0838)	LR 1.250e-04
0: TRAIN [4][610/3880]	Time 0.265 (0.321)	Data 1.34e-04 (7.43e-04)	Tok/s 39778 (43428)	Loss/tok 2.9142 (3.0837)	LR 1.250e-04
0: TRAIN [4][620/3880]	Time 0.265 (0.322)	Data 1.52e-04 (7.34e-04)	Tok/s 39717 (43478)	Loss/tok 2.8571 (3.0845)	LR 1.250e-04
0: TRAIN [4][630/3880]	Time 0.348 (0.322)	Data 1.28e-04 (7.24e-04)	Tok/s 47999 (43480)	Loss/tok 3.0543 (3.0849)	LR 1.250e-04
0: TRAIN [4][640/3880]	Time 0.431 (0.322)	Data 1.31e-04 (7.15e-04)	Tok/s 55491 (43519)	Loss/tok 3.1984 (3.0854)	LR 1.250e-04
0: TRAIN [4][650/3880]	Time 0.346 (0.322)	Data 1.74e-04 (7.07e-04)	Tok/s 48560 (43538)	Loss/tok 3.0105 (3.0848)	LR 1.250e-04
0: TRAIN [4][660/3880]	Time 0.430 (0.322)	Data 1.43e-04 (6.98e-04)	Tok/s 54562 (43569)	Loss/tok 3.2140 (3.0856)	LR 1.250e-04
0: TRAIN [4][670/3880]	Time 0.350 (0.322)	Data 1.34e-04 (6.90e-04)	Tok/s 48290 (43568)	Loss/tok 3.1790 (3.0847)	LR 1.250e-04
0: TRAIN [4][680/3880]	Time 0.430 (0.322)	Data 1.29e-04 (6.82e-04)	Tok/s 53939 (43553)	Loss/tok 3.2967 (3.0849)	LR 1.250e-04
0: TRAIN [4][690/3880]	Time 0.265 (0.322)	Data 1.43e-04 (6.74e-04)	Tok/s 38919 (43510)	Loss/tok 2.9055 (3.0853)	LR 1.250e-04
0: TRAIN [4][700/3880]	Time 0.430 (0.322)	Data 1.38e-04 (6.67e-04)	Tok/s 54454 (43524)	Loss/tok 3.1529 (3.0850)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][710/3880]	Time 0.348 (0.322)	Data 1.26e-04 (6.59e-04)	Tok/s 48533 (43561)	Loss/tok 3.0727 (3.0864)	LR 1.250e-04
0: TRAIN [4][720/3880]	Time 0.268 (0.322)	Data 1.35e-04 (6.53e-04)	Tok/s 38600 (43547)	Loss/tok 2.8959 (3.0869)	LR 1.250e-04
0: TRAIN [4][730/3880]	Time 0.265 (0.322)	Data 1.40e-04 (6.47e-04)	Tok/s 39469 (43547)	Loss/tok 2.8029 (3.0871)	LR 1.250e-04
0: TRAIN [4][740/3880]	Time 0.440 (0.323)	Data 1.58e-04 (6.40e-04)	Tok/s 53409 (43602)	Loss/tok 3.2587 (3.0878)	LR 1.250e-04
0: TRAIN [4][750/3880]	Time 0.266 (0.322)	Data 2.03e-04 (6.33e-04)	Tok/s 38849 (43558)	Loss/tok 3.0062 (3.0868)	LR 1.250e-04
0: TRAIN [4][760/3880]	Time 0.265 (0.322)	Data 1.32e-04 (6.27e-04)	Tok/s 39376 (43523)	Loss/tok 2.9098 (3.0855)	LR 1.250e-04
0: TRAIN [4][770/3880]	Time 0.348 (0.321)	Data 1.36e-04 (6.21e-04)	Tok/s 48385 (43496)	Loss/tok 3.0248 (3.0844)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][780/3880]	Time 0.266 (0.321)	Data 1.34e-04 (6.15e-04)	Tok/s 38226 (43449)	Loss/tok 2.9086 (3.0841)	LR 1.250e-04
0: TRAIN [4][790/3880]	Time 0.265 (0.321)	Data 1.65e-04 (6.09e-04)	Tok/s 38731 (43436)	Loss/tok 2.8818 (3.0832)	LR 1.250e-04
0: TRAIN [4][800/3880]	Time 0.266 (0.321)	Data 1.30e-04 (6.03e-04)	Tok/s 38385 (43462)	Loss/tok 2.9015 (3.0827)	LR 1.250e-04
0: TRAIN [4][810/3880]	Time 0.268 (0.320)	Data 1.50e-04 (5.97e-04)	Tok/s 39367 (43411)	Loss/tok 2.8341 (3.0817)	LR 1.250e-04
0: TRAIN [4][820/3880]	Time 0.347 (0.320)	Data 1.30e-04 (5.92e-04)	Tok/s 47991 (43443)	Loss/tok 3.1500 (3.0823)	LR 1.250e-04
0: TRAIN [4][830/3880]	Time 0.347 (0.321)	Data 1.33e-04 (5.86e-04)	Tok/s 48206 (43473)	Loss/tok 3.1074 (3.0835)	LR 1.250e-04
0: TRAIN [4][840/3880]	Time 0.266 (0.321)	Data 1.75e-04 (5.81e-04)	Tok/s 38160 (43489)	Loss/tok 2.8368 (3.0829)	LR 1.250e-04
0: TRAIN [4][850/3880]	Time 0.266 (0.321)	Data 1.34e-04 (5.76e-04)	Tok/s 38423 (43482)	Loss/tok 2.8887 (3.0833)	LR 1.250e-04
0: TRAIN [4][860/3880]	Time 0.546 (0.321)	Data 1.29e-04 (5.71e-04)	Tok/s 54404 (43515)	Loss/tok 3.4203 (3.0839)	LR 1.250e-04
0: TRAIN [4][870/3880]	Time 0.543 (0.322)	Data 1.45e-04 (5.66e-04)	Tok/s 55280 (43528)	Loss/tok 3.3734 (3.0843)	LR 1.250e-04
0: TRAIN [4][880/3880]	Time 0.195 (0.322)	Data 1.35e-04 (5.61e-04)	Tok/s 26839 (43514)	Loss/tok 2.4435 (3.0850)	LR 1.250e-04
0: TRAIN [4][890/3880]	Time 0.266 (0.322)	Data 1.41e-04 (5.57e-04)	Tok/s 39278 (43547)	Loss/tok 2.9063 (3.0857)	LR 1.250e-04
0: TRAIN [4][900/3880]	Time 0.430 (0.322)	Data 1.81e-04 (5.52e-04)	Tok/s 54608 (43570)	Loss/tok 3.2277 (3.0869)	LR 1.250e-04
0: TRAIN [4][910/3880]	Time 0.428 (0.322)	Data 2.10e-04 (5.48e-04)	Tok/s 54653 (43580)	Loss/tok 3.3363 (3.0865)	LR 1.250e-04
0: TRAIN [4][920/3880]	Time 0.266 (0.323)	Data 1.55e-04 (5.43e-04)	Tok/s 39263 (43620)	Loss/tok 2.7876 (3.0876)	LR 1.250e-04
0: TRAIN [4][930/3880]	Time 0.266 (0.322)	Data 1.49e-04 (5.39e-04)	Tok/s 39362 (43586)	Loss/tok 2.8654 (3.0874)	LR 1.250e-04
0: TRAIN [4][940/3880]	Time 0.347 (0.322)	Data 1.30e-04 (5.35e-04)	Tok/s 48182 (43586)	Loss/tok 2.9944 (3.0870)	LR 1.250e-04
0: TRAIN [4][950/3880]	Time 0.431 (0.322)	Data 1.45e-04 (5.30e-04)	Tok/s 54125 (43567)	Loss/tok 3.2275 (3.0864)	LR 1.250e-04
0: TRAIN [4][960/3880]	Time 0.347 (0.322)	Data 1.38e-04 (5.26e-04)	Tok/s 48634 (43599)	Loss/tok 3.1058 (3.0872)	LR 1.250e-04
0: TRAIN [4][970/3880]	Time 0.266 (0.322)	Data 1.85e-04 (5.23e-04)	Tok/s 40301 (43578)	Loss/tok 2.8632 (3.0865)	LR 1.250e-04
0: TRAIN [4][980/3880]	Time 0.192 (0.322)	Data 1.78e-04 (5.19e-04)	Tok/s 27069 (43560)	Loss/tok 2.4436 (3.0862)	LR 1.250e-04
0: TRAIN [4][990/3880]	Time 0.266 (0.322)	Data 1.34e-04 (5.15e-04)	Tok/s 38321 (43567)	Loss/tok 2.9053 (3.0861)	LR 1.250e-04
0: TRAIN [4][1000/3880]	Time 0.347 (0.322)	Data 1.24e-04 (5.12e-04)	Tok/s 48469 (43515)	Loss/tok 3.1680 (3.0850)	LR 1.250e-04
0: TRAIN [4][1010/3880]	Time 0.266 (0.321)	Data 1.21e-04 (5.08e-04)	Tok/s 38344 (43498)	Loss/tok 2.9076 (3.0842)	LR 1.250e-04
0: TRAIN [4][1020/3880]	Time 0.347 (0.321)	Data 2.18e-04 (5.05e-04)	Tok/s 48801 (43502)	Loss/tok 3.1362 (3.0840)	LR 1.250e-04
0: TRAIN [4][1030/3880]	Time 0.267 (0.321)	Data 1.49e-04 (5.01e-04)	Tok/s 39853 (43482)	Loss/tok 2.8448 (3.0831)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][1040/3880]	Time 0.347 (0.321)	Data 1.52e-04 (4.98e-04)	Tok/s 47879 (43463)	Loss/tok 3.1728 (3.0832)	LR 1.250e-04
0: TRAIN [4][1050/3880]	Time 0.347 (0.321)	Data 1.60e-04 (4.95e-04)	Tok/s 47993 (43502)	Loss/tok 3.0446 (3.0840)	LR 1.250e-04
0: TRAIN [4][1060/3880]	Time 0.265 (0.321)	Data 1.43e-04 (4.91e-04)	Tok/s 39348 (43486)	Loss/tok 2.8331 (3.0833)	LR 1.250e-04
0: TRAIN [4][1070/3880]	Time 0.265 (0.321)	Data 1.65e-04 (4.88e-04)	Tok/s 38815 (43475)	Loss/tok 2.9689 (3.0827)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1080/3880]	Time 0.266 (0.321)	Data 1.57e-04 (4.85e-04)	Tok/s 38423 (43445)	Loss/tok 2.8171 (3.0824)	LR 1.250e-04
0: TRAIN [4][1090/3880]	Time 0.265 (0.321)	Data 2.20e-04 (4.82e-04)	Tok/s 37991 (43463)	Loss/tok 2.9589 (3.0824)	LR 1.250e-04
0: TRAIN [4][1100/3880]	Time 0.347 (0.321)	Data 1.73e-04 (4.79e-04)	Tok/s 49348 (43467)	Loss/tok 3.0474 (3.0826)	LR 1.250e-04
0: TRAIN [4][1110/3880]	Time 0.348 (0.321)	Data 1.37e-04 (4.76e-04)	Tok/s 48146 (43502)	Loss/tok 3.1232 (3.0839)	LR 1.250e-04
0: TRAIN [4][1120/3880]	Time 0.347 (0.321)	Data 1.27e-04 (4.73e-04)	Tok/s 48848 (43494)	Loss/tok 3.0351 (3.0835)	LR 1.250e-04
0: TRAIN [4][1130/3880]	Time 0.347 (0.321)	Data 1.35e-04 (4.70e-04)	Tok/s 48019 (43461)	Loss/tok 3.0746 (3.0826)	LR 1.250e-04
0: TRAIN [4][1140/3880]	Time 0.265 (0.321)	Data 1.33e-04 (4.68e-04)	Tok/s 39567 (43469)	Loss/tok 2.8302 (3.0831)	LR 1.250e-04
0: TRAIN [4][1150/3880]	Time 0.266 (0.321)	Data 1.35e-04 (4.65e-04)	Tok/s 38094 (43481)	Loss/tok 3.0194 (3.0834)	LR 1.250e-04
0: TRAIN [4][1160/3880]	Time 0.266 (0.321)	Data 1.20e-04 (4.62e-04)	Tok/s 38527 (43498)	Loss/tok 2.9070 (3.0835)	LR 1.250e-04
0: TRAIN [4][1170/3880]	Time 0.267 (0.321)	Data 1.41e-04 (4.59e-04)	Tok/s 38060 (43503)	Loss/tok 2.8420 (3.0835)	LR 1.250e-04
0: TRAIN [4][1180/3880]	Time 0.544 (0.321)	Data 1.32e-04 (4.57e-04)	Tok/s 53980 (43544)	Loss/tok 3.4493 (3.0842)	LR 1.250e-04
0: TRAIN [4][1190/3880]	Time 0.276 (0.321)	Data 1.34e-04 (4.54e-04)	Tok/s 37709 (43541)	Loss/tok 2.8195 (3.0839)	LR 1.250e-04
0: TRAIN [4][1200/3880]	Time 0.266 (0.321)	Data 1.74e-04 (4.51e-04)	Tok/s 38484 (43507)	Loss/tok 3.0345 (3.0836)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1210/3880]	Time 0.256 (0.321)	Data 1.38e-04 (4.49e-04)	Tok/s 40719 (43515)	Loss/tok 2.8645 (3.0844)	LR 1.250e-04
0: TRAIN [4][1220/3880]	Time 0.347 (0.321)	Data 1.44e-04 (4.46e-04)	Tok/s 48525 (43530)	Loss/tok 3.1055 (3.0846)	LR 1.250e-04
0: TRAIN [4][1230/3880]	Time 0.266 (0.321)	Data 1.31e-04 (4.44e-04)	Tok/s 38536 (43513)	Loss/tok 2.8797 (3.0848)	LR 1.250e-04
0: TRAIN [4][1240/3880]	Time 0.347 (0.321)	Data 1.29e-04 (4.41e-04)	Tok/s 48329 (43526)	Loss/tok 3.0812 (3.0852)	LR 1.250e-04
0: TRAIN [4][1250/3880]	Time 0.348 (0.321)	Data 1.38e-04 (4.39e-04)	Tok/s 47602 (43507)	Loss/tok 3.0849 (3.0847)	LR 1.250e-04
0: TRAIN [4][1260/3880]	Time 0.430 (0.321)	Data 1.73e-04 (4.37e-04)	Tok/s 53974 (43510)	Loss/tok 3.2532 (3.0843)	LR 1.250e-04
0: TRAIN [4][1270/3880]	Time 0.266 (0.321)	Data 1.49e-04 (4.34e-04)	Tok/s 38246 (43499)	Loss/tok 2.9837 (3.0836)	LR 1.250e-04
0: TRAIN [4][1280/3880]	Time 0.431 (0.321)	Data 1.41e-04 (4.32e-04)	Tok/s 54118 (43497)	Loss/tok 3.2609 (3.0837)	LR 1.250e-04
0: TRAIN [4][1290/3880]	Time 0.432 (0.321)	Data 1.23e-04 (4.30e-04)	Tok/s 54748 (43490)	Loss/tok 3.2183 (3.0837)	LR 1.250e-04
0: TRAIN [4][1300/3880]	Time 0.348 (0.321)	Data 1.26e-04 (4.28e-04)	Tok/s 48252 (43507)	Loss/tok 3.0891 (3.0834)	LR 1.250e-04
0: TRAIN [4][1310/3880]	Time 0.433 (0.321)	Data 1.21e-04 (4.26e-04)	Tok/s 53054 (43512)	Loss/tok 3.3983 (3.0840)	LR 1.250e-04
0: TRAIN [4][1320/3880]	Time 0.265 (0.321)	Data 1.76e-04 (4.24e-04)	Tok/s 38317 (43509)	Loss/tok 2.8032 (3.0841)	LR 1.250e-04
0: TRAIN [4][1330/3880]	Time 0.267 (0.321)	Data 1.31e-04 (4.22e-04)	Tok/s 38322 (43512)	Loss/tok 2.8247 (3.0838)	LR 1.250e-04
0: TRAIN [4][1340/3880]	Time 0.266 (0.321)	Data 1.50e-04 (4.20e-04)	Tok/s 38522 (43511)	Loss/tok 2.9546 (3.0840)	LR 1.250e-04
0: TRAIN [4][1350/3880]	Time 0.430 (0.321)	Data 1.27e-04 (4.18e-04)	Tok/s 53612 (43519)	Loss/tok 3.3095 (3.0844)	LR 1.250e-04
0: TRAIN [4][1360/3880]	Time 0.348 (0.321)	Data 1.22e-04 (4.16e-04)	Tok/s 48356 (43508)	Loss/tok 3.0519 (3.0843)	LR 1.250e-04
0: TRAIN [4][1370/3880]	Time 0.347 (0.321)	Data 1.46e-04 (4.14e-04)	Tok/s 47720 (43517)	Loss/tok 3.1531 (3.0845)	LR 1.250e-04
0: TRAIN [4][1380/3880]	Time 0.347 (0.321)	Data 1.81e-04 (4.12e-04)	Tok/s 47935 (43531)	Loss/tok 3.0504 (3.0847)	LR 1.250e-04
0: TRAIN [4][1390/3880]	Time 0.194 (0.321)	Data 2.15e-04 (4.10e-04)	Tok/s 27219 (43502)	Loss/tok 2.5276 (3.0840)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1400/3880]	Time 0.192 (0.321)	Data 1.26e-04 (4.08e-04)	Tok/s 27130 (43521)	Loss/tok 2.5885 (3.0849)	LR 1.250e-04
0: TRAIN [4][1410/3880]	Time 0.347 (0.321)	Data 1.31e-04 (4.06e-04)	Tok/s 48265 (43527)	Loss/tok 3.0854 (3.0853)	LR 1.250e-04
0: TRAIN [4][1420/3880]	Time 0.266 (0.321)	Data 1.28e-04 (4.05e-04)	Tok/s 38268 (43521)	Loss/tok 2.8499 (3.0853)	LR 1.250e-04
0: TRAIN [4][1430/3880]	Time 0.431 (0.321)	Data 1.28e-04 (4.03e-04)	Tok/s 54281 (43550)	Loss/tok 3.0927 (3.0852)	LR 1.250e-04
0: TRAIN [4][1440/3880]	Time 0.192 (0.322)	Data 1.44e-04 (4.01e-04)	Tok/s 27022 (43549)	Loss/tok 2.4906 (3.0858)	LR 1.250e-04
0: TRAIN [4][1450/3880]	Time 0.430 (0.322)	Data 1.48e-04 (3.99e-04)	Tok/s 54657 (43577)	Loss/tok 3.2913 (3.0866)	LR 1.250e-04
0: TRAIN [4][1460/3880]	Time 0.266 (0.322)	Data 1.29e-04 (3.97e-04)	Tok/s 37958 (43596)	Loss/tok 2.9145 (3.0873)	LR 1.250e-04
0: TRAIN [4][1470/3880]	Time 0.348 (0.322)	Data 1.74e-04 (3.96e-04)	Tok/s 48698 (43590)	Loss/tok 2.9454 (3.0867)	LR 1.250e-04
0: TRAIN [4][1480/3880]	Time 0.346 (0.322)	Data 1.35e-04 (3.94e-04)	Tok/s 48527 (43594)	Loss/tok 3.0939 (3.0863)	LR 1.250e-04
0: TRAIN [4][1490/3880]	Time 0.266 (0.322)	Data 1.29e-04 (3.92e-04)	Tok/s 37945 (43595)	Loss/tok 2.9375 (3.0864)	LR 1.250e-04
0: TRAIN [4][1500/3880]	Time 0.265 (0.322)	Data 1.69e-04 (3.91e-04)	Tok/s 38116 (43586)	Loss/tok 2.8569 (3.0860)	LR 1.250e-04
0: TRAIN [4][1510/3880]	Time 0.266 (0.322)	Data 1.33e-04 (3.89e-04)	Tok/s 39325 (43602)	Loss/tok 2.9547 (3.0866)	LR 1.250e-04
0: TRAIN [4][1520/3880]	Time 0.349 (0.322)	Data 1.39e-04 (3.88e-04)	Tok/s 47853 (43603)	Loss/tok 2.9823 (3.0862)	LR 1.250e-04
0: TRAIN [4][1530/3880]	Time 0.347 (0.322)	Data 1.30e-04 (3.86e-04)	Tok/s 48458 (43595)	Loss/tok 3.1020 (3.0859)	LR 1.250e-04
0: TRAIN [4][1540/3880]	Time 0.430 (0.322)	Data 1.65e-04 (3.85e-04)	Tok/s 53624 (43599)	Loss/tok 3.2792 (3.0868)	LR 1.250e-04
0: TRAIN [4][1550/3880]	Time 0.265 (0.322)	Data 1.34e-04 (3.83e-04)	Tok/s 38868 (43634)	Loss/tok 2.8572 (3.0876)	LR 1.250e-04
0: TRAIN [4][1560/3880]	Time 0.432 (0.323)	Data 1.56e-04 (3.82e-04)	Tok/s 53469 (43639)	Loss/tok 3.2293 (3.0881)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1570/3880]	Time 0.419 (0.323)	Data 2.05e-04 (3.80e-04)	Tok/s 55014 (43653)	Loss/tok 3.3144 (3.0889)	LR 1.250e-04
0: TRAIN [4][1580/3880]	Time 0.265 (0.323)	Data 1.32e-04 (3.79e-04)	Tok/s 38012 (43639)	Loss/tok 2.9750 (3.0886)	LR 1.250e-04
0: TRAIN [4][1590/3880]	Time 0.193 (0.323)	Data 1.29e-04 (3.77e-04)	Tok/s 28031 (43623)	Loss/tok 2.3810 (3.0886)	LR 1.250e-04
0: TRAIN [4][1600/3880]	Time 0.266 (0.323)	Data 1.33e-04 (3.76e-04)	Tok/s 38588 (43635)	Loss/tok 2.9294 (3.0887)	LR 1.250e-04
0: TRAIN [4][1610/3880]	Time 0.432 (0.323)	Data 1.33e-04 (3.74e-04)	Tok/s 53029 (43633)	Loss/tok 3.3617 (3.0889)	LR 1.250e-04
0: TRAIN [4][1620/3880]	Time 0.266 (0.322)	Data 1.46e-04 (3.73e-04)	Tok/s 39231 (43627)	Loss/tok 2.8966 (3.0887)	LR 1.250e-04
0: TRAIN [4][1630/3880]	Time 0.429 (0.322)	Data 1.50e-04 (3.72e-04)	Tok/s 54893 (43615)	Loss/tok 3.1894 (3.0884)	LR 1.250e-04
0: TRAIN [4][1640/3880]	Time 0.348 (0.323)	Data 1.24e-04 (3.70e-04)	Tok/s 47481 (43631)	Loss/tok 3.0966 (3.0890)	LR 1.250e-04
0: TRAIN [4][1650/3880]	Time 0.348 (0.323)	Data 1.30e-04 (3.69e-04)	Tok/s 48269 (43632)	Loss/tok 3.0505 (3.0896)	LR 1.250e-04
0: TRAIN [4][1660/3880]	Time 0.348 (0.323)	Data 1.32e-04 (3.68e-04)	Tok/s 48535 (43650)	Loss/tok 3.0812 (3.0898)	LR 1.250e-04
0: TRAIN [4][1670/3880]	Time 0.266 (0.323)	Data 1.33e-04 (3.66e-04)	Tok/s 39156 (43662)	Loss/tok 2.8881 (3.0897)	LR 1.250e-04
0: TRAIN [4][1680/3880]	Time 0.347 (0.323)	Data 1.32e-04 (3.65e-04)	Tok/s 48466 (43679)	Loss/tok 2.9927 (3.0899)	LR 1.250e-04
0: TRAIN [4][1690/3880]	Time 0.265 (0.323)	Data 1.85e-04 (3.64e-04)	Tok/s 38452 (43688)	Loss/tok 2.9510 (3.0899)	LR 1.250e-04
0: TRAIN [4][1700/3880]	Time 0.195 (0.323)	Data 8.74e-04 (3.63e-04)	Tok/s 26901 (43699)	Loss/tok 2.5074 (3.0911)	LR 1.250e-04
0: TRAIN [4][1710/3880]	Time 0.265 (0.323)	Data 1.31e-04 (3.62e-04)	Tok/s 38690 (43691)	Loss/tok 2.7833 (3.0908)	LR 1.250e-04
0: TRAIN [4][1720/3880]	Time 0.266 (0.323)	Data 1.28e-04 (3.61e-04)	Tok/s 39025 (43675)	Loss/tok 2.8211 (3.0908)	LR 1.250e-04
0: TRAIN [4][1730/3880]	Time 0.265 (0.323)	Data 1.84e-04 (3.59e-04)	Tok/s 38880 (43674)	Loss/tok 2.8971 (3.0908)	LR 1.250e-04
0: TRAIN [4][1740/3880]	Time 0.266 (0.323)	Data 1.29e-04 (3.58e-04)	Tok/s 38623 (43680)	Loss/tok 2.9780 (3.0904)	LR 1.250e-04
0: TRAIN [4][1750/3880]	Time 0.347 (0.323)	Data 1.34e-04 (3.57e-04)	Tok/s 48118 (43678)	Loss/tok 3.0905 (3.0906)	LR 1.250e-04
0: TRAIN [4][1760/3880]	Time 0.267 (0.323)	Data 1.34e-04 (3.56e-04)	Tok/s 38767 (43682)	Loss/tok 3.0229 (3.0908)	LR 1.250e-04
0: TRAIN [4][1770/3880]	Time 0.266 (0.323)	Data 1.47e-04 (3.55e-04)	Tok/s 38753 (43684)	Loss/tok 2.8011 (3.0906)	LR 1.250e-04
0: TRAIN [4][1780/3880]	Time 0.267 (0.323)	Data 1.46e-04 (3.53e-04)	Tok/s 38698 (43686)	Loss/tok 2.9567 (3.0904)	LR 1.250e-04
0: TRAIN [4][1790/3880]	Time 0.347 (0.323)	Data 2.61e-04 (3.52e-04)	Tok/s 47962 (43688)	Loss/tok 3.1201 (3.0905)	LR 1.250e-04
0: TRAIN [4][1800/3880]	Time 0.267 (0.323)	Data 1.85e-04 (3.51e-04)	Tok/s 37865 (43686)	Loss/tok 2.8019 (3.0906)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1810/3880]	Time 0.347 (0.323)	Data 1.29e-04 (3.50e-04)	Tok/s 47602 (43709)	Loss/tok 3.1385 (3.0916)	LR 1.250e-04
0: TRAIN [4][1820/3880]	Time 0.347 (0.323)	Data 1.25e-04 (3.49e-04)	Tok/s 48672 (43717)	Loss/tok 3.0858 (3.0915)	LR 1.250e-04
0: TRAIN [4][1830/3880]	Time 0.347 (0.323)	Data 1.25e-04 (3.48e-04)	Tok/s 47893 (43713)	Loss/tok 3.2128 (3.0912)	LR 1.250e-04
0: TRAIN [4][1840/3880]	Time 0.266 (0.323)	Data 1.28e-04 (3.47e-04)	Tok/s 38412 (43712)	Loss/tok 2.9311 (3.0911)	LR 1.250e-04
0: TRAIN [4][1850/3880]	Time 0.193 (0.323)	Data 1.29e-04 (3.46e-04)	Tok/s 27710 (43701)	Loss/tok 2.5540 (3.0909)	LR 1.250e-04
0: TRAIN [4][1860/3880]	Time 0.192 (0.323)	Data 1.36e-04 (3.45e-04)	Tok/s 27416 (43675)	Loss/tok 2.6041 (3.0905)	LR 1.250e-04
0: TRAIN [4][1870/3880]	Time 0.430 (0.323)	Data 1.33e-04 (3.44e-04)	Tok/s 54815 (43703)	Loss/tok 3.2871 (3.0910)	LR 1.250e-04
0: TRAIN [4][1880/3880]	Time 0.432 (0.323)	Data 1.29e-04 (3.43e-04)	Tok/s 54269 (43725)	Loss/tok 3.2415 (3.0913)	LR 1.250e-04
0: TRAIN [4][1890/3880]	Time 0.266 (0.323)	Data 1.38e-04 (3.42e-04)	Tok/s 38529 (43753)	Loss/tok 2.9301 (3.0918)	LR 1.250e-04
0: TRAIN [4][1900/3880]	Time 0.266 (0.323)	Data 1.61e-04 (3.41e-04)	Tok/s 38752 (43738)	Loss/tok 2.8696 (3.0916)	LR 1.250e-04
0: TRAIN [4][1910/3880]	Time 0.270 (0.324)	Data 1.26e-04 (3.40e-04)	Tok/s 38030 (43757)	Loss/tok 2.9270 (3.0920)	LR 1.250e-04
0: TRAIN [4][1920/3880]	Time 0.267 (0.324)	Data 1.40e-04 (3.39e-04)	Tok/s 38417 (43754)	Loss/tok 2.8843 (3.0917)	LR 1.250e-04
0: TRAIN [4][1930/3880]	Time 0.266 (0.323)	Data 1.29e-04 (3.38e-04)	Tok/s 39335 (43729)	Loss/tok 2.9709 (3.0913)	LR 1.250e-04
0: TRAIN [4][1940/3880]	Time 0.193 (0.323)	Data 1.84e-04 (3.37e-04)	Tok/s 27375 (43711)	Loss/tok 2.4508 (3.0909)	LR 1.250e-04
0: TRAIN [4][1950/3880]	Time 0.265 (0.323)	Data 1.28e-04 (3.36e-04)	Tok/s 39291 (43697)	Loss/tok 2.8118 (3.0906)	LR 1.250e-04
0: TRAIN [4][1960/3880]	Time 0.268 (0.323)	Data 1.33e-04 (3.35e-04)	Tok/s 38400 (43700)	Loss/tok 2.9288 (3.0912)	LR 1.250e-04
0: TRAIN [4][1970/3880]	Time 0.269 (0.323)	Data 1.78e-04 (3.34e-04)	Tok/s 37712 (43700)	Loss/tok 2.9967 (3.0910)	LR 1.250e-04
0: TRAIN [4][1980/3880]	Time 0.266 (0.323)	Data 1.24e-04 (3.33e-04)	Tok/s 38747 (43696)	Loss/tok 2.9307 (3.0912)	LR 1.250e-04
0: TRAIN [4][1990/3880]	Time 0.430 (0.323)	Data 1.33e-04 (3.32e-04)	Tok/s 53779 (43716)	Loss/tok 3.2691 (3.0918)	LR 1.250e-04
0: TRAIN [4][2000/3880]	Time 0.432 (0.324)	Data 1.56e-04 (3.31e-04)	Tok/s 53094 (43743)	Loss/tok 3.3425 (3.0922)	LR 1.250e-04
0: TRAIN [4][2010/3880]	Time 0.266 (0.323)	Data 1.44e-04 (3.30e-04)	Tok/s 38954 (43733)	Loss/tok 2.8898 (3.0916)	LR 1.250e-04
0: TRAIN [4][2020/3880]	Time 0.265 (0.323)	Data 1.27e-04 (3.29e-04)	Tok/s 39904 (43738)	Loss/tok 2.9126 (3.0912)	LR 1.250e-04
0: TRAIN [4][2030/3880]	Time 0.266 (0.323)	Data 1.30e-04 (3.28e-04)	Tok/s 38839 (43737)	Loss/tok 2.9501 (3.0912)	LR 1.250e-04
0: TRAIN [4][2040/3880]	Time 0.430 (0.323)	Data 1.32e-04 (3.27e-04)	Tok/s 54167 (43736)	Loss/tok 3.2261 (3.0911)	LR 1.250e-04
0: TRAIN [4][2050/3880]	Time 0.430 (0.323)	Data 1.44e-04 (3.26e-04)	Tok/s 54457 (43727)	Loss/tok 3.2282 (3.0908)	LR 1.250e-04
0: TRAIN [4][2060/3880]	Time 0.347 (0.323)	Data 1.34e-04 (3.26e-04)	Tok/s 48453 (43752)	Loss/tok 3.0302 (3.0913)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][2070/3880]	Time 0.266 (0.324)	Data 2.43e-04 (3.25e-04)	Tok/s 38405 (43767)	Loss/tok 2.8784 (3.0923)	LR 1.250e-04
0: TRAIN [4][2080/3880]	Time 0.430 (0.324)	Data 1.31e-04 (3.24e-04)	Tok/s 54347 (43778)	Loss/tok 3.2955 (3.0925)	LR 1.250e-04
0: TRAIN [4][2090/3880]	Time 0.346 (0.324)	Data 1.41e-04 (3.23e-04)	Tok/s 48896 (43762)	Loss/tok 3.1486 (3.0920)	LR 1.250e-04
0: TRAIN [4][2100/3880]	Time 0.431 (0.324)	Data 1.24e-04 (3.23e-04)	Tok/s 54427 (43774)	Loss/tok 3.1794 (3.0921)	LR 1.250e-04
0: TRAIN [4][2110/3880]	Time 0.348 (0.324)	Data 1.34e-04 (3.22e-04)	Tok/s 48401 (43769)	Loss/tok 3.0433 (3.0921)	LR 1.250e-04
0: TRAIN [4][2120/3880]	Time 0.363 (0.323)	Data 1.83e-04 (3.21e-04)	Tok/s 46493 (43738)	Loss/tok 3.0696 (3.0915)	LR 1.250e-04
0: TRAIN [4][2130/3880]	Time 0.266 (0.323)	Data 1.43e-04 (3.20e-04)	Tok/s 38258 (43719)	Loss/tok 2.8966 (3.0910)	LR 1.250e-04
0: TRAIN [4][2140/3880]	Time 0.348 (0.323)	Data 1.30e-04 (3.19e-04)	Tok/s 48528 (43740)	Loss/tok 3.0358 (3.0912)	LR 1.250e-04
0: TRAIN [4][2150/3880]	Time 0.347 (0.323)	Data 1.51e-04 (3.19e-04)	Tok/s 47815 (43729)	Loss/tok 3.0603 (3.0910)	LR 1.250e-04
0: TRAIN [4][2160/3880]	Time 0.193 (0.323)	Data 1.32e-04 (3.18e-04)	Tok/s 27653 (43739)	Loss/tok 2.4636 (3.0914)	LR 1.250e-04
0: TRAIN [4][2170/3880]	Time 0.193 (0.323)	Data 1.36e-04 (3.17e-04)	Tok/s 27850 (43720)	Loss/tok 2.5231 (3.0909)	LR 1.250e-04
0: TRAIN [4][2180/3880]	Time 0.431 (0.323)	Data 1.28e-04 (3.16e-04)	Tok/s 54152 (43709)	Loss/tok 3.2460 (3.0906)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2190/3880]	Time 0.266 (0.323)	Data 1.58e-04 (3.16e-04)	Tok/s 38813 (43701)	Loss/tok 2.7840 (3.0909)	LR 1.250e-04
0: TRAIN [4][2200/3880]	Time 0.431 (0.323)	Data 1.25e-04 (3.15e-04)	Tok/s 53249 (43709)	Loss/tok 3.3583 (3.0912)	LR 1.250e-04
0: TRAIN [4][2210/3880]	Time 0.269 (0.323)	Data 1.32e-04 (3.14e-04)	Tok/s 38004 (43714)	Loss/tok 2.9081 (3.0914)	LR 1.250e-04
0: TRAIN [4][2220/3880]	Time 0.267 (0.323)	Data 1.41e-04 (3.13e-04)	Tok/s 39074 (43682)	Loss/tok 2.9251 (3.0907)	LR 1.250e-04
0: TRAIN [4][2230/3880]	Time 0.432 (0.323)	Data 1.59e-04 (3.12e-04)	Tok/s 54691 (43687)	Loss/tok 3.2599 (3.0908)	LR 1.250e-04
0: TRAIN [4][2240/3880]	Time 0.266 (0.323)	Data 1.31e-04 (3.12e-04)	Tok/s 38272 (43684)	Loss/tok 2.7837 (3.0909)	LR 1.250e-04
0: TRAIN [4][2250/3880]	Time 0.346 (0.323)	Data 2.19e-04 (3.11e-04)	Tok/s 48372 (43680)	Loss/tok 3.1029 (3.0910)	LR 1.250e-04
0: TRAIN [4][2260/3880]	Time 0.269 (0.323)	Data 1.79e-04 (3.10e-04)	Tok/s 38507 (43682)	Loss/tok 2.8999 (3.0909)	LR 1.250e-04
0: TRAIN [4][2270/3880]	Time 0.266 (0.323)	Data 1.39e-04 (3.09e-04)	Tok/s 39207 (43689)	Loss/tok 2.8767 (3.0911)	LR 1.250e-04
0: TRAIN [4][2280/3880]	Time 0.195 (0.323)	Data 1.31e-04 (3.09e-04)	Tok/s 27293 (43697)	Loss/tok 2.6025 (3.0913)	LR 1.250e-04
0: TRAIN [4][2290/3880]	Time 0.265 (0.323)	Data 1.25e-04 (3.08e-04)	Tok/s 38899 (43685)	Loss/tok 2.8846 (3.0910)	LR 1.250e-04
0: TRAIN [4][2300/3880]	Time 0.265 (0.323)	Data 1.41e-04 (3.07e-04)	Tok/s 39290 (43666)	Loss/tok 2.8059 (3.0906)	LR 1.250e-04
0: TRAIN [4][2310/3880]	Time 0.347 (0.323)	Data 1.94e-04 (3.07e-04)	Tok/s 48244 (43666)	Loss/tok 3.0541 (3.0905)	LR 1.250e-04
0: TRAIN [4][2320/3880]	Time 0.272 (0.323)	Data 1.28e-04 (3.06e-04)	Tok/s 37705 (43661)	Loss/tok 2.7779 (3.0904)	LR 1.250e-04
0: TRAIN [4][2330/3880]	Time 0.347 (0.323)	Data 1.60e-04 (3.05e-04)	Tok/s 49057 (43665)	Loss/tok 3.0732 (3.0906)	LR 1.250e-04
0: TRAIN [4][2340/3880]	Time 0.434 (0.323)	Data 1.32e-04 (3.05e-04)	Tok/s 53863 (43647)	Loss/tok 3.3175 (3.0903)	LR 1.250e-04
0: TRAIN [4][2350/3880]	Time 0.266 (0.323)	Data 1.38e-04 (3.04e-04)	Tok/s 38898 (43640)	Loss/tok 3.0052 (3.0900)	LR 1.250e-04
0: TRAIN [4][2360/3880]	Time 0.346 (0.323)	Data 1.34e-04 (3.04e-04)	Tok/s 48055 (43655)	Loss/tok 3.1073 (3.0901)	LR 1.250e-04
0: TRAIN [4][2370/3880]	Time 0.347 (0.323)	Data 1.43e-04 (3.03e-04)	Tok/s 47978 (43655)	Loss/tok 3.0423 (3.0902)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2380/3880]	Time 0.266 (0.323)	Data 1.41e-04 (3.02e-04)	Tok/s 38500 (43666)	Loss/tok 2.9412 (3.0910)	LR 1.250e-04
0: TRAIN [4][2390/3880]	Time 0.193 (0.323)	Data 2.60e-04 (3.02e-04)	Tok/s 27784 (43657)	Loss/tok 2.4749 (3.0908)	LR 1.250e-04
0: TRAIN [4][2400/3880]	Time 0.349 (0.323)	Data 1.61e-04 (3.01e-04)	Tok/s 48213 (43650)	Loss/tok 3.1132 (3.0906)	LR 1.250e-04
0: TRAIN [4][2410/3880]	Time 0.349 (0.323)	Data 1.65e-04 (3.01e-04)	Tok/s 47792 (43659)	Loss/tok 3.0594 (3.0905)	LR 1.250e-04
0: TRAIN [4][2420/3880]	Time 0.348 (0.323)	Data 1.27e-04 (3.00e-04)	Tok/s 47774 (43666)	Loss/tok 3.1430 (3.0907)	LR 1.250e-04
0: TRAIN [4][2430/3880]	Time 0.266 (0.323)	Data 1.22e-04 (2.99e-04)	Tok/s 38825 (43635)	Loss/tok 2.9427 (3.0902)	LR 1.250e-04
0: TRAIN [4][2440/3880]	Time 0.191 (0.323)	Data 1.60e-04 (2.99e-04)	Tok/s 27145 (43642)	Loss/tok 2.5597 (3.0906)	LR 1.250e-04
0: TRAIN [4][2450/3880]	Time 0.432 (0.323)	Data 1.34e-04 (2.98e-04)	Tok/s 54618 (43643)	Loss/tok 3.1582 (3.0904)	LR 1.250e-04
0: TRAIN [4][2460/3880]	Time 0.262 (0.322)	Data 1.81e-04 (2.97e-04)	Tok/s 39737 (43632)	Loss/tok 2.9038 (3.0899)	LR 1.250e-04
0: TRAIN [4][2470/3880]	Time 0.265 (0.322)	Data 1.90e-04 (2.97e-04)	Tok/s 38517 (43637)	Loss/tok 2.9610 (3.0897)	LR 1.250e-04
0: TRAIN [4][2480/3880]	Time 0.432 (0.322)	Data 1.47e-04 (2.96e-04)	Tok/s 54825 (43629)	Loss/tok 3.1760 (3.0894)	LR 1.250e-04
0: TRAIN [4][2490/3880]	Time 0.430 (0.322)	Data 1.65e-04 (2.96e-04)	Tok/s 53881 (43614)	Loss/tok 3.2413 (3.0893)	LR 1.250e-04
0: TRAIN [4][2500/3880]	Time 0.348 (0.322)	Data 1.28e-04 (2.95e-04)	Tok/s 48077 (43619)	Loss/tok 3.0247 (3.0893)	LR 1.250e-04
0: TRAIN [4][2510/3880]	Time 0.196 (0.322)	Data 1.67e-04 (2.94e-04)	Tok/s 27484 (43618)	Loss/tok 2.5809 (3.0892)	LR 1.250e-04
0: TRAIN [4][2520/3880]	Time 0.266 (0.322)	Data 1.25e-04 (2.94e-04)	Tok/s 39431 (43598)	Loss/tok 2.8690 (3.0886)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2530/3880]	Time 0.194 (0.322)	Data 1.40e-04 (2.93e-04)	Tok/s 27015 (43596)	Loss/tok 2.6156 (3.0886)	LR 1.250e-04
0: TRAIN [4][2540/3880]	Time 0.346 (0.322)	Data 1.90e-04 (2.92e-04)	Tok/s 48744 (43591)	Loss/tok 3.1090 (3.0886)	LR 1.250e-04
0: TRAIN [4][2550/3880]	Time 0.432 (0.322)	Data 1.31e-04 (2.92e-04)	Tok/s 54214 (43589)	Loss/tok 3.2946 (3.0885)	LR 1.250e-04
0: TRAIN [4][2560/3880]	Time 0.430 (0.322)	Data 2.70e-04 (2.91e-04)	Tok/s 54188 (43597)	Loss/tok 3.3021 (3.0885)	LR 1.250e-04
0: TRAIN [4][2570/3880]	Time 0.431 (0.322)	Data 1.26e-04 (2.91e-04)	Tok/s 55448 (43623)	Loss/tok 3.0976 (3.0892)	LR 1.250e-04
0: TRAIN [4][2580/3880]	Time 0.266 (0.322)	Data 1.66e-04 (2.90e-04)	Tok/s 38549 (43631)	Loss/tok 2.8684 (3.0894)	LR 1.250e-04
0: TRAIN [4][2590/3880]	Time 0.348 (0.322)	Data 1.27e-04 (2.90e-04)	Tok/s 47852 (43630)	Loss/tok 3.0783 (3.0891)	LR 1.250e-04
0: TRAIN [4][2600/3880]	Time 0.431 (0.323)	Data 1.37e-04 (2.89e-04)	Tok/s 53941 (43636)	Loss/tok 3.3418 (3.0895)	LR 1.250e-04
0: TRAIN [4][2610/3880]	Time 0.191 (0.322)	Data 1.37e-04 (2.89e-04)	Tok/s 27939 (43633)	Loss/tok 2.5452 (3.0894)	LR 1.250e-04
0: TRAIN [4][2620/3880]	Time 0.268 (0.322)	Data 1.31e-04 (2.88e-04)	Tok/s 38551 (43634)	Loss/tok 2.9384 (3.0893)	LR 1.250e-04
0: TRAIN [4][2630/3880]	Time 0.545 (0.322)	Data 1.36e-04 (2.87e-04)	Tok/s 54696 (43627)	Loss/tok 3.4045 (3.0891)	LR 1.250e-04
0: TRAIN [4][2640/3880]	Time 0.347 (0.322)	Data 1.22e-04 (2.87e-04)	Tok/s 48575 (43640)	Loss/tok 3.0516 (3.0893)	LR 1.250e-04
0: TRAIN [4][2650/3880]	Time 0.356 (0.322)	Data 1.64e-04 (2.86e-04)	Tok/s 47343 (43629)	Loss/tok 3.1272 (3.0890)	LR 1.250e-04
0: TRAIN [4][2660/3880]	Time 0.195 (0.322)	Data 2.45e-04 (2.86e-04)	Tok/s 27421 (43612)	Loss/tok 2.4941 (3.0888)	LR 1.250e-04
0: TRAIN [4][2670/3880]	Time 0.266 (0.322)	Data 1.31e-04 (2.85e-04)	Tok/s 38033 (43621)	Loss/tok 3.0062 (3.0893)	LR 1.250e-04
0: TRAIN [4][2680/3880]	Time 0.266 (0.322)	Data 1.28e-04 (2.85e-04)	Tok/s 39550 (43621)	Loss/tok 2.9336 (3.0892)	LR 1.250e-04
0: TRAIN [4][2690/3880]	Time 0.267 (0.322)	Data 1.35e-04 (2.84e-04)	Tok/s 38433 (43611)	Loss/tok 2.9436 (3.0891)	LR 1.250e-04
0: TRAIN [4][2700/3880]	Time 0.267 (0.322)	Data 1.45e-04 (2.84e-04)	Tok/s 38229 (43620)	Loss/tok 2.9448 (3.0895)	LR 1.250e-04
0: TRAIN [4][2710/3880]	Time 0.347 (0.322)	Data 1.39e-04 (2.83e-04)	Tok/s 48760 (43619)	Loss/tok 3.0026 (3.0892)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2720/3880]	Time 0.347 (0.322)	Data 1.40e-04 (2.83e-04)	Tok/s 48703 (43613)	Loss/tok 2.9654 (3.0892)	LR 1.250e-04
0: TRAIN [4][2730/3880]	Time 0.266 (0.322)	Data 1.35e-04 (2.82e-04)	Tok/s 38802 (43619)	Loss/tok 2.8961 (3.0893)	LR 1.250e-04
0: TRAIN [4][2740/3880]	Time 0.346 (0.322)	Data 1.36e-04 (2.82e-04)	Tok/s 48211 (43610)	Loss/tok 3.0753 (3.0892)	LR 1.250e-04
0: TRAIN [4][2750/3880]	Time 0.431 (0.322)	Data 1.59e-04 (2.81e-04)	Tok/s 54807 (43617)	Loss/tok 3.1469 (3.0892)	LR 1.250e-04
0: TRAIN [4][2760/3880]	Time 0.191 (0.322)	Data 1.45e-04 (2.81e-04)	Tok/s 27625 (43598)	Loss/tok 2.5373 (3.0887)	LR 1.250e-04
0: TRAIN [4][2770/3880]	Time 0.201 (0.322)	Data 1.27e-04 (2.80e-04)	Tok/s 26152 (43570)	Loss/tok 2.5261 (3.0882)	LR 1.250e-04
0: TRAIN [4][2780/3880]	Time 0.431 (0.322)	Data 1.52e-04 (2.80e-04)	Tok/s 54323 (43578)	Loss/tok 3.2257 (3.0885)	LR 1.250e-04
0: TRAIN [4][2790/3880]	Time 0.348 (0.322)	Data 1.28e-04 (2.79e-04)	Tok/s 48242 (43574)	Loss/tok 3.2270 (3.0884)	LR 1.250e-04
0: TRAIN [4][2800/3880]	Time 0.194 (0.322)	Data 1.63e-04 (2.79e-04)	Tok/s 27729 (43562)	Loss/tok 2.4723 (3.0883)	LR 1.250e-04
0: TRAIN [4][2810/3880]	Time 0.193 (0.322)	Data 1.39e-04 (2.78e-04)	Tok/s 27161 (43544)	Loss/tok 2.4636 (3.0878)	LR 1.250e-04
0: TRAIN [4][2820/3880]	Time 0.347 (0.322)	Data 1.64e-04 (2.78e-04)	Tok/s 48007 (43561)	Loss/tok 3.1080 (3.0885)	LR 1.250e-04
0: TRAIN [4][2830/3880]	Time 0.265 (0.322)	Data 1.26e-04 (2.77e-04)	Tok/s 39377 (43567)	Loss/tok 2.9120 (3.0887)	LR 1.250e-04
0: TRAIN [4][2840/3880]	Time 0.266 (0.322)	Data 1.35e-04 (2.77e-04)	Tok/s 39193 (43572)	Loss/tok 2.9370 (3.0885)	LR 1.250e-04
0: TRAIN [4][2850/3880]	Time 0.267 (0.322)	Data 1.52e-04 (2.76e-04)	Tok/s 38666 (43578)	Loss/tok 2.9576 (3.0886)	LR 1.250e-04
0: TRAIN [4][2860/3880]	Time 0.543 (0.322)	Data 1.67e-04 (2.76e-04)	Tok/s 55530 (43583)	Loss/tok 3.3909 (3.0890)	LR 1.250e-04
0: TRAIN [4][2870/3880]	Time 0.266 (0.322)	Data 1.44e-04 (2.76e-04)	Tok/s 38486 (43579)	Loss/tok 2.9217 (3.0894)	LR 1.250e-04
0: TRAIN [4][2880/3880]	Time 0.430 (0.322)	Data 1.66e-04 (2.75e-04)	Tok/s 53873 (43590)	Loss/tok 3.3011 (3.0897)	LR 1.250e-04
0: TRAIN [4][2890/3880]	Time 0.348 (0.322)	Data 1.68e-04 (2.75e-04)	Tok/s 48135 (43595)	Loss/tok 3.0536 (3.0895)	LR 1.250e-04
0: TRAIN [4][2900/3880]	Time 0.347 (0.322)	Data 1.65e-04 (2.74e-04)	Tok/s 47553 (43594)	Loss/tok 3.2151 (3.0895)	LR 1.250e-04
0: TRAIN [4][2910/3880]	Time 0.267 (0.322)	Data 1.63e-04 (2.74e-04)	Tok/s 39772 (43583)	Loss/tok 2.9663 (3.0891)	LR 1.250e-04
0: TRAIN [4][2920/3880]	Time 0.266 (0.322)	Data 1.31e-04 (2.74e-04)	Tok/s 38099 (43595)	Loss/tok 2.8789 (3.0892)	LR 1.250e-04
0: TRAIN [4][2930/3880]	Time 0.347 (0.322)	Data 1.33e-04 (2.73e-04)	Tok/s 47786 (43609)	Loss/tok 3.0550 (3.0894)	LR 1.250e-04
0: TRAIN [4][2940/3880]	Time 0.347 (0.322)	Data 1.70e-04 (2.73e-04)	Tok/s 47875 (43608)	Loss/tok 3.1444 (3.0895)	LR 1.250e-04
0: TRAIN [4][2950/3880]	Time 0.432 (0.322)	Data 1.32e-04 (2.72e-04)	Tok/s 53852 (43606)	Loss/tok 3.2085 (3.0894)	LR 1.250e-04
0: TRAIN [4][2960/3880]	Time 0.431 (0.322)	Data 1.47e-04 (2.72e-04)	Tok/s 53687 (43608)	Loss/tok 3.1694 (3.0896)	LR 1.250e-04
0: TRAIN [4][2970/3880]	Time 0.265 (0.322)	Data 1.81e-04 (2.72e-04)	Tok/s 38944 (43604)	Loss/tok 2.8314 (3.0894)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][2980/3880]	Time 0.347 (0.322)	Data 2.26e-04 (2.71e-04)	Tok/s 47608 (43594)	Loss/tok 3.1191 (3.0891)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2990/3880]	Time 0.267 (0.322)	Data 1.56e-04 (2.71e-04)	Tok/s 38905 (43612)	Loss/tok 2.9009 (3.0899)	LR 1.250e-04
0: TRAIN [4][3000/3880]	Time 0.266 (0.322)	Data 1.39e-04 (2.71e-04)	Tok/s 39343 (43595)	Loss/tok 2.9595 (3.0895)	LR 1.250e-04
0: TRAIN [4][3010/3880]	Time 0.266 (0.322)	Data 1.37e-04 (2.70e-04)	Tok/s 38471 (43602)	Loss/tok 2.8308 (3.0895)	LR 1.250e-04
0: TRAIN [4][3020/3880]	Time 0.265 (0.322)	Data 1.28e-04 (2.70e-04)	Tok/s 39296 (43607)	Loss/tok 2.8665 (3.0894)	LR 1.250e-04
0: TRAIN [4][3030/3880]	Time 0.432 (0.323)	Data 1.31e-04 (2.69e-04)	Tok/s 54061 (43618)	Loss/tok 3.3485 (3.0899)	LR 1.250e-04
0: TRAIN [4][3040/3880]	Time 0.266 (0.323)	Data 1.06e-03 (2.69e-04)	Tok/s 38265 (43618)	Loss/tok 2.9262 (3.0900)	LR 1.250e-04
0: TRAIN [4][3050/3880]	Time 0.267 (0.322)	Data 1.77e-04 (2.69e-04)	Tok/s 38989 (43612)	Loss/tok 2.8472 (3.0897)	LR 1.250e-04
0: TRAIN [4][3060/3880]	Time 0.266 (0.322)	Data 1.40e-04 (2.68e-04)	Tok/s 39088 (43610)	Loss/tok 2.8799 (3.0898)	LR 1.250e-04
0: TRAIN [4][3070/3880]	Time 0.347 (0.323)	Data 1.42e-04 (2.68e-04)	Tok/s 49405 (43617)	Loss/tok 3.0799 (3.0901)	LR 1.250e-04
0: TRAIN [4][3080/3880]	Time 0.347 (0.323)	Data 1.29e-04 (2.68e-04)	Tok/s 48126 (43619)	Loss/tok 3.1738 (3.0903)	LR 1.250e-04
0: TRAIN [4][3090/3880]	Time 0.348 (0.323)	Data 1.40e-04 (2.67e-04)	Tok/s 48129 (43619)	Loss/tok 3.0891 (3.0902)	LR 1.250e-04
0: TRAIN [4][3100/3880]	Time 0.193 (0.322)	Data 1.49e-04 (2.67e-04)	Tok/s 27458 (43605)	Loss/tok 2.5054 (3.0899)	LR 1.250e-04
0: TRAIN [4][3110/3880]	Time 0.435 (0.322)	Data 1.54e-04 (2.66e-04)	Tok/s 53843 (43612)	Loss/tok 3.1926 (3.0902)	LR 1.250e-04
0: TRAIN [4][3120/3880]	Time 0.192 (0.322)	Data 1.31e-04 (2.66e-04)	Tok/s 27190 (43608)	Loss/tok 2.4775 (3.0904)	LR 1.250e-04
0: TRAIN [4][3130/3880]	Time 0.266 (0.322)	Data 1.54e-04 (2.66e-04)	Tok/s 37660 (43604)	Loss/tok 2.9589 (3.0903)	LR 1.250e-04
0: TRAIN [4][3140/3880]	Time 0.194 (0.323)	Data 1.71e-04 (2.65e-04)	Tok/s 26455 (43610)	Loss/tok 2.4818 (3.0906)	LR 1.250e-04
0: TRAIN [4][3150/3880]	Time 0.265 (0.323)	Data 1.64e-04 (2.65e-04)	Tok/s 39231 (43610)	Loss/tok 2.9449 (3.0907)	LR 1.250e-04
0: TRAIN [4][3160/3880]	Time 0.347 (0.323)	Data 1.38e-04 (2.65e-04)	Tok/s 48081 (43612)	Loss/tok 3.0260 (3.0908)	LR 1.250e-04
0: TRAIN [4][3170/3880]	Time 0.275 (0.323)	Data 1.44e-04 (2.64e-04)	Tok/s 36959 (43609)	Loss/tok 2.8431 (3.0909)	LR 1.250e-04
0: TRAIN [4][3180/3880]	Time 0.431 (0.323)	Data 1.33e-04 (2.64e-04)	Tok/s 54552 (43612)	Loss/tok 3.1893 (3.0911)	LR 1.250e-04
0: TRAIN [4][3190/3880]	Time 0.431 (0.323)	Data 1.31e-04 (2.63e-04)	Tok/s 54052 (43616)	Loss/tok 3.2416 (3.0912)	LR 1.250e-04
0: TRAIN [4][3200/3880]	Time 0.348 (0.323)	Data 1.47e-04 (2.63e-04)	Tok/s 49047 (43627)	Loss/tok 2.9741 (3.0914)	LR 1.250e-04
0: TRAIN [4][3210/3880]	Time 0.430 (0.323)	Data 1.25e-04 (2.63e-04)	Tok/s 54086 (43636)	Loss/tok 3.2106 (3.0915)	LR 1.250e-04
0: TRAIN [4][3220/3880]	Time 0.265 (0.323)	Data 1.33e-04 (2.62e-04)	Tok/s 39833 (43635)	Loss/tok 2.8983 (3.0913)	LR 1.250e-04
0: TRAIN [4][3230/3880]	Time 0.266 (0.323)	Data 1.34e-04 (2.62e-04)	Tok/s 39173 (43633)	Loss/tok 2.9443 (3.0913)	LR 1.250e-04
0: TRAIN [4][3240/3880]	Time 0.266 (0.323)	Data 1.32e-04 (2.62e-04)	Tok/s 39239 (43630)	Loss/tok 2.8930 (3.0910)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][3250/3880]	Time 0.347 (0.323)	Data 1.44e-04 (2.61e-04)	Tok/s 48118 (43633)	Loss/tok 2.9799 (3.0908)	LR 1.250e-04
0: TRAIN [4][3260/3880]	Time 0.433 (0.323)	Data 1.31e-04 (2.61e-04)	Tok/s 53619 (43630)	Loss/tok 3.3768 (3.0909)	LR 1.250e-04
0: TRAIN [4][3270/3880]	Time 0.431 (0.323)	Data 1.50e-04 (2.60e-04)	Tok/s 54309 (43625)	Loss/tok 3.1385 (3.0906)	LR 1.250e-04
0: TRAIN [4][3280/3880]	Time 0.193 (0.323)	Data 1.34e-04 (2.60e-04)	Tok/s 26994 (43624)	Loss/tok 2.5138 (3.0908)	LR 1.250e-04
0: TRAIN [4][3290/3880]	Time 0.265 (0.323)	Data 1.70e-04 (2.60e-04)	Tok/s 38822 (43630)	Loss/tok 2.8718 (3.0908)	LR 1.250e-04
0: TRAIN [4][3300/3880]	Time 0.545 (0.323)	Data 1.37e-04 (2.59e-04)	Tok/s 55353 (43630)	Loss/tok 3.3646 (3.0909)	LR 1.250e-04
0: TRAIN [4][3310/3880]	Time 0.349 (0.323)	Data 1.31e-04 (2.59e-04)	Tok/s 48088 (43632)	Loss/tok 3.0602 (3.0907)	LR 1.250e-04
0: TRAIN [4][3320/3880]	Time 0.268 (0.323)	Data 1.70e-04 (2.59e-04)	Tok/s 38155 (43639)	Loss/tok 2.8932 (3.0908)	LR 1.250e-04
0: TRAIN [4][3330/3880]	Time 0.348 (0.323)	Data 1.61e-04 (2.58e-04)	Tok/s 48415 (43639)	Loss/tok 3.1021 (3.0906)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3340/3880]	Time 0.543 (0.323)	Data 1.36e-04 (2.58e-04)	Tok/s 55455 (43647)	Loss/tok 3.4149 (3.0908)	LR 1.250e-04
0: TRAIN [4][3350/3880]	Time 0.431 (0.323)	Data 1.34e-04 (2.58e-04)	Tok/s 53666 (43651)	Loss/tok 3.2343 (3.0908)	LR 1.250e-04
0: TRAIN [4][3360/3880]	Time 0.347 (0.323)	Data 1.36e-04 (2.57e-04)	Tok/s 48158 (43654)	Loss/tok 3.1628 (3.0907)	LR 1.250e-04
0: TRAIN [4][3370/3880]	Time 0.264 (0.323)	Data 1.28e-04 (2.57e-04)	Tok/s 39177 (43647)	Loss/tok 2.9504 (3.0905)	LR 1.250e-04
0: TRAIN [4][3380/3880]	Time 0.267 (0.323)	Data 1.31e-04 (2.57e-04)	Tok/s 38257 (43650)	Loss/tok 3.0061 (3.0906)	LR 1.250e-04
0: TRAIN [4][3390/3880]	Time 0.347 (0.323)	Data 1.24e-04 (2.56e-04)	Tok/s 48491 (43635)	Loss/tok 3.0634 (3.0903)	LR 1.250e-04
0: TRAIN [4][3400/3880]	Time 0.266 (0.323)	Data 1.84e-04 (2.56e-04)	Tok/s 38767 (43637)	Loss/tok 2.9349 (3.0905)	LR 1.250e-04
0: TRAIN [4][3410/3880]	Time 0.193 (0.323)	Data 1.28e-04 (2.56e-04)	Tok/s 27817 (43634)	Loss/tok 2.4956 (3.0903)	LR 1.250e-04
0: TRAIN [4][3420/3880]	Time 0.266 (0.322)	Data 1.33e-04 (2.55e-04)	Tok/s 38411 (43613)	Loss/tok 2.8511 (3.0901)	LR 1.250e-04
0: TRAIN [4][3430/3880]	Time 0.266 (0.322)	Data 1.31e-04 (2.55e-04)	Tok/s 38720 (43600)	Loss/tok 2.9400 (3.0898)	LR 1.250e-04
0: TRAIN [4][3440/3880]	Time 0.265 (0.322)	Data 1.34e-04 (2.55e-04)	Tok/s 38991 (43593)	Loss/tok 2.7995 (3.0896)	LR 1.250e-04
0: TRAIN [4][3450/3880]	Time 0.431 (0.322)	Data 1.43e-04 (2.54e-04)	Tok/s 53854 (43592)	Loss/tok 3.2692 (3.0894)	LR 1.250e-04
0: TRAIN [4][3460/3880]	Time 0.347 (0.322)	Data 1.31e-04 (2.54e-04)	Tok/s 47918 (43588)	Loss/tok 3.1882 (3.0896)	LR 1.250e-04
0: TRAIN [4][3470/3880]	Time 0.191 (0.322)	Data 1.43e-04 (2.54e-04)	Tok/s 27319 (43577)	Loss/tok 2.5114 (3.0892)	LR 1.250e-04
0: TRAIN [4][3480/3880]	Time 0.544 (0.322)	Data 1.52e-04 (2.54e-04)	Tok/s 54938 (43578)	Loss/tok 3.4617 (3.0894)	LR 1.250e-04
0: TRAIN [4][3490/3880]	Time 0.266 (0.322)	Data 1.37e-04 (2.53e-04)	Tok/s 39298 (43571)	Loss/tok 2.9662 (3.0892)	LR 1.250e-04
0: TRAIN [4][3500/3880]	Time 0.431 (0.322)	Data 1.39e-04 (2.53e-04)	Tok/s 53733 (43573)	Loss/tok 3.2971 (3.0893)	LR 1.250e-04
0: TRAIN [4][3510/3880]	Time 0.349 (0.322)	Data 1.32e-04 (2.53e-04)	Tok/s 47984 (43578)	Loss/tok 3.1438 (3.0891)	LR 1.250e-04
0: TRAIN [4][3520/3880]	Time 0.430 (0.322)	Data 1.23e-04 (2.52e-04)	Tok/s 53523 (43594)	Loss/tok 3.2806 (3.0896)	LR 1.250e-04
0: TRAIN [4][3530/3880]	Time 0.432 (0.322)	Data 1.35e-04 (2.52e-04)	Tok/s 54324 (43590)	Loss/tok 3.2684 (3.0895)	LR 1.250e-04
0: TRAIN [4][3540/3880]	Time 0.431 (0.322)	Data 1.37e-04 (2.52e-04)	Tok/s 54545 (43597)	Loss/tok 3.1542 (3.0897)	LR 1.250e-04
0: TRAIN [4][3550/3880]	Time 0.273 (0.322)	Data 1.26e-04 (2.52e-04)	Tok/s 38553 (43580)	Loss/tok 2.9420 (3.0893)	LR 1.250e-04
0: TRAIN [4][3560/3880]	Time 0.265 (0.322)	Data 1.40e-04 (2.51e-04)	Tok/s 38940 (43573)	Loss/tok 2.8361 (3.0891)	LR 1.250e-04
0: TRAIN [4][3570/3880]	Time 0.266 (0.322)	Data 1.31e-04 (2.51e-04)	Tok/s 38895 (43574)	Loss/tok 2.9047 (3.0889)	LR 1.250e-04
0: TRAIN [4][3580/3880]	Time 0.347 (0.322)	Data 1.76e-04 (2.51e-04)	Tok/s 48527 (43586)	Loss/tok 3.0782 (3.0892)	LR 1.250e-04
0: TRAIN [4][3590/3880]	Time 0.545 (0.322)	Data 1.40e-04 (2.50e-04)	Tok/s 54461 (43592)	Loss/tok 3.4054 (3.0895)	LR 1.250e-04
0: TRAIN [4][3600/3880]	Time 0.346 (0.322)	Data 1.53e-04 (2.50e-04)	Tok/s 49950 (43591)	Loss/tok 3.0567 (3.0893)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][3610/3880]	Time 0.430 (0.322)	Data 1.35e-04 (2.50e-04)	Tok/s 53514 (43581)	Loss/tok 3.2624 (3.0891)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3620/3880]	Time 0.543 (0.322)	Data 1.33e-04 (2.49e-04)	Tok/s 55237 (43581)	Loss/tok 3.4636 (3.0893)	LR 1.250e-04
0: TRAIN [4][3630/3880]	Time 0.346 (0.322)	Data 1.31e-04 (2.49e-04)	Tok/s 47952 (43577)	Loss/tok 3.1817 (3.0893)	LR 1.250e-04
0: TRAIN [4][3640/3880]	Time 0.346 (0.322)	Data 1.50e-04 (2.49e-04)	Tok/s 48581 (43575)	Loss/tok 2.9882 (3.0891)	LR 1.250e-04
0: TRAIN [4][3650/3880]	Time 0.266 (0.322)	Data 1.28e-04 (2.49e-04)	Tok/s 39268 (43578)	Loss/tok 3.0194 (3.0893)	LR 1.250e-04
0: TRAIN [4][3660/3880]	Time 0.347 (0.322)	Data 2.17e-04 (2.49e-04)	Tok/s 48388 (43582)	Loss/tok 3.0702 (3.0893)	LR 1.250e-04
0: TRAIN [4][3670/3880]	Time 0.431 (0.322)	Data 1.32e-04 (2.48e-04)	Tok/s 54479 (43575)	Loss/tok 3.2709 (3.0891)	LR 1.250e-04
0: TRAIN [4][3680/3880]	Time 0.545 (0.322)	Data 1.89e-04 (2.48e-04)	Tok/s 54636 (43578)	Loss/tok 3.4415 (3.0894)	LR 1.250e-04
0: TRAIN [4][3690/3880]	Time 0.265 (0.322)	Data 1.25e-04 (2.48e-04)	Tok/s 38710 (43575)	Loss/tok 2.9402 (3.0893)	LR 1.250e-04
0: TRAIN [4][3700/3880]	Time 0.191 (0.322)	Data 1.73e-04 (2.48e-04)	Tok/s 27542 (43568)	Loss/tok 2.4842 (3.0892)	LR 1.250e-04
0: TRAIN [4][3710/3880]	Time 0.266 (0.322)	Data 1.35e-04 (2.48e-04)	Tok/s 39339 (43567)	Loss/tok 2.9110 (3.0890)	LR 1.250e-04
0: TRAIN [4][3720/3880]	Time 0.347 (0.322)	Data 1.87e-04 (2.47e-04)	Tok/s 48489 (43567)	Loss/tok 2.9446 (3.0887)	LR 1.250e-04
0: TRAIN [4][3730/3880]	Time 0.269 (0.322)	Data 1.85e-04 (2.47e-04)	Tok/s 39035 (43551)	Loss/tok 2.8751 (3.0886)	LR 1.250e-04
0: TRAIN [4][3740/3880]	Time 0.265 (0.322)	Data 1.67e-04 (2.47e-04)	Tok/s 39348 (43551)	Loss/tok 2.9232 (3.0884)	LR 1.250e-04
0: TRAIN [4][3750/3880]	Time 0.347 (0.322)	Data 1.39e-04 (2.46e-04)	Tok/s 48369 (43550)	Loss/tok 3.0449 (3.0883)	LR 1.250e-04
0: TRAIN [4][3760/3880]	Time 0.265 (0.322)	Data 1.71e-04 (2.46e-04)	Tok/s 39666 (43545)	Loss/tok 2.9341 (3.0883)	LR 1.250e-04
0: TRAIN [4][3770/3880]	Time 0.265 (0.322)	Data 1.44e-04 (2.46e-04)	Tok/s 38617 (43542)	Loss/tok 2.7799 (3.0884)	LR 1.250e-04
0: TRAIN [4][3780/3880]	Time 0.348 (0.322)	Data 1.31e-04 (2.46e-04)	Tok/s 47855 (43547)	Loss/tok 3.1214 (3.0883)	LR 1.250e-04
0: TRAIN [4][3790/3880]	Time 0.347 (0.322)	Data 1.34e-04 (2.46e-04)	Tok/s 48909 (43548)	Loss/tok 3.0281 (3.0884)	LR 1.250e-04
0: TRAIN [4][3800/3880]	Time 0.431 (0.322)	Data 1.34e-04 (2.45e-04)	Tok/s 53635 (43550)	Loss/tok 3.2147 (3.0884)	LR 1.250e-04
0: TRAIN [4][3810/3880]	Time 0.347 (0.322)	Data 1.55e-04 (2.45e-04)	Tok/s 48369 (43552)	Loss/tok 3.0398 (3.0883)	LR 1.250e-04
0: TRAIN [4][3820/3880]	Time 0.347 (0.322)	Data 1.38e-04 (2.45e-04)	Tok/s 48365 (43544)	Loss/tok 3.1404 (3.0881)	LR 1.250e-04
0: TRAIN [4][3830/3880]	Time 0.432 (0.322)	Data 1.47e-04 (2.45e-04)	Tok/s 54591 (43537)	Loss/tok 3.2178 (3.0880)	LR 1.250e-04
0: TRAIN [4][3840/3880]	Time 0.266 (0.321)	Data 1.34e-04 (2.44e-04)	Tok/s 37689 (43535)	Loss/tok 3.0285 (3.0880)	LR 1.250e-04
0: TRAIN [4][3850/3880]	Time 0.195 (0.322)	Data 1.58e-04 (2.44e-04)	Tok/s 27274 (43535)	Loss/tok 2.5632 (3.0880)	LR 1.250e-04
0: TRAIN [4][3860/3880]	Time 0.194 (0.321)	Data 2.03e-04 (2.44e-04)	Tok/s 27823 (43529)	Loss/tok 2.5029 (3.0878)	LR 1.250e-04
0: TRAIN [4][3870/3880]	Time 0.348 (0.321)	Data 1.54e-04 (2.44e-04)	Tok/s 47313 (43527)	Loss/tok 3.0910 (3.0878)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
:::MLL 1576202749.196 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 524}}
:::MLL 1576202749.197 eval_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [4][0/6]	Time 0.841 (0.841)	Decoder iters 109.0 (109.0)	Tok/s 19426 (19426)
0: Running moses detokenizer
0: BLEU(score=24.04661760413825, counts=[37055, 18569, 10550, 6232], totals=[65245, 62242, 59239, 56242], precisions=[56.793624032492914, 29.83355290639761, 17.809213524873815, 11.080687031044414], bp=1.0, sys_len=65245, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1576202753.351 eval_accuracy: {"value": 24.05, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 535}}
:::MLL 1576202753.351 eval_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 4	Training Loss: 3.0874	Test BLEU: 24.05
0: Performance: Epoch: 4	Training: 174093 Tok/s
0: Finished epoch 4
:::MLL 1576202753.352 block_stop: {"value": null, "metadata": {"first_epoch_num": 5, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1576202753.352 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-12-13 02:05:59 AM
RESULT,RNN_TRANSLATOR,,6283,nvidia,2019-12-13 12:21:16 AM

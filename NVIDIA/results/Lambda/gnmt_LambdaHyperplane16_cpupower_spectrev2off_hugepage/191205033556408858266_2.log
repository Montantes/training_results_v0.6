Beginning trial 2 of 3
Gathering sys log on 9029gp-tnvrt-0
:::MLL 1575546536.965 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1575546536.965 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1575546536.966 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1575546536.966 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1575546536.967 submission_platform: {"value": "1xSYS-9029GP-TNVRT", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1575546536.967 submission_entry: {"value": "{'hardware': 'SYS-9029GP-TNVRT', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': 'Ubuntu 18.04.3 LTS / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.7-1.0.0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Platinum 8268 CPU @ 2.90GHz', 'num_cores': '48', 'num_vcpus': '96', 'accelerator': 'Tesla V100-SXM3-32GB', 'num_accelerators': '16', 'sys_mem_size': '754 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '1x 894.3G + 1x 3.7T', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27800 Family [ConnectX-5]', 'num_network_cards': '8', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1575546536.967 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1575546536.968 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1575546537.788 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node 9029gp-tnvrt-0
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=LambdaHyperplane16 -e 'MULTI_NODE= --master_port=5152' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=128 -e TEST_BATCH_SIZE=64 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191205033556408858266 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191205033556408858266 ./run_and_time.sh
Run vars: id 191205033556408858266 gpus 16 mparams  --master_port=5152
STARTING TIMING RUN AT 2019-12-05 11:48:58 AM
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=128
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 24 --nproc_per_node 16  --master_port=5152'
+ echo 'running benchmark'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --master_port=5152 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 128 --test-batch-size 64 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1575546539.948 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575546539.948 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575546539.948 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575546539.949 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575546539.950 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575546539.950 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575546539.956 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575546539.957 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575546539.960 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575546539.962 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575546539.964 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575546539.964 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575546539.964 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575546539.965 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575546539.968 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575546539.971 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=64, test_loader_workers=0, train_batch_size=128, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 119616925
0: Worker 0 is using worker seed: 2341203769
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1575546562.148 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1575546564.463 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1575546564.464 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1575546564.464 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1575546564.891 global_batch_size: {"value": 2048, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1575546564.893 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1575546564.893 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1575546564.893 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1575546564.894 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1575546564.894 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1575546564.894 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1575546564.894 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1575546564.897 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1575546564.898 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 1053753355
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/1938]	Time 0.392 (0.392)	Data 2.94e-01 (2.94e-01)	Tok/s 21339 (21339)	Loss/tok 10.7013 (10.7013)	LR 2.000e-05
0: TRAIN [0][10/1938]	Time 0.065 (0.099)	Data 8.65e-05 (2.68e-02)	Tok/s 79621 (70779)	Loss/tok 9.6774 (10.1812)	LR 2.518e-05
0: TRAIN [0][20/1938]	Time 0.117 (0.092)	Data 8.61e-05 (1.41e-02)	Tok/s 98643 (74062)	Loss/tok 9.3595 (9.8308)	LR 3.170e-05
0: TRAIN [0][30/1938]	Time 0.066 (0.086)	Data 8.34e-05 (9.57e-03)	Tok/s 76849 (76331)	Loss/tok 9.0136 (9.6267)	LR 3.991e-05
0: TRAIN [0][40/1938]	Time 0.044 (0.088)	Data 8.80e-05 (7.26e-03)	Tok/s 59464 (79258)	Loss/tok 8.6927 (9.4394)	LR 5.024e-05
0: TRAIN [0][50/1938]	Time 0.091 (0.085)	Data 8.51e-05 (5.85e-03)	Tok/s 92658 (79660)	Loss/tok 8.6631 (9.3053)	LR 6.325e-05
0: TRAIN [0][60/1938]	Time 0.117 (0.087)	Data 8.51e-05 (4.91e-03)	Tok/s 100335 (81074)	Loss/tok 8.6114 (9.1430)	LR 7.962e-05
0: TRAIN [0][70/1938]	Time 0.066 (0.087)	Data 8.01e-05 (4.23e-03)	Tok/s 78712 (81906)	Loss/tok 8.0877 (9.0093)	LR 1.002e-04
0: TRAIN [0][80/1938]	Time 0.119 (0.087)	Data 7.96e-05 (3.72e-03)	Tok/s 99023 (82786)	Loss/tok 8.2235 (8.8952)	LR 1.262e-04
0: TRAIN [0][90/1938]	Time 0.120 (0.088)	Data 9.25e-05 (3.32e-03)	Tok/s 95035 (83493)	Loss/tok 8.1315 (8.7905)	LR 1.589e-04
0: TRAIN [0][100/1938]	Time 0.065 (0.087)	Data 8.37e-05 (3.00e-03)	Tok/s 79071 (83450)	Loss/tok 7.8208 (8.7157)	LR 2.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][110/1938]	Time 0.066 (0.086)	Data 8.08e-05 (2.74e-03)	Tok/s 81254 (83580)	Loss/tok 7.7479 (8.6464)	LR 2.461e-04
0: TRAIN [0][120/1938]	Time 0.092 (0.085)	Data 1.13e-04 (2.52e-03)	Tok/s 91658 (83398)	Loss/tok 7.9192 (8.5874)	LR 3.098e-04
0: TRAIN [0][130/1938]	Time 0.066 (0.085)	Data 7.89e-05 (2.33e-03)	Tok/s 75753 (83227)	Loss/tok 7.5702 (8.5342)	LR 3.900e-04
0: TRAIN [0][140/1938]	Time 0.044 (0.084)	Data 8.15e-05 (2.17e-03)	Tok/s 59641 (82905)	Loss/tok 7.1874 (8.4879)	LR 4.909e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
0: TRAIN [0][150/1938]	Time 0.091 (0.084)	Data 8.73e-05 (2.03e-03)	Tok/s 93196 (82839)	Loss/tok 7.9654 (8.4638)	LR 5.902e-04
0: TRAIN [0][160/1938]	Time 0.095 (0.084)	Data 8.32e-05 (1.91e-03)	Tok/s 87924 (82880)	Loss/tok 7.8371 (8.4230)	LR 7.431e-04
0: TRAIN [0][170/1938]	Time 0.066 (0.084)	Data 9.82e-05 (1.81e-03)	Tok/s 76416 (83126)	Loss/tok 7.4621 (8.3771)	LR 9.355e-04
0: TRAIN [0][180/1938]	Time 0.065 (0.084)	Data 9.56e-05 (1.71e-03)	Tok/s 79972 (83114)	Loss/tok 7.4014 (8.3318)	LR 1.178e-03
0: TRAIN [0][190/1938]	Time 0.066 (0.085)	Data 8.73e-05 (1.63e-03)	Tok/s 78458 (83342)	Loss/tok 7.1393 (8.2768)	LR 1.483e-03
0: TRAIN [0][200/1938]	Time 0.092 (0.085)	Data 8.34e-05 (1.55e-03)	Tok/s 91642 (83559)	Loss/tok 7.2282 (8.2197)	LR 1.867e-03
0: TRAIN [0][210/1938]	Time 0.065 (0.084)	Data 9.47e-05 (1.48e-03)	Tok/s 80030 (83422)	Loss/tok 6.6541 (8.1675)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.043 (0.084)	Data 8.46e-05 (1.42e-03)	Tok/s 62546 (83316)	Loss/tok 5.7202 (8.1125)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.065 (0.084)	Data 8.39e-05 (1.36e-03)	Tok/s 78979 (83494)	Loss/tok 6.6060 (8.0517)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.092 (0.084)	Data 8.46e-05 (1.31e-03)	Tok/s 90565 (83561)	Loss/tok 6.5546 (7.9909)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.066 (0.084)	Data 8.18e-05 (1.26e-03)	Tok/s 80033 (83593)	Loss/tok 6.1920 (7.9255)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.066 (0.084)	Data 8.54e-05 (1.21e-03)	Tok/s 78009 (83587)	Loss/tok 5.9743 (7.8660)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.094 (0.084)	Data 8.32e-05 (1.17e-03)	Tok/s 90096 (83716)	Loss/tok 6.2612 (7.7984)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.043 (0.084)	Data 8.08e-05 (1.13e-03)	Tok/s 61191 (83773)	Loss/tok 5.1154 (7.7364)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.068 (0.085)	Data 8.68e-05 (1.10e-03)	Tok/s 74376 (83922)	Loss/tok 5.7043 (7.6686)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.093 (0.085)	Data 8.18e-05 (1.06e-03)	Tok/s 88767 (83876)	Loss/tok 6.0072 (7.6133)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.153 (0.085)	Data 8.65e-05 (1.03e-03)	Tok/s 95522 (84034)	Loss/tok 6.0580 (7.5460)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.092 (0.085)	Data 8.01e-05 (1.00e-03)	Tok/s 91897 (84038)	Loss/tok 5.8127 (7.4905)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.067 (0.085)	Data 8.23e-05 (9.74e-04)	Tok/s 75260 (83934)	Loss/tok 5.3310 (7.4400)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.066 (0.085)	Data 9.54e-05 (9.48e-04)	Tok/s 78986 (83890)	Loss/tok 5.1616 (7.3827)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.093 (0.085)	Data 9.35e-05 (9.23e-04)	Tok/s 89983 (83922)	Loss/tok 5.4138 (7.3264)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.093 (0.085)	Data 8.27e-05 (9.00e-04)	Tok/s 90285 (84013)	Loss/tok 5.3552 (7.2628)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.092 (0.084)	Data 8.06e-05 (8.78e-04)	Tok/s 89933 (83803)	Loss/tok 5.2280 (7.2205)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.092 (0.084)	Data 8.87e-05 (8.57e-04)	Tok/s 90050 (83817)	Loss/tok 5.2459 (7.1675)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.092 (0.084)	Data 8.34e-05 (8.37e-04)	Tok/s 91154 (83942)	Loss/tok 5.1108 (7.1077)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.044 (0.084)	Data 8.46e-05 (8.18e-04)	Tok/s 58821 (83853)	Loss/tok 3.8191 (7.0580)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.044 (0.084)	Data 8.65e-05 (8.00e-04)	Tok/s 58728 (83779)	Loss/tok 3.8256 (7.0099)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.119 (0.084)	Data 8.39e-05 (7.83e-04)	Tok/s 97909 (83953)	Loss/tok 5.2656 (6.9507)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.067 (0.084)	Data 8.51e-05 (7.67e-04)	Tok/s 77768 (83909)	Loss/tok 4.4497 (6.9038)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.044 (0.084)	Data 8.54e-05 (7.52e-04)	Tok/s 59625 (83774)	Loss/tok 3.6350 (6.8608)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.092 (0.084)	Data 8.15e-05 (7.37e-04)	Tok/s 91188 (83765)	Loss/tok 4.7432 (6.8147)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.066 (0.084)	Data 8.01e-05 (7.23e-04)	Tok/s 79353 (83694)	Loss/tok 4.3971 (6.7706)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.067 (0.084)	Data 8.49e-05 (7.09e-04)	Tok/s 74775 (83727)	Loss/tok 4.3726 (6.7226)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.092 (0.083)	Data 8.54e-05 (6.96e-04)	Tok/s 92495 (83665)	Loss/tok 4.5098 (6.6809)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.066 (0.084)	Data 8.68e-05 (6.84e-04)	Tok/s 77662 (83732)	Loss/tok 4.2917 (6.6334)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.067 (0.083)	Data 8.23e-05 (6.72e-04)	Tok/s 75487 (83691)	Loss/tok 4.2030 (6.5943)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.093 (0.084)	Data 8.20e-05 (6.60e-04)	Tok/s 88720 (83753)	Loss/tok 4.5455 (6.5506)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.066 (0.083)	Data 8.27e-05 (6.49e-04)	Tok/s 80512 (83690)	Loss/tok 4.1728 (6.5149)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.043 (0.083)	Data 8.54e-05 (6.38e-04)	Tok/s 61937 (83708)	Loss/tok 3.4291 (6.4728)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.068 (0.083)	Data 8.23e-05 (6.28e-04)	Tok/s 77784 (83660)	Loss/tok 4.1652 (6.4365)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.044 (0.083)	Data 8.42e-05 (6.18e-04)	Tok/s 61612 (83589)	Loss/tok 3.2707 (6.4019)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.092 (0.083)	Data 8.03e-05 (6.09e-04)	Tok/s 90622 (83583)	Loss/tok 4.3177 (6.3651)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.066 (0.083)	Data 8.30e-05 (6.00e-04)	Tok/s 77987 (83598)	Loss/tok 3.8946 (6.3282)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.093 (0.083)	Data 8.80e-05 (5.91e-04)	Tok/s 90729 (83655)	Loss/tok 4.3376 (6.2898)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.092 (0.083)	Data 9.04e-05 (5.82e-04)	Tok/s 89282 (83603)	Loss/tok 4.1216 (6.2588)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.067 (0.083)	Data 8.68e-05 (5.74e-04)	Tok/s 73734 (83562)	Loss/tok 3.8447 (6.2262)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.068 (0.083)	Data 8.49e-05 (5.66e-04)	Tok/s 74772 (83521)	Loss/tok 3.6540 (6.1937)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.092 (0.083)	Data 8.68e-05 (5.58e-04)	Tok/s 91334 (83587)	Loss/tok 4.1369 (6.1560)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.093 (0.083)	Data 8.80e-05 (5.51e-04)	Tok/s 90054 (83603)	Loss/tok 4.1789 (6.1250)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.093 (0.083)	Data 1.13e-04 (5.44e-04)	Tok/s 92212 (83633)	Loss/tok 4.1462 (6.0929)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.067 (0.083)	Data 8.54e-05 (5.37e-04)	Tok/s 76939 (83597)	Loss/tok 3.8085 (6.0646)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.093 (0.083)	Data 8.39e-05 (5.30e-04)	Tok/s 91156 (83524)	Loss/tok 4.1535 (6.0383)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.044 (0.083)	Data 8.58e-05 (5.23e-04)	Tok/s 58692 (83522)	Loss/tok 3.2335 (6.0095)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.067 (0.083)	Data 8.54e-05 (5.17e-04)	Tok/s 77152 (83535)	Loss/tok 3.8235 (5.9790)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.067 (0.083)	Data 8.34e-05 (5.10e-04)	Tok/s 76658 (83583)	Loss/tok 3.7237 (5.9489)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.093 (0.083)	Data 9.39e-05 (5.04e-04)	Tok/s 90979 (83621)	Loss/tok 4.0160 (5.9189)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.068 (0.083)	Data 8.15e-05 (4.99e-04)	Tok/s 76111 (83557)	Loss/tok 3.7941 (5.8942)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.068 (0.083)	Data 8.39e-05 (4.93e-04)	Tok/s 76337 (83539)	Loss/tok 3.6841 (5.8678)	LR 2.000e-03
0: TRAIN [0][730/1938]	Time 0.092 (0.083)	Data 8.77e-05 (4.87e-04)	Tok/s 91191 (83512)	Loss/tok 4.1005 (5.8440)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.067 (0.083)	Data 8.34e-05 (4.82e-04)	Tok/s 75893 (83521)	Loss/tok 3.8084 (5.8184)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.044 (0.083)	Data 8.39e-05 (4.77e-04)	Tok/s 61366 (83437)	Loss/tok 3.1119 (5.7984)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.044 (0.083)	Data 8.39e-05 (4.71e-04)	Tok/s 61058 (83377)	Loss/tok 3.0785 (5.7768)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.069 (0.083)	Data 8.44e-05 (4.66e-04)	Tok/s 76856 (83429)	Loss/tok 3.9327 (5.7498)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.067 (0.083)	Data 8.15e-05 (4.61e-04)	Tok/s 76389 (83394)	Loss/tok 3.8483 (5.7280)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.068 (0.083)	Data 8.70e-05 (4.57e-04)	Tok/s 76192 (83374)	Loss/tok 3.9196 (5.7060)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][800/1938]	Time 0.066 (0.083)	Data 8.34e-05 (4.52e-04)	Tok/s 77568 (83376)	Loss/tok 3.6890 (5.6840)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.068 (0.083)	Data 8.82e-05 (4.48e-04)	Tok/s 75661 (83392)	Loss/tok 3.5296 (5.6603)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.067 (0.083)	Data 9.23e-05 (4.43e-04)	Tok/s 76516 (83402)	Loss/tok 3.6577 (5.6383)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.068 (0.083)	Data 8.54e-05 (4.39e-04)	Tok/s 76427 (83410)	Loss/tok 3.6833 (5.6184)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.066 (0.083)	Data 8.42e-05 (4.35e-04)	Tok/s 78951 (83374)	Loss/tok 3.5181 (5.5996)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.153 (0.083)	Data 8.34e-05 (4.31e-04)	Tok/s 97080 (83400)	Loss/tok 4.1197 (5.5780)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.091 (0.083)	Data 8.27e-05 (4.27e-04)	Tok/s 90130 (83316)	Loss/tok 4.0184 (5.5608)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.044 (0.083)	Data 8.11e-05 (4.23e-04)	Tok/s 61649 (83289)	Loss/tok 3.2736 (5.5428)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.067 (0.083)	Data 8.13e-05 (4.19e-04)	Tok/s 76974 (83346)	Loss/tok 3.6582 (5.5227)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.118 (0.083)	Data 8.61e-05 (4.15e-04)	Tok/s 97520 (83366)	Loss/tok 3.9810 (5.5029)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.066 (0.083)	Data 8.08e-05 (4.12e-04)	Tok/s 76726 (83348)	Loss/tok 3.6539 (5.4853)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.066 (0.083)	Data 8.23e-05 (4.08e-04)	Tok/s 79845 (83364)	Loss/tok 3.6123 (5.4665)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.068 (0.083)	Data 8.63e-05 (4.05e-04)	Tok/s 73953 (83363)	Loss/tok 3.8957 (5.4490)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.094 (0.083)	Data 8.85e-05 (4.01e-04)	Tok/s 88469 (83404)	Loss/tok 3.9378 (5.4293)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.120 (0.083)	Data 8.56e-05 (3.98e-04)	Tok/s 96352 (83414)	Loss/tok 4.1231 (5.4118)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.067 (0.083)	Data 8.18e-05 (3.95e-04)	Tok/s 78070 (83441)	Loss/tok 3.5002 (5.3941)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.043 (0.083)	Data 8.34e-05 (3.91e-04)	Tok/s 61351 (83403)	Loss/tok 3.1105 (5.3799)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][970/1938]	Time 0.043 (0.083)	Data 8.73e-05 (3.88e-04)	Tok/s 61867 (83416)	Loss/tok 3.1054 (5.3646)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.045 (0.083)	Data 9.13e-05 (3.85e-04)	Tok/s 59074 (83449)	Loss/tok 3.0247 (5.3477)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.093 (0.084)	Data 8.73e-05 (3.82e-04)	Tok/s 91909 (83481)	Loss/tok 3.6763 (5.3299)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.068 (0.083)	Data 8.49e-05 (3.79e-04)	Tok/s 75263 (83439)	Loss/tok 3.4493 (5.3166)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.093 (0.083)	Data 7.87e-05 (3.76e-04)	Tok/s 90112 (83436)	Loss/tok 3.8346 (5.3009)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.069 (0.084)	Data 9.35e-05 (3.73e-04)	Tok/s 75071 (83456)	Loss/tok 3.5566 (5.2841)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.067 (0.084)	Data 8.46e-05 (3.71e-04)	Tok/s 74522 (83424)	Loss/tok 3.6138 (5.2704)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.118 (0.084)	Data 8.18e-05 (3.68e-04)	Tok/s 99655 (83446)	Loss/tok 3.8411 (5.2551)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.066 (0.083)	Data 8.06e-05 (3.65e-04)	Tok/s 77142 (83382)	Loss/tok 3.5018 (5.2430)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.119 (0.083)	Data 8.13e-05 (3.62e-04)	Tok/s 98459 (83408)	Loss/tok 4.0022 (5.2281)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.067 (0.084)	Data 7.99e-05 (3.60e-04)	Tok/s 75257 (83442)	Loss/tok 3.6188 (5.2133)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.068 (0.084)	Data 8.42e-05 (3.57e-04)	Tok/s 75114 (83441)	Loss/tok 3.5585 (5.1997)	LR 2.000e-03
0: TRAIN [0][1090/1938]	Time 0.091 (0.083)	Data 8.51e-05 (3.55e-04)	Tok/s 91969 (83434)	Loss/tok 3.6674 (5.1866)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.067 (0.084)	Data 8.61e-05 (3.52e-04)	Tok/s 80606 (83447)	Loss/tok 3.3976 (5.1726)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.154 (0.084)	Data 8.63e-05 (3.50e-04)	Tok/s 98258 (83469)	Loss/tok 4.1930 (5.1591)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.119 (0.084)	Data 8.80e-05 (3.48e-04)	Tok/s 97353 (83482)	Loss/tok 4.1248 (5.1463)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.045 (0.084)	Data 8.73e-05 (3.45e-04)	Tok/s 59490 (83495)	Loss/tok 2.9804 (5.1326)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.094 (0.084)	Data 8.39e-05 (3.43e-04)	Tok/s 89584 (83526)	Loss/tok 3.8931 (5.1189)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.066 (0.084)	Data 8.18e-05 (3.41e-04)	Tok/s 77302 (83501)	Loss/tok 3.5373 (5.1074)	LR 2.000e-03
0: TRAIN [0][1160/1938]	Time 0.066 (0.084)	Data 8.06e-05 (3.38e-04)	Tok/s 77619 (83460)	Loss/tok 3.4192 (5.0971)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.095 (0.084)	Data 8.44e-05 (3.36e-04)	Tok/s 88686 (83464)	Loss/tok 3.5733 (5.0850)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.043 (0.084)	Data 8.37e-05 (3.34e-04)	Tok/s 60597 (83419)	Loss/tok 3.0825 (5.0747)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.066 (0.084)	Data 8.39e-05 (3.32e-04)	Tok/s 78551 (83425)	Loss/tok 3.4878 (5.0637)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.094 (0.084)	Data 8.23e-05 (3.30e-04)	Tok/s 89340 (83474)	Loss/tok 3.6628 (5.0507)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.044 (0.084)	Data 9.70e-05 (3.28e-04)	Tok/s 60283 (83461)	Loss/tok 2.8811 (5.0404)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.067 (0.084)	Data 8.58e-05 (3.26e-04)	Tok/s 77488 (83443)	Loss/tok 3.4806 (5.0305)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.092 (0.084)	Data 8.75e-05 (3.24e-04)	Tok/s 90724 (83466)	Loss/tok 3.6196 (5.0185)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.119 (0.084)	Data 8.20e-05 (3.22e-04)	Tok/s 99260 (83455)	Loss/tok 3.8309 (5.0076)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1250/1938]	Time 0.119 (0.084)	Data 8.68e-05 (3.20e-04)	Tok/s 99073 (83518)	Loss/tok 3.8502 (4.9952)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.092 (0.084)	Data 8.27e-05 (3.18e-04)	Tok/s 91997 (83540)	Loss/tok 3.8478 (4.9849)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.067 (0.084)	Data 8.61e-05 (3.17e-04)	Tok/s 76918 (83550)	Loss/tok 3.5144 (4.9738)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.068 (0.084)	Data 9.37e-05 (3.15e-04)	Tok/s 74021 (83534)	Loss/tok 3.5850 (4.9641)	LR 2.000e-03
0: TRAIN [0][1290/1938]	Time 0.068 (0.084)	Data 8.51e-05 (3.13e-04)	Tok/s 75081 (83533)	Loss/tok 3.4566 (4.9536)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.066 (0.084)	Data 8.51e-05 (3.11e-04)	Tok/s 78853 (83519)	Loss/tok 3.1840 (4.9442)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.067 (0.084)	Data 8.27e-05 (3.10e-04)	Tok/s 75380 (83505)	Loss/tok 3.3953 (4.9350)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.093 (0.084)	Data 9.30e-05 (3.08e-04)	Tok/s 90210 (83514)	Loss/tok 3.5771 (4.9253)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.043 (0.084)	Data 8.11e-05 (3.06e-04)	Tok/s 61304 (83521)	Loss/tok 2.8945 (4.9157)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.118 (0.084)	Data 8.39e-05 (3.04e-04)	Tok/s 97691 (83539)	Loss/tok 3.8288 (4.9054)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.093 (0.084)	Data 8.70e-05 (3.03e-04)	Tok/s 91086 (83534)	Loss/tok 3.5936 (4.8958)	LR 2.000e-03
0: TRAIN [0][1360/1938]	Time 0.121 (0.084)	Data 8.65e-05 (3.01e-04)	Tok/s 98213 (83563)	Loss/tok 3.8586 (4.8859)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.067 (0.084)	Data 8.51e-05 (3.00e-04)	Tok/s 76466 (83594)	Loss/tok 3.4124 (4.8763)	LR 2.000e-03
0: TRAIN [0][1380/1938]	Time 0.093 (0.084)	Data 8.85e-05 (2.98e-04)	Tok/s 91256 (83610)	Loss/tok 3.5139 (4.8670)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.092 (0.084)	Data 8.75e-05 (2.97e-04)	Tok/s 89810 (83631)	Loss/tok 3.5926 (4.8579)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.044 (0.084)	Data 8.13e-05 (2.95e-04)	Tok/s 59472 (83626)	Loss/tok 3.0467 (4.8491)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.067 (0.084)	Data 8.70e-05 (2.94e-04)	Tok/s 78875 (83640)	Loss/tok 3.4011 (4.8398)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.044 (0.084)	Data 8.15e-05 (2.92e-04)	Tok/s 61499 (83626)	Loss/tok 2.9328 (4.8317)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.044 (0.084)	Data 8.42e-05 (2.91e-04)	Tok/s 60510 (83594)	Loss/tok 2.8812 (4.8242)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.067 (0.084)	Data 8.49e-05 (2.89e-04)	Tok/s 77087 (83600)	Loss/tok 3.3419 (4.8155)	LR 2.000e-03
0: TRAIN [0][1450/1938]	Time 0.067 (0.084)	Data 8.18e-05 (2.88e-04)	Tok/s 77015 (83566)	Loss/tok 3.3039 (4.8082)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.068 (0.084)	Data 8.82e-05 (2.86e-04)	Tok/s 76809 (83572)	Loss/tok 3.2011 (4.7999)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.067 (0.084)	Data 8.63e-05 (2.85e-04)	Tok/s 77767 (83572)	Loss/tok 3.3151 (4.7919)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1480/1938]	Time 0.154 (0.084)	Data 8.23e-05 (2.84e-04)	Tok/s 97094 (83629)	Loss/tok 4.0035 (4.7817)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.120 (0.084)	Data 9.20e-05 (2.82e-04)	Tok/s 97689 (83655)	Loss/tok 3.9070 (4.7734)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.067 (0.084)	Data 7.96e-05 (2.81e-04)	Tok/s 75236 (83675)	Loss/tok 3.2971 (4.7650)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.068 (0.084)	Data 8.25e-05 (2.80e-04)	Tok/s 76428 (83688)	Loss/tok 3.3900 (4.7570)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.067 (0.084)	Data 8.34e-05 (2.78e-04)	Tok/s 77325 (83696)	Loss/tok 3.4065 (4.7492)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.067 (0.084)	Data 7.99e-05 (2.77e-04)	Tok/s 77905 (83694)	Loss/tok 3.4944 (4.7415)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.094 (0.084)	Data 8.32e-05 (2.76e-04)	Tok/s 88863 (83683)	Loss/tok 3.4823 (4.7341)	LR 2.000e-03
0: TRAIN [0][1550/1938]	Time 0.093 (0.084)	Data 9.25e-05 (2.75e-04)	Tok/s 88922 (83699)	Loss/tok 3.6555 (4.7264)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.066 (0.084)	Data 8.39e-05 (2.73e-04)	Tok/s 75395 (83655)	Loss/tok 3.3457 (4.7199)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.093 (0.085)	Data 9.94e-05 (2.72e-04)	Tok/s 87918 (83698)	Loss/tok 3.5454 (4.7116)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.068 (0.084)	Data 8.08e-05 (2.71e-04)	Tok/s 80800 (83677)	Loss/tok 3.4249 (4.7054)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.094 (0.085)	Data 8.32e-05 (2.70e-04)	Tok/s 88935 (83673)	Loss/tok 3.8493 (4.6983)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.093 (0.085)	Data 9.80e-05 (2.69e-04)	Tok/s 90520 (83690)	Loss/tok 3.6941 (4.6905)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.119 (0.084)	Data 7.94e-05 (2.68e-04)	Tok/s 98107 (83664)	Loss/tok 3.7014 (4.6843)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.067 (0.084)	Data 8.32e-05 (2.66e-04)	Tok/s 75620 (83626)	Loss/tok 3.4769 (4.6782)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.119 (0.085)	Data 8.42e-05 (2.65e-04)	Tok/s 96863 (83646)	Loss/tok 3.6356 (4.6707)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.093 (0.085)	Data 8.20e-05 (2.64e-04)	Tok/s 89410 (83666)	Loss/tok 3.7398 (4.6635)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.093 (0.085)	Data 8.18e-05 (2.63e-04)	Tok/s 90733 (83673)	Loss/tok 3.5794 (4.6569)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.093 (0.085)	Data 8.03e-05 (2.62e-04)	Tok/s 90952 (83678)	Loss/tok 3.7541 (4.6506)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.067 (0.085)	Data 8.30e-05 (2.61e-04)	Tok/s 77959 (83682)	Loss/tok 3.3940 (4.6443)	LR 2.000e-03
0: TRAIN [0][1680/1938]	Time 0.068 (0.085)	Data 8.39e-05 (2.60e-04)	Tok/s 75676 (83685)	Loss/tok 3.4045 (4.6379)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.068 (0.085)	Data 8.03e-05 (2.59e-04)	Tok/s 76809 (83689)	Loss/tok 3.3174 (4.6313)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.092 (0.084)	Data 9.32e-05 (2.58e-04)	Tok/s 92194 (83649)	Loss/tok 3.4821 (4.6256)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.066 (0.084)	Data 8.06e-05 (2.57e-04)	Tok/s 79542 (83616)	Loss/tok 3.4279 (4.6203)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.120 (0.084)	Data 7.99e-05 (2.56e-04)	Tok/s 98025 (83635)	Loss/tok 3.7474 (4.6135)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.093 (0.084)	Data 8.06e-05 (2.55e-04)	Tok/s 90639 (83643)	Loss/tok 3.5791 (4.6067)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.067 (0.085)	Data 8.37e-05 (2.54e-04)	Tok/s 77570 (83657)	Loss/tok 3.2040 (4.6003)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1750/1938]	Time 0.094 (0.085)	Data 9.61e-05 (2.53e-04)	Tok/s 90703 (83665)	Loss/tok 3.3485 (4.5940)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.119 (0.084)	Data 8.27e-05 (2.52e-04)	Tok/s 96755 (83662)	Loss/tok 3.7822 (4.5882)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.067 (0.084)	Data 7.84e-05 (2.51e-04)	Tok/s 75397 (83623)	Loss/tok 3.3615 (4.5832)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.094 (0.084)	Data 8.65e-05 (2.50e-04)	Tok/s 91422 (83635)	Loss/tok 3.5643 (4.5771)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.093 (0.084)	Data 8.68e-05 (2.49e-04)	Tok/s 89635 (83620)	Loss/tok 3.5497 (4.5717)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.068 (0.084)	Data 9.80e-05 (2.48e-04)	Tok/s 76468 (83606)	Loss/tok 3.3020 (4.5664)	LR 2.000e-03
0: TRAIN [0][1810/1938]	Time 0.067 (0.084)	Data 8.11e-05 (2.47e-04)	Tok/s 78934 (83610)	Loss/tok 3.4364 (4.5607)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.043 (0.084)	Data 8.46e-05 (2.46e-04)	Tok/s 62065 (83587)	Loss/tok 2.8043 (4.5553)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.043 (0.084)	Data 8.99e-05 (2.46e-04)	Tok/s 61488 (83559)	Loss/tok 2.8257 (4.5506)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1840/1938]	Time 0.089 (0.084)	Data 8.77e-05 (2.45e-04)	Tok/s 95381 (83578)	Loss/tok 3.4554 (4.5445)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.065 (0.084)	Data 8.61e-05 (2.44e-04)	Tok/s 78816 (83582)	Loss/tok 3.3754 (4.5390)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][1860/1938]	Time 0.120 (0.084)	Data 8.18e-05 (2.43e-04)	Tok/s 96668 (83590)	Loss/tok 3.9229 (4.5339)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.044 (0.084)	Data 8.27e-05 (2.42e-04)	Tok/s 60277 (83570)	Loss/tok 2.8268 (4.5288)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.067 (0.084)	Data 8.32e-05 (2.41e-04)	Tok/s 77192 (83576)	Loss/tok 3.2900 (4.5237)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.120 (0.084)	Data 8.15e-05 (2.40e-04)	Tok/s 99912 (83590)	Loss/tok 3.5752 (4.5179)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.094 (0.084)	Data 8.13e-05 (2.40e-04)	Tok/s 87645 (83587)	Loss/tok 3.5484 (4.5128)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.152 (0.084)	Data 8.27e-05 (2.39e-04)	Tok/s 98554 (83569)	Loss/tok 3.8998 (4.5081)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.044 (0.084)	Data 8.20e-05 (2.38e-04)	Tok/s 59466 (83546)	Loss/tok 2.9593 (4.5033)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.094 (0.084)	Data 8.58e-05 (2.37e-04)	Tok/s 87875 (83559)	Loss/tok 3.5893 (4.4978)	LR 2.000e-03
:::MLL 1575546728.817 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1575546728.818 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.430 (0.430)	Decoder iters 112.0 (112.0)	Tok/s 21009 (21009)
0: Running moses detokenizer
0: BLEU(score=20.013131722061186, counts=[35099, 16217, 8681, 4822], totals=[66673, 63670, 60668, 57671], precisions=[52.643498867607576, 25.470394220197896, 14.309026175248896, 8.361221411107836], bp=1.0, sys_len=66673, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1575546730.001 eval_accuracy: {"value": 20.01, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1575546730.002 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.4982	Test BLEU: 20.01
0: Performance: Epoch: 0	Training: 1336660 Tok/s
0: Finished epoch 0
:::MLL 1575546730.002 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1575546730.002 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1575546730.003 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2208867108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [1][0/1938]	Time 0.352 (0.352)	Data 2.49e-01 (2.49e-01)	Tok/s 24022 (24022)	Loss/tok 3.3496 (3.3496)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.091 (0.113)	Data 9.13e-05 (2.27e-02)	Tok/s 92521 (78494)	Loss/tok 3.3759 (3.4425)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.066 (0.099)	Data 9.99e-05 (1.19e-02)	Tok/s 80665 (80587)	Loss/tok 3.2741 (3.4577)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.118 (0.094)	Data 8.44e-05 (8.10e-03)	Tok/s 99948 (82466)	Loss/tok 3.7323 (3.4560)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.092 (0.089)	Data 8.20e-05 (6.15e-03)	Tok/s 91595 (82165)	Loss/tok 3.5909 (3.4452)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.067 (0.085)	Data 8.27e-05 (4.96e-03)	Tok/s 77337 (80542)	Loss/tok 3.2036 (3.4260)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.066 (0.086)	Data 8.08e-05 (4.16e-03)	Tok/s 77587 (81356)	Loss/tok 3.3074 (3.4465)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.150 (0.089)	Data 8.82e-05 (3.58e-03)	Tok/s 99323 (82737)	Loss/tok 3.6874 (3.4728)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.066 (0.088)	Data 8.54e-05 (3.15e-03)	Tok/s 75612 (82859)	Loss/tok 3.2748 (3.4724)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.092 (0.089)	Data 8.30e-05 (2.82e-03)	Tok/s 92294 (83270)	Loss/tok 3.4241 (3.4846)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.067 (0.089)	Data 8.58e-05 (2.54e-03)	Tok/s 76323 (83758)	Loss/tok 3.2056 (3.4841)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.094 (0.089)	Data 8.68e-05 (2.32e-03)	Tok/s 88085 (83935)	Loss/tok 3.6129 (3.4770)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.067 (0.088)	Data 8.30e-05 (2.14e-03)	Tok/s 75241 (83930)	Loss/tok 3.3601 (3.4716)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.094 (0.089)	Data 8.30e-05 (1.98e-03)	Tok/s 89764 (84311)	Loss/tok 3.3666 (3.4736)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.118 (0.089)	Data 8.23e-05 (1.85e-03)	Tok/s 97465 (84419)	Loss/tok 3.6986 (3.4665)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.093 (0.089)	Data 8.49e-05 (1.73e-03)	Tok/s 90644 (84759)	Loss/tok 3.4690 (3.4694)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.067 (0.088)	Data 8.46e-05 (1.63e-03)	Tok/s 80610 (84603)	Loss/tok 3.1684 (3.4620)	LR 2.000e-03
0: TRAIN [1][170/1938]	Time 0.117 (0.088)	Data 8.51e-05 (1.54e-03)	Tok/s 98492 (84525)	Loss/tok 3.7839 (3.4598)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.067 (0.087)	Data 8.94e-05 (1.46e-03)	Tok/s 80469 (84450)	Loss/tok 3.2754 (3.4540)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.066 (0.087)	Data 8.42e-05 (1.39e-03)	Tok/s 78457 (84310)	Loss/tok 3.2070 (3.4532)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.093 (0.086)	Data 8.32e-05 (1.32e-03)	Tok/s 89081 (84072)	Loss/tok 3.3380 (3.4455)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.067 (0.085)	Data 8.01e-05 (1.26e-03)	Tok/s 75371 (83833)	Loss/tok 3.2351 (3.4387)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.092 (0.085)	Data 8.39e-05 (1.21e-03)	Tok/s 91280 (83913)	Loss/tok 3.5513 (3.4436)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.119 (0.085)	Data 8.11e-05 (1.16e-03)	Tok/s 98192 (83938)	Loss/tok 3.6582 (3.4433)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.067 (0.085)	Data 8.37e-05 (1.12e-03)	Tok/s 78528 (83769)	Loss/tok 3.2669 (3.4399)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.092 (0.085)	Data 8.54e-05 (1.08e-03)	Tok/s 91354 (83899)	Loss/tok 3.6284 (3.4407)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.066 (0.084)	Data 8.49e-05 (1.04e-03)	Tok/s 78396 (83696)	Loss/tok 3.1751 (3.4357)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.092 (0.085)	Data 8.89e-05 (1.00e-03)	Tok/s 92243 (83789)	Loss/tok 3.3370 (3.4355)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.067 (0.084)	Data 8.13e-05 (9.70e-04)	Tok/s 78039 (83716)	Loss/tok 3.2432 (3.4326)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.066 (0.084)	Data 8.80e-05 (9.39e-04)	Tok/s 77012 (83589)	Loss/tok 3.0340 (3.4280)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.043 (0.083)	Data 9.56e-05 (9.11e-04)	Tok/s 60744 (83439)	Loss/tok 2.8824 (3.4250)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.093 (0.083)	Data 9.08e-05 (8.85e-04)	Tok/s 89497 (83369)	Loss/tok 3.4863 (3.4208)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.067 (0.084)	Data 8.85e-05 (8.60e-04)	Tok/s 75123 (83373)	Loss/tok 3.4211 (3.4297)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.066 (0.084)	Data 8.18e-05 (8.37e-04)	Tok/s 77912 (83485)	Loss/tok 3.1417 (3.4310)	LR 2.000e-03
0: TRAIN [1][340/1938]	Time 0.067 (0.084)	Data 8.56e-05 (8.15e-04)	Tok/s 77557 (83484)	Loss/tok 3.1700 (3.4329)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.120 (0.084)	Data 8.54e-05 (7.94e-04)	Tok/s 97549 (83580)	Loss/tok 3.6088 (3.4366)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.043 (0.084)	Data 8.77e-05 (7.75e-04)	Tok/s 60842 (83477)	Loss/tok 2.7525 (3.4379)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.067 (0.084)	Data 8.54e-05 (7.56e-04)	Tok/s 77753 (83579)	Loss/tok 3.2242 (3.4375)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.119 (0.084)	Data 8.65e-05 (7.39e-04)	Tok/s 99113 (83533)	Loss/tok 3.5502 (3.4362)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.066 (0.084)	Data 1.00e-04 (7.22e-04)	Tok/s 76400 (83456)	Loss/tok 3.1457 (3.4337)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.067 (0.084)	Data 8.70e-05 (7.06e-04)	Tok/s 75443 (83441)	Loss/tok 3.3215 (3.4335)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.093 (0.084)	Data 8.75e-05 (6.91e-04)	Tok/s 92427 (83541)	Loss/tok 3.3718 (3.4352)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.091 (0.084)	Data 9.11e-05 (6.77e-04)	Tok/s 93204 (83573)	Loss/tok 3.4256 (3.4349)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.067 (0.084)	Data 1.02e-04 (6.63e-04)	Tok/s 75099 (83600)	Loss/tok 3.2239 (3.4356)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.093 (0.084)	Data 8.58e-05 (6.50e-04)	Tok/s 91090 (83647)	Loss/tok 3.5825 (3.4355)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.096 (0.085)	Data 8.58e-05 (6.38e-04)	Tok/s 88648 (83786)	Loss/tok 3.4761 (3.4406)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.093 (0.085)	Data 9.11e-05 (6.26e-04)	Tok/s 88868 (83846)	Loss/tok 3.4791 (3.4411)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.066 (0.085)	Data 9.04e-05 (6.14e-04)	Tok/s 80697 (83820)	Loss/tok 3.0623 (3.4406)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.066 (0.085)	Data 8.61e-05 (6.03e-04)	Tok/s 79149 (83867)	Loss/tok 3.3293 (3.4395)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.118 (0.085)	Data 8.82e-05 (5.93e-04)	Tok/s 99287 (83833)	Loss/tok 3.7495 (3.4396)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.067 (0.084)	Data 8.61e-05 (5.83e-04)	Tok/s 76182 (83790)	Loss/tok 3.2029 (3.4374)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.067 (0.084)	Data 9.08e-05 (5.73e-04)	Tok/s 76991 (83850)	Loss/tok 3.2366 (3.4365)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.093 (0.084)	Data 8.77e-05 (5.64e-04)	Tok/s 90258 (83854)	Loss/tok 3.5226 (3.4350)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.067 (0.085)	Data 8.65e-05 (5.55e-04)	Tok/s 75839 (83912)	Loss/tok 3.2357 (3.4357)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][540/1938]	Time 0.068 (0.085)	Data 8.20e-05 (5.46e-04)	Tok/s 75503 (84048)	Loss/tok 3.1780 (3.4390)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.068 (0.085)	Data 9.16e-05 (5.38e-04)	Tok/s 76013 (84072)	Loss/tok 3.3032 (3.4379)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.093 (0.085)	Data 9.35e-05 (5.30e-04)	Tok/s 91430 (84143)	Loss/tok 3.3776 (3.4392)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.043 (0.085)	Data 8.96e-05 (5.22e-04)	Tok/s 60414 (84137)	Loss/tok 2.7534 (3.4384)	LR 2.000e-03
0: TRAIN [1][580/1938]	Time 0.068 (0.085)	Data 8.73e-05 (5.14e-04)	Tok/s 79747 (84132)	Loss/tok 3.3116 (3.4400)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.044 (0.085)	Data 8.96e-05 (5.07e-04)	Tok/s 61280 (84126)	Loss/tok 2.7573 (3.4387)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.092 (0.085)	Data 8.37e-05 (5.00e-04)	Tok/s 91384 (84111)	Loss/tok 3.4344 (3.4375)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.092 (0.085)	Data 8.70e-05 (4.94e-04)	Tok/s 88627 (84103)	Loss/tok 3.5608 (3.4385)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.092 (0.085)	Data 8.27e-05 (4.87e-04)	Tok/s 90382 (84118)	Loss/tok 3.5892 (3.4387)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.093 (0.085)	Data 1.54e-04 (4.81e-04)	Tok/s 88077 (84172)	Loss/tok 3.3849 (3.4399)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.043 (0.085)	Data 9.20e-05 (4.75e-04)	Tok/s 61534 (84105)	Loss/tok 2.7819 (3.4382)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.091 (0.085)	Data 8.49e-05 (4.69e-04)	Tok/s 91501 (84173)	Loss/tok 3.4006 (3.4388)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.067 (0.085)	Data 8.49e-05 (4.63e-04)	Tok/s 76582 (84159)	Loss/tok 3.3061 (3.4377)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.153 (0.085)	Data 8.23e-05 (4.58e-04)	Tok/s 97766 (84109)	Loss/tok 3.7390 (3.4364)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.094 (0.085)	Data 8.20e-05 (4.52e-04)	Tok/s 89595 (84108)	Loss/tok 3.3728 (3.4349)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.066 (0.085)	Data 8.54e-05 (4.47e-04)	Tok/s 77852 (84082)	Loss/tok 3.4578 (3.4349)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][700/1938]	Time 0.119 (0.085)	Data 1.05e-04 (4.42e-04)	Tok/s 96615 (84094)	Loss/tok 3.6939 (3.4365)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.066 (0.085)	Data 8.39e-05 (4.37e-04)	Tok/s 77443 (84129)	Loss/tok 3.2744 (3.4351)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.067 (0.085)	Data 8.34e-05 (4.32e-04)	Tok/s 76737 (84129)	Loss/tok 3.0957 (3.4337)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.043 (0.085)	Data 8.54e-05 (4.27e-04)	Tok/s 61090 (84093)	Loss/tok 2.7772 (3.4329)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.092 (0.085)	Data 8.46e-05 (4.23e-04)	Tok/s 91244 (84148)	Loss/tok 3.3191 (3.4337)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.093 (0.085)	Data 8.51e-05 (4.18e-04)	Tok/s 89719 (84151)	Loss/tok 3.2998 (3.4339)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.067 (0.085)	Data 8.34e-05 (4.14e-04)	Tok/s 76093 (84175)	Loss/tok 3.2306 (3.4354)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.153 (0.086)	Data 8.65e-05 (4.09e-04)	Tok/s 97509 (84259)	Loss/tok 3.8682 (3.4368)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.093 (0.086)	Data 8.18e-05 (4.05e-04)	Tok/s 91331 (84312)	Loss/tok 3.3363 (3.4365)	LR 2.000e-03
0: TRAIN [1][790/1938]	Time 0.043 (0.085)	Data 8.13e-05 (4.01e-04)	Tok/s 59228 (84173)	Loss/tok 2.4516 (3.4346)	LR 2.000e-03
0: TRAIN [1][800/1938]	Time 0.067 (0.085)	Data 8.68e-05 (3.97e-04)	Tok/s 76030 (84138)	Loss/tok 3.3498 (3.4333)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.068 (0.085)	Data 9.54e-05 (3.94e-04)	Tok/s 75568 (84122)	Loss/tok 3.0221 (3.4327)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.044 (0.085)	Data 8.23e-05 (3.90e-04)	Tok/s 59908 (84059)	Loss/tok 2.6773 (3.4313)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][830/1938]	Time 0.094 (0.085)	Data 9.44e-05 (3.86e-04)	Tok/s 91403 (84115)	Loss/tok 3.4266 (3.4316)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.043 (0.085)	Data 8.15e-05 (3.83e-04)	Tok/s 62143 (83998)	Loss/tok 2.5711 (3.4293)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.068 (0.085)	Data 8.56e-05 (3.79e-04)	Tok/s 77706 (84033)	Loss/tok 3.2429 (3.4287)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.068 (0.085)	Data 8.56e-05 (3.76e-04)	Tok/s 79313 (84049)	Loss/tok 3.1939 (3.4284)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.117 (0.085)	Data 8.08e-05 (3.72e-04)	Tok/s 100869 (84020)	Loss/tok 3.4964 (3.4281)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.067 (0.085)	Data 1.10e-04 (3.69e-04)	Tok/s 79258 (83971)	Loss/tok 3.2979 (3.4269)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.093 (0.085)	Data 1.03e-04 (3.66e-04)	Tok/s 92361 (84017)	Loss/tok 3.4075 (3.4255)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.119 (0.085)	Data 1.02e-04 (3.63e-04)	Tok/s 97195 (84038)	Loss/tok 3.5403 (3.4261)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.092 (0.085)	Data 8.56e-05 (3.60e-04)	Tok/s 91736 (84051)	Loss/tok 3.4778 (3.4263)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.093 (0.085)	Data 1.04e-04 (3.57e-04)	Tok/s 89705 (84066)	Loss/tok 3.5186 (3.4274)	LR 2.000e-03
0: TRAIN [1][930/1938]	Time 0.067 (0.085)	Data 9.06e-05 (3.54e-04)	Tok/s 75195 (84077)	Loss/tok 3.3311 (3.4282)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.068 (0.085)	Data 8.49e-05 (3.51e-04)	Tok/s 76304 (84082)	Loss/tok 3.3284 (3.4283)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.092 (0.085)	Data 7.96e-05 (3.48e-04)	Tok/s 89951 (84096)	Loss/tok 3.4355 (3.4288)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.093 (0.085)	Data 8.27e-05 (3.46e-04)	Tok/s 87767 (84064)	Loss/tok 3.3393 (3.4277)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][970/1938]	Time 0.067 (0.085)	Data 8.34e-05 (3.43e-04)	Tok/s 77096 (84018)	Loss/tok 3.1791 (3.4282)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.067 (0.085)	Data 8.56e-05 (3.40e-04)	Tok/s 78870 (84015)	Loss/tok 3.3391 (3.4281)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.067 (0.085)	Data 8.27e-05 (3.38e-04)	Tok/s 74701 (84006)	Loss/tok 3.3819 (3.4273)	LR 2.000e-03
0: TRAIN [1][1000/1938]	Time 0.094 (0.085)	Data 9.49e-05 (3.35e-04)	Tok/s 86544 (84025)	Loss/tok 3.5088 (3.4268)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.066 (0.085)	Data 8.49e-05 (3.33e-04)	Tok/s 77588 (83982)	Loss/tok 3.0353 (3.4256)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.068 (0.085)	Data 8.54e-05 (3.30e-04)	Tok/s 75536 (83958)	Loss/tok 3.1119 (3.4242)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.067 (0.085)	Data 8.51e-05 (3.28e-04)	Tok/s 74690 (83966)	Loss/tok 3.1152 (3.4237)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.044 (0.085)	Data 9.16e-05 (3.26e-04)	Tok/s 59890 (83966)	Loss/tok 2.6172 (3.4229)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.068 (0.085)	Data 8.70e-05 (3.23e-04)	Tok/s 76786 (83975)	Loss/tok 3.1562 (3.4224)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.068 (0.085)	Data 8.39e-05 (3.21e-04)	Tok/s 74601 (83918)	Loss/tok 3.1550 (3.4211)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.068 (0.085)	Data 8.80e-05 (3.19e-04)	Tok/s 73798 (83884)	Loss/tok 3.2449 (3.4199)	LR 2.000e-03
0: TRAIN [1][1080/1938]	Time 0.093 (0.085)	Data 9.16e-05 (3.17e-04)	Tok/s 89771 (83904)	Loss/tok 3.4355 (3.4202)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.153 (0.085)	Data 8.58e-05 (3.15e-04)	Tok/s 97557 (83918)	Loss/tok 3.6940 (3.4200)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.067 (0.085)	Data 9.13e-05 (3.13e-04)	Tok/s 77332 (83893)	Loss/tok 3.1421 (3.4196)	LR 2.000e-03
0: TRAIN [1][1110/1938]	Time 0.068 (0.085)	Data 8.44e-05 (3.10e-04)	Tok/s 76548 (83881)	Loss/tok 3.3205 (3.4190)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.068 (0.085)	Data 8.58e-05 (3.08e-04)	Tok/s 75088 (83866)	Loss/tok 3.2475 (3.4186)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.067 (0.084)	Data 9.47e-05 (3.06e-04)	Tok/s 76658 (83837)	Loss/tok 3.1416 (3.4176)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1140/1938]	Time 0.045 (0.085)	Data 8.58e-05 (3.05e-04)	Tok/s 59823 (83844)	Loss/tok 2.7970 (3.4194)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.119 (0.084)	Data 8.58e-05 (3.03e-04)	Tok/s 97003 (83801)	Loss/tok 3.7004 (3.4189)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.093 (0.084)	Data 8.51e-05 (3.01e-04)	Tok/s 88859 (83823)	Loss/tok 3.3274 (3.4177)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.093 (0.084)	Data 8.42e-05 (2.99e-04)	Tok/s 90439 (83821)	Loss/tok 3.5223 (3.4174)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.067 (0.084)	Data 8.65e-05 (2.97e-04)	Tok/s 77077 (83816)	Loss/tok 3.2547 (3.4170)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.068 (0.085)	Data 8.23e-05 (2.95e-04)	Tok/s 74852 (83847)	Loss/tok 3.3157 (3.4201)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.091 (0.085)	Data 9.08e-05 (2.94e-04)	Tok/s 90598 (83843)	Loss/tok 3.4231 (3.4201)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.068 (0.085)	Data 8.37e-05 (2.92e-04)	Tok/s 77057 (83848)	Loss/tok 3.0884 (3.4205)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.093 (0.085)	Data 8.70e-05 (2.90e-04)	Tok/s 90738 (83865)	Loss/tok 3.3475 (3.4200)	LR 2.000e-03
0: TRAIN [1][1230/1938]	Time 0.066 (0.085)	Data 8.13e-05 (2.89e-04)	Tok/s 78148 (83844)	Loss/tok 2.9867 (3.4191)	LR 2.000e-03
0: TRAIN [1][1240/1938]	Time 0.067 (0.085)	Data 8.63e-05 (2.87e-04)	Tok/s 75525 (83845)	Loss/tok 3.1834 (3.4182)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.092 (0.085)	Data 8.80e-05 (2.85e-04)	Tok/s 91624 (83842)	Loss/tok 3.2035 (3.4175)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.093 (0.085)	Data 1.09e-04 (2.84e-04)	Tok/s 89307 (83839)	Loss/tok 3.4728 (3.4176)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.046 (0.085)	Data 8.73e-05 (2.82e-04)	Tok/s 58849 (83882)	Loss/tok 2.7085 (3.4188)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.067 (0.085)	Data 8.49e-05 (2.81e-04)	Tok/s 77149 (83854)	Loss/tok 3.1977 (3.4180)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.093 (0.085)	Data 8.06e-05 (2.79e-04)	Tok/s 92432 (83848)	Loss/tok 3.3981 (3.4174)	LR 2.000e-03
0: TRAIN [1][1300/1938]	Time 0.093 (0.085)	Data 8.37e-05 (2.78e-04)	Tok/s 88206 (83854)	Loss/tok 3.5008 (3.4171)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.066 (0.085)	Data 8.42e-05 (2.76e-04)	Tok/s 79045 (83835)	Loss/tok 3.3066 (3.4164)	LR 2.000e-03
0: TRAIN [1][1320/1938]	Time 0.118 (0.085)	Data 8.92e-05 (2.75e-04)	Tok/s 97043 (83793)	Loss/tok 3.6966 (3.4152)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.066 (0.085)	Data 8.49e-05 (2.73e-04)	Tok/s 77769 (83792)	Loss/tok 3.2388 (3.4149)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.068 (0.085)	Data 8.70e-05 (2.72e-04)	Tok/s 72736 (83775)	Loss/tok 3.0797 (3.4141)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.066 (0.085)	Data 8.11e-05 (2.70e-04)	Tok/s 77113 (83748)	Loss/tok 3.2894 (3.4133)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.152 (0.085)	Data 8.44e-05 (2.69e-04)	Tok/s 99432 (83748)	Loss/tok 3.6265 (3.4130)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.093 (0.084)	Data 8.06e-05 (2.68e-04)	Tok/s 90980 (83702)	Loss/tok 3.4001 (3.4119)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.093 (0.084)	Data 8.13e-05 (2.66e-04)	Tok/s 89911 (83709)	Loss/tok 3.3674 (3.4113)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.092 (0.084)	Data 8.63e-05 (2.65e-04)	Tok/s 90065 (83650)	Loss/tok 3.2496 (3.4098)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.067 (0.084)	Data 8.32e-05 (2.64e-04)	Tok/s 76543 (83602)	Loss/tok 3.0022 (3.4088)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1410/1938]	Time 0.066 (0.084)	Data 8.30e-05 (2.63e-04)	Tok/s 76542 (83624)	Loss/tok 3.2340 (3.4086)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1420/1938]	Time 0.043 (0.084)	Data 7.99e-05 (2.61e-04)	Tok/s 59754 (83604)	Loss/tok 2.5468 (3.4084)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.119 (0.084)	Data 8.73e-05 (2.60e-04)	Tok/s 97250 (83593)	Loss/tok 3.5830 (3.4079)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.121 (0.084)	Data 8.08e-05 (2.59e-04)	Tok/s 94867 (83608)	Loss/tok 3.4801 (3.4082)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.092 (0.084)	Data 1.04e-04 (2.58e-04)	Tok/s 90845 (83603)	Loss/tok 3.3691 (3.4078)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.092 (0.084)	Data 8.08e-05 (2.57e-04)	Tok/s 91394 (83608)	Loss/tok 3.4083 (3.4068)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.092 (0.084)	Data 8.56e-05 (2.55e-04)	Tok/s 89328 (83598)	Loss/tok 3.4832 (3.4065)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.092 (0.084)	Data 8.68e-05 (2.54e-04)	Tok/s 90141 (83590)	Loss/tok 3.3630 (3.4058)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.067 (0.084)	Data 8.39e-05 (2.53e-04)	Tok/s 80816 (83605)	Loss/tok 3.2825 (3.4063)	LR 2.000e-03
0: TRAIN [1][1500/1938]	Time 0.066 (0.084)	Data 8.13e-05 (2.52e-04)	Tok/s 76581 (83592)	Loss/tok 3.0285 (3.4049)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.044 (0.084)	Data 8.73e-05 (2.51e-04)	Tok/s 60358 (83595)	Loss/tok 2.5551 (3.4048)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.092 (0.084)	Data 8.39e-05 (2.50e-04)	Tok/s 91291 (83626)	Loss/tok 3.3837 (3.4052)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.119 (0.084)	Data 8.08e-05 (2.49e-04)	Tok/s 97554 (83625)	Loss/tok 3.4976 (3.4048)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.066 (0.084)	Data 8.49e-05 (2.48e-04)	Tok/s 80026 (83602)	Loss/tok 3.2334 (3.4040)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1550/1938]	Time 0.069 (0.084)	Data 8.92e-05 (2.47e-04)	Tok/s 76438 (83617)	Loss/tok 3.1406 (3.4049)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.068 (0.084)	Data 8.37e-05 (2.46e-04)	Tok/s 76565 (83581)	Loss/tok 3.1466 (3.4043)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.092 (0.084)	Data 8.49e-05 (2.45e-04)	Tok/s 91112 (83572)	Loss/tok 3.3224 (3.4036)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.120 (0.084)	Data 8.34e-05 (2.44e-04)	Tok/s 97114 (83584)	Loss/tok 3.6584 (3.4035)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.092 (0.084)	Data 8.77e-05 (2.43e-04)	Tok/s 91893 (83582)	Loss/tok 3.3084 (3.4036)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.067 (0.084)	Data 8.65e-05 (2.42e-04)	Tok/s 76576 (83623)	Loss/tok 3.1454 (3.4044)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.066 (0.084)	Data 8.51e-05 (2.41e-04)	Tok/s 73564 (83602)	Loss/tok 3.3117 (3.4035)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.094 (0.084)	Data 8.63e-05 (2.40e-04)	Tok/s 88426 (83614)	Loss/tok 3.2390 (3.4034)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.068 (0.084)	Data 8.42e-05 (2.39e-04)	Tok/s 75037 (83616)	Loss/tok 3.1667 (3.4029)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.068 (0.084)	Data 8.39e-05 (2.38e-04)	Tok/s 76853 (83620)	Loss/tok 2.9727 (3.4024)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.153 (0.085)	Data 8.63e-05 (2.37e-04)	Tok/s 97972 (83637)	Loss/tok 3.6402 (3.4030)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.067 (0.084)	Data 7.94e-05 (2.36e-04)	Tok/s 79746 (83636)	Loss/tok 3.0268 (3.4026)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.118 (0.084)	Data 8.68e-05 (2.35e-04)	Tok/s 99013 (83649)	Loss/tok 3.2569 (3.4022)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.092 (0.085)	Data 8.44e-05 (2.34e-04)	Tok/s 89573 (83661)	Loss/tok 3.5033 (3.4028)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.043 (0.085)	Data 7.96e-05 (2.33e-04)	Tok/s 61288 (83646)	Loss/tok 2.6532 (3.4027)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.093 (0.084)	Data 8.01e-05 (2.32e-04)	Tok/s 90919 (83636)	Loss/tok 3.2309 (3.4019)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.043 (0.084)	Data 8.06e-05 (2.32e-04)	Tok/s 58796 (83622)	Loss/tok 2.6791 (3.4006)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.153 (0.084)	Data 8.27e-05 (2.31e-04)	Tok/s 97395 (83614)	Loss/tok 3.6470 (3.4001)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1730/1938]	Time 0.093 (0.084)	Data 8.08e-05 (2.30e-04)	Tok/s 90220 (83623)	Loss/tok 3.3808 (3.4000)	LR 2.000e-03
0: TRAIN [1][1740/1938]	Time 0.094 (0.084)	Data 8.46e-05 (2.29e-04)	Tok/s 88830 (83626)	Loss/tok 3.3752 (3.3996)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.067 (0.085)	Data 7.99e-05 (2.28e-04)	Tok/s 76402 (83636)	Loss/tok 3.1216 (3.4000)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.044 (0.084)	Data 8.89e-05 (2.27e-04)	Tok/s 58444 (83616)	Loss/tok 2.5844 (3.3991)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.118 (0.084)	Data 8.30e-05 (2.27e-04)	Tok/s 99410 (83608)	Loss/tok 3.5543 (3.3990)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.069 (0.084)	Data 8.34e-05 (2.26e-04)	Tok/s 76229 (83578)	Loss/tok 2.9581 (3.3985)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.093 (0.084)	Data 9.30e-05 (2.25e-04)	Tok/s 91135 (83560)	Loss/tok 3.3736 (3.3978)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.092 (0.084)	Data 8.49e-05 (2.24e-04)	Tok/s 91332 (83593)	Loss/tok 3.3791 (3.3983)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.093 (0.084)	Data 1.33e-04 (2.24e-04)	Tok/s 91027 (83604)	Loss/tok 3.3628 (3.3982)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.068 (0.084)	Data 8.44e-05 (2.23e-04)	Tok/s 75036 (83592)	Loss/tok 3.0900 (3.3976)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.068 (0.084)	Data 8.15e-05 (2.22e-04)	Tok/s 76308 (83574)	Loss/tok 3.1799 (3.3969)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.092 (0.084)	Data 8.54e-05 (2.21e-04)	Tok/s 90602 (83580)	Loss/tok 3.3149 (3.3963)	LR 2.000e-03
0: TRAIN [1][1850/1938]	Time 0.043 (0.084)	Data 9.01e-05 (2.20e-04)	Tok/s 62771 (83563)	Loss/tok 2.6082 (3.3957)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1860/1938]	Time 0.152 (0.084)	Data 8.54e-05 (2.20e-04)	Tok/s 98771 (83573)	Loss/tok 3.7020 (3.3956)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.092 (0.084)	Data 8.27e-05 (2.19e-04)	Tok/s 90023 (83554)	Loss/tok 3.4023 (3.3949)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.119 (0.084)	Data 8.42e-05 (2.18e-04)	Tok/s 97537 (83544)	Loss/tok 3.5791 (3.3947)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.092 (0.084)	Data 8.20e-05 (2.18e-04)	Tok/s 90072 (83527)	Loss/tok 3.3802 (3.3940)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.093 (0.084)	Data 8.23e-05 (2.17e-04)	Tok/s 91551 (83523)	Loss/tok 3.1561 (3.3936)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.067 (0.084)	Data 8.20e-05 (2.16e-04)	Tok/s 76051 (83522)	Loss/tok 3.1615 (3.3933)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.069 (0.084)	Data 8.15e-05 (2.16e-04)	Tok/s 73797 (83516)	Loss/tok 3.1084 (3.3933)	LR 2.000e-03
0: TRAIN [1][1930/1938]	Time 0.119 (0.084)	Data 8.46e-05 (2.15e-04)	Tok/s 96486 (83529)	Loss/tok 3.4520 (3.3929)	LR 2.000e-03
:::MLL 1575546893.937 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1575546893.938 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.432 (0.432)	Decoder iters 114.0 (114.0)	Tok/s 20853 (20853)
0: Running moses detokenizer
0: BLEU(score=22.223160974033245, counts=[35984, 17344, 9610, 5559], totals=[65400, 62397, 59394, 56399], precisions=[55.02140672782875, 27.796208151032904, 16.18008553052497, 9.856557740385467], bp=1.0, sys_len=65400, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1575546895.097 eval_accuracy: {"value": 22.22, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1575546895.097 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.3936	Test BLEU: 22.22
0: Performance: Epoch: 1	Training: 1337199 Tok/s
0: Finished epoch 1
:::MLL 1575546895.098 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1575546895.098 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1575546895.098 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 1622369142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [2][0/1938]	Time 0.354 (0.354)	Data 2.53e-01 (2.53e-01)	Tok/s 23625 (23625)	Loss/tok 3.2803 (3.2803)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.162 (0.115)	Data 8.49e-05 (2.31e-02)	Tok/s 73137 (79769)	Loss/tok 3.4372 (3.2566)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.065 (0.098)	Data 8.11e-05 (1.21e-02)	Tok/s 79115 (81379)	Loss/tok 3.0001 (3.2219)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.092 (0.091)	Data 8.20e-05 (8.24e-03)	Tok/s 91751 (81578)	Loss/tok 3.2344 (3.2251)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.044 (0.087)	Data 8.61e-05 (6.25e-03)	Tok/s 61979 (81270)	Loss/tok 2.6280 (3.2060)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.065 (0.085)	Data 7.58e-05 (5.04e-03)	Tok/s 78932 (81451)	Loss/tok 3.1308 (3.1967)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.093 (0.085)	Data 8.39e-05 (4.23e-03)	Tok/s 89738 (82010)	Loss/tok 3.3040 (3.2067)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.095 (0.088)	Data 8.51e-05 (3.65e-03)	Tok/s 90673 (83114)	Loss/tok 3.3259 (3.2387)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.069 (0.089)	Data 8.27e-05 (3.21e-03)	Tok/s 75391 (83539)	Loss/tok 2.9946 (3.2543)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.121 (0.089)	Data 9.04e-05 (2.86e-03)	Tok/s 97603 (83690)	Loss/tok 3.2880 (3.2578)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.067 (0.091)	Data 8.18e-05 (2.59e-03)	Tok/s 76873 (84147)	Loss/tok 3.0278 (3.2767)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.068 (0.090)	Data 8.82e-05 (2.36e-03)	Tok/s 76984 (83754)	Loss/tok 3.0059 (3.2730)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.093 (0.089)	Data 1.10e-04 (2.17e-03)	Tok/s 91296 (83644)	Loss/tok 3.3351 (3.2709)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.066 (0.088)	Data 8.25e-05 (2.01e-03)	Tok/s 79736 (83235)	Loss/tok 3.0106 (3.2640)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.093 (0.087)	Data 7.96e-05 (1.88e-03)	Tok/s 89211 (83356)	Loss/tok 3.2076 (3.2621)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.067 (0.088)	Data 8.11e-05 (1.76e-03)	Tok/s 78324 (83636)	Loss/tok 3.1590 (3.2619)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.094 (0.087)	Data 8.70e-05 (1.65e-03)	Tok/s 89592 (83710)	Loss/tok 3.3521 (3.2601)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.091 (0.088)	Data 1.02e-04 (1.56e-03)	Tok/s 92234 (83700)	Loss/tok 3.2183 (3.2637)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.066 (0.087)	Data 8.11e-05 (1.48e-03)	Tok/s 78889 (83600)	Loss/tok 3.0661 (3.2569)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.067 (0.087)	Data 8.18e-05 (1.41e-03)	Tok/s 78631 (83662)	Loss/tok 2.9534 (3.2572)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.094 (0.088)	Data 9.25e-05 (1.34e-03)	Tok/s 89335 (83970)	Loss/tok 3.3352 (3.2641)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.093 (0.087)	Data 1.31e-04 (1.28e-03)	Tok/s 89716 (83759)	Loss/tok 3.2486 (3.2593)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.069 (0.088)	Data 8.46e-05 (1.23e-03)	Tok/s 75515 (83870)	Loss/tok 3.0656 (3.2622)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.119 (0.087)	Data 8.51e-05 (1.18e-03)	Tok/s 97875 (83809)	Loss/tok 3.4285 (3.2620)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.067 (0.087)	Data 1.24e-04 (1.13e-03)	Tok/s 74125 (83811)	Loss/tok 2.9671 (3.2617)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.152 (0.088)	Data 8.68e-05 (1.09e-03)	Tok/s 95579 (83937)	Loss/tok 3.6944 (3.2675)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.066 (0.087)	Data 7.99e-05 (1.05e-03)	Tok/s 79052 (83950)	Loss/tok 3.0041 (3.2691)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.066 (0.087)	Data 8.30e-05 (1.02e-03)	Tok/s 75404 (83992)	Loss/tok 3.2119 (3.2674)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.092 (0.087)	Data 8.82e-05 (9.84e-04)	Tok/s 91195 (83973)	Loss/tok 3.3353 (3.2678)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.045 (0.087)	Data 8.44e-05 (9.53e-04)	Tok/s 58608 (83994)	Loss/tok 2.6479 (3.2682)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.093 (0.087)	Data 8.68e-05 (9.25e-04)	Tok/s 90613 (84164)	Loss/tok 3.2198 (3.2676)	LR 2.000e-03
0: TRAIN [2][310/1938]	Time 0.066 (0.087)	Data 9.85e-05 (8.98e-04)	Tok/s 78138 (84085)	Loss/tok 3.0216 (3.2661)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.094 (0.087)	Data 8.80e-05 (8.72e-04)	Tok/s 89221 (84031)	Loss/tok 3.0894 (3.2633)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][330/1938]	Time 0.067 (0.087)	Data 8.58e-05 (8.49e-04)	Tok/s 78231 (84052)	Loss/tok 3.0579 (3.2666)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.066 (0.087)	Data 8.56e-05 (8.26e-04)	Tok/s 77758 (84070)	Loss/tok 3.1975 (3.2672)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.066 (0.087)	Data 8.34e-05 (8.05e-04)	Tok/s 78652 (83969)	Loss/tok 3.0236 (3.2662)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.066 (0.086)	Data 8.73e-05 (7.85e-04)	Tok/s 77454 (83848)	Loss/tok 3.1371 (3.2629)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.094 (0.086)	Data 8.70e-05 (7.67e-04)	Tok/s 91878 (83812)	Loss/tok 3.0413 (3.2598)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.120 (0.086)	Data 9.27e-05 (7.49e-04)	Tok/s 95620 (83913)	Loss/tok 3.5215 (3.2622)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.066 (0.086)	Data 8.65e-05 (7.32e-04)	Tok/s 77290 (83826)	Loss/tok 3.0452 (3.2610)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.120 (0.086)	Data 8.13e-05 (7.16e-04)	Tok/s 100069 (83791)	Loss/tok 3.3052 (3.2604)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.066 (0.086)	Data 9.37e-05 (7.01e-04)	Tok/s 78658 (83855)	Loss/tok 2.9414 (3.2594)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.093 (0.086)	Data 8.51e-05 (6.86e-04)	Tok/s 90566 (83803)	Loss/tok 3.2437 (3.2583)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.093 (0.085)	Data 8.56e-05 (6.72e-04)	Tok/s 90547 (83729)	Loss/tok 3.2717 (3.2591)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.066 (0.085)	Data 8.44e-05 (6.59e-04)	Tok/s 80726 (83660)	Loss/tok 2.9728 (3.2564)	LR 2.000e-03
0: TRAIN [2][450/1938]	Time 0.093 (0.085)	Data 8.61e-05 (6.47e-04)	Tok/s 88888 (83736)	Loss/tok 3.2956 (3.2590)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.067 (0.086)	Data 8.94e-05 (6.34e-04)	Tok/s 75250 (83778)	Loss/tok 3.0822 (3.2586)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.044 (0.085)	Data 8.63e-05 (6.23e-04)	Tok/s 59718 (83716)	Loss/tok 2.6505 (3.2562)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.066 (0.085)	Data 8.34e-05 (6.12e-04)	Tok/s 80616 (83663)	Loss/tok 3.0833 (3.2568)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.066 (0.085)	Data 8.30e-05 (6.01e-04)	Tok/s 80146 (83679)	Loss/tok 3.1162 (3.2558)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.067 (0.085)	Data 8.77e-05 (5.91e-04)	Tok/s 77930 (83656)	Loss/tok 2.9013 (3.2557)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][510/1938]	Time 0.151 (0.085)	Data 8.51e-05 (5.81e-04)	Tok/s 97523 (83601)	Loss/tok 3.5414 (3.2568)	LR 2.000e-03
0: TRAIN [2][520/1938]	Time 0.092 (0.085)	Data 8.77e-05 (5.72e-04)	Tok/s 91646 (83626)	Loss/tok 3.3630 (3.2573)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.092 (0.085)	Data 9.44e-05 (5.62e-04)	Tok/s 90633 (83622)	Loss/tok 3.2064 (3.2556)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.067 (0.085)	Data 8.34e-05 (5.54e-04)	Tok/s 77608 (83586)	Loss/tok 2.9572 (3.2546)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.092 (0.085)	Data 8.42e-05 (5.45e-04)	Tok/s 92014 (83578)	Loss/tok 3.2964 (3.2550)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.094 (0.085)	Data 8.32e-05 (5.37e-04)	Tok/s 90209 (83645)	Loss/tok 3.1837 (3.2567)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.066 (0.085)	Data 7.92e-05 (5.29e-04)	Tok/s 75445 (83621)	Loss/tok 2.9633 (3.2564)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.121 (0.085)	Data 8.34e-05 (5.22e-04)	Tok/s 98246 (83660)	Loss/tok 3.3725 (3.2561)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.120 (0.085)	Data 8.82e-05 (5.14e-04)	Tok/s 94699 (83599)	Loss/tok 3.6465 (3.2562)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.093 (0.085)	Data 9.08e-05 (5.07e-04)	Tok/s 89368 (83586)	Loss/tok 3.2928 (3.2559)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.066 (0.085)	Data 9.47e-05 (5.01e-04)	Tok/s 78538 (83588)	Loss/tok 3.0127 (3.2557)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.117 (0.085)	Data 8.32e-05 (4.94e-04)	Tok/s 98650 (83573)	Loss/tok 3.6084 (3.2562)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.151 (0.085)	Data 8.06e-05 (4.87e-04)	Tok/s 100576 (83522)	Loss/tok 3.3806 (3.2541)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.066 (0.085)	Data 1.40e-04 (4.81e-04)	Tok/s 79931 (83508)	Loss/tok 3.0003 (3.2537)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.122 (0.085)	Data 8.73e-05 (4.75e-04)	Tok/s 95177 (83546)	Loss/tok 3.4458 (3.2544)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][660/1938]	Time 0.094 (0.085)	Data 8.75e-05 (4.69e-04)	Tok/s 88883 (83565)	Loss/tok 3.2241 (3.2571)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.066 (0.085)	Data 8.63e-05 (4.64e-04)	Tok/s 75959 (83605)	Loss/tok 2.9078 (3.2592)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.094 (0.085)	Data 8.87e-05 (4.58e-04)	Tok/s 88657 (83589)	Loss/tok 3.3286 (3.2598)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.068 (0.085)	Data 9.06e-05 (4.53e-04)	Tok/s 76381 (83607)	Loss/tok 3.1317 (3.2593)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.096 (0.085)	Data 8.99e-05 (4.48e-04)	Tok/s 88449 (83631)	Loss/tok 3.2457 (3.2608)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.154 (0.085)	Data 1.04e-04 (4.43e-04)	Tok/s 94691 (83645)	Loss/tok 3.6272 (3.2622)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.044 (0.085)	Data 8.63e-05 (4.38e-04)	Tok/s 61613 (83589)	Loss/tok 2.6894 (3.2615)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.119 (0.085)	Data 9.94e-05 (4.33e-04)	Tok/s 97608 (83638)	Loss/tok 3.4049 (3.2635)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.151 (0.085)	Data 8.65e-05 (4.28e-04)	Tok/s 98621 (83617)	Loss/tok 3.4953 (3.2626)	LR 2.000e-03
0: TRAIN [2][750/1938]	Time 0.067 (0.085)	Data 8.27e-05 (4.24e-04)	Tok/s 75314 (83540)	Loss/tok 3.0287 (3.2617)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.092 (0.085)	Data 8.03e-05 (4.19e-04)	Tok/s 90937 (83505)	Loss/tok 3.2975 (3.2611)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.044 (0.085)	Data 8.01e-05 (4.15e-04)	Tok/s 55777 (83532)	Loss/tok 2.5278 (3.2616)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.065 (0.085)	Data 8.23e-05 (4.11e-04)	Tok/s 80994 (83505)	Loss/tok 2.9546 (3.2605)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.066 (0.085)	Data 8.37e-05 (4.06e-04)	Tok/s 79464 (83509)	Loss/tok 3.1421 (3.2608)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.067 (0.085)	Data 7.96e-05 (4.02e-04)	Tok/s 75681 (83480)	Loss/tok 2.8767 (3.2607)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.092 (0.085)	Data 7.99e-05 (3.98e-04)	Tok/s 88985 (83445)	Loss/tok 3.3215 (3.2601)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.044 (0.085)	Data 7.94e-05 (3.95e-04)	Tok/s 60864 (83499)	Loss/tok 2.7033 (3.2608)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.067 (0.085)	Data 8.65e-05 (3.91e-04)	Tok/s 75146 (83407)	Loss/tok 3.0846 (3.2593)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.094 (0.085)	Data 8.54e-05 (3.87e-04)	Tok/s 89582 (83520)	Loss/tok 3.1654 (3.2613)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.093 (0.085)	Data 8.99e-05 (3.84e-04)	Tok/s 91310 (83497)	Loss/tok 3.3036 (3.2611)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.067 (0.085)	Data 8.75e-05 (3.80e-04)	Tok/s 77776 (83516)	Loss/tok 2.8952 (3.2617)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.094 (0.085)	Data 8.65e-05 (3.77e-04)	Tok/s 88267 (83549)	Loss/tok 3.2816 (3.2624)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.093 (0.085)	Data 8.51e-05 (3.74e-04)	Tok/s 90871 (83536)	Loss/tok 3.3467 (3.2615)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.066 (0.085)	Data 7.89e-05 (3.70e-04)	Tok/s 76921 (83531)	Loss/tok 3.0084 (3.2622)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.119 (0.085)	Data 7.99e-05 (3.67e-04)	Tok/s 98633 (83498)	Loss/tok 3.5064 (3.2620)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][910/1938]	Time 0.092 (0.085)	Data 8.42e-05 (3.64e-04)	Tok/s 92410 (83509)	Loss/tok 3.1153 (3.2633)	LR 2.000e-03
0: TRAIN [2][920/1938]	Time 0.066 (0.085)	Data 8.39e-05 (3.61e-04)	Tok/s 77926 (83519)	Loss/tok 3.1714 (3.2630)	LR 2.000e-03
0: TRAIN [2][930/1938]	Time 0.093 (0.085)	Data 8.39e-05 (3.58e-04)	Tok/s 91597 (83510)	Loss/tok 3.2876 (3.2629)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.067 (0.085)	Data 8.32e-05 (3.55e-04)	Tok/s 75534 (83542)	Loss/tok 3.0398 (3.2642)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.066 (0.085)	Data 8.34e-05 (3.52e-04)	Tok/s 78149 (83588)	Loss/tok 3.0597 (3.2650)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.092 (0.085)	Data 9.23e-05 (3.49e-04)	Tok/s 91774 (83568)	Loss/tok 3.2473 (3.2649)	LR 2.000e-03
0: TRAIN [2][970/1938]	Time 0.068 (0.085)	Data 8.56e-05 (3.47e-04)	Tok/s 77362 (83602)	Loss/tok 3.0877 (3.2657)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.091 (0.085)	Data 8.30e-05 (3.44e-04)	Tok/s 93636 (83550)	Loss/tok 3.3560 (3.2647)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.067 (0.085)	Data 9.44e-05 (3.41e-04)	Tok/s 78204 (83519)	Loss/tok 3.1114 (3.2643)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.093 (0.085)	Data 8.25e-05 (3.39e-04)	Tok/s 92961 (83502)	Loss/tok 3.0778 (3.2634)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.068 (0.085)	Data 8.58e-05 (3.36e-04)	Tok/s 74060 (83496)	Loss/tok 3.0678 (3.2628)	LR 2.000e-03
0: TRAIN [2][1020/1938]	Time 0.153 (0.085)	Data 8.39e-05 (3.34e-04)	Tok/s 99525 (83503)	Loss/tok 3.3893 (3.2635)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.092 (0.085)	Data 8.37e-05 (3.31e-04)	Tok/s 91172 (83504)	Loss/tok 3.3258 (3.2631)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.092 (0.085)	Data 8.75e-05 (3.29e-04)	Tok/s 92278 (83479)	Loss/tok 3.2209 (3.2621)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.093 (0.085)	Data 8.37e-05 (3.27e-04)	Tok/s 91220 (83433)	Loss/tok 3.2321 (3.2612)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.094 (0.085)	Data 8.68e-05 (3.24e-04)	Tok/s 89063 (83410)	Loss/tok 3.1762 (3.2606)	LR 2.000e-03
0: TRAIN [2][1070/1938]	Time 0.118 (0.085)	Data 8.13e-05 (3.22e-04)	Tok/s 99421 (83412)	Loss/tok 3.2701 (3.2595)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.093 (0.085)	Data 8.27e-05 (3.20e-04)	Tok/s 89481 (83401)	Loss/tok 3.2943 (3.2588)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.066 (0.085)	Data 8.20e-05 (3.18e-04)	Tok/s 78266 (83380)	Loss/tok 3.0172 (3.2585)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.067 (0.085)	Data 7.89e-05 (3.16e-04)	Tok/s 75766 (83421)	Loss/tok 2.8331 (3.2590)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.067 (0.085)	Data 8.15e-05 (3.14e-04)	Tok/s 75303 (83435)	Loss/tok 3.2174 (3.2594)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.067 (0.085)	Data 8.92e-05 (3.12e-04)	Tok/s 77224 (83400)	Loss/tok 3.0072 (3.2586)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.120 (0.085)	Data 8.42e-05 (3.10e-04)	Tok/s 96926 (83404)	Loss/tok 3.3666 (3.2592)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.066 (0.085)	Data 1.02e-04 (3.08e-04)	Tok/s 78702 (83419)	Loss/tok 2.9376 (3.2592)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][1150/1938]	Time 0.066 (0.085)	Data 8.61e-05 (3.06e-04)	Tok/s 77250 (83440)	Loss/tok 3.0410 (3.2599)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.043 (0.085)	Data 8.37e-05 (3.04e-04)	Tok/s 61174 (83458)	Loss/tok 2.8396 (3.2609)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.151 (0.085)	Data 9.32e-05 (3.02e-04)	Tok/s 98156 (83441)	Loss/tok 3.6079 (3.2602)	LR 2.000e-03
0: TRAIN [2][1180/1938]	Time 0.094 (0.085)	Data 9.68e-05 (3.00e-04)	Tok/s 87109 (83393)	Loss/tok 3.3798 (3.2601)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.065 (0.085)	Data 8.42e-05 (2.98e-04)	Tok/s 77949 (83368)	Loss/tok 3.0709 (3.2594)	LR 2.000e-03
0: TRAIN [2][1200/1938]	Time 0.094 (0.085)	Data 8.58e-05 (2.96e-04)	Tok/s 90194 (83422)	Loss/tok 3.1747 (3.2601)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.092 (0.085)	Data 8.46e-05 (2.95e-04)	Tok/s 91687 (83397)	Loss/tok 3.1710 (3.2588)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.119 (0.085)	Data 8.27e-05 (2.93e-04)	Tok/s 96862 (83385)	Loss/tok 3.4164 (3.2580)	LR 2.000e-03
0: TRAIN [2][1230/1938]	Time 0.066 (0.085)	Data 8.23e-05 (2.91e-04)	Tok/s 77004 (83408)	Loss/tok 3.0628 (3.2578)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.066 (0.084)	Data 8.51e-05 (2.90e-04)	Tok/s 75625 (83401)	Loss/tok 3.0717 (3.2577)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.094 (0.085)	Data 8.61e-05 (2.88e-04)	Tok/s 89446 (83450)	Loss/tok 3.4441 (3.2585)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.093 (0.085)	Data 8.34e-05 (2.86e-04)	Tok/s 91631 (83470)	Loss/tok 3.1462 (3.2582)	LR 2.000e-03
0: TRAIN [2][1270/1938]	Time 0.065 (0.084)	Data 8.56e-05 (2.85e-04)	Tok/s 78524 (83423)	Loss/tok 3.1448 (3.2574)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.093 (0.084)	Data 8.18e-05 (2.83e-04)	Tok/s 88967 (83442)	Loss/tok 3.3447 (3.2576)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.067 (0.085)	Data 8.56e-05 (2.82e-04)	Tok/s 76280 (83477)	Loss/tok 3.1681 (3.2591)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.092 (0.085)	Data 7.99e-05 (2.80e-04)	Tok/s 90164 (83491)	Loss/tok 3.2405 (3.2600)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][1310/1938]	Time 0.093 (0.085)	Data 8.58e-05 (2.79e-04)	Tok/s 90712 (83494)	Loss/tok 3.2591 (3.2602)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.155 (0.085)	Data 8.34e-05 (2.77e-04)	Tok/s 95025 (83487)	Loss/tok 3.5655 (3.2603)	LR 2.000e-03
0: TRAIN [2][1330/1938]	Time 0.067 (0.085)	Data 8.18e-05 (2.76e-04)	Tok/s 77157 (83499)	Loss/tok 3.1149 (3.2601)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.092 (0.085)	Data 8.32e-05 (2.74e-04)	Tok/s 91015 (83503)	Loss/tok 3.1565 (3.2594)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.043 (0.085)	Data 8.70e-05 (2.73e-04)	Tok/s 61449 (83516)	Loss/tok 2.8793 (3.2594)	LR 2.000e-03
0: TRAIN [2][1360/1938]	Time 0.066 (0.085)	Data 8.13e-05 (2.71e-04)	Tok/s 76821 (83506)	Loss/tok 3.0923 (3.2590)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.091 (0.085)	Data 8.01e-05 (2.70e-04)	Tok/s 91357 (83502)	Loss/tok 3.1360 (3.2583)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.123 (0.085)	Data 8.37e-05 (2.69e-04)	Tok/s 94858 (83516)	Loss/tok 3.4938 (3.2587)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.153 (0.085)	Data 8.44e-05 (2.67e-04)	Tok/s 96413 (83509)	Loss/tok 3.7001 (3.2587)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.120 (0.085)	Data 7.89e-05 (2.66e-04)	Tok/s 97823 (83547)	Loss/tok 3.4857 (3.2588)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.066 (0.085)	Data 8.11e-05 (2.65e-04)	Tok/s 75796 (83560)	Loss/tok 3.0051 (3.2592)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.092 (0.085)	Data 8.30e-05 (2.63e-04)	Tok/s 91383 (83601)	Loss/tok 3.0662 (3.2600)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.094 (0.085)	Data 8.96e-05 (2.62e-04)	Tok/s 89052 (83620)	Loss/tok 3.2890 (3.2601)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.151 (0.085)	Data 8.30e-05 (2.61e-04)	Tok/s 100216 (83608)	Loss/tok 3.5591 (3.2595)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][1450/1938]	Time 0.066 (0.085)	Data 7.96e-05 (2.60e-04)	Tok/s 79604 (83593)	Loss/tok 3.2358 (3.2594)	LR 2.000e-03
0: TRAIN [2][1460/1938]	Time 0.096 (0.085)	Data 1.39e-04 (2.59e-04)	Tok/s 85475 (83603)	Loss/tok 3.5206 (3.2596)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.121 (0.085)	Data 8.54e-05 (2.57e-04)	Tok/s 98330 (83618)	Loss/tok 3.3624 (3.2595)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.069 (0.085)	Data 8.13e-05 (2.56e-04)	Tok/s 73018 (83645)	Loss/tok 3.1004 (3.2601)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.044 (0.085)	Data 8.44e-05 (2.55e-04)	Tok/s 60312 (83645)	Loss/tok 2.6853 (3.2603)	LR 2.000e-03
0: TRAIN [2][1500/1938]	Time 0.092 (0.085)	Data 8.13e-05 (2.54e-04)	Tok/s 90231 (83649)	Loss/tok 3.1851 (3.2599)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.153 (0.085)	Data 8.49e-05 (2.53e-04)	Tok/s 96353 (83664)	Loss/tok 3.5149 (3.2603)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.067 (0.085)	Data 8.30e-05 (2.52e-04)	Tok/s 78364 (83655)	Loss/tok 2.9639 (3.2601)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.067 (0.085)	Data 8.20e-05 (2.51e-04)	Tok/s 76931 (83623)	Loss/tok 3.1668 (3.2592)	LR 2.000e-03
0: TRAIN [2][1540/1938]	Time 0.093 (0.085)	Data 9.35e-05 (2.50e-04)	Tok/s 92114 (83631)	Loss/tok 3.2079 (3.2597)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.093 (0.085)	Data 8.20e-05 (2.48e-04)	Tok/s 91115 (83646)	Loss/tok 3.0802 (3.2602)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.066 (0.085)	Data 8.39e-05 (2.47e-04)	Tok/s 76408 (83653)	Loss/tok 3.0850 (3.2603)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.065 (0.085)	Data 8.77e-05 (2.46e-04)	Tok/s 78727 (83637)	Loss/tok 3.1428 (3.2602)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.067 (0.085)	Data 9.01e-05 (2.45e-04)	Tok/s 80584 (83641)	Loss/tok 3.0767 (3.2601)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.092 (0.085)	Data 8.73e-05 (2.44e-04)	Tok/s 91241 (83621)	Loss/tok 3.3623 (3.2595)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.067 (0.085)	Data 7.82e-05 (2.43e-04)	Tok/s 75369 (83634)	Loss/tok 2.9891 (3.2590)	LR 2.000e-03
0: TRAIN [2][1610/1938]	Time 0.066 (0.085)	Data 7.92e-05 (2.42e-04)	Tok/s 77384 (83637)	Loss/tok 2.9753 (3.2591)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.066 (0.085)	Data 7.94e-05 (2.41e-04)	Tok/s 79698 (83632)	Loss/tok 3.0366 (3.2590)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.094 (0.085)	Data 8.15e-05 (2.40e-04)	Tok/s 87508 (83648)	Loss/tok 3.2426 (3.2588)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.065 (0.085)	Data 8.18e-05 (2.39e-04)	Tok/s 78680 (83621)	Loss/tok 3.0098 (3.2584)	LR 2.000e-03
0: TRAIN [2][1650/1938]	Time 0.093 (0.085)	Data 7.99e-05 (2.39e-04)	Tok/s 89078 (83638)	Loss/tok 3.1583 (3.2583)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.094 (0.085)	Data 8.18e-05 (2.38e-04)	Tok/s 89601 (83637)	Loss/tok 3.2143 (3.2579)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.093 (0.085)	Data 8.37e-05 (2.37e-04)	Tok/s 90089 (83625)	Loss/tok 3.2636 (3.2578)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.043 (0.085)	Data 8.77e-05 (2.36e-04)	Tok/s 61585 (83594)	Loss/tok 2.5307 (3.2573)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.068 (0.085)	Data 8.99e-05 (2.35e-04)	Tok/s 75797 (83549)	Loss/tok 2.9308 (3.2564)	LR 2.000e-03
0: TRAIN [2][1700/1938]	Time 0.120 (0.085)	Data 8.06e-05 (2.34e-04)	Tok/s 93816 (83554)	Loss/tok 3.5397 (3.2563)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1710/1938]	Time 0.120 (0.085)	Data 8.34e-05 (2.33e-04)	Tok/s 95867 (83561)	Loss/tok 3.4788 (3.2563)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][1720/1938]	Time 0.093 (0.085)	Data 8.56e-05 (2.32e-04)	Tok/s 89866 (83571)	Loss/tok 3.1440 (3.2566)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.119 (0.085)	Data 7.99e-05 (2.31e-04)	Tok/s 98643 (83580)	Loss/tok 3.4668 (3.2565)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.093 (0.085)	Data 8.42e-05 (2.31e-04)	Tok/s 90357 (83592)	Loss/tok 3.1905 (3.2566)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.094 (0.085)	Data 9.32e-05 (2.30e-04)	Tok/s 91249 (83590)	Loss/tok 3.3287 (3.2564)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.091 (0.085)	Data 8.11e-05 (2.29e-04)	Tok/s 92067 (83551)	Loss/tok 3.2164 (3.2559)	LR 2.000e-03
0: TRAIN [2][1770/1938]	Time 0.118 (0.085)	Data 8.68e-05 (2.28e-04)	Tok/s 98806 (83558)	Loss/tok 3.4203 (3.2556)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.153 (0.085)	Data 8.06e-05 (2.27e-04)	Tok/s 95457 (83573)	Loss/tok 3.6600 (3.2560)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.093 (0.085)	Data 8.20e-05 (2.27e-04)	Tok/s 87239 (83563)	Loss/tok 3.2854 (3.2558)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.067 (0.085)	Data 8.37e-05 (2.26e-04)	Tok/s 76630 (83534)	Loss/tok 2.9773 (3.2551)	LR 2.000e-03
0: TRAIN [2][1810/1938]	Time 0.093 (0.085)	Data 8.54e-05 (2.25e-04)	Tok/s 91205 (83536)	Loss/tok 3.2369 (3.2549)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.067 (0.085)	Data 7.87e-05 (2.24e-04)	Tok/s 77245 (83528)	Loss/tok 2.9876 (3.2542)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.120 (0.085)	Data 8.70e-05 (2.23e-04)	Tok/s 97644 (83531)	Loss/tok 3.1807 (3.2540)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.067 (0.085)	Data 8.25e-05 (2.23e-04)	Tok/s 76725 (83523)	Loss/tok 3.0261 (3.2539)	LR 2.000e-03
0: TRAIN [2][1850/1938]	Time 0.066 (0.085)	Data 8.37e-05 (2.22e-04)	Tok/s 76763 (83500)	Loss/tok 3.2363 (3.2535)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.095 (0.084)	Data 8.39e-05 (2.21e-04)	Tok/s 89558 (83497)	Loss/tok 3.2450 (3.2529)	LR 2.000e-03
0: TRAIN [2][1870/1938]	Time 0.066 (0.084)	Data 8.20e-05 (2.20e-04)	Tok/s 78001 (83479)	Loss/tok 3.1117 (3.2527)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][1880/1938]	Time 0.093 (0.084)	Data 8.58e-05 (2.20e-04)	Tok/s 90702 (83472)	Loss/tok 3.3415 (3.2528)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.092 (0.084)	Data 8.49e-05 (2.19e-04)	Tok/s 91888 (83461)	Loss/tok 3.2198 (3.2526)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.067 (0.084)	Data 8.06e-05 (2.18e-04)	Tok/s 75402 (83451)	Loss/tok 2.9741 (3.2523)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.151 (0.084)	Data 8.06e-05 (2.18e-04)	Tok/s 99958 (83467)	Loss/tok 3.4640 (3.2522)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.067 (0.084)	Data 8.68e-05 (2.17e-04)	Tok/s 79766 (83472)	Loss/tok 2.9600 (3.2527)	LR 2.000e-03
0: TRAIN [2][1930/1938]	Time 0.092 (0.084)	Data 8.34e-05 (2.16e-04)	Tok/s 90941 (83486)	Loss/tok 3.5361 (3.2535)	LR 2.000e-03
:::MLL 1575547059.173 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1575547059.173 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.399 (0.399)	Decoder iters 100.0 (100.0)	Tok/s 22737 (22737)
0: Running moses detokenizer
0: BLEU(score=22.99670994730711, counts=[36700, 18011, 10056, 5866], totals=[65701, 62698, 59695, 56695], precisions=[55.8591193436934, 28.726594149733643, 16.84563196247592, 10.346591410177265], bp=1.0, sys_len=65701, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1575547060.279 eval_accuracy: {"value": 23.0, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1575547060.279 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.2563	Test BLEU: 23.00
0: Performance: Epoch: 2	Training: 1336171 Tok/s
0: Finished epoch 2
:::MLL 1575547060.280 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1575547060.280 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1575547060.281 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 3384035468
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [3][0/1938]	Time 0.330 (0.330)	Data 2.49e-01 (2.49e-01)	Tok/s 15631 (15631)	Loss/tok 2.8900 (2.8900)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.043 (0.112)	Data 8.73e-05 (2.27e-02)	Tok/s 62244 (78915)	Loss/tok 2.6101 (3.1352)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.154 (0.095)	Data 8.30e-05 (1.20e-02)	Tok/s 95086 (78804)	Loss/tok 3.5818 (3.1182)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.066 (0.094)	Data 8.23e-05 (8.13e-03)	Tok/s 77467 (80914)	Loss/tok 2.8973 (3.1594)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.044 (0.092)	Data 8.92e-05 (6.17e-03)	Tok/s 57124 (81836)	Loss/tok 2.4752 (3.1628)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.093 (0.087)	Data 7.94e-05 (4.97e-03)	Tok/s 88084 (81251)	Loss/tok 3.1910 (3.1489)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.068 (0.085)	Data 8.37e-05 (4.17e-03)	Tok/s 76651 (80805)	Loss/tok 2.8979 (3.1414)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.154 (0.085)	Data 7.99e-05 (3.60e-03)	Tok/s 96815 (81282)	Loss/tok 3.4010 (3.1374)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.094 (0.085)	Data 9.11e-05 (3.16e-03)	Tok/s 90506 (81674)	Loss/tok 3.0228 (3.1395)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.093 (0.086)	Data 7.99e-05 (2.82e-03)	Tok/s 91700 (82403)	Loss/tok 3.2656 (3.1460)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.066 (0.085)	Data 8.51e-05 (2.55e-03)	Tok/s 77090 (82317)	Loss/tok 3.0556 (3.1401)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.095 (0.086)	Data 7.87e-05 (2.33e-03)	Tok/s 87906 (82669)	Loss/tok 3.1381 (3.1426)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.066 (0.086)	Data 9.04e-05 (2.14e-03)	Tok/s 79616 (82795)	Loss/tok 2.8990 (3.1461)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.043 (0.086)	Data 8.75e-05 (1.99e-03)	Tok/s 60221 (83059)	Loss/tok 2.6181 (3.1498)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.093 (0.086)	Data 8.63e-05 (1.85e-03)	Tok/s 90682 (83182)	Loss/tok 3.1399 (3.1516)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.152 (0.087)	Data 8.25e-05 (1.74e-03)	Tok/s 97741 (83595)	Loss/tok 3.4596 (3.1632)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.093 (0.087)	Data 8.51e-05 (1.63e-03)	Tok/s 87124 (83645)	Loss/tok 3.3162 (3.1619)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.120 (0.088)	Data 8.51e-05 (1.54e-03)	Tok/s 96087 (83797)	Loss/tok 3.3425 (3.1718)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.067 (0.087)	Data 8.61e-05 (1.46e-03)	Tok/s 78577 (83570)	Loss/tok 2.9138 (3.1631)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.118 (0.087)	Data 8.58e-05 (1.39e-03)	Tok/s 98196 (83521)	Loss/tok 3.4592 (3.1618)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.093 (0.087)	Data 8.56e-05 (1.33e-03)	Tok/s 88190 (83682)	Loss/tok 3.3467 (3.1618)	LR 2.000e-03
0: TRAIN [3][210/1938]	Time 0.092 (0.086)	Data 8.20e-05 (1.27e-03)	Tok/s 89857 (83611)	Loss/tok 3.2405 (3.1594)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.043 (0.087)	Data 8.49e-05 (1.21e-03)	Tok/s 62772 (83623)	Loss/tok 2.4660 (3.1614)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.067 (0.086)	Data 8.44e-05 (1.16e-03)	Tok/s 74571 (83575)	Loss/tok 3.0845 (3.1606)	LR 2.000e-03
0: TRAIN [3][240/1938]	Time 0.066 (0.087)	Data 8.82e-05 (1.12e-03)	Tok/s 77457 (83801)	Loss/tok 2.9863 (3.1662)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.066 (0.087)	Data 8.58e-05 (1.08e-03)	Tok/s 78057 (83718)	Loss/tok 2.9677 (3.1670)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.119 (0.087)	Data 8.65e-05 (1.04e-03)	Tok/s 99269 (83814)	Loss/tok 3.3486 (3.1686)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.118 (0.087)	Data 9.78e-05 (1.01e-03)	Tok/s 99373 (83865)	Loss/tok 3.2031 (3.1727)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.093 (0.087)	Data 9.47e-05 (9.73e-04)	Tok/s 90503 (83823)	Loss/tok 3.1248 (3.1707)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.093 (0.087)	Data 1.04e-04 (9.42e-04)	Tok/s 90240 (83946)	Loss/tok 3.1502 (3.1692)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.092 (0.087)	Data 8.82e-05 (9.14e-04)	Tok/s 90276 (83866)	Loss/tok 3.3012 (3.1681)	LR 2.000e-03
0: TRAIN [3][310/1938]	Time 0.066 (0.087)	Data 8.56e-05 (8.87e-04)	Tok/s 74397 (83908)	Loss/tok 2.9614 (3.1710)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.092 (0.086)	Data 8.58e-05 (8.62e-04)	Tok/s 90372 (83813)	Loss/tok 3.3813 (3.1695)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][330/1938]	Time 0.066 (0.086)	Data 8.63e-05 (8.39e-04)	Tok/s 77581 (83828)	Loss/tok 3.0645 (3.1700)	LR 2.000e-03
0: TRAIN [3][340/1938]	Time 0.093 (0.086)	Data 8.63e-05 (8.17e-04)	Tok/s 91021 (83763)	Loss/tok 3.1189 (3.1665)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.120 (0.086)	Data 8.63e-05 (7.96e-04)	Tok/s 97442 (83713)	Loss/tok 3.2868 (3.1663)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.092 (0.086)	Data 9.47e-05 (7.76e-04)	Tok/s 89952 (83742)	Loss/tok 3.0860 (3.1655)	LR 2.000e-03
0: TRAIN [3][370/1938]	Time 0.152 (0.086)	Data 8.39e-05 (7.58e-04)	Tok/s 97268 (83654)	Loss/tok 3.5207 (3.1660)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.093 (0.085)	Data 8.49e-05 (7.40e-04)	Tok/s 90323 (83593)	Loss/tok 3.2588 (3.1648)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.092 (0.086)	Data 8.46e-05 (7.23e-04)	Tok/s 90101 (83616)	Loss/tok 3.2544 (3.1682)	LR 2.000e-03
0: TRAIN [3][400/1938]	Time 0.093 (0.086)	Data 8.15e-05 (7.08e-04)	Tok/s 87143 (83650)	Loss/tok 3.1723 (3.1685)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.066 (0.085)	Data 8.32e-05 (6.92e-04)	Tok/s 76171 (83566)	Loss/tok 3.0226 (3.1657)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.065 (0.085)	Data 8.63e-05 (6.78e-04)	Tok/s 78703 (83569)	Loss/tok 3.0367 (3.1647)	LR 2.000e-03
0: TRAIN [3][430/1938]	Time 0.065 (0.085)	Data 8.03e-05 (6.64e-04)	Tok/s 80983 (83543)	Loss/tok 3.0359 (3.1645)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.067 (0.085)	Data 8.46e-05 (6.51e-04)	Tok/s 76119 (83459)	Loss/tok 2.8754 (3.1634)	LR 2.000e-03
0: TRAIN [3][450/1938]	Time 0.120 (0.085)	Data 9.30e-05 (6.39e-04)	Tok/s 97002 (83452)	Loss/tok 3.3356 (3.1646)	LR 2.000e-03
0: TRAIN [3][460/1938]	Time 0.067 (0.085)	Data 7.94e-05 (6.27e-04)	Tok/s 79525 (83497)	Loss/tok 3.0765 (3.1662)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.092 (0.085)	Data 8.80e-05 (6.15e-04)	Tok/s 90312 (83463)	Loss/tok 3.1417 (3.1674)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.067 (0.085)	Data 8.23e-05 (6.04e-04)	Tok/s 77942 (83381)	Loss/tok 3.0568 (3.1650)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.096 (0.085)	Data 8.34e-05 (5.94e-04)	Tok/s 86586 (83499)	Loss/tok 3.1032 (3.1657)	LR 2.000e-03
0: TRAIN [3][500/1938]	Time 0.066 (0.085)	Data 8.18e-05 (5.84e-04)	Tok/s 77209 (83578)	Loss/tok 2.7928 (3.1674)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.119 (0.085)	Data 8.42e-05 (5.74e-04)	Tok/s 96816 (83562)	Loss/tok 3.4100 (3.1669)	LR 2.000e-03
0: TRAIN [3][520/1938]	Time 0.066 (0.085)	Data 8.01e-05 (5.65e-04)	Tok/s 78736 (83506)	Loss/tok 3.0250 (3.1648)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][530/1938]	Time 0.093 (0.085)	Data 8.54e-05 (5.56e-04)	Tok/s 90202 (83578)	Loss/tok 3.3443 (3.1677)	LR 2.000e-03
0: TRAIN [3][540/1938]	Time 0.091 (0.085)	Data 8.68e-05 (5.47e-04)	Tok/s 90681 (83520)	Loss/tok 3.1217 (3.1662)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.119 (0.085)	Data 8.44e-05 (5.39e-04)	Tok/s 96863 (83501)	Loss/tok 3.5127 (3.1680)	LR 2.000e-03
0: TRAIN [3][560/1938]	Time 0.067 (0.085)	Data 9.06e-05 (5.30e-04)	Tok/s 77394 (83389)	Loss/tok 3.0771 (3.1663)	LR 2.000e-03
0: TRAIN [3][570/1938]	Time 0.093 (0.085)	Data 8.68e-05 (5.23e-04)	Tok/s 90206 (83458)	Loss/tok 3.3705 (3.1662)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.066 (0.085)	Data 7.99e-05 (5.15e-04)	Tok/s 77416 (83446)	Loss/tok 2.8791 (3.1640)	LR 2.000e-03
0: TRAIN [3][590/1938]	Time 0.094 (0.085)	Data 8.61e-05 (5.08e-04)	Tok/s 89223 (83487)	Loss/tok 3.1526 (3.1652)	LR 2.000e-03
0: TRAIN [3][600/1938]	Time 0.066 (0.085)	Data 8.44e-05 (5.01e-04)	Tok/s 78666 (83432)	Loss/tok 2.9440 (3.1654)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.067 (0.085)	Data 8.42e-05 (4.94e-04)	Tok/s 77856 (83476)	Loss/tok 2.9814 (3.1639)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.044 (0.084)	Data 8.18e-05 (4.87e-04)	Tok/s 60022 (83408)	Loss/tok 2.7188 (3.1629)	LR 2.000e-03
0: TRAIN [3][630/1938]	Time 0.092 (0.084)	Data 8.51e-05 (4.81e-04)	Tok/s 88793 (83425)	Loss/tok 3.1763 (3.1627)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.043 (0.084)	Data 8.75e-05 (4.75e-04)	Tok/s 61178 (83367)	Loss/tok 2.6468 (3.1615)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.066 (0.084)	Data 8.30e-05 (4.69e-04)	Tok/s 80339 (83435)	Loss/tok 3.1384 (3.1620)	LR 2.000e-03
0: TRAIN [3][660/1938]	Time 0.091 (0.084)	Data 8.27e-05 (4.63e-04)	Tok/s 91433 (83432)	Loss/tok 3.2306 (3.1616)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][670/1938]	Time 0.066 (0.084)	Data 8.49e-05 (4.58e-04)	Tok/s 77946 (83530)	Loss/tok 3.1107 (3.1644)	LR 2.000e-03
0: TRAIN [3][680/1938]	Time 0.067 (0.084)	Data 9.51e-05 (4.52e-04)	Tok/s 76994 (83577)	Loss/tok 3.0221 (3.1648)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.067 (0.084)	Data 8.39e-05 (4.47e-04)	Tok/s 76325 (83608)	Loss/tok 2.9016 (3.1658)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.067 (0.085)	Data 8.77e-05 (4.42e-04)	Tok/s 75991 (83646)	Loss/tok 2.9593 (3.1675)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.095 (0.085)	Data 8.30e-05 (4.37e-04)	Tok/s 88907 (83664)	Loss/tok 3.2007 (3.1683)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.067 (0.084)	Data 9.73e-05 (4.32e-04)	Tok/s 74657 (83630)	Loss/tok 2.8212 (3.1665)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.068 (0.084)	Data 8.01e-05 (4.27e-04)	Tok/s 75895 (83592)	Loss/tok 2.8733 (3.1655)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.119 (0.084)	Data 8.32e-05 (4.23e-04)	Tok/s 97181 (83557)	Loss/tok 3.3010 (3.1654)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.069 (0.084)	Data 8.03e-05 (4.18e-04)	Tok/s 76436 (83565)	Loss/tok 2.9559 (3.1654)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.067 (0.085)	Data 8.54e-05 (4.14e-04)	Tok/s 76122 (83620)	Loss/tok 3.1103 (3.1664)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.119 (0.085)	Data 8.25e-05 (4.10e-04)	Tok/s 98877 (83641)	Loss/tok 3.2884 (3.1669)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.067 (0.084)	Data 1.27e-04 (4.06e-04)	Tok/s 76916 (83576)	Loss/tok 3.0909 (3.1660)	LR 1.000e-03
0: TRAIN [3][790/1938]	Time 0.067 (0.084)	Data 8.39e-05 (4.01e-04)	Tok/s 78599 (83553)	Loss/tok 2.9436 (3.1655)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.068 (0.085)	Data 8.44e-05 (3.98e-04)	Tok/s 75560 (83588)	Loss/tok 3.0727 (3.1674)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.121 (0.085)	Data 8.49e-05 (3.94e-04)	Tok/s 95135 (83630)	Loss/tok 3.4497 (3.1685)	LR 1.000e-03
0: TRAIN [3][820/1938]	Time 0.096 (0.085)	Data 8.37e-05 (3.90e-04)	Tok/s 87938 (83626)	Loss/tok 3.0495 (3.1678)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.093 (0.085)	Data 1.09e-04 (3.86e-04)	Tok/s 91133 (83656)	Loss/tok 3.2142 (3.1681)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.069 (0.085)	Data 8.51e-05 (3.83e-04)	Tok/s 74040 (83628)	Loss/tok 2.8847 (3.1678)	LR 1.000e-03
0: TRAIN [3][850/1938]	Time 0.067 (0.085)	Data 8.44e-05 (3.79e-04)	Tok/s 75146 (83712)	Loss/tok 3.0132 (3.1699)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.121 (0.085)	Data 8.56e-05 (3.76e-04)	Tok/s 94350 (83769)	Loss/tok 3.3632 (3.1716)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.067 (0.085)	Data 8.37e-05 (3.73e-04)	Tok/s 79144 (83744)	Loss/tok 2.9515 (3.1720)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.093 (0.085)	Data 8.37e-05 (3.69e-04)	Tok/s 92092 (83739)	Loss/tok 3.0219 (3.1712)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.066 (0.085)	Data 8.20e-05 (3.66e-04)	Tok/s 77303 (83734)	Loss/tok 2.9743 (3.1718)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.092 (0.085)	Data 8.39e-05 (3.63e-04)	Tok/s 91556 (83766)	Loss/tok 2.9521 (3.1713)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.068 (0.085)	Data 1.10e-04 (3.60e-04)	Tok/s 74213 (83707)	Loss/tok 2.8978 (3.1697)	LR 1.000e-03
0: TRAIN [3][920/1938]	Time 0.152 (0.085)	Data 8.89e-05 (3.57e-04)	Tok/s 97697 (83704)	Loss/tok 3.5357 (3.1698)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.068 (0.085)	Data 8.30e-05 (3.54e-04)	Tok/s 77443 (83701)	Loss/tok 2.8597 (3.1694)	LR 1.000e-03
0: TRAIN [3][940/1938]	Time 0.094 (0.085)	Data 7.96e-05 (3.51e-04)	Tok/s 89113 (83709)	Loss/tok 3.2554 (3.1690)	LR 1.000e-03
0: TRAIN [3][950/1938]	Time 0.068 (0.085)	Data 8.56e-05 (3.48e-04)	Tok/s 78050 (83725)	Loss/tok 2.9380 (3.1697)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][960/1938]	Time 0.093 (0.085)	Data 8.51e-05 (3.46e-04)	Tok/s 89844 (83757)	Loss/tok 3.0209 (3.1692)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [3][970/1938]	Time 0.067 (0.085)	Data 8.56e-05 (3.43e-04)	Tok/s 75134 (83737)	Loss/tok 2.9518 (3.1692)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.043 (0.085)	Data 8.11e-05 (3.40e-04)	Tok/s 61292 (83784)	Loss/tok 2.5107 (3.1699)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.067 (0.085)	Data 8.51e-05 (3.38e-04)	Tok/s 74032 (83739)	Loss/tok 2.9435 (3.1686)	LR 1.000e-03
0: TRAIN [3][1000/1938]	Time 0.069 (0.086)	Data 8.70e-05 (3.35e-04)	Tok/s 77265 (83806)	Loss/tok 3.0022 (3.1702)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.068 (0.085)	Data 8.32e-05 (3.33e-04)	Tok/s 75960 (83796)	Loss/tok 2.9862 (3.1696)	LR 1.000e-03
0: TRAIN [3][1020/1938]	Time 0.067 (0.085)	Data 8.46e-05 (3.30e-04)	Tok/s 76036 (83730)	Loss/tok 2.8590 (3.1692)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.096 (0.085)	Data 8.42e-05 (3.28e-04)	Tok/s 87392 (83746)	Loss/tok 3.1447 (3.1697)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.120 (0.085)	Data 8.08e-05 (3.26e-04)	Tok/s 96984 (83748)	Loss/tok 3.2133 (3.1700)	LR 1.000e-03
0: TRAIN [3][1050/1938]	Time 0.095 (0.085)	Data 8.23e-05 (3.23e-04)	Tok/s 89734 (83735)	Loss/tok 3.1254 (3.1693)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.092 (0.085)	Data 8.61e-05 (3.21e-04)	Tok/s 90349 (83739)	Loss/tok 3.0990 (3.1684)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.067 (0.085)	Data 8.61e-05 (3.19e-04)	Tok/s 77727 (83732)	Loss/tok 3.1383 (3.1675)	LR 1.000e-03
0: TRAIN [3][1080/1938]	Time 0.067 (0.085)	Data 8.37e-05 (3.17e-04)	Tok/s 77651 (83675)	Loss/tok 2.9022 (3.1666)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.118 (0.085)	Data 7.94e-05 (3.15e-04)	Tok/s 99177 (83657)	Loss/tok 3.4409 (3.1660)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.043 (0.085)	Data 1.22e-04 (3.13e-04)	Tok/s 62832 (83632)	Loss/tok 2.6338 (3.1652)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.067 (0.085)	Data 8.80e-05 (3.11e-04)	Tok/s 76962 (83660)	Loss/tok 2.9941 (3.1649)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.067 (0.085)	Data 9.47e-05 (3.09e-04)	Tok/s 78112 (83686)	Loss/tok 2.8252 (3.1650)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.094 (0.085)	Data 8.85e-05 (3.07e-04)	Tok/s 88925 (83705)	Loss/tok 3.1063 (3.1650)	LR 1.000e-03
0: TRAIN [3][1140/1938]	Time 0.121 (0.085)	Data 8.32e-05 (3.05e-04)	Tok/s 97500 (83737)	Loss/tok 3.3138 (3.1653)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.066 (0.086)	Data 8.25e-05 (3.03e-04)	Tok/s 78745 (83778)	Loss/tok 2.9082 (3.1660)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.120 (0.085)	Data 8.32e-05 (3.01e-04)	Tok/s 95971 (83744)	Loss/tok 3.2691 (3.1657)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.068 (0.085)	Data 8.44e-05 (2.99e-04)	Tok/s 76049 (83736)	Loss/tok 3.0117 (3.1658)	LR 1.000e-03
0: TRAIN [3][1180/1938]	Time 0.067 (0.085)	Data 8.44e-05 (2.97e-04)	Tok/s 76869 (83734)	Loss/tok 3.0662 (3.1659)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.094 (0.085)	Data 8.32e-05 (2.95e-04)	Tok/s 88690 (83744)	Loss/tok 3.1793 (3.1654)	LR 1.000e-03
0: TRAIN [3][1200/1938]	Time 0.067 (0.085)	Data 8.30e-05 (2.94e-04)	Tok/s 79110 (83770)	Loss/tok 2.9537 (3.1650)	LR 1.000e-03
0: TRAIN [3][1210/1938]	Time 0.095 (0.085)	Data 8.27e-05 (2.92e-04)	Tok/s 89230 (83742)	Loss/tok 3.0844 (3.1642)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.043 (0.085)	Data 8.27e-05 (2.90e-04)	Tok/s 62583 (83693)	Loss/tok 2.5517 (3.1639)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.067 (0.085)	Data 8.39e-05 (2.89e-04)	Tok/s 77515 (83681)	Loss/tok 2.9221 (3.1634)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.068 (0.085)	Data 8.39e-05 (2.87e-04)	Tok/s 74660 (83677)	Loss/tok 3.0065 (3.1627)	LR 1.000e-03
0: TRAIN [3][1250/1938]	Time 0.095 (0.085)	Data 8.37e-05 (2.85e-04)	Tok/s 87197 (83701)	Loss/tok 3.1934 (3.1625)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.066 (0.085)	Data 1.16e-04 (2.84e-04)	Tok/s 78860 (83658)	Loss/tok 2.9801 (3.1616)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.095 (0.085)	Data 8.80e-05 (2.82e-04)	Tok/s 87815 (83687)	Loss/tok 3.1263 (3.1620)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.067 (0.085)	Data 8.03e-05 (2.81e-04)	Tok/s 77919 (83651)	Loss/tok 2.9889 (3.1612)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.092 (0.085)	Data 7.94e-05 (2.79e-04)	Tok/s 90855 (83636)	Loss/tok 3.1387 (3.1605)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.094 (0.085)	Data 8.46e-05 (2.78e-04)	Tok/s 90174 (83612)	Loss/tok 3.0062 (3.1597)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][1310/1938]	Time 0.152 (0.085)	Data 8.25e-05 (2.76e-04)	Tok/s 98534 (83597)	Loss/tok 3.4485 (3.1598)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.069 (0.085)	Data 8.01e-05 (2.75e-04)	Tok/s 77304 (83580)	Loss/tok 2.8976 (3.1601)	LR 1.000e-03
0: TRAIN [3][1330/1938]	Time 0.095 (0.085)	Data 8.70e-05 (2.73e-04)	Tok/s 89560 (83575)	Loss/tok 2.9324 (3.1595)	LR 1.000e-03
0: TRAIN [3][1340/1938]	Time 0.045 (0.085)	Data 8.46e-05 (2.72e-04)	Tok/s 57661 (83530)	Loss/tok 2.4906 (3.1589)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.067 (0.085)	Data 9.08e-05 (2.71e-04)	Tok/s 77541 (83530)	Loss/tok 2.8809 (3.1590)	LR 1.000e-03
0: TRAIN [3][1360/1938]	Time 0.093 (0.085)	Data 8.92e-05 (2.69e-04)	Tok/s 91477 (83503)	Loss/tok 3.0477 (3.1582)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.067 (0.085)	Data 8.39e-05 (2.68e-04)	Tok/s 78725 (83508)	Loss/tok 2.8520 (3.1577)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.122 (0.085)	Data 8.34e-05 (2.67e-04)	Tok/s 95325 (83521)	Loss/tok 3.3384 (3.1574)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.094 (0.085)	Data 8.08e-05 (2.65e-04)	Tok/s 89216 (83500)	Loss/tok 3.0918 (3.1567)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.152 (0.085)	Data 8.34e-05 (2.64e-04)	Tok/s 98666 (83508)	Loss/tok 3.3690 (3.1569)	LR 1.000e-03
0: TRAIN [3][1410/1938]	Time 0.153 (0.085)	Data 8.30e-05 (2.63e-04)	Tok/s 99103 (83494)	Loss/tok 3.4559 (3.1569)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.093 (0.085)	Data 8.65e-05 (2.62e-04)	Tok/s 89778 (83534)	Loss/tok 3.1957 (3.1578)	LR 1.000e-03
0: TRAIN [3][1430/1938]	Time 0.068 (0.085)	Data 8.65e-05 (2.60e-04)	Tok/s 75104 (83534)	Loss/tok 2.9160 (3.1575)	LR 1.000e-03
0: TRAIN [3][1440/1938]	Time 0.093 (0.085)	Data 8.75e-05 (2.59e-04)	Tok/s 90479 (83546)	Loss/tok 3.1847 (3.1575)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.044 (0.085)	Data 8.06e-05 (2.58e-04)	Tok/s 59718 (83524)	Loss/tok 2.4847 (3.1572)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.093 (0.085)	Data 8.51e-05 (2.57e-04)	Tok/s 90792 (83528)	Loss/tok 3.0772 (3.1570)	LR 1.000e-03
0: TRAIN [3][1470/1938]	Time 0.068 (0.085)	Data 8.58e-05 (2.56e-04)	Tok/s 76971 (83479)	Loss/tok 2.9398 (3.1565)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.044 (0.085)	Data 8.27e-05 (2.55e-04)	Tok/s 59264 (83451)	Loss/tok 2.5069 (3.1561)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][1490/1938]	Time 0.119 (0.085)	Data 8.56e-05 (2.53e-04)	Tok/s 99128 (83445)	Loss/tok 3.3117 (3.1560)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.151 (0.085)	Data 8.06e-05 (2.52e-04)	Tok/s 98549 (83431)	Loss/tok 3.5071 (3.1560)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.093 (0.085)	Data 7.94e-05 (2.51e-04)	Tok/s 90533 (83448)	Loss/tok 2.9579 (3.1558)	LR 5.000e-04
0: TRAIN [3][1520/1938]	Time 0.120 (0.085)	Data 8.44e-05 (2.50e-04)	Tok/s 97352 (83453)	Loss/tok 3.3413 (3.1556)	LR 5.000e-04
0: TRAIN [3][1530/1938]	Time 0.066 (0.085)	Data 8.54e-05 (2.49e-04)	Tok/s 77621 (83442)	Loss/tok 3.0067 (3.1556)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.043 (0.085)	Data 8.49e-05 (2.48e-04)	Tok/s 61803 (83398)	Loss/tok 2.6257 (3.1547)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.066 (0.085)	Data 8.15e-05 (2.47e-04)	Tok/s 77252 (83364)	Loss/tok 2.8866 (3.1538)	LR 5.000e-04
0: TRAIN [3][1560/1938]	Time 0.093 (0.085)	Data 8.32e-05 (2.46e-04)	Tok/s 88582 (83394)	Loss/tok 3.1206 (3.1537)	LR 5.000e-04
0: TRAIN [3][1570/1938]	Time 0.067 (0.085)	Data 8.37e-05 (2.45e-04)	Tok/s 79150 (83408)	Loss/tok 2.8008 (3.1533)	LR 5.000e-04
0: TRAIN [3][1580/1938]	Time 0.066 (0.085)	Data 8.08e-05 (2.44e-04)	Tok/s 78420 (83427)	Loss/tok 2.8506 (3.1533)	LR 5.000e-04
0: TRAIN [3][1590/1938]	Time 0.066 (0.085)	Data 8.54e-05 (2.43e-04)	Tok/s 77689 (83399)	Loss/tok 2.8628 (3.1524)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.069 (0.085)	Data 8.20e-05 (2.42e-04)	Tok/s 75573 (83380)	Loss/tok 3.0716 (3.1518)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.120 (0.085)	Data 8.37e-05 (2.41e-04)	Tok/s 98772 (83359)	Loss/tok 3.1839 (3.1510)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.093 (0.085)	Data 9.44e-05 (2.40e-04)	Tok/s 90384 (83368)	Loss/tok 2.9942 (3.1506)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.152 (0.085)	Data 9.44e-05 (2.39e-04)	Tok/s 97114 (83354)	Loss/tok 3.4256 (3.1509)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.118 (0.085)	Data 8.18e-05 (2.38e-04)	Tok/s 98916 (83346)	Loss/tok 3.3274 (3.1502)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.067 (0.085)	Data 8.42e-05 (2.37e-04)	Tok/s 77131 (83363)	Loss/tok 2.8939 (3.1505)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][1660/1938]	Time 0.068 (0.085)	Data 8.32e-05 (2.36e-04)	Tok/s 75561 (83356)	Loss/tok 2.7873 (3.1505)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.094 (0.085)	Data 8.42e-05 (2.35e-04)	Tok/s 90295 (83370)	Loss/tok 3.1421 (3.1507)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.067 (0.085)	Data 1.21e-04 (2.35e-04)	Tok/s 76762 (83397)	Loss/tok 3.0030 (3.1511)	LR 5.000e-04
0: TRAIN [3][1690/1938]	Time 0.093 (0.085)	Data 8.77e-05 (2.34e-04)	Tok/s 91138 (83382)	Loss/tok 2.9843 (3.1506)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.066 (0.085)	Data 8.11e-05 (2.33e-04)	Tok/s 77712 (83375)	Loss/tok 2.9665 (3.1502)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.094 (0.085)	Data 8.06e-05 (2.32e-04)	Tok/s 90479 (83375)	Loss/tok 3.1209 (3.1503)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.121 (0.085)	Data 8.27e-05 (2.31e-04)	Tok/s 93317 (83343)	Loss/tok 3.4041 (3.1497)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.067 (0.085)	Data 8.49e-05 (2.30e-04)	Tok/s 76284 (83329)	Loss/tok 3.0124 (3.1491)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.067 (0.085)	Data 8.70e-05 (2.29e-04)	Tok/s 77360 (83355)	Loss/tok 2.7874 (3.1494)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.120 (0.085)	Data 8.61e-05 (2.29e-04)	Tok/s 97741 (83370)	Loss/tok 3.2348 (3.1493)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.093 (0.085)	Data 8.32e-05 (2.28e-04)	Tok/s 90074 (83375)	Loss/tok 3.1302 (3.1490)	LR 5.000e-04
0: TRAIN [3][1770/1938]	Time 0.119 (0.085)	Data 8.39e-05 (2.27e-04)	Tok/s 99099 (83381)	Loss/tok 3.3009 (3.1486)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.067 (0.085)	Data 8.06e-05 (2.26e-04)	Tok/s 76523 (83383)	Loss/tok 2.8727 (3.1483)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.067 (0.085)	Data 8.15e-05 (2.25e-04)	Tok/s 76529 (83380)	Loss/tok 2.7915 (3.1478)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.094 (0.085)	Data 8.39e-05 (2.25e-04)	Tok/s 88100 (83384)	Loss/tok 3.0748 (3.1479)	LR 5.000e-04
0: TRAIN [3][1810/1938]	Time 0.067 (0.085)	Data 8.39e-05 (2.24e-04)	Tok/s 76913 (83378)	Loss/tok 2.8669 (3.1476)	LR 5.000e-04
0: TRAIN [3][1820/1938]	Time 0.120 (0.085)	Data 8.27e-05 (2.23e-04)	Tok/s 99097 (83391)	Loss/tok 3.2343 (3.1472)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.093 (0.085)	Data 8.15e-05 (2.22e-04)	Tok/s 88926 (83408)	Loss/tok 3.2160 (3.1479)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.044 (0.085)	Data 8.37e-05 (2.22e-04)	Tok/s 60564 (83427)	Loss/tok 2.6232 (3.1476)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][1850/1938]	Time 0.043 (0.085)	Data 8.49e-05 (2.21e-04)	Tok/s 62501 (83390)	Loss/tok 2.4203 (3.1471)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.094 (0.085)	Data 8.51e-05 (2.20e-04)	Tok/s 91299 (83400)	Loss/tok 2.9660 (3.1470)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.043 (0.085)	Data 8.32e-05 (2.19e-04)	Tok/s 58447 (83391)	Loss/tok 2.5247 (3.1470)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.092 (0.085)	Data 8.25e-05 (2.19e-04)	Tok/s 90858 (83362)	Loss/tok 3.1665 (3.1467)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.093 (0.085)	Data 8.23e-05 (2.18e-04)	Tok/s 90112 (83347)	Loss/tok 3.1511 (3.1462)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.067 (0.085)	Data 8.01e-05 (2.17e-04)	Tok/s 77310 (83348)	Loss/tok 2.9960 (3.1465)	LR 5.000e-04
0: TRAIN [3][1910/1938]	Time 0.092 (0.085)	Data 8.08e-05 (2.17e-04)	Tok/s 90383 (83347)	Loss/tok 3.2040 (3.1461)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.067 (0.085)	Data 8.03e-05 (2.16e-04)	Tok/s 76302 (83354)	Loss/tok 2.8315 (3.1460)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.068 (0.085)	Data 8.32e-05 (2.15e-04)	Tok/s 75263 (83347)	Loss/tok 3.0089 (3.1456)	LR 5.000e-04
:::MLL 1575547224.676 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1575547224.676 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.404 (0.404)	Decoder iters 100.0 (100.0)	Tok/s 22277 (22277)
0: Running moses detokenizer
0: BLEU(score=23.952155232824257, counts=[37279, 18647, 10579, 6249], totals=[65723, 62720, 59717, 56719], precisions=[56.72139129376322, 29.730548469387756, 17.715223470703485, 11.017472099296532], bp=1.0, sys_len=65723, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1575547225.822 eval_accuracy: {"value": 23.95, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1575547225.822 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.1471	Test BLEU: 23.95
0: Performance: Epoch: 3	Training: 1333145 Tok/s
0: Finished epoch 3
:::MLL 1575547225.822 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
:::MLL 1575547225.823 block_start: {"value": null, "metadata": {"first_epoch_num": 5, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1575547225.823 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 514}}
0: Starting epoch 4
0: Executing preallocation
0: Sampler for epoch 4 uses seed 246334159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [4][0/1938]	Time 0.326 (0.326)	Data 2.54e-01 (2.54e-01)	Tok/s 15947 (15947)	Loss/tok 2.8011 (2.8011)	LR 5.000e-04
0: TRAIN [4][10/1938]	Time 0.043 (0.108)	Data 8.77e-05 (2.31e-02)	Tok/s 63377 (78053)	Loss/tok 2.5160 (3.0049)	LR 5.000e-04
0: TRAIN [4][20/1938]	Time 0.043 (0.094)	Data 1.15e-04 (1.22e-02)	Tok/s 62533 (78347)	Loss/tok 2.4761 (2.9743)	LR 5.000e-04
0: TRAIN [4][30/1938]	Time 0.066 (0.090)	Data 8.61e-05 (8.27e-03)	Tok/s 79427 (80329)	Loss/tok 2.8296 (2.9802)	LR 5.000e-04
0: TRAIN [4][40/1938]	Time 0.066 (0.086)	Data 8.61e-05 (6.27e-03)	Tok/s 75568 (80411)	Loss/tok 2.7653 (2.9745)	LR 5.000e-04
0: TRAIN [4][50/1938]	Time 0.067 (0.083)	Data 8.68e-05 (5.06e-03)	Tok/s 76139 (80412)	Loss/tok 2.7839 (2.9689)	LR 5.000e-04
0: TRAIN [4][60/1938]	Time 0.151 (0.084)	Data 8.49e-05 (4.24e-03)	Tok/s 97016 (81296)	Loss/tok 3.3784 (2.9923)	LR 5.000e-04
0: TRAIN [4][70/1938]	Time 0.093 (0.083)	Data 8.70e-05 (3.66e-03)	Tok/s 87959 (80970)	Loss/tok 3.0452 (2.9886)	LR 5.000e-04
0: TRAIN [4][80/1938]	Time 0.067 (0.083)	Data 8.37e-05 (3.22e-03)	Tok/s 76128 (81227)	Loss/tok 2.8533 (2.9892)	LR 5.000e-04
0: TRAIN [4][90/1938]	Time 0.119 (0.083)	Data 9.20e-05 (2.87e-03)	Tok/s 99078 (81638)	Loss/tok 3.2360 (2.9923)	LR 5.000e-04
0: TRAIN [4][100/1938]	Time 0.044 (0.084)	Data 9.51e-05 (2.60e-03)	Tok/s 59164 (81823)	Loss/tok 2.5632 (3.0112)	LR 5.000e-04
0: TRAIN [4][110/1938]	Time 0.120 (0.084)	Data 8.34e-05 (2.37e-03)	Tok/s 97436 (81895)	Loss/tok 3.1689 (3.0078)	LR 5.000e-04
0: TRAIN [4][120/1938]	Time 0.121 (0.085)	Data 8.63e-05 (2.18e-03)	Tok/s 97441 (82686)	Loss/tok 3.1928 (3.0172)	LR 5.000e-04
0: TRAIN [4][130/1938]	Time 0.092 (0.085)	Data 8.37e-05 (2.02e-03)	Tok/s 89180 (82758)	Loss/tok 3.1445 (3.0229)	LR 5.000e-04
0: TRAIN [4][140/1938]	Time 0.093 (0.085)	Data 8.49e-05 (1.89e-03)	Tok/s 89564 (82811)	Loss/tok 3.0518 (3.0276)	LR 5.000e-04
0: TRAIN [4][150/1938]	Time 0.092 (0.086)	Data 8.49e-05 (1.77e-03)	Tok/s 90135 (83144)	Loss/tok 2.9534 (3.0284)	LR 5.000e-04
0: TRAIN [4][160/1938]	Time 0.119 (0.085)	Data 8.58e-05 (1.66e-03)	Tok/s 98053 (83127)	Loss/tok 3.1927 (3.0277)	LR 5.000e-04
0: TRAIN [4][170/1938]	Time 0.067 (0.085)	Data 8.27e-05 (1.57e-03)	Tok/s 77512 (82933)	Loss/tok 2.8671 (3.0246)	LR 5.000e-04
0: TRAIN [4][180/1938]	Time 0.118 (0.085)	Data 8.54e-05 (1.49e-03)	Tok/s 98769 (83204)	Loss/tok 3.2608 (3.0273)	LR 5.000e-04
0: TRAIN [4][190/1938]	Time 0.043 (0.085)	Data 8.25e-05 (1.41e-03)	Tok/s 59304 (83234)	Loss/tok 2.4331 (3.0278)	LR 5.000e-04
0: TRAIN [4][200/1938]	Time 0.154 (0.085)	Data 8.15e-05 (1.35e-03)	Tok/s 95372 (83035)	Loss/tok 3.3879 (3.0303)	LR 5.000e-04
0: TRAIN [4][210/1938]	Time 0.119 (0.085)	Data 8.68e-05 (1.29e-03)	Tok/s 97217 (83196)	Loss/tok 3.1795 (3.0323)	LR 5.000e-04
0: TRAIN [4][220/1938]	Time 0.123 (0.086)	Data 8.61e-05 (1.23e-03)	Tok/s 96360 (83437)	Loss/tok 3.0913 (3.0370)	LR 5.000e-04
0: TRAIN [4][230/1938]	Time 0.066 (0.086)	Data 8.37e-05 (1.18e-03)	Tok/s 77001 (83494)	Loss/tok 2.8617 (3.0387)	LR 5.000e-04
0: TRAIN [4][240/1938]	Time 0.121 (0.086)	Data 8.11e-05 (1.14e-03)	Tok/s 93448 (83396)	Loss/tok 3.4037 (3.0380)	LR 5.000e-04
0: TRAIN [4][250/1938]	Time 0.152 (0.085)	Data 8.65e-05 (1.10e-03)	Tok/s 97021 (83298)	Loss/tok 3.3588 (3.0369)	LR 5.000e-04
0: TRAIN [4][260/1938]	Time 0.044 (0.085)	Data 8.63e-05 (1.06e-03)	Tok/s 59244 (83246)	Loss/tok 2.7683 (3.0344)	LR 5.000e-04
0: TRAIN [4][270/1938]	Time 0.066 (0.085)	Data 8.68e-05 (1.02e-03)	Tok/s 77990 (83120)	Loss/tok 2.8350 (3.0316)	LR 5.000e-04
0: TRAIN [4][280/1938]	Time 0.065 (0.085)	Data 9.51e-05 (9.89e-04)	Tok/s 77946 (83204)	Loss/tok 2.9094 (3.0344)	LR 5.000e-04
0: TRAIN [4][290/1938]	Time 0.091 (0.085)	Data 8.23e-05 (9.58e-04)	Tok/s 92289 (83301)	Loss/tok 2.9106 (3.0332)	LR 5.000e-04
0: TRAIN [4][300/1938]	Time 0.068 (0.085)	Data 8.49e-05 (9.29e-04)	Tok/s 76102 (83471)	Loss/tok 2.8961 (3.0355)	LR 5.000e-04
0: TRAIN [4][310/1938]	Time 0.067 (0.086)	Data 8.30e-05 (9.02e-04)	Tok/s 76892 (83614)	Loss/tok 2.8207 (3.0347)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [4][320/1938]	Time 0.093 (0.085)	Data 9.39e-05 (8.76e-04)	Tok/s 91170 (83550)	Loss/tok 3.0992 (3.0334)	LR 5.000e-04
0: TRAIN [4][330/1938]	Time 0.093 (0.085)	Data 8.51e-05 (8.53e-04)	Tok/s 90553 (83469)	Loss/tok 3.0327 (3.0303)	LR 5.000e-04
0: TRAIN [4][340/1938]	Time 0.066 (0.085)	Data 9.16e-05 (8.30e-04)	Tok/s 78479 (83430)	Loss/tok 2.8450 (3.0293)	LR 5.000e-04
0: TRAIN [4][350/1938]	Time 0.094 (0.085)	Data 7.99e-05 (8.09e-04)	Tok/s 89737 (83307)	Loss/tok 3.0916 (3.0280)	LR 5.000e-04
0: TRAIN [4][360/1938]	Time 0.067 (0.085)	Data 8.06e-05 (7.89e-04)	Tok/s 76479 (83253)	Loss/tok 2.7672 (3.0277)	LR 2.500e-04
0: TRAIN [4][370/1938]	Time 0.119 (0.085)	Data 8.39e-05 (7.70e-04)	Tok/s 99508 (83283)	Loss/tok 3.2192 (3.0289)	LR 2.500e-04
0: TRAIN [4][380/1938]	Time 0.118 (0.084)	Data 8.77e-05 (7.52e-04)	Tok/s 96877 (83161)	Loss/tok 3.3036 (3.0280)	LR 2.500e-04
0: TRAIN [4][390/1938]	Time 0.068 (0.085)	Data 8.44e-05 (7.35e-04)	Tok/s 74078 (83320)	Loss/tok 2.8036 (3.0290)	LR 2.500e-04
0: TRAIN [4][400/1938]	Time 0.066 (0.085)	Data 8.18e-05 (7.19e-04)	Tok/s 80589 (83283)	Loss/tok 2.9141 (3.0285)	LR 2.500e-04
0: TRAIN [4][410/1938]	Time 0.066 (0.084)	Data 8.25e-05 (7.03e-04)	Tok/s 78829 (83233)	Loss/tok 2.9346 (3.0288)	LR 2.500e-04
0: TRAIN [4][420/1938]	Time 0.092 (0.085)	Data 8.77e-05 (6.88e-04)	Tok/s 88895 (83279)	Loss/tok 3.0699 (3.0295)	LR 2.500e-04
0: TRAIN [4][430/1938]	Time 0.043 (0.084)	Data 8.01e-05 (6.74e-04)	Tok/s 61332 (83238)	Loss/tok 2.4455 (3.0282)	LR 2.500e-04
0: TRAIN [4][440/1938]	Time 0.067 (0.084)	Data 8.42e-05 (6.61e-04)	Tok/s 76957 (83192)	Loss/tok 2.7297 (3.0278)	LR 2.500e-04
0: TRAIN [4][450/1938]	Time 0.066 (0.084)	Data 7.82e-05 (6.48e-04)	Tok/s 79155 (83061)	Loss/tok 2.8729 (3.0256)	LR 2.500e-04
0: TRAIN [4][460/1938]	Time 0.095 (0.084)	Data 7.94e-05 (6.36e-04)	Tok/s 89753 (83007)	Loss/tok 2.9431 (3.0235)	LR 2.500e-04
0: TRAIN [4][470/1938]	Time 0.155 (0.084)	Data 8.11e-05 (6.24e-04)	Tok/s 94583 (83107)	Loss/tok 3.5041 (3.0277)	LR 2.500e-04
0: TRAIN [4][480/1938]	Time 0.151 (0.084)	Data 8.06e-05 (6.13e-04)	Tok/s 98465 (83133)	Loss/tok 3.4187 (3.0282)	LR 2.500e-04
0: TRAIN [4][490/1938]	Time 0.067 (0.084)	Data 8.37e-05 (6.02e-04)	Tok/s 78936 (83087)	Loss/tok 2.7953 (3.0262)	LR 2.500e-04
0: TRAIN [4][500/1938]	Time 0.044 (0.084)	Data 8.23e-05 (5.92e-04)	Tok/s 57911 (83095)	Loss/tok 2.5299 (3.0263)	LR 2.500e-04
0: TRAIN [4][510/1938]	Time 0.068 (0.084)	Data 8.54e-05 (5.82e-04)	Tok/s 74389 (83072)	Loss/tok 2.6962 (3.0252)	LR 2.500e-04
0: TRAIN [4][520/1938]	Time 0.066 (0.084)	Data 8.56e-05 (5.72e-04)	Tok/s 80220 (83048)	Loss/tok 2.8753 (3.0242)	LR 2.500e-04
0: TRAIN [4][530/1938]	Time 0.092 (0.084)	Data 8.37e-05 (5.63e-04)	Tok/s 90352 (83088)	Loss/tok 2.9157 (3.0248)	LR 2.500e-04
0: TRAIN [4][540/1938]	Time 0.067 (0.084)	Data 8.63e-05 (5.55e-04)	Tok/s 76971 (83157)	Loss/tok 2.8918 (3.0262)	LR 2.500e-04
0: TRAIN [4][550/1938]	Time 0.066 (0.084)	Data 8.58e-05 (5.46e-04)	Tok/s 79441 (83115)	Loss/tok 2.6910 (3.0246)	LR 2.500e-04
0: TRAIN [4][560/1938]	Time 0.155 (0.084)	Data 8.49e-05 (5.38e-04)	Tok/s 96268 (83070)	Loss/tok 3.3079 (3.0243)	LR 2.500e-04
0: TRAIN [4][570/1938]	Time 0.066 (0.084)	Data 8.18e-05 (5.30e-04)	Tok/s 79925 (83010)	Loss/tok 2.8024 (3.0236)	LR 2.500e-04
0: TRAIN [4][580/1938]	Time 0.094 (0.084)	Data 8.18e-05 (5.22e-04)	Tok/s 88979 (82974)	Loss/tok 2.9849 (3.0231)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][590/1938]	Time 0.066 (0.084)	Data 8.37e-05 (5.15e-04)	Tok/s 78173 (83105)	Loss/tok 2.8255 (3.0261)	LR 2.500e-04
0: TRAIN [4][600/1938]	Time 0.066 (0.084)	Data 8.56e-05 (5.08e-04)	Tok/s 77696 (83007)	Loss/tok 2.7832 (3.0237)	LR 2.500e-04
0: TRAIN [4][610/1938]	Time 0.067 (0.084)	Data 8.61e-05 (5.01e-04)	Tok/s 75865 (83055)	Loss/tok 2.9600 (3.0236)	LR 2.500e-04
0: TRAIN [4][620/1938]	Time 0.066 (0.084)	Data 8.46e-05 (4.94e-04)	Tok/s 75965 (83041)	Loss/tok 2.8693 (3.0242)	LR 2.500e-04
0: TRAIN [4][630/1938]	Time 0.067 (0.084)	Data 8.42e-05 (4.88e-04)	Tok/s 77397 (83066)	Loss/tok 2.8425 (3.0236)	LR 2.500e-04
0: TRAIN [4][640/1938]	Time 0.067 (0.084)	Data 8.73e-05 (4.81e-04)	Tok/s 77967 (83161)	Loss/tok 2.8159 (3.0236)	LR 2.500e-04
0: TRAIN [4][650/1938]	Time 0.092 (0.084)	Data 8.13e-05 (4.75e-04)	Tok/s 92147 (83127)	Loss/tok 2.9935 (3.0223)	LR 2.500e-04
0: TRAIN [4][660/1938]	Time 0.092 (0.084)	Data 8.68e-05 (4.69e-04)	Tok/s 90573 (83150)	Loss/tok 3.0595 (3.0227)	LR 2.500e-04
0: TRAIN [4][670/1938]	Time 0.067 (0.084)	Data 8.08e-05 (4.64e-04)	Tok/s 73730 (83088)	Loss/tok 2.9110 (3.0240)	LR 2.500e-04
0: TRAIN [4][680/1938]	Time 0.065 (0.083)	Data 8.92e-05 (4.58e-04)	Tok/s 75950 (82960)	Loss/tok 2.7396 (3.0225)	LR 2.500e-04
0: TRAIN [4][690/1938]	Time 0.067 (0.083)	Data 1.01e-04 (4.53e-04)	Tok/s 77831 (82970)	Loss/tok 2.7379 (3.0222)	LR 2.500e-04
0: TRAIN [4][700/1938]	Time 0.152 (0.083)	Data 8.25e-05 (4.48e-04)	Tok/s 97138 (82988)	Loss/tok 3.4347 (3.0221)	LR 2.500e-04
0: TRAIN [4][710/1938]	Time 0.093 (0.084)	Data 8.42e-05 (4.43e-04)	Tok/s 89905 (83055)	Loss/tok 2.9762 (3.0246)	LR 2.500e-04
0: TRAIN [4][720/1938]	Time 0.066 (0.084)	Data 8.15e-05 (4.38e-04)	Tok/s 76842 (83047)	Loss/tok 2.7543 (3.0237)	LR 2.500e-04
0: TRAIN [4][730/1938]	Time 0.092 (0.084)	Data 8.42e-05 (4.33e-04)	Tok/s 91465 (83059)	Loss/tok 3.0139 (3.0233)	LR 2.500e-04
0: TRAIN [4][740/1938]	Time 0.067 (0.084)	Data 8.70e-05 (4.28e-04)	Tok/s 78009 (83119)	Loss/tok 2.8516 (3.0242)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][750/1938]	Time 0.066 (0.084)	Data 8.85e-05 (4.24e-04)	Tok/s 76459 (83156)	Loss/tok 2.8424 (3.0261)	LR 2.500e-04
0: TRAIN [4][760/1938]	Time 0.068 (0.084)	Data 8.92e-05 (4.19e-04)	Tok/s 73745 (83150)	Loss/tok 2.7696 (3.0259)	LR 2.500e-04
0: TRAIN [4][770/1938]	Time 0.154 (0.084)	Data 8.85e-05 (4.15e-04)	Tok/s 96364 (83228)	Loss/tok 3.3889 (3.0279)	LR 2.500e-04
0: TRAIN [4][780/1938]	Time 0.152 (0.084)	Data 9.20e-05 (4.11e-04)	Tok/s 97701 (83236)	Loss/tok 3.3121 (3.0279)	LR 2.500e-04
0: TRAIN [4][790/1938]	Time 0.067 (0.084)	Data 8.80e-05 (4.06e-04)	Tok/s 77253 (83216)	Loss/tok 2.8711 (3.0274)	LR 2.500e-04
0: TRAIN [4][800/1938]	Time 0.044 (0.084)	Data 9.54e-05 (4.03e-04)	Tok/s 59884 (83261)	Loss/tok 2.5562 (3.0294)	LR 2.500e-04
0: TRAIN [4][810/1938]	Time 0.067 (0.084)	Data 8.49e-05 (3.99e-04)	Tok/s 77418 (83280)	Loss/tok 2.8807 (3.0289)	LR 2.500e-04
0: TRAIN [4][820/1938]	Time 0.067 (0.084)	Data 9.35e-05 (3.95e-04)	Tok/s 77302 (83246)	Loss/tok 2.9096 (3.0279)	LR 2.500e-04
0: TRAIN [4][830/1938]	Time 0.119 (0.084)	Data 8.25e-05 (3.91e-04)	Tok/s 97815 (83211)	Loss/tok 3.1525 (3.0271)	LR 2.500e-04
0: TRAIN [4][840/1938]	Time 0.067 (0.084)	Data 8.70e-05 (3.87e-04)	Tok/s 77159 (83249)	Loss/tok 2.7549 (3.0279)	LR 2.500e-04
0: TRAIN [4][850/1938]	Time 0.068 (0.084)	Data 8.18e-05 (3.84e-04)	Tok/s 76174 (83269)	Loss/tok 2.8755 (3.0285)	LR 2.500e-04
0: TRAIN [4][860/1938]	Time 0.066 (0.084)	Data 8.44e-05 (3.80e-04)	Tok/s 79870 (83200)	Loss/tok 2.8412 (3.0270)	LR 2.500e-04
0: TRAIN [4][870/1938]	Time 0.094 (0.084)	Data 9.94e-05 (3.77e-04)	Tok/s 89528 (83222)	Loss/tok 2.9401 (3.0269)	LR 2.500e-04
0: TRAIN [4][880/1938]	Time 0.093 (0.084)	Data 8.51e-05 (3.74e-04)	Tok/s 89147 (83270)	Loss/tok 3.1656 (3.0289)	LR 2.500e-04
0: TRAIN [4][890/1938]	Time 0.093 (0.084)	Data 8.37e-05 (3.70e-04)	Tok/s 92347 (83308)	Loss/tok 2.9804 (3.0290)	LR 2.500e-04
0: TRAIN [4][900/1938]	Time 0.093 (0.084)	Data 8.63e-05 (3.67e-04)	Tok/s 91049 (83352)	Loss/tok 2.9307 (3.0300)	LR 2.500e-04
0: TRAIN [4][910/1938]	Time 0.067 (0.084)	Data 9.68e-05 (3.64e-04)	Tok/s 74680 (83355)	Loss/tok 2.7763 (3.0296)	LR 2.500e-04
0: TRAIN [4][920/1938]	Time 0.092 (0.084)	Data 8.39e-05 (3.61e-04)	Tok/s 90633 (83325)	Loss/tok 3.0319 (3.0288)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][930/1938]	Time 0.068 (0.084)	Data 9.73e-05 (3.58e-04)	Tok/s 77725 (83349)	Loss/tok 2.7653 (3.0284)	LR 2.500e-04
0: TRAIN [4][940/1938]	Time 0.152 (0.084)	Data 8.37e-05 (3.55e-04)	Tok/s 98133 (83297)	Loss/tok 3.5716 (3.0284)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [4][950/1938]	Time 0.068 (0.084)	Data 8.37e-05 (3.53e-04)	Tok/s 76121 (83310)	Loss/tok 3.0037 (3.0291)	LR 2.500e-04
0: TRAIN [4][960/1938]	Time 0.094 (0.084)	Data 9.23e-05 (3.50e-04)	Tok/s 88499 (83340)	Loss/tok 2.9382 (3.0286)	LR 2.500e-04
0: TRAIN [4][970/1938]	Time 0.119 (0.084)	Data 8.39e-05 (3.47e-04)	Tok/s 97803 (83347)	Loss/tok 3.3354 (3.0288)	LR 2.500e-04
0: TRAIN [4][980/1938]	Time 0.043 (0.084)	Data 9.56e-05 (3.44e-04)	Tok/s 61420 (83229)	Loss/tok 2.4658 (3.0274)	LR 2.500e-04
0: TRAIN [4][990/1938]	Time 0.067 (0.084)	Data 8.34e-05 (3.42e-04)	Tok/s 75681 (83253)	Loss/tok 2.9137 (3.0280)	LR 2.500e-04
0: TRAIN [4][1000/1938]	Time 0.093 (0.084)	Data 8.18e-05 (3.39e-04)	Tok/s 90491 (83264)	Loss/tok 2.9301 (3.0274)	LR 2.500e-04
0: TRAIN [4][1010/1938]	Time 0.044 (0.084)	Data 8.23e-05 (3.37e-04)	Tok/s 60103 (83227)	Loss/tok 2.5601 (3.0275)	LR 2.500e-04
0: TRAIN [4][1020/1938]	Time 0.066 (0.084)	Data 9.61e-05 (3.34e-04)	Tok/s 79121 (83206)	Loss/tok 2.8101 (3.0263)	LR 2.500e-04
0: TRAIN [4][1030/1938]	Time 0.068 (0.084)	Data 8.25e-05 (3.32e-04)	Tok/s 74421 (83179)	Loss/tok 2.9019 (3.0257)	LR 2.500e-04
0: TRAIN [4][1040/1938]	Time 0.121 (0.084)	Data 9.06e-05 (3.29e-04)	Tok/s 97014 (83268)	Loss/tok 3.2492 (3.0272)	LR 2.500e-04
0: TRAIN [4][1050/1938]	Time 0.043 (0.084)	Data 8.18e-05 (3.27e-04)	Tok/s 61115 (83261)	Loss/tok 2.5019 (3.0264)	LR 2.500e-04
0: TRAIN [4][1060/1938]	Time 0.154 (0.084)	Data 9.06e-05 (3.25e-04)	Tok/s 96048 (83249)	Loss/tok 3.3530 (3.0272)	LR 2.500e-04
0: TRAIN [4][1070/1938]	Time 0.093 (0.084)	Data 8.56e-05 (3.23e-04)	Tok/s 91203 (83253)	Loss/tok 2.9634 (3.0269)	LR 2.500e-04
0: TRAIN [4][1080/1938]	Time 0.119 (0.084)	Data 9.16e-05 (3.20e-04)	Tok/s 98517 (83254)	Loss/tok 3.2780 (3.0264)	LR 2.500e-04
0: TRAIN [4][1090/1938]	Time 0.044 (0.084)	Data 8.49e-05 (3.18e-04)	Tok/s 57883 (83237)	Loss/tok 2.3764 (3.0272)	LR 2.500e-04
0: TRAIN [4][1100/1938]	Time 0.067 (0.084)	Data 8.54e-05 (3.16e-04)	Tok/s 79160 (83230)	Loss/tok 2.9042 (3.0275)	LR 2.500e-04
0: TRAIN [4][1110/1938]	Time 0.094 (0.084)	Data 8.49e-05 (3.14e-04)	Tok/s 88347 (83226)	Loss/tok 3.2266 (3.0272)	LR 2.500e-04
0: TRAIN [4][1120/1938]	Time 0.067 (0.084)	Data 8.39e-05 (3.12e-04)	Tok/s 78260 (83203)	Loss/tok 2.7908 (3.0264)	LR 2.500e-04
0: TRAIN [4][1130/1938]	Time 0.119 (0.084)	Data 8.18e-05 (3.10e-04)	Tok/s 100003 (83205)	Loss/tok 3.0359 (3.0260)	LR 2.500e-04
0: TRAIN [4][1140/1938]	Time 0.044 (0.084)	Data 8.37e-05 (3.08e-04)	Tok/s 58466 (83208)	Loss/tok 2.5186 (3.0256)	LR 2.500e-04
0: TRAIN [4][1150/1938]	Time 0.094 (0.084)	Data 8.34e-05 (3.06e-04)	Tok/s 89829 (83236)	Loss/tok 2.9739 (3.0264)	LR 2.500e-04
0: TRAIN [4][1160/1938]	Time 0.067 (0.084)	Data 8.37e-05 (3.04e-04)	Tok/s 77846 (83227)	Loss/tok 2.7138 (3.0256)	LR 2.500e-04
0: TRAIN [4][1170/1938]	Time 0.121 (0.084)	Data 8.63e-05 (3.02e-04)	Tok/s 97390 (83237)	Loss/tok 3.2162 (3.0254)	LR 2.500e-04
0: TRAIN [4][1180/1938]	Time 0.120 (0.084)	Data 8.87e-05 (3.00e-04)	Tok/s 98218 (83245)	Loss/tok 3.2332 (3.0258)	LR 1.250e-04
0: TRAIN [4][1190/1938]	Time 0.093 (0.084)	Data 8.63e-05 (2.99e-04)	Tok/s 89355 (83247)	Loss/tok 3.0026 (3.0261)	LR 1.250e-04
0: TRAIN [4][1200/1938]	Time 0.066 (0.084)	Data 8.13e-05 (2.97e-04)	Tok/s 78662 (83217)	Loss/tok 2.8792 (3.0258)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][1210/1938]	Time 0.090 (0.084)	Data 1.04e-04 (2.95e-04)	Tok/s 93045 (83203)	Loss/tok 3.0693 (3.0256)	LR 1.250e-04
0: TRAIN [4][1220/1938]	Time 0.120 (0.084)	Data 8.63e-05 (2.93e-04)	Tok/s 96348 (83205)	Loss/tok 3.2717 (3.0266)	LR 1.250e-04
0: TRAIN [4][1230/1938]	Time 0.121 (0.084)	Data 8.65e-05 (2.92e-04)	Tok/s 97924 (83244)	Loss/tok 3.0777 (3.0283)	LR 1.250e-04
0: TRAIN [4][1240/1938]	Time 0.043 (0.084)	Data 8.46e-05 (2.90e-04)	Tok/s 60895 (83256)	Loss/tok 2.4719 (3.0286)	LR 1.250e-04
0: TRAIN [4][1250/1938]	Time 0.066 (0.084)	Data 8.30e-05 (2.88e-04)	Tok/s 78318 (83244)	Loss/tok 2.8132 (3.0278)	LR 1.250e-04
0: TRAIN [4][1260/1938]	Time 0.067 (0.084)	Data 8.13e-05 (2.87e-04)	Tok/s 76041 (83260)	Loss/tok 2.8713 (3.0288)	LR 1.250e-04
0: TRAIN [4][1270/1938]	Time 0.067 (0.084)	Data 8.44e-05 (2.85e-04)	Tok/s 78124 (83264)	Loss/tok 2.8849 (3.0287)	LR 1.250e-04
0: TRAIN [4][1280/1938]	Time 0.068 (0.084)	Data 1.04e-04 (2.84e-04)	Tok/s 76717 (83295)	Loss/tok 2.9127 (3.0292)	LR 1.250e-04
0: TRAIN [4][1290/1938]	Time 0.066 (0.084)	Data 9.61e-05 (2.82e-04)	Tok/s 79036 (83266)	Loss/tok 2.7607 (3.0287)	LR 1.250e-04
0: TRAIN [4][1300/1938]	Time 0.067 (0.084)	Data 9.16e-05 (2.81e-04)	Tok/s 78680 (83301)	Loss/tok 2.9153 (3.0293)	LR 1.250e-04
0: TRAIN [4][1310/1938]	Time 0.095 (0.084)	Data 8.80e-05 (2.79e-04)	Tok/s 88077 (83370)	Loss/tok 3.0800 (3.0313)	LR 1.250e-04
0: TRAIN [4][1320/1938]	Time 0.092 (0.085)	Data 8.27e-05 (2.78e-04)	Tok/s 90403 (83415)	Loss/tok 3.0566 (3.0323)	LR 1.250e-04
0: TRAIN [4][1330/1938]	Time 0.067 (0.085)	Data 8.25e-05 (2.76e-04)	Tok/s 79706 (83436)	Loss/tok 3.0191 (3.0318)	LR 1.250e-04
0: TRAIN [4][1340/1938]	Time 0.066 (0.084)	Data 7.92e-05 (2.75e-04)	Tok/s 77717 (83384)	Loss/tok 2.8425 (3.0312)	LR 1.250e-04
0: TRAIN [4][1350/1938]	Time 0.066 (0.084)	Data 8.34e-05 (2.73e-04)	Tok/s 77307 (83357)	Loss/tok 2.8735 (3.0308)	LR 1.250e-04
0: TRAIN [4][1360/1938]	Time 0.066 (0.084)	Data 7.99e-05 (2.72e-04)	Tok/s 77207 (83335)	Loss/tok 2.7973 (3.0303)	LR 1.250e-04
0: TRAIN [4][1370/1938]	Time 0.067 (0.084)	Data 9.20e-05 (2.71e-04)	Tok/s 78345 (83358)	Loss/tok 2.8945 (3.0300)	LR 1.250e-04
0: TRAIN [4][1380/1938]	Time 0.067 (0.084)	Data 8.46e-05 (2.69e-04)	Tok/s 79400 (83368)	Loss/tok 2.7790 (3.0299)	LR 1.250e-04
0: TRAIN [4][1390/1938]	Time 0.094 (0.084)	Data 8.39e-05 (2.68e-04)	Tok/s 89893 (83372)	Loss/tok 2.8840 (3.0305)	LR 1.250e-04
0: TRAIN [4][1400/1938]	Time 0.066 (0.084)	Data 8.03e-05 (2.67e-04)	Tok/s 77722 (83357)	Loss/tok 2.8787 (3.0301)	LR 1.250e-04
0: TRAIN [4][1410/1938]	Time 0.152 (0.084)	Data 9.78e-05 (2.65e-04)	Tok/s 96152 (83387)	Loss/tok 3.5564 (3.0311)	LR 1.250e-04
0: TRAIN [4][1420/1938]	Time 0.067 (0.084)	Data 8.73e-05 (2.64e-04)	Tok/s 78548 (83350)	Loss/tok 2.8958 (3.0307)	LR 1.250e-04
0: TRAIN [4][1430/1938]	Time 0.119 (0.084)	Data 8.61e-05 (2.63e-04)	Tok/s 100359 (83365)	Loss/tok 3.1084 (3.0306)	LR 1.250e-04
0: TRAIN [4][1440/1938]	Time 0.118 (0.084)	Data 1.03e-04 (2.62e-04)	Tok/s 99361 (83363)	Loss/tok 3.1160 (3.0299)	LR 1.250e-04
0: TRAIN [4][1450/1938]	Time 0.066 (0.084)	Data 8.54e-05 (2.60e-04)	Tok/s 76684 (83326)	Loss/tok 2.8492 (3.0294)	LR 1.250e-04
0: TRAIN [4][1460/1938]	Time 0.093 (0.084)	Data 8.06e-05 (2.59e-04)	Tok/s 90633 (83362)	Loss/tok 2.8220 (3.0302)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][1470/1938]	Time 0.064 (0.084)	Data 1.02e-04 (2.58e-04)	Tok/s 79965 (83386)	Loss/tok 2.8754 (3.0307)	LR 1.250e-04
0: TRAIN [4][1480/1938]	Time 0.045 (0.084)	Data 8.94e-05 (2.57e-04)	Tok/s 58598 (83312)	Loss/tok 2.4309 (3.0301)	LR 1.250e-04
0: TRAIN [4][1490/1938]	Time 0.121 (0.084)	Data 8.65e-05 (2.56e-04)	Tok/s 95147 (83307)	Loss/tok 3.3054 (3.0301)	LR 1.250e-04
0: TRAIN [4][1500/1938]	Time 0.094 (0.084)	Data 8.23e-05 (2.55e-04)	Tok/s 88771 (83332)	Loss/tok 3.0123 (3.0312)	LR 1.250e-04
0: TRAIN [4][1510/1938]	Time 0.067 (0.084)	Data 8.63e-05 (2.54e-04)	Tok/s 78064 (83341)	Loss/tok 2.7887 (3.0314)	LR 1.250e-04
0: TRAIN [4][1520/1938]	Time 0.093 (0.084)	Data 8.20e-05 (2.52e-04)	Tok/s 90049 (83359)	Loss/tok 3.0845 (3.0314)	LR 1.250e-04
0: TRAIN [4][1530/1938]	Time 0.092 (0.084)	Data 8.25e-05 (2.51e-04)	Tok/s 90877 (83342)	Loss/tok 3.1149 (3.0310)	LR 1.250e-04
0: TRAIN [4][1540/1938]	Time 0.066 (0.084)	Data 8.08e-05 (2.50e-04)	Tok/s 77291 (83339)	Loss/tok 2.7117 (3.0317)	LR 1.250e-04
0: TRAIN [4][1550/1938]	Time 0.066 (0.084)	Data 8.06e-05 (2.49e-04)	Tok/s 78697 (83334)	Loss/tok 2.8485 (3.0315)	LR 1.250e-04
0: TRAIN [4][1560/1938]	Time 0.152 (0.084)	Data 8.65e-05 (2.48e-04)	Tok/s 96975 (83361)	Loss/tok 3.3346 (3.0323)	LR 1.250e-04
0: TRAIN [4][1570/1938]	Time 0.119 (0.084)	Data 7.99e-05 (2.47e-04)	Tok/s 98785 (83384)	Loss/tok 3.2105 (3.0324)	LR 1.250e-04
0: TRAIN [4][1580/1938]	Time 0.066 (0.084)	Data 8.23e-05 (2.46e-04)	Tok/s 75953 (83386)	Loss/tok 2.8370 (3.0323)	LR 1.250e-04
0: TRAIN [4][1590/1938]	Time 0.067 (0.084)	Data 8.51e-05 (2.45e-04)	Tok/s 77500 (83380)	Loss/tok 2.8955 (3.0316)	LR 1.250e-04
0: TRAIN [4][1600/1938]	Time 0.066 (0.084)	Data 9.27e-05 (2.44e-04)	Tok/s 75864 (83382)	Loss/tok 2.9141 (3.0323)	LR 1.250e-04
0: TRAIN [4][1610/1938]	Time 0.067 (0.084)	Data 8.34e-05 (2.43e-04)	Tok/s 78235 (83352)	Loss/tok 2.7394 (3.0316)	LR 1.250e-04
0: TRAIN [4][1620/1938]	Time 0.067 (0.084)	Data 8.11e-05 (2.42e-04)	Tok/s 76298 (83364)	Loss/tok 2.8527 (3.0321)	LR 1.250e-04
0: TRAIN [4][1630/1938]	Time 0.093 (0.084)	Data 8.96e-05 (2.41e-04)	Tok/s 88793 (83375)	Loss/tok 3.0296 (3.0322)	LR 1.250e-04
0: TRAIN [4][1640/1938]	Time 0.094 (0.084)	Data 8.25e-05 (2.40e-04)	Tok/s 89302 (83402)	Loss/tok 3.0404 (3.0324)	LR 1.250e-04
0: TRAIN [4][1650/1938]	Time 0.066 (0.084)	Data 8.13e-05 (2.39e-04)	Tok/s 78010 (83389)	Loss/tok 2.6570 (3.0320)	LR 1.250e-04
0: TRAIN [4][1660/1938]	Time 0.155 (0.084)	Data 8.49e-05 (2.38e-04)	Tok/s 95692 (83402)	Loss/tok 3.2877 (3.0324)	LR 1.250e-04
0: TRAIN [4][1670/1938]	Time 0.067 (0.084)	Data 9.23e-05 (2.37e-04)	Tok/s 76602 (83397)	Loss/tok 2.9207 (3.0320)	LR 1.250e-04
0: TRAIN [4][1680/1938]	Time 0.118 (0.084)	Data 8.18e-05 (2.36e-04)	Tok/s 96185 (83401)	Loss/tok 3.4391 (3.0322)	LR 1.250e-04
0: TRAIN [4][1690/1938]	Time 0.067 (0.084)	Data 8.75e-05 (2.36e-04)	Tok/s 78740 (83411)	Loss/tok 2.8328 (3.0322)	LR 1.250e-04
0: TRAIN [4][1700/1938]	Time 0.067 (0.084)	Data 8.51e-05 (2.35e-04)	Tok/s 78737 (83412)	Loss/tok 2.8538 (3.0317)	LR 1.250e-04
0: TRAIN [4][1710/1938]	Time 0.093 (0.085)	Data 8.46e-05 (2.34e-04)	Tok/s 89617 (83452)	Loss/tok 2.9045 (3.0332)	LR 1.250e-04
0: TRAIN [4][1720/1938]	Time 0.120 (0.085)	Data 8.30e-05 (2.33e-04)	Tok/s 96864 (83470)	Loss/tok 3.0913 (3.0333)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][1730/1938]	Time 0.067 (0.085)	Data 8.42e-05 (2.32e-04)	Tok/s 75133 (83473)	Loss/tok 2.9465 (3.0334)	LR 1.250e-04
0: TRAIN [4][1740/1938]	Time 0.121 (0.085)	Data 1.02e-04 (2.31e-04)	Tok/s 95054 (83486)	Loss/tok 3.1316 (3.0334)	LR 1.250e-04
0: TRAIN [4][1750/1938]	Time 0.044 (0.085)	Data 8.56e-05 (2.30e-04)	Tok/s 61064 (83483)	Loss/tok 2.3703 (3.0330)	LR 1.250e-04
0: TRAIN [4][1760/1938]	Time 0.092 (0.085)	Data 8.11e-05 (2.30e-04)	Tok/s 90559 (83485)	Loss/tok 3.0516 (3.0328)	LR 1.250e-04
0: TRAIN [4][1770/1938]	Time 0.093 (0.085)	Data 8.51e-05 (2.29e-04)	Tok/s 90663 (83506)	Loss/tok 2.8580 (3.0327)	LR 1.250e-04
0: TRAIN [4][1780/1938]	Time 0.067 (0.085)	Data 8.13e-05 (2.28e-04)	Tok/s 75382 (83500)	Loss/tok 2.7985 (3.0323)	LR 1.250e-04
0: TRAIN [4][1790/1938]	Time 0.044 (0.085)	Data 8.87e-05 (2.27e-04)	Tok/s 59675 (83484)	Loss/tok 2.3491 (3.0322)	LR 1.250e-04
0: TRAIN [4][1800/1938]	Time 0.092 (0.085)	Data 8.11e-05 (2.26e-04)	Tok/s 91297 (83464)	Loss/tok 2.9925 (3.0318)	LR 1.250e-04
0: TRAIN [4][1810/1938]	Time 0.067 (0.085)	Data 9.63e-05 (2.26e-04)	Tok/s 78912 (83450)	Loss/tok 2.8027 (3.0316)	LR 1.250e-04
0: TRAIN [4][1820/1938]	Time 0.045 (0.085)	Data 8.32e-05 (2.25e-04)	Tok/s 58356 (83475)	Loss/tok 2.3952 (3.0319)	LR 1.250e-04
0: TRAIN [4][1830/1938]	Time 0.043 (0.085)	Data 8.46e-05 (2.24e-04)	Tok/s 62037 (83449)	Loss/tok 2.4767 (3.0318)	LR 1.250e-04
0: TRAIN [4][1840/1938]	Time 0.093 (0.085)	Data 8.39e-05 (2.23e-04)	Tok/s 90660 (83434)	Loss/tok 3.0491 (3.0312)	LR 1.250e-04
0: TRAIN [4][1850/1938]	Time 0.068 (0.085)	Data 8.39e-05 (2.23e-04)	Tok/s 76108 (83431)	Loss/tok 2.7783 (3.0315)	LR 1.250e-04
0: TRAIN [4][1860/1938]	Time 0.093 (0.085)	Data 8.25e-05 (2.22e-04)	Tok/s 90447 (83425)	Loss/tok 3.0626 (3.0312)	LR 1.250e-04
0: TRAIN [4][1870/1938]	Time 0.044 (0.085)	Data 8.23e-05 (2.21e-04)	Tok/s 59748 (83414)	Loss/tok 2.3564 (3.0310)	LR 1.250e-04
0: TRAIN [4][1880/1938]	Time 0.067 (0.085)	Data 8.77e-05 (2.21e-04)	Tok/s 75981 (83406)	Loss/tok 2.7912 (3.0314)	LR 1.250e-04
0: TRAIN [4][1890/1938]	Time 0.067 (0.085)	Data 8.37e-05 (2.20e-04)	Tok/s 77590 (83410)	Loss/tok 2.9235 (3.0311)	LR 1.250e-04
0: TRAIN [4][1900/1938]	Time 0.095 (0.085)	Data 8.68e-05 (2.19e-04)	Tok/s 88652 (83433)	Loss/tok 2.9891 (3.0314)	LR 1.250e-04
0: TRAIN [4][1910/1938]	Time 0.093 (0.085)	Data 8.73e-05 (2.18e-04)	Tok/s 91767 (83450)	Loss/tok 3.0425 (3.0314)	LR 1.250e-04
0: TRAIN [4][1920/1938]	Time 0.067 (0.085)	Data 8.54e-05 (2.18e-04)	Tok/s 76971 (83424)	Loss/tok 2.9539 (3.0311)	LR 1.250e-04
0: TRAIN [4][1930/1938]	Time 0.067 (0.085)	Data 9.63e-05 (2.17e-04)	Tok/s 79581 (83403)	Loss/tok 2.7625 (3.0308)	LR 1.250e-04
:::MLL 1575547390.145 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 524}}
:::MLL 1575547390.145 eval_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [4][0/3]	Time 0.392 (0.392)	Decoder iters 95.0 (95.0)	Tok/s 22537 (22537)
0: Running moses detokenizer
0: BLEU(score=24.348651923520343, counts=[37140, 18698, 10713, 6411], totals=[65290, 62287, 59284, 56285], precisions=[56.88466840251187, 30.019105110215616, 18.070643006544767, 11.390246069112552], bp=1.0, sys_len=65290, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1575547391.241 eval_accuracy: {"value": 24.35, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 535}}
:::MLL 1575547391.242 eval_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 4	Training Loss: 3.0296	Test BLEU: 24.35
0: Performance: Epoch: 4	Training: 1334538 Tok/s
0: Finished epoch 4
:::MLL 1575547391.242 block_stop: {"value": null, "metadata": {"first_epoch_num": 5, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1575547391.243 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-12-05 12:03:18 PM
RESULT,RNN_TRANSLATOR,,860,nvidia,2019-12-05 11:48:58 AM

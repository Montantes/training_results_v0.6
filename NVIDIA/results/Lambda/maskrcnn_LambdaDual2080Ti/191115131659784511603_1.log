Beginning trial 1 of 1
Gathering sys log on lambda-quad
:::MLL 1573852656.190 submission_benchmark: {"value": "maskrcnn", "metadata": {"file": "mlperf_logger.py", "lineno": 213}}
:::MLL 1573852656.192 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_logger.py", "lineno": 218}}
WARNING: Log validation: Key "submission_division" is not in known maskrcnn keys.
:::MLL 1573852656.193 submission_division: {"value": "closed", "metadata": {"file": "mlperf_logger.py", "lineno": 222}}
:::MLL 1573852656.194 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_logger.py", "lineno": 226}}
:::MLL 1573852656.195 submission_platform: {"value": "1xSystem Product Name", "metadata": {"file": "mlperf_logger.py", "lineno": 230}}
:::MLL 1573852656.196 submission_entry: {"value": "{'hardware': 'System Product Name', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': ' ', 'os': 'Ubuntu 18.04.3 LTS / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.7-1.0.0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '1x Intel(R) Core(TM) i9-9820X CPU @ 3.30GHz', 'num_cores': '10', 'num_vcpus': '20', 'accelerator': 'GeForce RTX 2080 Ti', 'num_accelerators': '4', 'sys_mem_size': '125 GB', 'sys_storage_type': '<unknown bus> SSD', 'sys_storage_size': '2x 54.5M + 1x 1.8T + 2x 14.8M + 2x 140.7M + 1x 156M + 2x 3.7M + 1x 2.3M + 1x 34.6M + 1x 14.5M + 1x 956K + 1x 44.2M + 1x 4.2M + 2x 89.1M + 1x 156.7M', 'cpu_accel_interconnect': 'UPI', 'network_card': '', 'num_network_cards': '0', 'notes': ''}\"}", "metadata": {"file": "mlperf_logger.py", "lineno": 234}}
:::MLL 1573852656.197 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_logger.py", "lineno": 238}}
:::MLL 1573852656.198 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_logger.py", "lineno": 242}}
Clearing caches
:::MLL 1573852657.136 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node lambda-quad
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=LambdaDual2080Ti -e 'MULTI_NODE= --master_port=4710' -e SLURM_JOB_ID=191115131659784511603 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191115131659784511603 ./run_and_time.sh
Run vars: id 191115131659784511603 gpus 2 mparams  --master_port=4710
STARTING TIMING RUN AT 2019-11-15 09:17:37 PM
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ '[' '!' -f /coco ']'
+ ln -sf /data/coco2017 /coco
++ ls /data
+ echo coco2017 torchvision
coco2017 torchvision
+ python -m bind_launch --nsockets_per_node 1 --ncores_per_socket 10 --nproc_per_node 2 --master_port=4710 tools/train_mlperf.py --config-file configs/e2e_mask_rcnn_R_50_FPN_1x.yaml DTYPE float16 PATHS_CATALOG maskrcnn_benchmark/config/paths_catalog_dbcluster.py MODEL.WEIGHT /coco/models/R-50.pkl DISABLE_REDUCED_LOGGING True SOLVER.BASE_LR 0.06 SOLVER.MAX_ITER 80000 SOLVER.WARMUP_FACTOR 0.000096 SOLVER.WARMUP_ITERS 625 SOLVER.WARMUP_METHOD mlperf_linear SOLVER.STEPS '(24000, 32000)' SOLVER.IMS_PER_BATCH 8 TEST.IMS_PER_BATCH 4 MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN 6000 NHWC True
:::MLL 1573852659.233 init_start: {"value": null, "metadata": {"file": "tools/train_mlperf.py", "lineno": 262}}
:::MLL 1573852659.233 init_start: {"value": null, "metadata": {"file": "tools/train_mlperf.py", "lineno": 262}}
2019-11-15 21:17:44,175 maskrcnn_benchmark INFO: Using 2 GPUs
2019-11-15 21:17:44,175 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_mask_rcnn_R_50_FPN_1x.yaml', distributed=True, local_rank=0, opts=['DTYPE', 'float16', 'PATHS_CATALOG', 'maskrcnn_benchmark/config/paths_catalog_dbcluster.py', 'MODEL.WEIGHT', '/coco/models/R-50.pkl', 'DISABLE_REDUCED_LOGGING', 'True', 'SOLVER.BASE_LR', '0.06', 'SOLVER.MAX_ITER', '80000', 'SOLVER.WARMUP_FACTOR', '0.000096', 'SOLVER.WARMUP_ITERS', '625', 'SOLVER.WARMUP_METHOD', 'mlperf_linear', 'SOLVER.STEPS', '(24000, 32000)', 'SOLVER.IMS_PER_BATCH', '8', 'TEST.IMS_PER_BATCH', '4', 'MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN', '6000', 'NHWC', 'True'], seed=1412225024)
2019-11-15 21:17:44,176 maskrcnn_benchmark INFO: Worker 0: Setting seed 4014329629
2019-11-15 21:17:44,176 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2019-11-15 21:17:46,483 maskrcnn_benchmark INFO: 
PyTorch version: 1.1.0a0+828a6a3
Is debug build: No
CUDA used to build PyTorch: 10.1.163

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.1.163
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti

Nvidia driver version: 418.88
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.0

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.16.3
[pip] torch==1.1.0a0+828a6a3
[pip] torchtext==0.4.0
[pip] torchvision==0.2.1
[conda] magma-cuda100             2.1.0                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] torch                     1.1.0a0+828a6a3          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.2.1                    pypi_0    pypi
        Pillow (5.3.0.post1)
2019-11-15 21:17:46,484 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_mask_rcnn_R_50_FPN_1x.yaml
2019-11-15 21:17:46,484 maskrcnn_benchmark INFO: 
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/MSRA/R-50"
  BACKBONE:
    CONV_BODY: "R-50-FPN"
    OUT_CHANNELS: 256
  RPN:
    USE_FPN: True
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    PRE_NMS_TOP_N_TRAIN: 2000
    PRE_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
  ROI_HEADS:
    USE_FPN: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
  ROI_MASK_HEAD:
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    FEATURE_EXTRACTOR: "MaskRCNNFPNFeatureExtractor"
    PREDICTOR: "MaskRCNNC4Predictor"
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 2
    RESOLUTION: 28
    SHARE_BOX_FEATURE_EXTRACTOR: False
  MASK_ON: True
DATASETS:
  TRAIN: ("coco_2017_train",)
  TEST: ("coco_2017_val",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BASE_LR: 0.02
  WEIGHT_DECAY: 0.0001
  STEPS: (60000, 80000)
  MAX_ITER: 90000

2019-11-15 21:17:46,485 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
DISABLE_REDUCED_LOGGING: True
DTYPE: float16
INPUT:
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (800,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  TO_BGR255: True
MLPERF:
  MIN_BBOX_MAP: 0.377
  MIN_SEGM_MAP: 0.339
MODEL:
  BACKBONE:
    CONV_BODY: R-50-FPN
    FREEZE_CONV_BODY_AT: 2
    OUT_CHANNELS: 256
    USE_GN: False
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  RESNETS:
    NUM_GROUPS: 1
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 64
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 81
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.5
    DETECTIONS_PER_IMG: 100
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.5
    POSITIVE_FRACTION: 0.25
    SCORE_THRESH: 0.05
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: MaskRCNNFPNFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 28
    SHARE_BOX_FEATURE_EXTRACTOR: False
    USE_GN: False
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 6000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 2000
    PRE_NMS_TOP_N_TEST: 1000
    PRE_NMS_TOP_N_TRAIN: 2000
    RPN_HEAD: SingleConvRPNHead
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  WEIGHT: /coco/models/R-50.pkl
NHWC: True
OUTPUT_DIR: .
PATHS_CATALOG: maskrcnn_benchmark/config/paths_catalog_dbcluster.py
PER_EPOCH_EVAL: True
SAVE_CHECKPOINTS: False
SOLVER:
  BASE_LR: 0.06
  BIAS_LR_FACTOR: 2
  CHECKPOINT_PERIOD: 2500
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  MAX_ITER: 80000
  MOMENTUM: 0.9
  STEPS: (24000, 32000)
  WARMUP_FACTOR: 9.6e-05
  WARMUP_ITERS: 625
  WARMUP_METHOD: mlperf_linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0
TEST:
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 4
:::MLL 1573852666.491 global_batch_size: {"value": 8, "metadata": {"file": "tools/train_mlperf.py", "lineno": 147}}
:::MLL 1573852666.492 num_image_candidates: {"value": 6000, "metadata": {"file": "tools/train_mlperf.py", "lineno": 148}}
:::MLL 1573852667.220 opt_base_learning_rate: {"value": 0.06, "metadata": {"file": "tools/train_mlperf.py", "lineno": 157}}
:::MLL 1573852667.220 opt_learning_rate_warmup_steps: {"value": 625, "metadata": {"file": "tools/train_mlperf.py", "lineno": 158}}
:::MLL 1573852667.220 opt_learning_rate_warmup_factor: {"value": 9.6e-05, "metadata": {"file": "tools/train_mlperf.py", "lineno": 159}}
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
2019-11-15 21:17:47,231 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from /coco/models/R-50.pkl
2019-11-15 21:17:47,339 maskrcnn_benchmark.utils.c2_model_loading INFO: Remapping C2 weights
2019-11-15 21:17:47,339 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: conv1_b              mapped name: conv1.bias
2019-11-15 21:17:47,339 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: conv1_w              mapped name: conv1.weight
2019-11-15 21:17:47,339 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: fc1000_b             mapped name: fc1000.bias
2019-11-15 21:17:47,339 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: fc1000_w             mapped name: fc1000.weight
2019-11-15 21:17:47,339 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch1_b     mapped name: layer1.0.downsample.0.bias
2019-11-15 21:17:47,339 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch1_bn_b  mapped name: layer1.0.downsample.1.bias
2019-11-15 21:17:47,339 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch1_bn_s  mapped name: layer1.0.downsample.1.weight
2019-11-15 21:17:47,339 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch1_w     mapped name: layer1.0.downsample.0.weight
2019-11-15 21:17:47,339 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2a_b    mapped name: layer1.0.conv1.bias
2019-11-15 21:17:47,339 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2a_bn_b mapped name: layer1.0.bn1.bias
2019-11-15 21:17:47,339 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2a_bn_s mapped name: layer1.0.bn1.weight
2019-11-15 21:17:47,339 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2a_w    mapped name: layer1.0.conv1.weight
2019-11-15 21:17:47,339 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2b_b    mapped name: layer1.0.conv2.bias
2019-11-15 21:17:47,339 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2b_bn_b mapped name: layer1.0.bn2.bias
2019-11-15 21:17:47,339 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2b_bn_s mapped name: layer1.0.bn2.weight
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2b_w    mapped name: layer1.0.conv2.weight
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2c_b    mapped name: layer1.0.conv3.bias
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2c_bn_b mapped name: layer1.0.bn3.bias
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2c_bn_s mapped name: layer1.0.bn3.weight
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_0_branch2c_w    mapped name: layer1.0.conv3.weight
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2a_b    mapped name: layer1.1.conv1.bias
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2a_bn_b mapped name: layer1.1.bn1.bias
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2a_bn_s mapped name: layer1.1.bn1.weight
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2a_w    mapped name: layer1.1.conv1.weight
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2b_b    mapped name: layer1.1.conv2.bias
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2b_bn_b mapped name: layer1.1.bn2.bias
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2b_bn_s mapped name: layer1.1.bn2.weight
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2b_w    mapped name: layer1.1.conv2.weight
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2c_b    mapped name: layer1.1.conv3.bias
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2c_bn_b mapped name: layer1.1.bn3.bias
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2c_bn_s mapped name: layer1.1.bn3.weight
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_1_branch2c_w    mapped name: layer1.1.conv3.weight
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2a_b    mapped name: layer1.2.conv1.bias
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2a_bn_b mapped name: layer1.2.bn1.bias
2019-11-15 21:17:47,340 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2a_bn_s mapped name: layer1.2.bn1.weight
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2a_w    mapped name: layer1.2.conv1.weight
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2b_b    mapped name: layer1.2.conv2.bias
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2b_bn_b mapped name: layer1.2.bn2.bias
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2b_bn_s mapped name: layer1.2.bn2.weight
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2b_w    mapped name: layer1.2.conv2.weight
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2c_b    mapped name: layer1.2.conv3.bias
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2c_bn_b mapped name: layer1.2.bn3.bias
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2c_bn_s mapped name: layer1.2.bn3.weight
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res2_2_branch2c_w    mapped name: layer1.2.conv3.weight
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch1_b     mapped name: layer2.0.downsample.0.bias
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch1_bn_b  mapped name: layer2.0.downsample.1.bias
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch1_bn_s  mapped name: layer2.0.downsample.1.weight
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch1_w     mapped name: layer2.0.downsample.0.weight
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2a_b    mapped name: layer2.0.conv1.bias
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2a_bn_b mapped name: layer2.0.bn1.bias
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2a_bn_s mapped name: layer2.0.bn1.weight
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2a_w    mapped name: layer2.0.conv1.weight
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2b_b    mapped name: layer2.0.conv2.bias
2019-11-15 21:17:47,341 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2b_bn_b mapped name: layer2.0.bn2.bias
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2b_bn_s mapped name: layer2.0.bn2.weight
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2b_w    mapped name: layer2.0.conv2.weight
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2c_b    mapped name: layer2.0.conv3.bias
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2c_bn_b mapped name: layer2.0.bn3.bias
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2c_bn_s mapped name: layer2.0.bn3.weight
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_0_branch2c_w    mapped name: layer2.0.conv3.weight
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2a_b    mapped name: layer2.1.conv1.bias
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2a_bn_b mapped name: layer2.1.bn1.bias
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2a_bn_s mapped name: layer2.1.bn1.weight
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2a_w    mapped name: layer2.1.conv1.weight
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2b_b    mapped name: layer2.1.conv2.bias
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2b_bn_b mapped name: layer2.1.bn2.bias
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2b_bn_s mapped name: layer2.1.bn2.weight
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2b_w    mapped name: layer2.1.conv2.weight
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2c_b    mapped name: layer2.1.conv3.bias
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2c_bn_b mapped name: layer2.1.bn3.bias
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2c_bn_s mapped name: layer2.1.bn3.weight
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_1_branch2c_w    mapped name: layer2.1.conv3.weight
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2a_b    mapped name: layer2.2.conv1.bias
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2a_bn_b mapped name: layer2.2.bn1.bias
2019-11-15 21:17:47,342 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2a_bn_s mapped name: layer2.2.bn1.weight
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2a_w    mapped name: layer2.2.conv1.weight
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2b_b    mapped name: layer2.2.conv2.bias
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2b_bn_b mapped name: layer2.2.bn2.bias
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2b_bn_s mapped name: layer2.2.bn2.weight
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2b_w    mapped name: layer2.2.conv2.weight
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2c_b    mapped name: layer2.2.conv3.bias
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2c_bn_b mapped name: layer2.2.bn3.bias
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2c_bn_s mapped name: layer2.2.bn3.weight
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_2_branch2c_w    mapped name: layer2.2.conv3.weight
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2a_b    mapped name: layer2.3.conv1.bias
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2a_bn_b mapped name: layer2.3.bn1.bias
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2a_bn_s mapped name: layer2.3.bn1.weight
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2a_w    mapped name: layer2.3.conv1.weight
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2b_b    mapped name: layer2.3.conv2.bias
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2b_bn_b mapped name: layer2.3.bn2.bias
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2b_bn_s mapped name: layer2.3.bn2.weight
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2b_w    mapped name: layer2.3.conv2.weight
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2c_b    mapped name: layer2.3.conv3.bias
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2c_bn_b mapped name: layer2.3.bn3.bias
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2c_bn_s mapped name: layer2.3.bn3.weight
2019-11-15 21:17:47,343 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res3_3_branch2c_w    mapped name: layer2.3.conv3.weight
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch1_b     mapped name: layer3.0.downsample.0.bias
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch1_bn_b  mapped name: layer3.0.downsample.1.bias
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch1_bn_s  mapped name: layer3.0.downsample.1.weight
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch1_w     mapped name: layer3.0.downsample.0.weight
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2a_b    mapped name: layer3.0.conv1.bias
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2a_bn_b mapped name: layer3.0.bn1.bias
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2a_bn_s mapped name: layer3.0.bn1.weight
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2a_w    mapped name: layer3.0.conv1.weight
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2b_b    mapped name: layer3.0.conv2.bias
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2b_bn_b mapped name: layer3.0.bn2.bias
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2b_bn_s mapped name: layer3.0.bn2.weight
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2b_w    mapped name: layer3.0.conv2.weight
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2c_b    mapped name: layer3.0.conv3.bias
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2c_bn_b mapped name: layer3.0.bn3.bias
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2c_bn_s mapped name: layer3.0.bn3.weight
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_0_branch2c_w    mapped name: layer3.0.conv3.weight
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2a_b    mapped name: layer3.1.conv1.bias
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2a_bn_b mapped name: layer3.1.bn1.bias
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2a_bn_s mapped name: layer3.1.bn1.weight
2019-11-15 21:17:47,344 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2a_w    mapped name: layer3.1.conv1.weight
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2b_b    mapped name: layer3.1.conv2.bias
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2b_bn_b mapped name: layer3.1.bn2.bias
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2b_bn_s mapped name: layer3.1.bn2.weight
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2b_w    mapped name: layer3.1.conv2.weight
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2c_b    mapped name: layer3.1.conv3.bias
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2c_bn_b mapped name: layer3.1.bn3.bias
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2c_bn_s mapped name: layer3.1.bn3.weight
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_1_branch2c_w    mapped name: layer3.1.conv3.weight
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2a_b    mapped name: layer3.2.conv1.bias
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2a_bn_b mapped name: layer3.2.bn1.bias
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2a_bn_s mapped name: layer3.2.bn1.weight
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2a_w    mapped name: layer3.2.conv1.weight
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2b_b    mapped name: layer3.2.conv2.bias
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2b_bn_b mapped name: layer3.2.bn2.bias
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2b_bn_s mapped name: layer3.2.bn2.weight
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2b_w    mapped name: layer3.2.conv2.weight
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2c_b    mapped name: layer3.2.conv3.bias
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2c_bn_b mapped name: layer3.2.bn3.bias
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2c_bn_s mapped name: layer3.2.bn3.weight
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_2_branch2c_w    mapped name: layer3.2.conv3.weight
2019-11-15 21:17:47,345 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2a_b    mapped name: layer3.3.conv1.bias
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2a_bn_b mapped name: layer3.3.bn1.bias
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2a_bn_s mapped name: layer3.3.bn1.weight
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2a_w    mapped name: layer3.3.conv1.weight
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2b_b    mapped name: layer3.3.conv2.bias
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2b_bn_b mapped name: layer3.3.bn2.bias
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2b_bn_s mapped name: layer3.3.bn2.weight
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2b_w    mapped name: layer3.3.conv2.weight
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2c_b    mapped name: layer3.3.conv3.bias
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2c_bn_b mapped name: layer3.3.bn3.bias
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2c_bn_s mapped name: layer3.3.bn3.weight
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_3_branch2c_w    mapped name: layer3.3.conv3.weight
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2a_b    mapped name: layer3.4.conv1.bias
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2a_bn_b mapped name: layer3.4.bn1.bias
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2a_bn_s mapped name: layer3.4.bn1.weight
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2a_w    mapped name: layer3.4.conv1.weight
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2b_b    mapped name: layer3.4.conv2.bias
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2b_bn_b mapped name: layer3.4.bn2.bias
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2b_bn_s mapped name: layer3.4.bn2.weight
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2b_w    mapped name: layer3.4.conv2.weight
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2c_b    mapped name: layer3.4.conv3.bias
2019-11-15 21:17:47,346 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2c_bn_b mapped name: layer3.4.bn3.bias
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2c_bn_s mapped name: layer3.4.bn3.weight
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_4_branch2c_w    mapped name: layer3.4.conv3.weight
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2a_b    mapped name: layer3.5.conv1.bias
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2a_bn_b mapped name: layer3.5.bn1.bias
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2a_bn_s mapped name: layer3.5.bn1.weight
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2a_w    mapped name: layer3.5.conv1.weight
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2b_b    mapped name: layer3.5.conv2.bias
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2b_bn_b mapped name: layer3.5.bn2.bias
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2b_bn_s mapped name: layer3.5.bn2.weight
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2b_w    mapped name: layer3.5.conv2.weight
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2c_b    mapped name: layer3.5.conv3.bias
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2c_bn_b mapped name: layer3.5.bn3.bias
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2c_bn_s mapped name: layer3.5.bn3.weight
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res4_5_branch2c_w    mapped name: layer3.5.conv3.weight
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch1_b     mapped name: layer4.0.downsample.0.bias
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch1_bn_b  mapped name: layer4.0.downsample.1.bias
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch1_bn_s  mapped name: layer4.0.downsample.1.weight
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch1_w     mapped name: layer4.0.downsample.0.weight
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2a_b    mapped name: layer4.0.conv1.bias
2019-11-15 21:17:47,347 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2a_bn_b mapped name: layer4.0.bn1.bias
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2a_bn_s mapped name: layer4.0.bn1.weight
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2a_w    mapped name: layer4.0.conv1.weight
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2b_b    mapped name: layer4.0.conv2.bias
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2b_bn_b mapped name: layer4.0.bn2.bias
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2b_bn_s mapped name: layer4.0.bn2.weight
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2b_w    mapped name: layer4.0.conv2.weight
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2c_b    mapped name: layer4.0.conv3.bias
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2c_bn_b mapped name: layer4.0.bn3.bias
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2c_bn_s mapped name: layer4.0.bn3.weight
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_0_branch2c_w    mapped name: layer4.0.conv3.weight
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2a_b    mapped name: layer4.1.conv1.bias
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2a_bn_b mapped name: layer4.1.bn1.bias
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2a_bn_s mapped name: layer4.1.bn1.weight
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2a_w    mapped name: layer4.1.conv1.weight
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2b_b    mapped name: layer4.1.conv2.bias
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2b_bn_b mapped name: layer4.1.bn2.bias
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2b_bn_s mapped name: layer4.1.bn2.weight
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2b_w    mapped name: layer4.1.conv2.weight
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2c_b    mapped name: layer4.1.conv3.bias
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2c_bn_b mapped name: layer4.1.bn3.bias
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2c_bn_s mapped name: layer4.1.bn3.weight
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_1_branch2c_w    mapped name: layer4.1.conv3.weight
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2a_b    mapped name: layer4.2.conv1.bias
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2a_bn_b mapped name: layer4.2.bn1.bias
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2a_bn_s mapped name: layer4.2.bn1.weight
2019-11-15 21:17:47,348 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2a_w    mapped name: layer4.2.conv1.weight
2019-11-15 21:17:47,349 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2b_b    mapped name: layer4.2.conv2.bias
2019-11-15 21:17:47,349 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2b_bn_b mapped name: layer4.2.bn2.bias
2019-11-15 21:17:47,349 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2b_bn_s mapped name: layer4.2.bn2.weight
2019-11-15 21:17:47,349 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2b_w    mapped name: layer4.2.conv2.weight
2019-11-15 21:17:47,349 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2c_b    mapped name: layer4.2.conv3.bias
2019-11-15 21:17:47,349 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2c_bn_b mapped name: layer4.2.bn3.bias
2019-11-15 21:17:47,349 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2c_bn_s mapped name: layer4.2.bn3.weight
2019-11-15 21:17:47,349 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res5_2_branch2c_w    mapped name: layer4.2.conv3.weight
2019-11-15 21:17:47,349 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res_conv1_bn_b       mapped name: bn1.bias
2019-11-15 21:17:47,349 maskrcnn_benchmark.utils.c2_model_loading INFO: C2 name: res_conv1_bn_s       mapped name: bn1.weight
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.bn1.bias                   loaded from layer1.0.bn1.bias            of shape (64,)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.bn1.weight                 loaded from layer1.0.bn1.weight          of shape (64,)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.bn2.bias                   loaded from layer1.0.bn2.bias            of shape (64,)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.bn2.weight                 loaded from layer1.0.bn2.weight          of shape (64,)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.bn3.bias                   loaded from layer1.0.bn3.bias            of shape (256,)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.bn3.weight                 loaded from layer1.0.bn3.weight          of shape (256,)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.conv1.weight               loaded from layer1.0.conv1.weight        of shape (64, 64, 1, 1)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.conv2.weight               loaded from layer1.0.conv2.weight        of shape (64, 64, 3, 3)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.conv3.weight               loaded from layer1.0.conv3.weight        of shape (256, 64, 1, 1)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.downsample.0.weight        loaded from layer1.0.downsample.0.weight of shape (256, 64, 1, 1)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.downsample.1.bias          loaded from layer1.0.downsample.1.bias   of shape (256,)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.0.downsample.1.weight        loaded from layer1.0.downsample.1.weight of shape (256,)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.bn1.bias                   loaded from layer1.1.bn1.bias            of shape (64,)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.bn1.weight                 loaded from layer1.1.bn1.weight          of shape (64,)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.bn2.bias                   loaded from layer1.1.bn2.bias            of shape (64,)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.bn2.weight                 loaded from layer1.1.bn2.weight          of shape (64,)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.bn3.bias                   loaded from layer1.1.bn3.bias            of shape (256,)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.bn3.weight                 loaded from layer1.1.bn3.weight          of shape (256,)
2019-11-15 21:17:47,358 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.conv1.weight               loaded from layer1.1.conv1.weight        of shape (64, 256, 1, 1)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.conv2.weight               loaded from layer1.1.conv2.weight        of shape (64, 64, 3, 3)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.1.conv3.weight               loaded from layer1.1.conv3.weight        of shape (256, 64, 1, 1)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.bn1.bias                   loaded from layer1.2.bn1.bias            of shape (64,)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.bn1.weight                 loaded from layer1.2.bn1.weight          of shape (64,)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.bn2.bias                   loaded from layer1.2.bn2.bias            of shape (64,)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.bn2.weight                 loaded from layer1.2.bn2.weight          of shape (64,)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.bn3.bias                   loaded from layer1.2.bn3.bias            of shape (256,)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.bn3.weight                 loaded from layer1.2.bn3.weight          of shape (256,)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.conv1.weight               loaded from layer1.2.conv1.weight        of shape (64, 256, 1, 1)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.conv2.weight               loaded from layer1.2.conv2.weight        of shape (64, 64, 3, 3)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer1.2.conv3.weight               loaded from layer1.2.conv3.weight        of shape (256, 64, 1, 1)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.bn1.bias                   loaded from layer2.0.bn1.bias            of shape (128,)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.bn1.weight                 loaded from layer2.0.bn1.weight          of shape (128,)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.bn2.bias                   loaded from layer2.0.bn2.bias            of shape (128,)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.bn2.weight                 loaded from layer2.0.bn2.weight          of shape (128,)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.bn3.bias                   loaded from layer2.0.bn3.bias            of shape (512,)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.bn3.weight                 loaded from layer2.0.bn3.weight          of shape (512,)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.conv1.weight               loaded from layer2.0.conv1.weight        of shape (128, 256, 1, 1)
2019-11-15 21:17:47,359 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.conv2.weight               loaded from layer2.0.conv2.weight        of shape (128, 128, 3, 3)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.conv3.weight               loaded from layer2.0.conv3.weight        of shape (512, 128, 1, 1)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.downsample.0.weight        loaded from layer2.0.downsample.0.weight of shape (512, 256, 1, 1)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.downsample.1.bias          loaded from layer2.0.downsample.1.bias   of shape (512,)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.0.downsample.1.weight        loaded from layer2.0.downsample.1.weight of shape (512,)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.bn1.bias                   loaded from layer2.1.bn1.bias            of shape (128,)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.bn1.weight                 loaded from layer2.1.bn1.weight          of shape (128,)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.bn2.bias                   loaded from layer2.1.bn2.bias            of shape (128,)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.bn2.weight                 loaded from layer2.1.bn2.weight          of shape (128,)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.bn3.bias                   loaded from layer2.1.bn3.bias            of shape (512,)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.bn3.weight                 loaded from layer2.1.bn3.weight          of shape (512,)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.conv1.weight               loaded from layer2.1.conv1.weight        of shape (128, 512, 1, 1)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.conv2.weight               loaded from layer2.1.conv2.weight        of shape (128, 128, 3, 3)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.1.conv3.weight               loaded from layer2.1.conv3.weight        of shape (512, 128, 1, 1)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.bn1.bias                   loaded from layer2.2.bn1.bias            of shape (128,)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.bn1.weight                 loaded from layer2.2.bn1.weight          of shape (128,)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.bn2.bias                   loaded from layer2.2.bn2.bias            of shape (128,)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.bn2.weight                 loaded from layer2.2.bn2.weight          of shape (128,)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.bn3.bias                   loaded from layer2.2.bn3.bias            of shape (512,)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.bn3.weight                 loaded from layer2.2.bn3.weight          of shape (512,)
2019-11-15 21:17:47,360 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.conv1.weight               loaded from layer2.2.conv1.weight        of shape (128, 512, 1, 1)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.conv2.weight               loaded from layer2.2.conv2.weight        of shape (128, 128, 3, 3)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.2.conv3.weight               loaded from layer2.2.conv3.weight        of shape (512, 128, 1, 1)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.bn1.bias                   loaded from layer2.3.bn1.bias            of shape (128,)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.bn1.weight                 loaded from layer2.3.bn1.weight          of shape (128,)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.bn2.bias                   loaded from layer2.3.bn2.bias            of shape (128,)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.bn2.weight                 loaded from layer2.3.bn2.weight          of shape (128,)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.bn3.bias                   loaded from layer2.3.bn3.bias            of shape (512,)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.bn3.weight                 loaded from layer2.3.bn3.weight          of shape (512,)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.conv1.weight               loaded from layer2.3.conv1.weight        of shape (128, 512, 1, 1)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.conv2.weight               loaded from layer2.3.conv2.weight        of shape (128, 128, 3, 3)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer2.3.conv3.weight               loaded from layer2.3.conv3.weight        of shape (512, 128, 1, 1)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.bn1.bias                   loaded from layer3.0.bn1.bias            of shape (256,)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.bn1.weight                 loaded from layer3.0.bn1.weight          of shape (256,)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.bn2.bias                   loaded from layer3.0.bn2.bias            of shape (256,)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.bn2.weight                 loaded from layer3.0.bn2.weight          of shape (256,)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.bn3.bias                   loaded from layer3.0.bn3.bias            of shape (1024,)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.bn3.weight                 loaded from layer3.0.bn3.weight          of shape (1024,)
2019-11-15 21:17:47,361 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.conv1.weight               loaded from layer3.0.conv1.weight        of shape (256, 512, 1, 1)
2019-11-15 21:17:47,362 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.conv2.weight               loaded from layer3.0.conv2.weight        of shape (256, 256, 3, 3)
2019-11-15 21:17:47,362 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.conv3.weight               loaded from layer3.0.conv3.weight        of shape (1024, 256, 1, 1)
2019-11-15 21:17:47,362 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.downsample.0.weight        loaded from layer3.0.downsample.0.weight of shape (1024, 512, 1, 1)
2019-11-15 21:17:47,362 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.downsample.1.bias          loaded from layer3.0.downsample.1.bias   of shape (1024,)
2019-11-15 21:17:47,362 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.0.downsample.1.weight        loaded from layer3.0.downsample.1.weight of shape (1024,)
2019-11-15 21:17:47,362 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.bn1.bias                   loaded from layer3.1.bn1.bias            of shape (256,)
2019-11-15 21:17:47,362 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.bn1.weight                 loaded from layer3.1.bn1.weight          of shape (256,)
2019-11-15 21:17:47,362 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.bn2.bias                   loaded from layer3.1.bn2.bias            of shape (256,)
2019-11-15 21:17:47,362 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.bn2.weight                 loaded from layer3.1.bn2.weight          of shape (256,)
2019-11-15 21:17:47,362 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.bn3.bias                   loaded from layer3.1.bn3.bias            of shape (1024,)
2019-11-15 21:17:47,363 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.bn3.weight                 loaded from layer3.1.bn3.weight          of shape (1024,)
2019-11-15 21:17:47,363 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.conv1.weight               loaded from layer3.1.conv1.weight        of shape (256, 1024, 1, 1)
2019-11-15 21:17:47,363 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.conv2.weight               loaded from layer3.1.conv2.weight        of shape (256, 256, 3, 3)
2019-11-15 21:17:47,363 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.1.conv3.weight               loaded from layer3.1.conv3.weight        of shape (1024, 256, 1, 1)
2019-11-15 21:17:47,363 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.bn1.bias                   loaded from layer3.2.bn1.bias            of shape (256,)
2019-11-15 21:17:47,363 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.bn1.weight                 loaded from layer3.2.bn1.weight          of shape (256,)
2019-11-15 21:17:47,363 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.bn2.bias                   loaded from layer3.2.bn2.bias            of shape (256,)
2019-11-15 21:17:47,363 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.bn2.weight                 loaded from layer3.2.bn2.weight          of shape (256,)
2019-11-15 21:17:47,363 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.bn3.bias                   loaded from layer3.2.bn3.bias            of shape (1024,)
2019-11-15 21:17:47,363 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.bn3.weight                 loaded from layer3.2.bn3.weight          of shape (1024,)
2019-11-15 21:17:47,364 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.conv1.weight               loaded from layer3.2.conv1.weight        of shape (256, 1024, 1, 1)
2019-11-15 21:17:47,364 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.conv2.weight               loaded from layer3.2.conv2.weight        of shape (256, 256, 3, 3)
2019-11-15 21:17:47,364 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.2.conv3.weight               loaded from layer3.2.conv3.weight        of shape (1024, 256, 1, 1)
2019-11-15 21:17:47,364 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.bn1.bias                   loaded from layer3.3.bn1.bias            of shape (256,)
2019-11-15 21:17:47,364 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.bn1.weight                 loaded from layer3.3.bn1.weight          of shape (256,)
2019-11-15 21:17:47,364 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.bn2.bias                   loaded from layer3.3.bn2.bias            of shape (256,)
2019-11-15 21:17:47,364 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.bn2.weight                 loaded from layer3.3.bn2.weight          of shape (256,)
2019-11-15 21:17:47,364 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.bn3.bias                   loaded from layer3.3.bn3.bias            of shape (1024,)
2019-11-15 21:17:47,364 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.bn3.weight                 loaded from layer3.3.bn3.weight          of shape (1024,)
2019-11-15 21:17:47,365 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.conv1.weight               loaded from layer3.3.conv1.weight        of shape (256, 1024, 1, 1)
2019-11-15 21:17:47,365 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.conv2.weight               loaded from layer3.3.conv2.weight        of shape (256, 256, 3, 3)
2019-11-15 21:17:47,365 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.3.conv3.weight               loaded from layer3.3.conv3.weight        of shape (1024, 256, 1, 1)
2019-11-15 21:17:47,365 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.bn1.bias                   loaded from layer3.4.bn1.bias            of shape (256,)
2019-11-15 21:17:47,365 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.bn1.weight                 loaded from layer3.4.bn1.weight          of shape (256,)
2019-11-15 21:17:47,365 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.bn2.bias                   loaded from layer3.4.bn2.bias            of shape (256,)
2019-11-15 21:17:47,365 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.bn2.weight                 loaded from layer3.4.bn2.weight          of shape (256,)
2019-11-15 21:17:47,365 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.bn3.bias                   loaded from layer3.4.bn3.bias            of shape (1024,)
2019-11-15 21:17:47,365 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.bn3.weight                 loaded from layer3.4.bn3.weight          of shape (1024,)
2019-11-15 21:17:47,366 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.conv1.weight               loaded from layer3.4.conv1.weight        of shape (256, 1024, 1, 1)
2019-11-15 21:17:47,366 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.conv2.weight               loaded from layer3.4.conv2.weight        of shape (256, 256, 3, 3)
2019-11-15 21:17:47,366 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.4.conv3.weight               loaded from layer3.4.conv3.weight        of shape (1024, 256, 1, 1)
2019-11-15 21:17:47,366 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.bn1.bias                   loaded from layer3.5.bn1.bias            of shape (256,)
2019-11-15 21:17:47,366 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.bn1.weight                 loaded from layer3.5.bn1.weight          of shape (256,)
2019-11-15 21:17:47,366 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.bn2.bias                   loaded from layer3.5.bn2.bias            of shape (256,)
2019-11-15 21:17:47,366 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.bn2.weight                 loaded from layer3.5.bn2.weight          of shape (256,)
2019-11-15 21:17:47,366 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.bn3.bias                   loaded from layer3.5.bn3.bias            of shape (1024,)
2019-11-15 21:17:47,367 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.bn3.weight                 loaded from layer3.5.bn3.weight          of shape (1024,)
2019-11-15 21:17:47,367 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.conv1.weight               loaded from layer3.5.conv1.weight        of shape (256, 1024, 1, 1)
2019-11-15 21:17:47,367 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.conv2.weight               loaded from layer3.5.conv2.weight        of shape (256, 256, 3, 3)
2019-11-15 21:17:47,367 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer3.5.conv3.weight               loaded from layer3.5.conv3.weight        of shape (1024, 256, 1, 1)
2019-11-15 21:17:47,367 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.bn1.bias                   loaded from layer4.0.bn1.bias            of shape (512,)
2019-11-15 21:17:47,367 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.bn1.weight                 loaded from layer4.0.bn1.weight          of shape (512,)
2019-11-15 21:17:47,367 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.bn2.bias                   loaded from layer4.0.bn2.bias            of shape (512,)
2019-11-15 21:17:47,367 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.bn2.weight                 loaded from layer4.0.bn2.weight          of shape (512,)
2019-11-15 21:17:47,368 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.bn3.bias                   loaded from layer4.0.bn3.bias            of shape (2048,)
2019-11-15 21:17:47,368 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.bn3.weight                 loaded from layer4.0.bn3.weight          of shape (2048,)
2019-11-15 21:17:47,368 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.conv1.weight               loaded from layer4.0.conv1.weight        of shape (512, 1024, 1, 1)
2019-11-15 21:17:47,370 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.conv2.weight               loaded from layer4.0.conv2.weight        of shape (512, 512, 3, 3)
2019-11-15 21:17:47,370 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.conv3.weight               loaded from layer4.0.conv3.weight        of shape (2048, 512, 1, 1)
2019-11-15 21:17:47,370 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.downsample.0.weight        loaded from layer4.0.downsample.0.weight of shape (2048, 1024, 1, 1)
2019-11-15 21:17:47,370 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.downsample.1.bias          loaded from layer4.0.downsample.1.bias   of shape (2048,)
2019-11-15 21:17:47,370 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.0.downsample.1.weight        loaded from layer4.0.downsample.1.weight of shape (2048,)
2019-11-15 21:17:47,370 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.bn1.bias                   loaded from layer4.1.bn1.bias            of shape (512,)
2019-11-15 21:17:47,370 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.bn1.weight                 loaded from layer4.1.bn1.weight          of shape (512,)
2019-11-15 21:17:47,370 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.bn2.bias                   loaded from layer4.1.bn2.bias            of shape (512,)
2019-11-15 21:17:47,370 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.bn2.weight                 loaded from layer4.1.bn2.weight          of shape (512,)
2019-11-15 21:17:47,370 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.bn3.bias                   loaded from layer4.1.bn3.bias            of shape (2048,)
2019-11-15 21:17:47,370 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.bn3.weight                 loaded from layer4.1.bn3.weight          of shape (2048,)
2019-11-15 21:17:47,371 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.conv1.weight               loaded from layer4.1.conv1.weight        of shape (512, 2048, 1, 1)
2019-11-15 21:17:47,376 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.conv2.weight               loaded from layer4.1.conv2.weight        of shape (512, 512, 3, 3)
2019-11-15 21:17:47,376 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.1.conv3.weight               loaded from layer4.1.conv3.weight        of shape (2048, 512, 1, 1)
2019-11-15 21:17:47,376 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.bn1.bias                   loaded from layer4.2.bn1.bias            of shape (512,)
2019-11-15 21:17:47,376 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.bn1.weight                 loaded from layer4.2.bn1.weight          of shape (512,)
2019-11-15 21:17:47,376 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.bn2.bias                   loaded from layer4.2.bn2.bias            of shape (512,)
2019-11-15 21:17:47,376 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.bn2.weight                 loaded from layer4.2.bn2.weight          of shape (512,)
2019-11-15 21:17:47,376 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.bn3.bias                   loaded from layer4.2.bn3.bias            of shape (2048,)
2019-11-15 21:17:47,376 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.bn3.weight                 loaded from layer4.2.bn3.weight          of shape (2048,)
2019-11-15 21:17:47,376 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.conv1.weight               loaded from layer4.2.conv1.weight        of shape (512, 2048, 1, 1)
2019-11-15 21:17:47,381 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.conv2.weight               loaded from layer4.2.conv2.weight        of shape (512, 512, 3, 3)
2019-11-15 21:17:47,381 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.layer4.2.conv3.weight               loaded from layer4.2.conv3.weight        of shape (2048, 512, 1, 1)
2019-11-15 21:17:47,381 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.stem.bn1.bias                       loaded from bn1.bias                     of shape (64,)
2019-11-15 21:17:47,381 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.stem.bn1.weight                     loaded from bn1.weight                   of shape (64,)
2019-11-15 21:17:47,381 maskrcnn_benchmark.utils.model_serialization INFO: module.backbone.body.stem.conv1.weight                   loaded from conv1.weight                 of shape (64, 3, 7, 7)
:::MLL 1573852667.410 init_stop: {"value": null, "metadata": {"file": "tools/train_mlperf.py", "lineno": 197}}
:::MLL 1573852667.410 run_start: {"value": null, "metadata": {"file": "tools/train_mlperf.py", "lineno": 198}}
2019-11-15 21:17:47,410 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
loading annotations into memory...
loading annotations into memory...
Done (t=5.75s)
creating index...
Done (t=5.72s)
creating index...
index created!
index created!
2019-11-15 21:17:55,594 maskrcnn_benchmark.trainer INFO: Start training
:::MLL 1573852676.428 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "tools/train_mlperf.py", "lineno": 129}}
:::MLL 1573852676.431 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "tools/train_mlperf.py", "lineno": 130}}
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
2019-11-15 21:18:05,754 maskrcnn_benchmark.trainer INFO: eta: 11:17:03  iter: 20  loss: 2.3758 (3.4587)  loss_classifier: 0.6634 (1.6955)  loss_box_reg: 0.0241 (0.0383)  loss_mask: 0.9991 (1.0792)  loss_objectness: 0.5506 (0.5472)  loss_rpn_box_reg: 0.0657 (0.0985)  time: 0.3371 (0.5079)  data: 0.0026 (0.0447)  lr: 0.001824  max mem: 7455
2019-11-15 21:18:12,735 maskrcnn_benchmark.trainer INFO: eta: 9:31:01  iter: 40  loss: 1.7525 (2.6557)  loss_classifier: 0.4491 (1.1258)  loss_box_reg: 0.0686 (0.0647)  loss_mask: 0.7046 (0.8941)  loss_objectness: 0.3374 (0.4600)  loss_rpn_box_reg: 0.0950 (0.1110)  time: 0.3457 (0.4285)  data: 0.0027 (0.0237)  lr: 0.003744  max mem: 7778
2019-11-15 21:18:19,510 maskrcnn_benchmark.trainer INFO: eta: 8:51:02  iter: 60  loss: 1.5282 (2.2855)  loss_classifier: 0.3195 (0.8713)  loss_box_reg: 0.0592 (0.0742)  loss_mask: 0.6844 (0.8249)  loss_objectness: 0.3038 (0.4036)  loss_rpn_box_reg: 0.0952 (0.1116)  time: 0.3403 (0.3986)  data: 0.0028 (0.0168)  lr: 0.005664  max mem: 7830
2019-11-15 21:18:26,243 maskrcnn_benchmark.trainer INFO: eta: 8:30:16  iter: 80  loss: 1.1860 (2.0258)  loss_classifier: 0.2160 (0.7118)  loss_box_reg: 0.0470 (0.0675)  loss_mask: 0.6730 (0.7874)  loss_objectness: 0.1951 (0.3563)  loss_rpn_box_reg: 0.0659 (0.1027)  time: 0.3407 (0.3831)  data: 0.0027 (0.0133)  lr: 0.007584  max mem: 7830
2019-11-15 21:18:32,964 maskrcnn_benchmark.trainer INFO: eta: 8:17:36  iter: 100  loss: 1.3644 (1.8935)  loss_classifier: 0.2531 (0.6287)  loss_box_reg: 0.0500 (0.0683)  loss_mask: 0.6721 (0.7636)  loss_objectness: 0.2086 (0.3290)  loss_rpn_box_reg: 0.1009 (0.1039)  time: 0.3384 (0.3737)  data: 0.0028 (0.0112)  lr: 0.009504  max mem: 7830
2019-11-15 21:18:39,721 maskrcnn_benchmark.trainer INFO: eta: 8:09:32  iter: 120  loss: 1.3926 (1.8080)  loss_classifier: 0.3094 (0.5796)  loss_box_reg: 0.0909 (0.0740)  loss_mask: 0.6644 (0.7483)  loss_objectness: 0.1659 (0.3040)  loss_rpn_box_reg: 0.0674 (0.1021)  time: 0.3322 (0.3677)  data: 0.0028 (0.0098)  lr: 0.011424  max mem: 7874
2019-11-15 21:18:46,708 maskrcnn_benchmark.trainer INFO: eta: 8:05:55  iter: 140  loss: 1.4947 (1.7682)  loss_classifier: 0.4628 (0.5585)  loss_box_reg: 0.1259 (0.0840)  loss_mask: 0.6616 (0.7356)  loss_objectness: 0.1632 (0.2834)  loss_rpn_box_reg: 0.0976 (0.1067)  time: 0.3458 (0.3651)  data: 0.0028 (0.0088)  lr: 0.013344  max mem: 7879
2019-11-15 21:18:53,667 maskrcnn_benchmark.trainer INFO: eta: 8:02:57  iter: 160  loss: 1.5250 (1.7554)  loss_classifier: 0.4797 (0.5541)  loss_box_reg: 0.1856 (0.0971)  loss_mask: 0.6380 (0.7238)  loss_objectness: 0.1702 (0.2733)  loss_rpn_box_reg: 0.0958 (0.1072)  time: 0.3500 (0.3629)  data: 0.0028 (0.0080)  lr: 0.015264  max mem: 7879
2019-11-15 21:19:00,726 maskrcnn_benchmark.trainer INFO: eta: 8:01:21  iter: 180  loss: 1.5451 (1.7327)  loss_classifier: 0.5232 (0.5503)  loss_box_reg: 0.2136 (0.1091)  loss_mask: 0.6358 (0.7144)  loss_objectness: 0.1025 (0.2555)  loss_rpn_box_reg: 0.0528 (0.1033)  time: 0.3531 (0.3618)  data: 0.0028 (0.0075)  lr: 0.017184  max mem: 7879
2019-11-15 21:19:07,680 maskrcnn_benchmark.trainer INFO: eta: 7:59:21  iter: 200  loss: 1.4993 (1.7117)  loss_classifier: 0.5094 (0.5430)  loss_box_reg: 0.1711 (0.1161)  loss_mask: 0.6127 (0.7039)  loss_objectness: 0.1305 (0.2446)  loss_rpn_box_reg: 0.0668 (0.1041)  time: 0.3437 (0.3604)  data: 0.0028 (0.0070)  lr: 0.019104  max mem: 7973
2019-11-15 21:19:14,758 maskrcnn_benchmark.trainer INFO: eta: 7:58:26  iter: 220  loss: 1.4657 (1.6917)  loss_classifier: 0.4413 (0.5365)  loss_box_reg: 0.1588 (0.1204)  loss_mask: 0.5869 (0.6935)  loss_objectness: 0.1309 (0.2366)  loss_rpn_box_reg: 0.0857 (0.1048)  time: 0.3508 (0.3598)  data: 0.0028 (0.0066)  lr: 0.021024  max mem: 7973
2019-11-15 21:19:21,771 maskrcnn_benchmark.trainer INFO: eta: 7:57:18  iter: 240  loss: 1.6161 (1.6864)  loss_classifier: 0.4978 (0.5366)  loss_box_reg: 0.1783 (0.1265)  loss_mask: 0.5950 (0.6842)  loss_objectness: 0.1723 (0.2323)  loss_rpn_box_reg: 0.1009 (0.1067)  time: 0.3504 (0.3591)  data: 0.0028 (0.0063)  lr: 0.022944  max mem: 7973
2019-11-15 21:19:28,836 maskrcnn_benchmark.trainer INFO: eta: 7:56:35  iter: 260  loss: 1.4613 (1.6731)  loss_classifier: 0.4689 (0.5322)  loss_box_reg: 0.1669 (0.1304)  loss_mask: 0.6054 (0.6779)  loss_objectness: 0.1100 (0.2265)  loss_rpn_box_reg: 0.0549 (0.1060)  time: 0.3520 (0.3586)  data: 0.0027 (0.0060)  lr: 0.024864  max mem: 7973
2019-11-15 21:19:35,871 maskrcnn_benchmark.trainer INFO: eta: 7:55:49  iter: 280  loss: 1.6099 (1.6658)  loss_classifier: 0.4993 (0.5321)  loss_box_reg: 0.2056 (0.1352)  loss_mask: 0.5806 (0.6715)  loss_objectness: 0.1364 (0.2206)  loss_rpn_box_reg: 0.0835 (0.1064)  time: 0.3494 (0.3581)  data: 0.0028 (0.0058)  lr: 0.026784  max mem: 7973
2019-11-15 21:19:42,864 maskrcnn_benchmark.trainer INFO: eta: 7:54:57  iter: 300  loss: 1.3913 (1.6517)  loss_classifier: 0.4188 (0.5281)  loss_box_reg: 0.1454 (0.1375)  loss_mask: 0.5911 (0.6658)  loss_objectness: 0.1324 (0.2152)  loss_rpn_box_reg: 0.0658 (0.1051)  time: 0.3483 (0.3576)  data: 0.0028 (0.0056)  lr: 0.028704  max mem: 7973
2019-11-15 21:19:49,897 maskrcnn_benchmark.trainer INFO: eta: 7:54:20  iter: 320  loss: 1.2488 (1.6344)  loss_classifier: 0.3909 (0.5210)  loss_box_reg: 0.1408 (0.1382)  loss_mask: 0.5851 (0.6602)  loss_objectness: 0.1250 (0.2106)  loss_rpn_box_reg: 0.0783 (0.1043)  time: 0.3490 (0.3572)  data: 0.0027 (0.0054)  lr: 0.030624  max mem: 7973
2019-11-15 21:19:56,976 maskrcnn_benchmark.trainer INFO: eta: 7:53:58  iter: 340  loss: 1.3307 (1.6205)  loss_classifier: 0.4023 (0.5143)  loss_box_reg: 0.1416 (0.1388)  loss_mask: 0.5759 (0.6551)  loss_objectness: 0.1486 (0.2079)  loss_rpn_box_reg: 0.0633 (0.1044)  time: 0.3504 (0.3570)  data: 0.0027 (0.0053)  lr: 0.032544  max mem: 7973
2019-11-15 21:20:03,982 maskrcnn_benchmark.trainer INFO: eta: 7:53:21  iter: 360  loss: 1.5980 (1.6304)  loss_classifier: 0.3736 (0.5095)  loss_box_reg: 0.0947 (0.1371)  loss_mask: 0.6468 (0.6545)  loss_objectness: 0.4108 (0.2200)  loss_rpn_box_reg: 0.1274 (0.1092)  time: 0.3463 (0.3566)  data: 0.0028 (0.0051)  lr: 0.034464  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.5
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.5
2019-11-15 21:20:10,816 maskrcnn_benchmark.trainer INFO: eta: 7:52:12  iter: 380  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 4.0964 (0.8479)  loss_objectness: 0.5346 (0.2644)  loss_rpn_box_reg: 0.1132 (0.1158)  time: 0.3404 (0.3558)  data: 0.0009 (0.0049)  lr: 0.036384  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.03125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.03125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.015625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.015625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0078125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0078125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.00390625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.00390625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.001953125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.001953125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0009765625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0009765625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.00048828125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.00048828125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.000244140625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.000244140625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0001220703125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0001220703125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.103515625e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.103515625e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0517578125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0517578125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.52587890625e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.52587890625e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.62939453125e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.62939453125e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.814697265625e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.814697265625e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9073486328125e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9073486328125e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.5367431640625e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.5367431640625e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.76837158203125e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.76837158203125e-07
2019-11-15 21:20:17,504 maskrcnn_benchmark.trainer INFO: eta: 7:50:39  iter: 400  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 5.5595 (1.1396)  loss_objectness: 0.5490 (nan)  loss_rpn_box_reg: 0.0616 (0.1256)  time: 0.3361 (0.3548)  data: 0.0009 (0.0047)  lr: 0.038304  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.384185791015625e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.384185791015625e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1920928955078125e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1920928955078125e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.960464477539063e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.960464477539063e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9802322387695312e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9802322387695312e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4901161193847656e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4901161193847656e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.450580596923828e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.450580596923828e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.725290298461914e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.725290298461914e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.862645149230957e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.862645149230957e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.313225746154785e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.313225746154785e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.656612873077393e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.656612873077393e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3283064365386963e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3283064365386963e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1641532182693481e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1641532182693481e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.820766091346741e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.820766091346741e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9103830456733704e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9103830456733704e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4551915228366852e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4551915228366852e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.275957614183426e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.275957614183426e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.637978807091713e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.637978807091713e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8189894035458565e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8189894035458565e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.094947017729282e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.094947017729282e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.547473508864641e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.547473508864641e-13
2019-11-15 21:20:24,298 maskrcnn_benchmark.trainer INFO: eta: 7:49:35  iter: 420  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 4.6258 (1.3643)  loss_objectness: 0.5909 (nan)  loss_rpn_box_reg: 0.1705 (0.1306)  time: 0.3399 (0.3541)  data: 0.0009 (0.0046)  lr: 0.040224  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2737367544323206e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2737367544323206e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1368683772161603e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1368683772161603e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.684341886080802e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.684341886080802e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.842170943040401e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.842170943040401e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4210854715202004e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4210854715202004e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.105427357601002e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.105427357601002e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.552713678800501e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.552713678800501e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7763568394002505e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7763568394002505e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.881784197001252e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.881784197001252e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.440892098500626e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.440892098500626e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.220446049250313e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.220446049250313e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1102230246251565e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1102230246251565e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.551115123125783e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.551115123125783e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7755575615628914e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7755575615628914e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3877787807814457e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3877787807814457e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.938893903907228e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.938893903907228e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.469446951953614e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.469446951953614e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.734723475976807e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.734723475976807e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.673617379884035e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.673617379884035e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.336808689942018e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.336808689942018e-19
2019-11-15 21:20:31,083 maskrcnn_benchmark.trainer INFO: eta: 7:48:34  iter: 440  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 4.8472 (1.5621)  loss_objectness: 0.5673 (nan)  loss_rpn_box_reg: 0.0762 (0.1323)  time: 0.3396 (0.3534)  data: 0.0008 (0.0044)  lr: 0.042144  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.168404344971009e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.168404344971009e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0842021724855044e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0842021724855044e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.421010862427522e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.421010862427522e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.710505431213761e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.710505431213761e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3552527156068805e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3552527156068805e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.776263578034403e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.776263578034403e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3881317890172014e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3881317890172014e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6940658945086007e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6940658945086007e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.470329472543003e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.470329472543003e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.235164736271502e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.235164736271502e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.117582368135751e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.117582368135751e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0587911840678754e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0587911840678754e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.293955920339377e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.293955920339377e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6469779601696886e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6469779601696886e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3234889800848443e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3234889800848443e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.617444900424222e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.617444900424222e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.308722450212111e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.308722450212111e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6543612251060553e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6543612251060553e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.271806125530277e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.271806125530277e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1359030627651384e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1359030627651384e-25
2019-11-15 21:20:37,908 maskrcnn_benchmark.trainer INFO: eta: 7:47:45  iter: 460  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 5.0787 (1.7905)  loss_objectness: 0.5838 (nan)  loss_rpn_box_reg: 0.1317 (0.1359)  time: 0.3412 (0.3529)  data: 0.0009 (0.0042)  lr: 0.044064  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0679515313825692e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0679515313825692e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0339757656912846e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0339757656912846e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.169878828456423e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.169878828456423e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5849394142282115e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5849394142282115e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2924697071141057e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2924697071141057e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.462348535570529e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.462348535570529e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2311742677852644e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2311742677852644e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6155871338926322e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6155871338926322e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.077935669463161e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.077935669463161e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0389678347315804e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0389678347315804e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0194839173657902e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0194839173657902e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0097419586828951e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0097419586828951e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.048709793414476e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.048709793414476e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.524354896707238e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.524354896707238e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.262177448353619e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.262177448353619e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.310887241768095e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.310887241768095e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1554436208840472e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1554436208840472e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5777218104420236e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5777218104420236e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.888609052210118e-31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.888609052210118e-31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.944304526105059e-31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.944304526105059e-31
2019-11-15 21:20:44,778 maskrcnn_benchmark.trainer INFO: eta: 7:47:07  iter: 480  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 4.9912 (1.9636)  loss_objectness: 0.5693 (nan)  loss_rpn_box_reg: 0.1053 (0.1372)  time: 0.3445 (0.3525)  data: 0.0009 (0.0041)  lr: 0.045984  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9721522630525295e-31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9721522630525295e-31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.860761315262648e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.860761315262648e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.930380657631324e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.930380657631324e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.465190328815662e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.465190328815662e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.232595164407831e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.232595164407831e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.162975822039155e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.162975822039155e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0814879110195774e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0814879110195774e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5407439555097887e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5407439555097887e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.703719777548943e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.703719777548943e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.851859888774472e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.851859888774472e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.925929944387236e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.925929944387236e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.62964972193618e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.62964972193618e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.81482486096809e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.81482486096809e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.407412430484045e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.407412430484045e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2037062152420224e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2037062152420224e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.018531076210112e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.018531076210112e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.009265538105056e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.009265538105056e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.504632769052528e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.504632769052528e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.52316384526264e-37
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.52316384526264e-37
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.76158192263132e-37
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.76158192263132e-37
2019-11-15 21:20:51,608 maskrcnn_benchmark.trainer INFO: eta: 7:46:25  iter: 500  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 3.8622 (2.1390)  loss_objectness: 0.6322 (nan)  loss_rpn_box_reg: 0.1370 (0.1515)  time: 0.3395 (0.3520)  data: 0.0009 (0.0040)  lr: 0.047904  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.88079096131566e-37
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.88079096131566e-37
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.4039548065783e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.4039548065783e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.70197740328915e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.70197740328915e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.350988701644575e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.350988701644575e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1754943508222875e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1754943508222875e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.877471754111438e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.877471754111438e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.938735877055719e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.938735877055719e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4693679385278594e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4693679385278594e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.346839692639297e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.346839692639297e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6734198463196485e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6734198463196485e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8367099231598242e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8367099231598242e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.183549615799121e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.183549615799121e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.591774807899561e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.591774807899561e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2958874039497803e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2958874039497803e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1479437019748901e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1479437019748901e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.739718509874451e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.739718509874451e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8698592549372254e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8698592549372254e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4349296274686127e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4349296274686127e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.174648137343064e-43
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.174648137343064e-43
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.587324068671532e-43
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.587324068671532e-43
2019-11-15 21:20:58,447 maskrcnn_benchmark.trainer INFO: eta: 7:45:48  iter: 520  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 4.0013 (2.2595)  loss_objectness: 0.5925 (nan)  loss_rpn_box_reg: 0.1229 (0.1554)  time: 0.3435 (0.3516)  data: 0.0009 (0.0039)  lr: 0.049824  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.793662034335766e-43
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.793662034335766e-43
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.96831017167883e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.96831017167883e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.484155085839415e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.484155085839415e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2420775429197073e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2420775429197073e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1210387714598537e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1210387714598537e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.605193857299268e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.605193857299268e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.802596928649634e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.802596928649634e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.401298464324817e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.401298464324817e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.006492321624085e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.006492321624085e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.503246160812043e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.503246160812043e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7516230804060213e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7516230804060213e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.758115402030107e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.758115402030107e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3790577010150533e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3790577010150533e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1895288505075267e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1895288505075267e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0947644252537633e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0947644252537633e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.473822126268817e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.473822126268817e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7369110631344083e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7369110631344083e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3684555315672042e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3684555315672042e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.842277657836021e-49
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.842277657836021e-49
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4211388289180104e-49
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4211388289180104e-49
2019-11-15 21:21:05,344 maskrcnn_benchmark.trainer INFO: eta: 7:45:21  iter: 540  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 3.8986 (2.3501)  loss_objectness: 0.5765 (nan)  loss_rpn_box_reg: 0.1188 (0.1572)  time: 0.3419 (0.3514)  data: 0.0009 (0.0037)  lr: 0.051744  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7105694144590052e-49
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7105694144590052e-49
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.552847072295026e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.552847072295026e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.276423536147513e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.276423536147513e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1382117680737565e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1382117680737565e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0691058840368783e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0691058840368783e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.345529420184391e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.345529420184391e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6727647100921956e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6727647100921956e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3363823550460978e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3363823550460978e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.681911775230489e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.681911775230489e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3409558876152446e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3409558876152446e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6704779438076223e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6704779438076223e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.352389719038111e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.352389719038111e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.176194859519056e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.176194859519056e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.088097429759528e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.088097429759528e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.044048714879764e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.044048714879764e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.22024357439882e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.22024357439882e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.61012178719941e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.61012178719941e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.305060893599705e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.305060893599705e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.525304467998525e-55
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.525304467998525e-55
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2626522339992623e-55
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2626522339992623e-55
2019-11-15 21:21:12,259 maskrcnn_benchmark.trainer INFO: eta: 7:44:58  iter: 560  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 5.4878 (2.4897)  loss_objectness: 0.5954 (nan)  loss_rpn_box_reg: 0.1510 (0.1616)  time: 0.3457 (0.3512)  data: 0.0009 (0.0036)  lr: 0.053664  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6313261169996311e-55
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6313261169996311e-55
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.156630584998156e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.156630584998156e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.078315292499078e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.078315292499078e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.039157646249539e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.039157646249539e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0195788231247695e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0195788231247695e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.0978941156238473e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.0978941156238473e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5489470578119236e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5489470578119236e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2744735289059618e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2744735289059618e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.372367644529809e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.372367644529809e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1861838222649046e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1861838222649046e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5930919111324523e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5930919111324523e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.965459555662261e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.965459555662261e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.982729777831131e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.982729777831131e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9913648889155653e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9913648889155653e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.956824444577827e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.956824444577827e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.9784122222889134e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.9784122222889134e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4892061111444567e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4892061111444567e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2446030555722283e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2446030555722283e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.223015277861142e-61
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.223015277861142e-61
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.111507638930571e-61
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.111507638930571e-61
2019-11-15 21:21:19,107 maskrcnn_benchmark.trainer INFO: eta: 7:44:26  iter: 580  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 4.9113 (2.5987)  loss_objectness: 0.5803 (nan)  loss_rpn_box_reg: 0.1382 (0.2348)  time: 0.3423 (0.3509)  data: 0.0009 (0.0035)  lr: 0.055584  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5557538194652854e-61
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5557538194652854e-61
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.778769097326427e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.778769097326427e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8893845486632136e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8893845486632136e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9446922743316068e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9446922743316068e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.723461371658034e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.723461371658034e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.861730685829017e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.861730685829017e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4308653429145085e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4308653429145085e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2154326714572542e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2154326714572542e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.077163357286271e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.077163357286271e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0385816786431356e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0385816786431356e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5192908393215678e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5192908393215678e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.596454196607839e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.596454196607839e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7982270983039195e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7982270983039195e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8991135491519597e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8991135491519597e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.495567745759799e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.495567745759799e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.7477838728798994e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.7477838728798994e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3738919364399497e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3738919364399497e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1869459682199748e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1869459682199748e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.934729841099874e-67
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.934729841099874e-67
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.967364920549937e-67
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.967364920549937e-67
2019-11-15 21:21:26,014 maskrcnn_benchmark.trainer INFO: eta: 7:44:05  iter: 600  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 5.3165 (2.7643)  loss_objectness: 0.6048 (nan)  loss_rpn_box_reg: 0.0955 (0.2855)  time: 0.3438 (0.3507)  data: 0.0009 (0.0035)  lr: 0.057504  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4836824602749686e-67
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4836824602749686e-67
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.418412301374843e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.418412301374843e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7092061506874214e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7092061506874214e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8546030753437107e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8546030753437107e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.273015376718553e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.273015376718553e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.636507688359277e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.636507688359277e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3182538441796384e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3182538441796384e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1591269220898192e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1591269220898192e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.795634610449096e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.795634610449096e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.897817305224548e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.897817305224548e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.448908652612274e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.448908652612274e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.24454326306137e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.24454326306137e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.622271631530685e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.622271631530685e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8111358157653425e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8111358157653425e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.055679078826712e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.055679078826712e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.527839539413356e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.527839539413356e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.263919769706678e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.263919769706678e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.131959884853339e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.131959884853339e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.659799424266695e-73
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.659799424266695e-73
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8298997121333476e-73
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8298997121333476e-73
2019-11-15 21:21:32,952 maskrcnn_benchmark.trainer INFO: eta: 7:43:48  iter: 620  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 4.3104 (2.9593)  loss_objectness: 0.5552 (nan)  loss_rpn_box_reg: 0.1089 (0.3531)  time: 0.3437 (0.3506)  data: 0.0009 (0.0034)  lr: 0.059424  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4149498560666738e-73
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4149498560666738e-73
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.074749280333369e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.074749280333369e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5373746401666845e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5373746401666845e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7686873200833423e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7686873200833423e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.843436600416711e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.843436600416711e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.421718300208356e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.421718300208356e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.210859150104178e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.210859150104178e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.105429575052089e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.105429575052089e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.527147875260445e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.527147875260445e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7635739376302223e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7635739376302223e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3817869688151111e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3817869688151111e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.908934844075556e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.908934844075556e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.454467422037778e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.454467422037778e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.727233711018889e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.727233711018889e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.636168555094445e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.636168555094445e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3180842775472223e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3180842775472223e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1590421387736112e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1590421387736112e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0795210693868056e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0795210693868056e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.397605346934028e-79
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.397605346934028e-79
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.698802673467014e-79
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.698802673467014e-79
2019-11-15 21:21:39,767 maskrcnn_benchmark.trainer INFO: eta: 7:43:17  iter: 640  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 4.3454 (3.0336)  loss_objectness: 0.6133 (nan)  loss_rpn_box_reg: 0.0929 (0.3509)  time: 0.3408 (0.3503)  data: 0.0009 (0.0033)  lr: 0.060000  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.349401336733507e-79
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.349401336733507e-79
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.747006683667535e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.747006683667535e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3735033418337674e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3735033418337674e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6867516709168837e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6867516709168837e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.433758354584419e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.433758354584419e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.2168791772922093e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.2168791772922093e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1084395886461046e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1084395886461046e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0542197943230523e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0542197943230523e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.271098971615262e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.271098971615262e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.635549485807631e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.635549485807631e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3177747429038154e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3177747429038154e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.588873714519077e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.588873714519077e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2944368572595385e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2944368572595385e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6472184286297693e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6472184286297693e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.236092143148846e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.236092143148846e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.118046071574423e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.118046071574423e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0590230357872116e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0590230357872116e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0295115178936058e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0295115178936058e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.147557589468029e-85
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.147557589468029e-85
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5737787947340145e-85
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5737787947340145e-85
2019-11-15 21:21:46,748 maskrcnn_benchmark.trainer INFO: eta: 7:43:07  iter: 660  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 4.6751 (3.1596)  loss_objectness: 0.5782 (nan)  loss_rpn_box_reg: 0.1609 (0.3492)  time: 0.3480 (0.3502)  data: 0.0009 (0.0032)  lr: 0.060000  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2868893973670072e-85
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2868893973670072e-85
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.434446986835036e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.434446986835036e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.217223493417518e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.217223493417518e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.608611746708759e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.608611746708759e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.043058733543795e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.043058733543795e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.021529366771898e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.021529366771898e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.010764683385949e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.010764683385949e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0053823416929744e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0053823416929744e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.026911708464872e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.026911708464872e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.513455854232436e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.513455854232436e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.256727927116218e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.256727927116218e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.28363963558109e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.28363963558109e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.141819817790545e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.141819817790545e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5709099088952725e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5709099088952725e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.854549544476363e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.854549544476363e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.9272747722381812e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.9272747722381812e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9636373861190906e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9636373861190906e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.818186930595453e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.818186930595453e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.909093465297727e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.909093465297727e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4545467326488633e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4545467326488633e-91
2019-11-15 21:21:53,588 maskrcnn_benchmark.trainer INFO: eta: 7:42:41  iter: 680  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 5.1638 (3.2884)  loss_objectness: 0.5863 (nan)  loss_rpn_box_reg: 0.0933 (0.3655)  time: 0.3403 (0.3500)  data: 0.0009 (0.0032)  lr: 0.060000  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2272733663244316e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2272733663244316e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.136366831622158e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.136366831622158e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.068183415811079e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.068183415811079e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5340917079055395e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5340917079055395e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.670458539527698e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.670458539527698e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.835229269763849e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.835229269763849e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9176146348819244e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9176146348819244e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.588073174409622e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.588073174409622e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.794036587204811e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.794036587204811e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3970182936024055e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3970182936024055e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1985091468012028e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1985091468012028e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.992545734006014e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.992545734006014e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.996272867003007e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.996272867003007e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4981364335015035e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4981364335015035e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.490682167507517e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.490682167507517e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.745341083753759e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.745341083753759e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8726705418768793e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8726705418768793e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.363352709384397e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.363352709384397e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.6816763546921983e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.6816763546921983e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3408381773460992e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3408381773460992e-97
2019-11-15 21:22:00,436 maskrcnn_benchmark.trainer INFO: eta: 7:42:16  iter: 700  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 6.9333 (3.4262)  loss_objectness: 0.6220 (nan)  loss_rpn_box_reg: 0.2653 (0.3782)  time: 0.3424 (0.3498)  data: 0.0009 (0.0031)  lr: 0.060000  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1704190886730496e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1704190886730496e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.852095443365248e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.852095443365248e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.926047721682624e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.926047721682624e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.463023860841312e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.463023860841312e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.31511930420656e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.31511930420656e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.65755965210328e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.65755965210328e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.82877982605164e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.82877982605164e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.1438991302582e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.1438991302582e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5719495651291e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5719495651291e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.28597478256455e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.28597478256455e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.142987391282275e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.142987391282275e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.714936956411375e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.714936956411375e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8574684782056875e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8574684782056875e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4287342391028437e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4287342391028437e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.143671195514219e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.143671195514219e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5718355977571093e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5718355977571093e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7859177988785547e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7859177988785547e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.929588994392773e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.929588994392773e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.464794497196387e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.464794497196387e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2323972485981933e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2323972485981933e-103
2019-11-15 21:22:07,491 maskrcnn_benchmark.trainer INFO: eta: 7:42:16  iter: 720  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 4.1964 (3.5133)  loss_objectness: 0.5638 (nan)  loss_rpn_box_reg: 0.0745 (0.3724)  time: 0.3458 (0.3499)  data: 0.0009 (0.0030)  lr: 0.060000  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1161986242990967e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1161986242990967e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.5809931214954833e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.5809931214954833e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7904965607477417e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7904965607477417e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3952482803738708e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3952482803738708e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.976241401869354e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.976241401869354e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.488120700934677e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.488120700934677e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7440603504673385e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7440603504673385e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.720301752336693e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.720301752336693e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3601508761683463e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3601508761683463e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1800754380841732e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1800754380841732e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0900377190420866e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0900377190420866e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.450188595210433e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.450188595210433e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7250942976052165e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7250942976052165e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3625471488026082e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3625471488026082e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.812735744013041e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.812735744013041e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4063678720065206e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4063678720065206e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7031839360032603e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7031839360032603e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.515919680016301e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.515919680016301e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.257959840008151e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.257959840008151e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1289799200040754e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1289799200040754e-109
2019-11-15 21:22:14,371 maskrcnn_benchmark.trainer INFO: eta: 7:41:56  iter: 740  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 5.3935 (3.5929)  loss_objectness: 0.5523 (nan)  loss_rpn_box_reg: 0.0895 (0.3744)  time: 0.3439 (0.3497)  data: 0.0010 (0.0030)  lr: 0.060000  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0644899600020377e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0644899600020377e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.3224498000101884e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.3224498000101884e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6612249000050942e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6612249000050942e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3306124500025471e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3306124500025471e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.653062250012736e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.653062250012736e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.326531125006368e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.326531125006368e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.663265562503184e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.663265562503184e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.31632781251592e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.31632781251592e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.15816390625796e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.15816390625796e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.07908195312898e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.07908195312898e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.03954097656449e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.03954097656449e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.19770488282245e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.19770488282245e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.598852441411225e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.598852441411225e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2994262207056124e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2994262207056124e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.497131103528062e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.497131103528062e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.248565551764031e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.248565551764031e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6242827758820155e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6242827758820155e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.121413879410078e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.121413879410078e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.060706939705039e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.060706939705039e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0303534698525194e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0303534698525194e-115
2019-11-15 21:22:21,332 maskrcnn_benchmark.trainer INFO: eta: 7:41:46  iter: 760  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 4.5828 (3.6645)  loss_objectness: 0.5451 (nan)  loss_rpn_box_reg: 0.0882 (0.3677)  time: 0.3462 (0.3497)  data: 0.0009 (0.0029)  lr: 0.060000  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0151767349262597e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0151767349262597e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.075883674631299e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.075883674631299e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5379418373156492e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5379418373156492e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2689709186578246e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2689709186578246e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.344854593289123e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.344854593289123e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1724272966445615e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1724272966445615e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5862136483222808e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5862136483222808e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.931068241611404e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.931068241611404e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.965534120805702e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.965534120805702e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.982767060402851e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.982767060402851e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.913835302014255e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.913835302014255e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.9569176510071274e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.9569176510071274e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4784588255035637e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4784588255035637e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2392294127517818e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2392294127517818e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.196147063758909e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.196147063758909e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0980735318794546e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0980735318794546e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5490367659397273e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5490367659397273e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.745183829698637e-121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.745183829698637e-121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8725919148493183e-121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8725919148493183e-121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9362959574246591e-121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9362959574246591e-121
2019-11-15 21:22:28,248 maskrcnn_benchmark.trainer INFO: eta: 7:41:31  iter: 780  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 5.6297 (3.7250)  loss_objectness: 0.5822 (nan)  loss_rpn_box_reg: 0.1096 (0.3946)  time: 0.3443 (0.3496)  data: 0.0009 (0.0029)  lr: 0.060000  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.681479787123296e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.681479787123296e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.840739893561648e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.840739893561648e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.420369946780824e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.420369946780824e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.210184973390412e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.210184973390412e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.05092486695206e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.05092486695206e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.02546243347603e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.02546243347603e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.512731216738015e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.512731216738015e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.563656083690075e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.563656083690075e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7818280418450374e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7818280418450374e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8909140209225187e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8909140209225187e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.454570104612593e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.454570104612593e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.727285052306297e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.727285052306297e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3636425261531484e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3636425261531484e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1818212630765742e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1818212630765742e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.909106315382871e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.909106315382871e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9545531576914354e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9545531576914354e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4772765788457177e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4772765788457177e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.386382894228589e-127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.386382894228589e-127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6931914471142943e-127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6931914471142943e-127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8465957235571472e-127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8465957235571472e-127
2019-11-15 21:22:35,171 maskrcnn_benchmark.trainer INFO: eta: 7:41:17  iter: 800  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 5.7898 (3.8238)  loss_objectness: 0.5838 (nan)  loss_rpn_box_reg: 0.1146 (0.4252)  time: 0.3458 (0.3495)  data: 0.0009 (0.0028)  lr: 0.060000  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.232978617785736e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.232978617785736e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.616489308892868e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.616489308892868e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.308244654446434e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.308244654446434e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.154122327223217e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.154122327223217e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.770611636116085e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.770611636116085e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8853058180580424e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8853058180580424e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4426529090290212e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4426529090290212e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.213264545145106e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.213264545145106e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.606632272572553e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.606632272572553e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8033161362862765e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8033161362862765e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.016580681431383e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.016580681431383e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5082903407156913e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5082903407156913e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2541451703578456e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2541451703578456e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1270725851789228e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1270725851789228e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.635362925894614e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.635362925894614e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.817681462947307e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.817681462947307e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4088407314736535e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4088407314736535e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.044203657368268e-133
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.044203657368268e-133
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.522101828684134e-133
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.522101828684134e-133
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.761050914342067e-133
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.761050914342067e-133
2019-11-15 21:22:42,172 maskrcnn_benchmark.trainer INFO: eta: 7:41:12  iter: 820  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 5.2661 (3.9205)  loss_objectness: 0.5621 (nan)  loss_rpn_box_reg: 0.1009 (0.4207)  time: 0.3456 (0.3495)  data: 0.0009 (0.0028)  lr: 0.060000  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.805254571710335e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.805254571710335e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4026272858551673e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4026272858551673e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2013136429275836e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2013136429275836e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1006568214637918e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1006568214637918e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.503284107318959e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.503284107318959e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7516420536594796e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7516420536594796e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3758210268297398e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3758210268297398e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.879105134148699e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.879105134148699e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4395525670743494e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4395525670743494e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7197762835371747e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7197762835371747e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.598881417685874e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.598881417685874e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.299440708842937e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.299440708842937e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1497203544214684e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1497203544214684e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0748601772107342e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0748601772107342e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.374300886053671e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.374300886053671e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6871504430268355e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6871504430268355e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3435752215134178e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3435752215134178e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.717876107567089e-139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.717876107567089e-139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3589380537835444e-139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3589380537835444e-139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6794690268917722e-139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6794690268917722e-139
2019-11-15 21:22:49,110 maskrcnn_benchmark.trainer INFO: eta: 7:41:00  iter: 840  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 5.4220 (4.0092)  loss_objectness: 0.6183 (nan)  loss_rpn_box_reg: 0.0719 (0.4288)  time: 0.3442 (0.3494)  data: 0.0009 (0.0027)  lr: 0.060000  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.397345134458861e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.397345134458861e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1986725672294305e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1986725672294305e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0993362836147152e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0993362836147152e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0496681418073576e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0496681418073576e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.248340709036788e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.248340709036788e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.624170354518394e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.624170354518394e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.312085177259197e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.312085177259197e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.560425886295985e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.560425886295985e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2802129431479926e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2802129431479926e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6401064715739963e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6401064715739963e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.200532357869981e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.200532357869981e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.100266178934991e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.100266178934991e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0501330894674953e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0501330894674953e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0250665447337477e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0250665447337477e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.1253327236687384e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.1253327236687384e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5626663618343692e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5626663618343692e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2813331809171846e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2813331809171846e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.406665904585923e-145
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.406665904585923e-145
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2033329522929615e-145
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2033329522929615e-145
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6016664761464807e-145
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6016664761464807e-145
2019-11-15 21:22:56,001 maskrcnn_benchmark.trainer INFO: eta: 7:40:44  iter: 860  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 6.8069 (4.0982)  loss_objectness: 0.6315 (nan)  loss_rpn_box_reg: 0.0923 (0.4702)  time: 0.3450 (0.3493)  data: 0.0009 (0.0027)  lr: 0.060000  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.008332380732404e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.008332380732404e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.004166190366202e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.004166190366202e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.002083095183101e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.002083095183101e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0010415475915505e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0010415475915505e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.0052077379577523e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.0052077379577523e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5026038689788762e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5026038689788762e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2513019344894381e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2513019344894381e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.256509672447191e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.256509672447191e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1282548362235952e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1282548362235952e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5641274181117976e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5641274181117976e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.820637090558988e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.820637090558988e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.910318545279494e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.910318545279494e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.955159272639747e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.955159272639747e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.775796363198735e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.775796363198735e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.887898181599368e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.887898181599368e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.443949090799684e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.443949090799684e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.221974545399842e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.221974545399842e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.10987272699921e-151
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.10987272699921e-151
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.054936363499605e-151
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.054936363499605e-151
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5274681817498023e-151
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5274681817498023e-151
2019-11-15 21:23:03,041 maskrcnn_benchmark.trainer INFO: eta: 7:40:42  iter: 880  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 4.9994 (4.1663)  loss_objectness: 0.5787 (nan)  loss_rpn_box_reg: 0.1017 (0.4820)  time: 0.3477 (0.3494)  data: 0.0009 (0.0027)  lr: 0.060000  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.637340908749012e-152
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.637340908749012e-152
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.818670454374506e-152
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.818670454374506e-152
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.909335227187253e-152
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.909335227187253e-152
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.546676135936265e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.546676135936265e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.7733380679681323e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.7733380679681323e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3866690339840662e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3866690339840662e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1933345169920331e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1933345169920331e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.966672584960166e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.966672584960166e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.983336292480083e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.983336292480083e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4916681462400413e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4916681462400413e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.458340731200207e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.458340731200207e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7291703656001034e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7291703656001034e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8645851828000517e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8645851828000517e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.322925914000258e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.322925914000258e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.661462957000129e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.661462957000129e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3307314785000646e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3307314785000646e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1653657392500323e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1653657392500323e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.826828696250162e-157
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.826828696250162e-157
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.913414348125081e-157
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.913414348125081e-157
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4567071740625404e-157
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4567071740625404e-157
2019-11-15 21:23:10,036 maskrcnn_benchmark.trainer INFO: eta: 7:40:35  iter: 900  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 6.2597 (4.2287)  loss_objectness: 0.6176 (nan)  loss_rpn_box_reg: 0.1561 (0.4861)  time: 0.3495 (0.3494)  data: 0.0009 (0.0026)  lr: 0.060000  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.283535870312702e-158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.283535870312702e-158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.641767935156351e-158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.641767935156351e-158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8208839675781755e-158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8208839675781755e-158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.104419837890877e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.104419837890877e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.552209918945439e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.552209918945439e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2761049594727193e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2761049594727193e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1380524797363597e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1380524797363597e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.6902623986817984e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.6902623986817984e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8451311993408992e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8451311993408992e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4225655996704496e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4225655996704496e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.112827998352248e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.112827998352248e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.556413999176124e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.556413999176124e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.778206999588062e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.778206999588062e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.89103499794031e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.89103499794031e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.445517498970155e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.445517498970155e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2227587494850775e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2227587494850775e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1113793747425387e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1113793747425387e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.556896873712694e-163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.556896873712694e-163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.778448436856347e-163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.778448436856347e-163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3892242184281734e-163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3892242184281734e-163
2019-11-15 21:23:16,964 maskrcnn_benchmark.trainer INFO: eta: 7:40:23  iter: 920  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 6.7687 (4.2916)  loss_objectness: 0.6743 (nan)  loss_rpn_box_reg: 0.0697 (0.4786)  time: 0.3471 (0.3493)  data: 0.0009 (0.0026)  lr: 0.060000  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.946121092140867e-164
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.946121092140867e-164
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4730605460704336e-164
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4730605460704336e-164
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7365302730352168e-164
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7365302730352168e-164
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.682651365176084e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.682651365176084e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.341325682588042e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.341325682588042e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.170662841294021e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.170662841294021e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0853314206470105e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0853314206470105e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.426657103235053e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.426657103235053e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7133285516175262e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7133285516175262e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3566642758087631e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3566642758087631e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.783321379043816e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.783321379043816e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.391660689521908e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.391660689521908e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.695830344760954e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.695830344760954e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.47915172380477e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.47915172380477e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.239575861902385e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.239575861902385e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1197879309511924e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1197879309511924e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0598939654755962e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0598939654755962e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.299469827377981e-169
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.299469827377981e-169
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6497349136889905e-169
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6497349136889905e-169
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3248674568444952e-169
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3248674568444952e-169
2019-11-15 21:23:23,866 maskrcnn_benchmark.trainer INFO: eta: 7:40:09  iter: 940  loss: nan (nan)  loss_classifier: nan (nan)  loss_box_reg: nan (nan)  loss_mask: 5.3918 (4.3354)  loss_objectness: 0.5550 (nan)  loss_rpn_box_reg: 0.0831 (0.4723)  time: 0.3430 (0.3492)  data: 0.0009 (0.0026)  lr: 0.060000  max mem: 7973
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.624337284222476e-170
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.624337284222476e-170
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.312168642111238e-170
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.312168642111238e-170
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.656084321055619e-170
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.656084321055619e-170
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.280421605278095e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.280421605278095e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.140210802639048e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.140210802639048e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.070105401319524e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.070105401319524e-171

Beginning trial 2 of 3
Gathering sys log on 4029gp-tvrt-1
:::MLL 1575775215.644 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1575775215.645 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1575775215.645 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1575775215.645 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1575775215.646 submission_platform: {"value": "1xSYS-4029GP-TVRT", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1575775215.646 submission_entry: {"value": "{'hardware': 'SYS-4029GP-TVRT', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': 'Ubuntu 18.04.3 LTS / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.7-1.0.0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz', 'num_cores': '40', 'num_vcpus': '80', 'accelerator': 'Tesla V100-SXM2-32GB', 'num_accelerators': '8', 'sys_mem_size': '754 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '1x 3.7T', 'cpu_accel_interconnect': 'UPI', 'network_card': '', 'num_network_cards': '0', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1575775215.647 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1575775215.647 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1575775216.442 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node 4029gp-tvrt-1
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=LambdaHyperplaneBasic -e 'MULTI_NODE= --master_port=4876' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191207185714287884518 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191207185714287884518 ./run_and_time.sh
Run vars: id 191207185714287884518 gpus 8 mparams  --master_port=4876
STARTING TIMING RUN AT 2019-12-08 03:20:17 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
running benchmark
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 20 --nproc_per_node 8  --master_port=4876'
+ echo 'running benchmark'
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --master_port=4876 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1575775218.710 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575775218.710 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575775218.714 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575775218.718 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575775218.721 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575775218.721 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575775218.721 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1575775218.729 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 4210053157
0: Worker 0 is using worker seed: 1746806829
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1575775228.996 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1575775229.843 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1575775229.843 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1575775229.843 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1575775230.213 global_batch_size: {"value": 2048, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1575775230.214 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1575775230.215 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1575775230.215 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1575775230.215 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1575775230.215 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1575775230.216 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1575775230.216 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1575775230.217 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1575775230.217 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 2675315268
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/1938]	Time 0.480 (0.480)	Data 3.36e-01 (3.36e-01)	Tok/s 21384 (21384)	Loss/tok 10.6892 (10.6892)	LR 2.000e-05
0: TRAIN [0][10/1938]	Time 0.181 (0.224)	Data 1.02e-04 (3.07e-02)	Tok/s 93177 (88181)	Loss/tok 9.7698 (10.2296)	LR 2.518e-05
0: TRAIN [0][20/1938]	Time 0.181 (0.198)	Data 1.04e-04 (1.61e-02)	Tok/s 93688 (89278)	Loss/tok 9.4057 (9.9195)	LR 3.170e-05
0: TRAIN [0][30/1938]	Time 0.180 (0.184)	Data 1.00e-04 (1.09e-02)	Tok/s 92487 (88537)	Loss/tok 9.1149 (9.7252)	LR 3.991e-05
0: TRAIN [0][40/1938]	Time 0.122 (0.178)	Data 1.01e-04 (8.30e-03)	Tok/s 82873 (88323)	Loss/tok 8.7092 (9.5553)	LR 5.024e-05
0: TRAIN [0][50/1938]	Time 0.124 (0.176)	Data 9.94e-05 (6.70e-03)	Tok/s 82478 (88614)	Loss/tok 8.5594 (9.3913)	LR 6.325e-05
0: TRAIN [0][60/1938]	Time 0.124 (0.175)	Data 9.80e-05 (5.61e-03)	Tok/s 82589 (88864)	Loss/tok 8.2502 (9.2475)	LR 7.962e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][70/1938]	Time 0.122 (0.172)	Data 9.94e-05 (4.84e-03)	Tok/s 83616 (88836)	Loss/tok 8.1123 (9.1498)	LR 9.796e-05
0: TRAIN [0][80/1938]	Time 0.179 (0.170)	Data 9.94e-05 (4.25e-03)	Tok/s 95780 (89056)	Loss/tok 8.1468 (9.0331)	LR 1.233e-04
0: TRAIN [0][90/1938]	Time 0.122 (0.170)	Data 1.01e-04 (3.80e-03)	Tok/s 85174 (89047)	Loss/tok 7.9510 (8.9277)	LR 1.552e-04
0: TRAIN [0][100/1938]	Time 0.122 (0.170)	Data 9.78e-05 (3.43e-03)	Tok/s 82109 (89178)	Loss/tok 7.7455 (8.8355)	LR 1.954e-04
0: TRAIN [0][110/1938]	Time 0.121 (0.167)	Data 9.68e-05 (3.13e-03)	Tok/s 87540 (88783)	Loss/tok 7.7776 (8.7675)	LR 2.461e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
0: TRAIN [0][120/1938]	Time 0.240 (0.166)	Data 9.80e-05 (2.88e-03)	Tok/s 97354 (88678)	Loss/tok 8.1276 (8.7093)	LR 3.027e-04
0: TRAIN [0][130/1938]	Time 0.070 (0.165)	Data 9.75e-05 (2.67e-03)	Tok/s 75078 (88629)	Loss/tok 7.1015 (8.6511)	LR 3.811e-04
0: TRAIN [0][140/1938]	Time 0.122 (0.165)	Data 9.89e-05 (2.49e-03)	Tok/s 82713 (88572)	Loss/tok 7.7944 (8.6257)	LR 4.798e-04
0: TRAIN [0][150/1938]	Time 0.121 (0.165)	Data 9.56e-05 (2.33e-03)	Tok/s 86880 (88636)	Loss/tok 7.6139 (8.5707)	LR 6.040e-04
0: TRAIN [0][160/1938]	Time 0.124 (0.165)	Data 9.58e-05 (2.19e-03)	Tok/s 82961 (88789)	Loss/tok 7.5119 (8.5180)	LR 7.604e-04
0: TRAIN [0][170/1938]	Time 0.182 (0.165)	Data 9.47e-05 (2.07e-03)	Tok/s 91938 (88792)	Loss/tok 7.6270 (8.4690)	LR 9.573e-04
0: TRAIN [0][180/1938]	Time 0.067 (0.164)	Data 9.63e-05 (1.96e-03)	Tok/s 78933 (88761)	Loss/tok 6.7186 (8.4212)	LR 1.205e-03
0: TRAIN [0][190/1938]	Time 0.180 (0.163)	Data 9.49e-05 (1.86e-03)	Tok/s 92696 (88745)	Loss/tok 7.3466 (8.3707)	LR 1.517e-03
0: TRAIN [0][200/1938]	Time 0.315 (0.163)	Data 9.49e-05 (1.77e-03)	Tok/s 94027 (88668)	Loss/tok 7.4072 (8.3151)	LR 1.910e-03
0: TRAIN [0][210/1938]	Time 0.068 (0.161)	Data 9.44e-05 (1.69e-03)	Tok/s 77834 (88485)	Loss/tok 6.0211 (8.2671)	LR 2.000e-03
0: TRAIN [0][220/1938]	Time 0.178 (0.161)	Data 9.42e-05 (1.62e-03)	Tok/s 93545 (88535)	Loss/tok 6.9896 (8.2099)	LR 2.000e-03
0: TRAIN [0][230/1938]	Time 0.182 (0.160)	Data 9.68e-05 (1.55e-03)	Tok/s 92075 (88441)	Loss/tok 6.8392 (8.1583)	LR 2.000e-03
0: TRAIN [0][240/1938]	Time 0.122 (0.161)	Data 9.68e-05 (1.49e-03)	Tok/s 83619 (88540)	Loss/tok 6.3825 (8.0928)	LR 2.000e-03
0: TRAIN [0][250/1938]	Time 0.123 (0.160)	Data 9.56e-05 (1.44e-03)	Tok/s 83801 (88515)	Loss/tok 6.2887 (8.0343)	LR 2.000e-03
0: TRAIN [0][260/1938]	Time 0.123 (0.161)	Data 9.75e-05 (1.39e-03)	Tok/s 84042 (88471)	Loss/tok 6.2248 (7.9684)	LR 2.000e-03
0: TRAIN [0][270/1938]	Time 0.179 (0.162)	Data 9.82e-05 (1.34e-03)	Tok/s 95074 (88567)	Loss/tok 6.3765 (7.9013)	LR 2.000e-03
0: TRAIN [0][280/1938]	Time 0.123 (0.162)	Data 9.63e-05 (1.30e-03)	Tok/s 83160 (88615)	Loss/tok 5.9138 (7.8385)	LR 2.000e-03
0: TRAIN [0][290/1938]	Time 0.122 (0.162)	Data 9.66e-05 (1.25e-03)	Tok/s 81670 (88668)	Loss/tok 5.8203 (7.7754)	LR 2.000e-03
0: TRAIN [0][300/1938]	Time 0.179 (0.162)	Data 1.00e-04 (1.22e-03)	Tok/s 92446 (88679)	Loss/tok 6.0031 (7.7147)	LR 2.000e-03
0: TRAIN [0][310/1938]	Time 0.124 (0.162)	Data 9.35e-05 (1.18e-03)	Tok/s 84141 (88631)	Loss/tok 5.6759 (7.6580)	LR 2.000e-03
0: TRAIN [0][320/1938]	Time 0.182 (0.162)	Data 9.42e-05 (1.15e-03)	Tok/s 91890 (88630)	Loss/tok 5.8630 (7.6025)	LR 2.000e-03
0: TRAIN [0][330/1938]	Time 0.126 (0.161)	Data 1.16e-04 (1.11e-03)	Tok/s 81007 (88473)	Loss/tok 5.4261 (7.5551)	LR 2.000e-03
0: TRAIN [0][340/1938]	Time 0.123 (0.161)	Data 9.56e-05 (1.08e-03)	Tok/s 86061 (88460)	Loss/tok 5.4200 (7.5029)	LR 2.000e-03
0: TRAIN [0][350/1938]	Time 0.240 (0.162)	Data 9.80e-05 (1.06e-03)	Tok/s 97875 (88549)	Loss/tok 5.7413 (7.4371)	LR 2.000e-03
0: TRAIN [0][360/1938]	Time 0.069 (0.161)	Data 9.73e-05 (1.03e-03)	Tok/s 76795 (88467)	Loss/tok 4.2737 (7.3875)	LR 2.000e-03
0: TRAIN [0][370/1938]	Time 0.068 (0.161)	Data 9.87e-05 (1.00e-03)	Tok/s 77892 (88450)	Loss/tok 4.2818 (7.3354)	LR 2.000e-03
0: TRAIN [0][380/1938]	Time 0.125 (0.162)	Data 9.61e-05 (9.81e-04)	Tok/s 82091 (88438)	Loss/tok 5.1261 (7.2850)	LR 2.000e-03
0: TRAIN [0][390/1938]	Time 0.123 (0.161)	Data 1.17e-04 (9.58e-04)	Tok/s 85483 (88389)	Loss/tok 4.7949 (7.2378)	LR 2.000e-03
0: TRAIN [0][400/1938]	Time 0.126 (0.161)	Data 1.01e-04 (9.37e-04)	Tok/s 81516 (88379)	Loss/tok 4.9000 (7.1879)	LR 2.000e-03
0: TRAIN [0][410/1938]	Time 0.242 (0.162)	Data 9.68e-05 (9.16e-04)	Tok/s 96660 (88415)	Loss/tok 5.3371 (7.1332)	LR 2.000e-03
0: TRAIN [0][420/1938]	Time 0.124 (0.162)	Data 1.14e-04 (8.97e-04)	Tok/s 84283 (88412)	Loss/tok 4.7540 (7.0824)	LR 2.000e-03
0: TRAIN [0][430/1938]	Time 0.123 (0.162)	Data 9.89e-05 (8.79e-04)	Tok/s 84761 (88388)	Loss/tok 4.6201 (7.0366)	LR 2.000e-03
0: TRAIN [0][440/1938]	Time 0.126 (0.162)	Data 1.00e-04 (8.61e-04)	Tok/s 82658 (88401)	Loss/tok 4.6496 (6.9879)	LR 2.000e-03
0: TRAIN [0][450/1938]	Time 0.181 (0.161)	Data 9.35e-05 (8.44e-04)	Tok/s 93079 (88355)	Loss/tok 4.7421 (6.9468)	LR 2.000e-03
0: TRAIN [0][460/1938]	Time 0.181 (0.161)	Data 9.56e-05 (8.28e-04)	Tok/s 91529 (88385)	Loss/tok 4.8362 (6.8969)	LR 2.000e-03
0: TRAIN [0][470/1938]	Time 0.125 (0.161)	Data 9.11e-05 (8.12e-04)	Tok/s 82654 (88305)	Loss/tok 4.4203 (6.8583)	LR 2.000e-03
0: TRAIN [0][480/1938]	Time 0.123 (0.162)	Data 1.00e-04 (7.98e-04)	Tok/s 83260 (88378)	Loss/tok 4.4447 (6.8048)	LR 2.000e-03
0: TRAIN [0][490/1938]	Time 0.067 (0.161)	Data 9.68e-05 (7.83e-04)	Tok/s 78771 (88335)	Loss/tok 3.5478 (6.7659)	LR 2.000e-03
0: TRAIN [0][500/1938]	Time 0.124 (0.161)	Data 9.78e-05 (7.70e-04)	Tok/s 83903 (88308)	Loss/tok 4.2835 (6.7248)	LR 2.000e-03
0: TRAIN [0][510/1938]	Time 0.242 (0.161)	Data 1.01e-04 (7.57e-04)	Tok/s 98205 (88285)	Loss/tok 4.7955 (6.6849)	LR 2.000e-03
0: TRAIN [0][520/1938]	Time 0.181 (0.161)	Data 9.61e-05 (7.44e-04)	Tok/s 92254 (88303)	Loss/tok 4.5232 (6.6405)	LR 2.000e-03
0: TRAIN [0][530/1938]	Time 0.179 (0.161)	Data 1.02e-04 (7.32e-04)	Tok/s 93820 (88341)	Loss/tok 4.4823 (6.5966)	LR 2.000e-03
0: TRAIN [0][540/1938]	Time 0.124 (0.162)	Data 1.01e-04 (7.20e-04)	Tok/s 82379 (88382)	Loss/tok 4.0389 (6.5514)	LR 2.000e-03
0: TRAIN [0][550/1938]	Time 0.178 (0.162)	Data 1.03e-04 (7.09e-04)	Tok/s 93012 (88361)	Loss/tok 4.5026 (6.5134)	LR 2.000e-03
0: TRAIN [0][560/1938]	Time 0.124 (0.162)	Data 1.02e-04 (6.98e-04)	Tok/s 82737 (88381)	Loss/tok 4.0964 (6.4728)	LR 2.000e-03
0: TRAIN [0][570/1938]	Time 0.123 (0.161)	Data 1.13e-04 (6.88e-04)	Tok/s 83485 (88343)	Loss/tok 4.1890 (6.4411)	LR 2.000e-03
0: TRAIN [0][580/1938]	Time 0.183 (0.162)	Data 1.11e-04 (6.78e-04)	Tok/s 92162 (88394)	Loss/tok 4.3718 (6.4010)	LR 2.000e-03
0: TRAIN [0][590/1938]	Time 0.122 (0.162)	Data 1.00e-04 (6.68e-04)	Tok/s 84018 (88404)	Loss/tok 3.9741 (6.3650)	LR 2.000e-03
0: TRAIN [0][600/1938]	Time 0.123 (0.162)	Data 9.92e-05 (6.59e-04)	Tok/s 84785 (88450)	Loss/tok 3.9965 (6.3247)	LR 2.000e-03
0: TRAIN [0][610/1938]	Time 0.180 (0.162)	Data 9.80e-05 (6.49e-04)	Tok/s 92768 (88444)	Loss/tok 4.3371 (6.2931)	LR 2.000e-03
0: TRAIN [0][620/1938]	Time 0.124 (0.162)	Data 1.15e-04 (6.41e-04)	Tok/s 83258 (88413)	Loss/tok 3.8922 (6.2634)	LR 2.000e-03
0: TRAIN [0][630/1938]	Time 0.122 (0.162)	Data 9.54e-05 (6.32e-04)	Tok/s 83696 (88417)	Loss/tok 3.9747 (6.2292)	LR 2.000e-03
0: TRAIN [0][640/1938]	Time 0.311 (0.162)	Data 9.70e-05 (6.24e-04)	Tok/s 95612 (88437)	Loss/tok 4.5999 (6.1947)	LR 2.000e-03
0: TRAIN [0][650/1938]	Time 0.179 (0.162)	Data 1.00e-04 (6.16e-04)	Tok/s 92442 (88416)	Loss/tok 4.2593 (6.1659)	LR 2.000e-03
0: TRAIN [0][660/1938]	Time 0.238 (0.162)	Data 9.78e-05 (6.08e-04)	Tok/s 96501 (88406)	Loss/tok 4.4479 (6.1368)	LR 2.000e-03
0: TRAIN [0][670/1938]	Time 0.242 (0.162)	Data 1.24e-04 (6.01e-04)	Tok/s 96498 (88399)	Loss/tok 4.4773 (6.1081)	LR 2.000e-03
0: TRAIN [0][680/1938]	Time 0.124 (0.162)	Data 1.08e-04 (5.93e-04)	Tok/s 83907 (88415)	Loss/tok 3.9287 (6.0796)	LR 2.000e-03
0: TRAIN [0][690/1938]	Time 0.311 (0.161)	Data 9.78e-05 (5.86e-04)	Tok/s 95692 (88363)	Loss/tok 4.6867 (6.0548)	LR 2.000e-03
0: TRAIN [0][700/1938]	Time 0.123 (0.161)	Data 9.80e-05 (5.79e-04)	Tok/s 83821 (88313)	Loss/tok 3.9015 (6.0314)	LR 2.000e-03
0: TRAIN [0][710/1938]	Time 0.069 (0.161)	Data 9.89e-05 (5.73e-04)	Tok/s 75351 (88292)	Loss/tok 3.2620 (6.0056)	LR 2.000e-03
0: TRAIN [0][720/1938]	Time 0.123 (0.160)	Data 9.92e-05 (5.66e-04)	Tok/s 81644 (88260)	Loss/tok 3.8055 (5.9823)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][730/1938]	Time 0.123 (0.161)	Data 1.03e-04 (5.60e-04)	Tok/s 83081 (88281)	Loss/tok 3.8833 (5.9530)	LR 2.000e-03
0: TRAIN [0][740/1938]	Time 0.071 (0.161)	Data 1.01e-04 (5.54e-04)	Tok/s 74818 (88274)	Loss/tok 3.0789 (5.9283)	LR 2.000e-03
0: TRAIN [0][750/1938]	Time 0.123 (0.161)	Data 9.75e-05 (5.47e-04)	Tok/s 84360 (88285)	Loss/tok 3.6671 (5.9019)	LR 2.000e-03
0: TRAIN [0][760/1938]	Time 0.181 (0.161)	Data 9.99e-05 (5.42e-04)	Tok/s 92184 (88334)	Loss/tok 4.0856 (5.8735)	LR 2.000e-03
0: TRAIN [0][770/1938]	Time 0.068 (0.161)	Data 9.63e-05 (5.36e-04)	Tok/s 76801 (88313)	Loss/tok 3.1433 (5.8503)	LR 2.000e-03
0: TRAIN [0][780/1938]	Time 0.178 (0.161)	Data 9.94e-05 (5.30e-04)	Tok/s 94461 (88340)	Loss/tok 4.0090 (5.8255)	LR 2.000e-03
0: TRAIN [0][790/1938]	Time 0.181 (0.161)	Data 1.43e-04 (5.25e-04)	Tok/s 92478 (88358)	Loss/tok 4.0478 (5.8017)	LR 2.000e-03
0: TRAIN [0][800/1938]	Time 0.069 (0.161)	Data 1.01e-04 (5.20e-04)	Tok/s 76455 (88315)	Loss/tok 3.2511 (5.7821)	LR 2.000e-03
0: TRAIN [0][810/1938]	Time 0.123 (0.161)	Data 1.03e-04 (5.14e-04)	Tok/s 85989 (88317)	Loss/tok 3.7706 (5.7592)	LR 2.000e-03
0: TRAIN [0][820/1938]	Time 0.182 (0.161)	Data 9.87e-05 (5.09e-04)	Tok/s 91977 (88360)	Loss/tok 4.1407 (5.7338)	LR 2.000e-03
0: TRAIN [0][830/1938]	Time 0.311 (0.161)	Data 1.06e-04 (5.05e-04)	Tok/s 95807 (88358)	Loss/tok 4.4797 (5.7123)	LR 2.000e-03
0: TRAIN [0][840/1938]	Time 0.122 (0.161)	Data 1.01e-04 (5.00e-04)	Tok/s 84349 (88363)	Loss/tok 3.5857 (5.6915)	LR 2.000e-03
0: TRAIN [0][850/1938]	Time 0.241 (0.161)	Data 1.00e-04 (4.95e-04)	Tok/s 97161 (88361)	Loss/tok 4.1663 (5.6706)	LR 2.000e-03
0: TRAIN [0][860/1938]	Time 0.122 (0.161)	Data 1.05e-04 (4.90e-04)	Tok/s 86325 (88350)	Loss/tok 3.6151 (5.6504)	LR 2.000e-03
0: TRAIN [0][870/1938]	Time 0.241 (0.161)	Data 1.02e-04 (4.86e-04)	Tok/s 96423 (88338)	Loss/tok 4.2129 (5.6311)	LR 2.000e-03
0: TRAIN [0][880/1938]	Time 0.181 (0.161)	Data 1.01e-04 (4.82e-04)	Tok/s 92341 (88353)	Loss/tok 3.8423 (5.6108)	LR 2.000e-03
0: TRAIN [0][890/1938]	Time 0.122 (0.161)	Data 9.99e-05 (4.77e-04)	Tok/s 83165 (88350)	Loss/tok 3.6155 (5.5919)	LR 2.000e-03
0: TRAIN [0][900/1938]	Time 0.239 (0.161)	Data 1.05e-04 (4.73e-04)	Tok/s 100171 (88358)	Loss/tok 3.9679 (5.5720)	LR 2.000e-03
0: TRAIN [0][910/1938]	Time 0.181 (0.162)	Data 1.03e-04 (4.69e-04)	Tok/s 93207 (88397)	Loss/tok 3.8622 (5.5501)	LR 2.000e-03
0: TRAIN [0][920/1938]	Time 0.123 (0.162)	Data 9.73e-05 (4.65e-04)	Tok/s 85017 (88397)	Loss/tok 3.5096 (5.5324)	LR 2.000e-03
0: TRAIN [0][930/1938]	Time 0.124 (0.161)	Data 9.94e-05 (4.61e-04)	Tok/s 83765 (88364)	Loss/tok 3.6530 (5.5170)	LR 2.000e-03
0: TRAIN [0][940/1938]	Time 0.122 (0.161)	Data 9.99e-05 (4.58e-04)	Tok/s 83691 (88380)	Loss/tok 3.4744 (5.4991)	LR 2.000e-03
0: TRAIN [0][950/1938]	Time 0.312 (0.162)	Data 9.85e-05 (4.54e-04)	Tok/s 95006 (88412)	Loss/tok 4.2859 (5.4792)	LR 2.000e-03
0: TRAIN [0][960/1938]	Time 0.122 (0.162)	Data 1.01e-04 (4.50e-04)	Tok/s 85946 (88422)	Loss/tok 3.5600 (5.4614)	LR 2.000e-03
0: TRAIN [0][970/1938]	Time 0.122 (0.162)	Data 9.73e-05 (4.47e-04)	Tok/s 83937 (88410)	Loss/tok 3.6430 (5.4456)	LR 2.000e-03
0: TRAIN [0][980/1938]	Time 0.182 (0.162)	Data 1.01e-04 (4.43e-04)	Tok/s 94399 (88409)	Loss/tok 3.7942 (5.4292)	LR 2.000e-03
0: TRAIN [0][990/1938]	Time 0.182 (0.162)	Data 9.63e-05 (4.40e-04)	Tok/s 93637 (88420)	Loss/tok 3.7721 (5.4125)	LR 2.000e-03
0: TRAIN [0][1000/1938]	Time 0.069 (0.162)	Data 1.09e-04 (4.36e-04)	Tok/s 77403 (88440)	Loss/tok 3.1079 (5.3952)	LR 2.000e-03
0: TRAIN [0][1010/1938]	Time 0.243 (0.161)	Data 1.01e-04 (4.33e-04)	Tok/s 94435 (88430)	Loss/tok 4.1090 (5.3805)	LR 2.000e-03
0: TRAIN [0][1020/1938]	Time 0.180 (0.161)	Data 1.23e-04 (4.30e-04)	Tok/s 93220 (88421)	Loss/tok 3.9750 (5.3661)	LR 2.000e-03
0: TRAIN [0][1030/1938]	Time 0.123 (0.161)	Data 9.80e-05 (4.27e-04)	Tok/s 84005 (88419)	Loss/tok 3.6580 (5.3518)	LR 2.000e-03
0: TRAIN [0][1040/1938]	Time 0.122 (0.161)	Data 1.02e-04 (4.24e-04)	Tok/s 84548 (88408)	Loss/tok 3.5199 (5.3382)	LR 2.000e-03
0: TRAIN [0][1050/1938]	Time 0.125 (0.161)	Data 9.75e-05 (4.20e-04)	Tok/s 84373 (88413)	Loss/tok 3.4061 (5.3237)	LR 2.000e-03
0: TRAIN [0][1060/1938]	Time 0.067 (0.161)	Data 9.99e-05 (4.18e-04)	Tok/s 77083 (88419)	Loss/tok 2.9465 (5.3089)	LR 2.000e-03
0: TRAIN [0][1070/1938]	Time 0.183 (0.161)	Data 9.89e-05 (4.15e-04)	Tok/s 91327 (88397)	Loss/tok 3.7316 (5.2958)	LR 2.000e-03
0: TRAIN [0][1080/1938]	Time 0.122 (0.161)	Data 9.94e-05 (4.12e-04)	Tok/s 84204 (88385)	Loss/tok 3.6615 (5.2826)	LR 2.000e-03
0: TRAIN [0][1090/1938]	Time 0.126 (0.161)	Data 1.03e-04 (4.09e-04)	Tok/s 80568 (88375)	Loss/tok 3.5862 (5.2686)	LR 2.000e-03
0: TRAIN [0][1100/1938]	Time 0.316 (0.161)	Data 1.02e-04 (4.06e-04)	Tok/s 93230 (88406)	Loss/tok 4.2158 (5.2524)	LR 2.000e-03
0: TRAIN [0][1110/1938]	Time 0.244 (0.161)	Data 9.89e-05 (4.03e-04)	Tok/s 95418 (88431)	Loss/tok 4.0205 (5.2374)	LR 2.000e-03
0: TRAIN [0][1120/1938]	Time 0.180 (0.161)	Data 9.82e-05 (4.01e-04)	Tok/s 93075 (88424)	Loss/tok 3.6983 (5.2239)	LR 2.000e-03
0: TRAIN [0][1130/1938]	Time 0.126 (0.161)	Data 1.02e-04 (3.98e-04)	Tok/s 82236 (88410)	Loss/tok 3.5629 (5.2110)	LR 2.000e-03
0: TRAIN [0][1140/1938]	Time 0.069 (0.161)	Data 9.73e-05 (3.95e-04)	Tok/s 76126 (88406)	Loss/tok 2.9302 (5.1981)	LR 2.000e-03
0: TRAIN [0][1150/1938]	Time 0.122 (0.161)	Data 1.01e-04 (3.93e-04)	Tok/s 84422 (88412)	Loss/tok 3.6549 (5.1848)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1160/1938]	Time 0.185 (0.161)	Data 9.89e-05 (3.90e-04)	Tok/s 89628 (88427)	Loss/tok 3.7333 (5.1715)	LR 2.000e-03
0: TRAIN [0][1170/1938]	Time 0.178 (0.161)	Data 9.58e-05 (3.88e-04)	Tok/s 94794 (88414)	Loss/tok 3.7386 (5.1595)	LR 2.000e-03
0: TRAIN [0][1180/1938]	Time 0.069 (0.161)	Data 1.11e-04 (3.85e-04)	Tok/s 76480 (88372)	Loss/tok 3.0056 (5.1503)	LR 2.000e-03
0: TRAIN [0][1190/1938]	Time 0.242 (0.161)	Data 1.06e-04 (3.83e-04)	Tok/s 96539 (88367)	Loss/tok 4.0365 (5.1386)	LR 2.000e-03
0: TRAIN [0][1200/1938]	Time 0.241 (0.161)	Data 9.89e-05 (3.81e-04)	Tok/s 97978 (88373)	Loss/tok 3.9755 (5.1260)	LR 2.000e-03
0: TRAIN [0][1210/1938]	Time 0.181 (0.161)	Data 9.80e-05 (3.78e-04)	Tok/s 92719 (88369)	Loss/tok 3.6927 (5.1143)	LR 2.000e-03
0: TRAIN [0][1220/1938]	Time 0.183 (0.161)	Data 1.06e-04 (3.76e-04)	Tok/s 92126 (88371)	Loss/tok 3.7161 (5.1025)	LR 2.000e-03
0: TRAIN [0][1230/1938]	Time 0.125 (0.161)	Data 1.03e-04 (3.74e-04)	Tok/s 81342 (88389)	Loss/tok 3.4206 (5.0899)	LR 2.000e-03
0: TRAIN [0][1240/1938]	Time 0.123 (0.162)	Data 9.92e-05 (3.72e-04)	Tok/s 83989 (88403)	Loss/tok 3.4615 (5.0779)	LR 2.000e-03
0: TRAIN [0][1250/1938]	Time 0.315 (0.162)	Data 1.05e-04 (3.70e-04)	Tok/s 96453 (88420)	Loss/tok 4.0909 (5.0657)	LR 2.000e-03
0: TRAIN [0][1260/1938]	Time 0.069 (0.162)	Data 1.00e-04 (3.68e-04)	Tok/s 75290 (88414)	Loss/tok 2.9750 (5.0547)	LR 2.000e-03
0: TRAIN [0][1270/1938]	Time 0.068 (0.162)	Data 1.13e-04 (3.65e-04)	Tok/s 78650 (88420)	Loss/tok 3.0052 (5.0437)	LR 2.000e-03
0: TRAIN [0][1280/1938]	Time 0.180 (0.162)	Data 1.01e-04 (3.63e-04)	Tok/s 92833 (88414)	Loss/tok 3.7421 (5.0338)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1290/1938]	Time 0.124 (0.162)	Data 9.68e-05 (3.61e-04)	Tok/s 81048 (88426)	Loss/tok 3.3557 (5.0226)	LR 2.000e-03
0: TRAIN [0][1300/1938]	Time 0.124 (0.162)	Data 9.99e-05 (3.59e-04)	Tok/s 83737 (88432)	Loss/tok 3.3730 (5.0114)	LR 2.000e-03
0: TRAIN [0][1310/1938]	Time 0.122 (0.162)	Data 9.78e-05 (3.57e-04)	Tok/s 85022 (88402)	Loss/tok 3.3817 (5.0028)	LR 2.000e-03
0: TRAIN [0][1320/1938]	Time 0.180 (0.162)	Data 9.85e-05 (3.56e-04)	Tok/s 93301 (88411)	Loss/tok 3.6954 (4.9927)	LR 2.000e-03
0: TRAIN [0][1330/1938]	Time 0.242 (0.162)	Data 1.08e-04 (3.54e-04)	Tok/s 96145 (88441)	Loss/tok 3.8564 (4.9805)	LR 2.000e-03
0: TRAIN [0][1340/1938]	Time 0.180 (0.162)	Data 1.07e-04 (3.52e-04)	Tok/s 93320 (88430)	Loss/tok 3.7613 (4.9714)	LR 2.000e-03
0: TRAIN [0][1350/1938]	Time 0.123 (0.162)	Data 1.04e-04 (3.50e-04)	Tok/s 80916 (88441)	Loss/tok 3.3739 (4.9605)	LR 2.000e-03
0: TRAIN [0][1360/1938]	Time 0.180 (0.162)	Data 9.80e-05 (3.48e-04)	Tok/s 91775 (88459)	Loss/tok 3.7129 (4.9507)	LR 2.000e-03
0: TRAIN [0][1370/1938]	Time 0.239 (0.163)	Data 1.13e-04 (3.46e-04)	Tok/s 97580 (88487)	Loss/tok 3.7685 (4.9394)	LR 2.000e-03
0: TRAIN [0][1380/1938]	Time 0.123 (0.163)	Data 9.73e-05 (3.44e-04)	Tok/s 84616 (88466)	Loss/tok 3.4810 (4.9315)	LR 2.000e-03
0: TRAIN [0][1390/1938]	Time 0.179 (0.163)	Data 9.73e-05 (3.43e-04)	Tok/s 92784 (88450)	Loss/tok 3.6427 (4.9229)	LR 2.000e-03
0: TRAIN [0][1400/1938]	Time 0.122 (0.163)	Data 1.01e-04 (3.41e-04)	Tok/s 84195 (88446)	Loss/tok 3.5166 (4.9141)	LR 2.000e-03
0: TRAIN [0][1410/1938]	Time 0.123 (0.162)	Data 9.51e-05 (3.39e-04)	Tok/s 83576 (88430)	Loss/tok 3.2332 (4.9057)	LR 2.000e-03
0: TRAIN [0][1420/1938]	Time 0.070 (0.162)	Data 1.00e-04 (3.38e-04)	Tok/s 75043 (88414)	Loss/tok 2.8461 (4.8972)	LR 2.000e-03
0: TRAIN [0][1430/1938]	Time 0.123 (0.162)	Data 9.92e-05 (3.36e-04)	Tok/s 85112 (88409)	Loss/tok 3.4932 (4.8884)	LR 2.000e-03
0: TRAIN [0][1440/1938]	Time 0.182 (0.162)	Data 9.70e-05 (3.34e-04)	Tok/s 92336 (88391)	Loss/tok 3.5541 (4.8807)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1450/1938]	Time 0.181 (0.162)	Data 9.73e-05 (3.33e-04)	Tok/s 92808 (88380)	Loss/tok 3.7141 (4.8728)	LR 2.000e-03
0: TRAIN [0][1460/1938]	Time 0.126 (0.162)	Data 9.73e-05 (3.31e-04)	Tok/s 82176 (88352)	Loss/tok 3.3740 (4.8657)	LR 2.000e-03
0: TRAIN [0][1470/1938]	Time 0.181 (0.162)	Data 9.70e-05 (3.30e-04)	Tok/s 93633 (88349)	Loss/tok 3.6374 (4.8575)	LR 2.000e-03
0: TRAIN [0][1480/1938]	Time 0.124 (0.162)	Data 9.73e-05 (3.28e-04)	Tok/s 82979 (88347)	Loss/tok 3.4268 (4.8492)	LR 2.000e-03
0: TRAIN [0][1490/1938]	Time 0.243 (0.162)	Data 1.13e-04 (3.27e-04)	Tok/s 95272 (88345)	Loss/tok 3.8342 (4.8409)	LR 2.000e-03
0: TRAIN [0][1500/1938]	Time 0.124 (0.162)	Data 1.16e-04 (3.25e-04)	Tok/s 80456 (88315)	Loss/tok 3.4034 (4.8342)	LR 2.000e-03
0: TRAIN [0][1510/1938]	Time 0.125 (0.161)	Data 9.99e-05 (3.24e-04)	Tok/s 83164 (88289)	Loss/tok 3.2709 (4.8272)	LR 2.000e-03
0: TRAIN [0][1520/1938]	Time 0.123 (0.161)	Data 9.94e-05 (3.22e-04)	Tok/s 83877 (88282)	Loss/tok 3.3310 (4.8192)	LR 2.000e-03
0: TRAIN [0][1530/1938]	Time 0.243 (0.161)	Data 1.14e-04 (3.21e-04)	Tok/s 95673 (88271)	Loss/tok 3.9827 (4.8119)	LR 2.000e-03
0: TRAIN [0][1540/1938]	Time 0.123 (0.162)	Data 1.01e-04 (3.19e-04)	Tok/s 84807 (88290)	Loss/tok 3.3481 (4.8029)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][1550/1938]	Time 0.244 (0.162)	Data 9.89e-05 (3.18e-04)	Tok/s 94873 (88302)	Loss/tok 3.8603 (4.7948)	LR 2.000e-03
0: TRAIN [0][1560/1938]	Time 0.125 (0.162)	Data 9.85e-05 (3.17e-04)	Tok/s 82682 (88316)	Loss/tok 3.3781 (4.7864)	LR 2.000e-03
0: TRAIN [0][1570/1938]	Time 0.181 (0.162)	Data 1.01e-04 (3.15e-04)	Tok/s 93450 (88296)	Loss/tok 3.5891 (4.7796)	LR 2.000e-03
0: TRAIN [0][1580/1938]	Time 0.068 (0.162)	Data 9.44e-05 (3.14e-04)	Tok/s 76722 (88287)	Loss/tok 2.8784 (4.7726)	LR 2.000e-03
0: TRAIN [0][1590/1938]	Time 0.125 (0.162)	Data 9.63e-05 (3.13e-04)	Tok/s 82821 (88276)	Loss/tok 3.3527 (4.7655)	LR 2.000e-03
0: TRAIN [0][1600/1938]	Time 0.122 (0.161)	Data 9.23e-05 (3.11e-04)	Tok/s 84213 (88250)	Loss/tok 3.2534 (4.7592)	LR 2.000e-03
0: TRAIN [0][1610/1938]	Time 0.311 (0.162)	Data 9.61e-05 (3.10e-04)	Tok/s 95448 (88254)	Loss/tok 3.9003 (4.7515)	LR 2.000e-03
0: TRAIN [0][1620/1938]	Time 0.124 (0.161)	Data 9.37e-05 (3.09e-04)	Tok/s 82950 (88221)	Loss/tok 3.3920 (4.7457)	LR 2.000e-03
0: TRAIN [0][1630/1938]	Time 0.316 (0.161)	Data 1.01e-04 (3.07e-04)	Tok/s 94878 (88218)	Loss/tok 4.0445 (4.7388)	LR 2.000e-03
0: TRAIN [0][1640/1938]	Time 0.243 (0.162)	Data 1.03e-04 (3.06e-04)	Tok/s 96576 (88242)	Loss/tok 3.8067 (4.7308)	LR 2.000e-03
0: TRAIN [0][1650/1938]	Time 0.124 (0.162)	Data 9.78e-05 (3.05e-04)	Tok/s 83418 (88242)	Loss/tok 3.3233 (4.7238)	LR 2.000e-03
0: TRAIN [0][1660/1938]	Time 0.070 (0.162)	Data 1.16e-04 (3.04e-04)	Tok/s 74967 (88225)	Loss/tok 2.8014 (4.7177)	LR 2.000e-03
0: TRAIN [0][1670/1938]	Time 0.180 (0.162)	Data 9.75e-05 (3.02e-04)	Tok/s 93761 (88218)	Loss/tok 3.4585 (4.7110)	LR 2.000e-03
0: TRAIN [0][1680/1938]	Time 0.242 (0.162)	Data 9.75e-05 (3.01e-04)	Tok/s 95402 (88225)	Loss/tok 3.8248 (4.7041)	LR 2.000e-03
0: TRAIN [0][1690/1938]	Time 0.069 (0.162)	Data 9.37e-05 (3.00e-04)	Tok/s 77139 (88208)	Loss/tok 2.7924 (4.6979)	LR 2.000e-03
0: TRAIN [0][1700/1938]	Time 0.183 (0.162)	Data 9.63e-05 (2.99e-04)	Tok/s 92859 (88217)	Loss/tok 3.6092 (4.6913)	LR 2.000e-03
0: TRAIN [0][1710/1938]	Time 0.242 (0.162)	Data 9.44e-05 (2.98e-04)	Tok/s 95702 (88237)	Loss/tok 3.7233 (4.6837)	LR 2.000e-03
0: TRAIN [0][1720/1938]	Time 0.242 (0.162)	Data 9.82e-05 (2.96e-04)	Tok/s 97001 (88255)	Loss/tok 3.7451 (4.6767)	LR 2.000e-03
0: TRAIN [0][1730/1938]	Time 0.242 (0.162)	Data 9.92e-05 (2.95e-04)	Tok/s 95759 (88261)	Loss/tok 3.7673 (4.6698)	LR 2.000e-03
0: TRAIN [0][1740/1938]	Time 0.124 (0.162)	Data 9.87e-05 (2.94e-04)	Tok/s 83386 (88241)	Loss/tok 3.4008 (4.6644)	LR 2.000e-03
0: TRAIN [0][1750/1938]	Time 0.068 (0.162)	Data 9.61e-05 (2.93e-04)	Tok/s 76922 (88221)	Loss/tok 2.8341 (4.6591)	LR 2.000e-03
0: TRAIN [0][1760/1938]	Time 0.239 (0.162)	Data 1.21e-04 (2.92e-04)	Tok/s 97626 (88216)	Loss/tok 3.6498 (4.6531)	LR 2.000e-03
0: TRAIN [0][1770/1938]	Time 0.241 (0.162)	Data 9.06e-05 (2.91e-04)	Tok/s 96852 (88228)	Loss/tok 3.8869 (4.6464)	LR 2.000e-03
0: TRAIN [0][1780/1938]	Time 0.181 (0.162)	Data 1.01e-04 (2.90e-04)	Tok/s 92880 (88236)	Loss/tok 3.6267 (4.6399)	LR 2.000e-03
0: TRAIN [0][1790/1938]	Time 0.122 (0.162)	Data 9.42e-05 (2.89e-04)	Tok/s 84700 (88245)	Loss/tok 3.4128 (4.6335)	LR 2.000e-03
0: TRAIN [0][1800/1938]	Time 0.181 (0.162)	Data 9.92e-05 (2.88e-04)	Tok/s 92753 (88248)	Loss/tok 3.6355 (4.6273)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1810/1938]	Time 0.181 (0.162)	Data 1.00e-04 (2.87e-04)	Tok/s 92092 (88254)	Loss/tok 3.5606 (4.6212)	LR 2.000e-03
0: TRAIN [0][1820/1938]	Time 0.123 (0.162)	Data 1.10e-04 (2.86e-04)	Tok/s 86126 (88249)	Loss/tok 3.1900 (4.6155)	LR 2.000e-03
0: TRAIN [0][1830/1938]	Time 0.182 (0.162)	Data 1.13e-04 (2.85e-04)	Tok/s 91278 (88250)	Loss/tok 3.6465 (4.6097)	LR 2.000e-03
0: TRAIN [0][1840/1938]	Time 0.238 (0.162)	Data 9.78e-05 (2.84e-04)	Tok/s 96554 (88252)	Loss/tok 3.7333 (4.6039)	LR 2.000e-03
0: TRAIN [0][1850/1938]	Time 0.123 (0.162)	Data 9.73e-05 (2.83e-04)	Tok/s 84174 (88246)	Loss/tok 3.3432 (4.5984)	LR 2.000e-03
0: TRAIN [0][1860/1938]	Time 0.181 (0.162)	Data 9.25e-05 (2.82e-04)	Tok/s 92241 (88246)	Loss/tok 3.5520 (4.5929)	LR 2.000e-03
0: TRAIN [0][1870/1938]	Time 0.243 (0.162)	Data 9.80e-05 (2.81e-04)	Tok/s 96385 (88244)	Loss/tok 3.7142 (4.5873)	LR 2.000e-03
0: TRAIN [0][1880/1938]	Time 0.123 (0.162)	Data 9.75e-05 (2.80e-04)	Tok/s 84394 (88223)	Loss/tok 3.2644 (4.5823)	LR 2.000e-03
0: TRAIN [0][1890/1938]	Time 0.126 (0.162)	Data 9.94e-05 (2.79e-04)	Tok/s 82113 (88227)	Loss/tok 3.2786 (4.5766)	LR 2.000e-03
0: TRAIN [0][1900/1938]	Time 0.069 (0.162)	Data 9.70e-05 (2.78e-04)	Tok/s 75052 (88212)	Loss/tok 2.8406 (4.5715)	LR 2.000e-03
0: TRAIN [0][1910/1938]	Time 0.124 (0.162)	Data 9.73e-05 (2.77e-04)	Tok/s 82530 (88211)	Loss/tok 3.3579 (4.5661)	LR 2.000e-03
0: TRAIN [0][1920/1938]	Time 0.180 (0.162)	Data 9.70e-05 (2.76e-04)	Tok/s 93098 (88220)	Loss/tok 3.5658 (4.5607)	LR 2.000e-03
0: TRAIN [0][1930/1938]	Time 0.184 (0.162)	Data 9.82e-05 (2.75e-04)	Tok/s 91974 (88206)	Loss/tok 3.5434 (4.5559)	LR 2.000e-03
:::MLL 1575775544.229 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1575775544.230 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.654 (0.654)	Decoder iters 118.0 (118.0)	Tok/s 25078 (25078)
0: Running moses detokenizer
0: BLEU(score=20.159122237800467, counts=[34744, 15989, 8585, 4787], totals=[65571, 62568, 59565, 56567], precisions=[52.98683869393482, 25.554596598900396, 14.412826324183666, 8.462531157742147], bp=1.0, sys_len=65571, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1575775546.172 eval_accuracy: {"value": 20.16, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1575775546.172 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.5524	Test BLEU: 20.16
0: Performance: Epoch: 0	Training: 705882 Tok/s
0: Finished epoch 0
:::MLL 1575775546.173 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1575775546.173 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1575775546.173 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 3115306043
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][0/1938]	Time 0.459 (0.459)	Data 2.59e-01 (2.59e-01)	Tok/s 36235 (36235)	Loss/tok 3.5825 (3.5825)	LR 2.000e-03
0: TRAIN [1][10/1938]	Time 0.317 (0.226)	Data 1.02e-04 (2.37e-02)	Tok/s 93649 (86150)	Loss/tok 3.8850 (3.5886)	LR 2.000e-03
0: TRAIN [1][20/1938]	Time 0.124 (0.192)	Data 1.01e-04 (1.24e-02)	Tok/s 83256 (86606)	Loss/tok 3.1718 (3.5190)	LR 2.000e-03
0: TRAIN [1][30/1938]	Time 0.312 (0.189)	Data 9.82e-05 (8.46e-03)	Tok/s 95230 (87423)	Loss/tok 3.8291 (3.5351)	LR 2.000e-03
0: TRAIN [1][40/1938]	Time 0.239 (0.181)	Data 1.02e-04 (6.42e-03)	Tok/s 96084 (87411)	Loss/tok 3.7671 (3.5075)	LR 2.000e-03
0: TRAIN [1][50/1938]	Time 0.124 (0.180)	Data 1.06e-04 (5.18e-03)	Tok/s 83478 (87866)	Loss/tok 3.1069 (3.4975)	LR 2.000e-03
0: TRAIN [1][60/1938]	Time 0.123 (0.178)	Data 9.99e-05 (4.35e-03)	Tok/s 83646 (88062)	Loss/tok 3.2784 (3.4935)	LR 2.000e-03
0: TRAIN [1][70/1938]	Time 0.069 (0.175)	Data 1.16e-04 (3.75e-03)	Tok/s 75634 (87992)	Loss/tok 2.7593 (3.4855)	LR 2.000e-03
0: TRAIN [1][80/1938]	Time 0.069 (0.173)	Data 1.00e-04 (3.30e-03)	Tok/s 77448 (88050)	Loss/tok 2.7076 (3.4884)	LR 2.000e-03
0: TRAIN [1][90/1938]	Time 0.312 (0.173)	Data 1.13e-04 (2.95e-03)	Tok/s 94270 (88019)	Loss/tok 3.7559 (3.4921)	LR 2.000e-03
0: TRAIN [1][100/1938]	Time 0.126 (0.171)	Data 1.01e-04 (2.67e-03)	Tok/s 84554 (87960)	Loss/tok 3.3335 (3.4935)	LR 2.000e-03
0: TRAIN [1][110/1938]	Time 0.122 (0.169)	Data 1.02e-04 (2.44e-03)	Tok/s 82114 (87699)	Loss/tok 3.3312 (3.4913)	LR 2.000e-03
0: TRAIN [1][120/1938]	Time 0.069 (0.168)	Data 9.99e-05 (2.25e-03)	Tok/s 76238 (87691)	Loss/tok 2.8139 (3.4926)	LR 2.000e-03
0: TRAIN [1][130/1938]	Time 0.243 (0.168)	Data 9.87e-05 (2.08e-03)	Tok/s 95735 (87705)	Loss/tok 3.7196 (3.4944)	LR 2.000e-03
0: TRAIN [1][140/1938]	Time 0.182 (0.169)	Data 1.00e-04 (1.94e-03)	Tok/s 92099 (87879)	Loss/tok 3.4628 (3.5007)	LR 2.000e-03
0: TRAIN [1][150/1938]	Time 0.122 (0.170)	Data 1.56e-04 (1.82e-03)	Tok/s 84095 (88001)	Loss/tok 3.3069 (3.5009)	LR 2.000e-03
0: TRAIN [1][160/1938]	Time 0.181 (0.171)	Data 1.00e-04 (1.71e-03)	Tok/s 94638 (88189)	Loss/tok 3.4212 (3.5006)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][170/1938]	Time 0.123 (0.171)	Data 1.04e-04 (1.62e-03)	Tok/s 83489 (88213)	Loss/tok 3.2881 (3.5003)	LR 2.000e-03
0: TRAIN [1][180/1938]	Time 0.123 (0.169)	Data 9.61e-05 (1.54e-03)	Tok/s 83032 (88033)	Loss/tok 3.1830 (3.4925)	LR 2.000e-03
0: TRAIN [1][190/1938]	Time 0.181 (0.170)	Data 1.32e-04 (1.46e-03)	Tok/s 92486 (88160)	Loss/tok 3.4412 (3.4943)	LR 2.000e-03
0: TRAIN [1][200/1938]	Time 0.123 (0.169)	Data 1.01e-04 (1.39e-03)	Tok/s 83545 (88149)	Loss/tok 3.1609 (3.4924)	LR 2.000e-03
0: TRAIN [1][210/1938]	Time 0.181 (0.169)	Data 9.99e-05 (1.33e-03)	Tok/s 92163 (88157)	Loss/tok 3.3690 (3.4897)	LR 2.000e-03
0: TRAIN [1][220/1938]	Time 0.123 (0.167)	Data 9.80e-05 (1.28e-03)	Tok/s 84511 (88088)	Loss/tok 3.3366 (3.4847)	LR 2.000e-03
0: TRAIN [1][230/1938]	Time 0.124 (0.167)	Data 1.01e-04 (1.23e-03)	Tok/s 83090 (88070)	Loss/tok 3.1624 (3.4814)	LR 2.000e-03
0: TRAIN [1][240/1938]	Time 0.125 (0.166)	Data 1.78e-04 (1.18e-03)	Tok/s 83544 (88055)	Loss/tok 3.1301 (3.4806)	LR 2.000e-03
0: TRAIN [1][250/1938]	Time 0.122 (0.165)	Data 1.34e-04 (1.14e-03)	Tok/s 85058 (87970)	Loss/tok 3.2381 (3.4748)	LR 2.000e-03
0: TRAIN [1][260/1938]	Time 0.240 (0.164)	Data 1.01e-04 (1.10e-03)	Tok/s 97166 (87903)	Loss/tok 3.5351 (3.4700)	LR 2.000e-03
0: TRAIN [1][270/1938]	Time 0.179 (0.164)	Data 9.89e-05 (1.06e-03)	Tok/s 94754 (87994)	Loss/tok 3.3482 (3.4666)	LR 2.000e-03
0: TRAIN [1][280/1938]	Time 0.311 (0.164)	Data 1.06e-04 (1.03e-03)	Tok/s 95571 (88011)	Loss/tok 3.9309 (3.4664)	LR 2.000e-03
0: TRAIN [1][290/1938]	Time 0.122 (0.164)	Data 1.03e-04 (9.95e-04)	Tok/s 86016 (88033)	Loss/tok 3.0966 (3.4640)	LR 2.000e-03
0: TRAIN [1][300/1938]	Time 0.123 (0.163)	Data 1.15e-04 (9.65e-04)	Tok/s 85905 (87974)	Loss/tok 3.1977 (3.4600)	LR 2.000e-03
0: TRAIN [1][310/1938]	Time 0.181 (0.163)	Data 9.89e-05 (9.37e-04)	Tok/s 92580 (88036)	Loss/tok 3.4165 (3.4629)	LR 2.000e-03
0: TRAIN [1][320/1938]	Time 0.242 (0.163)	Data 1.15e-04 (9.11e-04)	Tok/s 97044 (88013)	Loss/tok 3.5271 (3.4600)	LR 2.000e-03
0: TRAIN [1][330/1938]	Time 0.315 (0.163)	Data 1.12e-04 (8.87e-04)	Tok/s 94521 (88022)	Loss/tok 3.9704 (3.4631)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][340/1938]	Time 0.244 (0.164)	Data 1.72e-04 (8.64e-04)	Tok/s 96598 (88033)	Loss/tok 3.5058 (3.4661)	LR 2.000e-03
0: TRAIN [1][350/1938]	Time 0.124 (0.164)	Data 1.02e-04 (8.43e-04)	Tok/s 82509 (88055)	Loss/tok 3.2139 (3.4672)	LR 2.000e-03
0: TRAIN [1][360/1938]	Time 0.069 (0.163)	Data 9.82e-05 (8.22e-04)	Tok/s 75484 (87973)	Loss/tok 2.6818 (3.4643)	LR 2.000e-03
0: TRAIN [1][370/1938]	Time 0.242 (0.163)	Data 1.69e-04 (8.03e-04)	Tok/s 93573 (87993)	Loss/tok 3.7668 (3.4635)	LR 2.000e-03
0: TRAIN [1][380/1938]	Time 0.068 (0.163)	Data 9.89e-05 (7.85e-04)	Tok/s 78346 (87959)	Loss/tok 2.7571 (3.4608)	LR 2.000e-03
0: TRAIN [1][390/1938]	Time 0.123 (0.162)	Data 1.02e-04 (7.67e-04)	Tok/s 84702 (87897)	Loss/tok 3.2187 (3.4579)	LR 2.000e-03
0: TRAIN [1][400/1938]	Time 0.124 (0.162)	Data 1.03e-04 (7.51e-04)	Tok/s 80947 (87880)	Loss/tok 3.2918 (3.4585)	LR 2.000e-03
0: TRAIN [1][410/1938]	Time 0.181 (0.163)	Data 1.01e-04 (7.35e-04)	Tok/s 93085 (87960)	Loss/tok 3.4103 (3.4594)	LR 2.000e-03
0: TRAIN [1][420/1938]	Time 0.242 (0.163)	Data 9.97e-05 (7.20e-04)	Tok/s 96780 (87916)	Loss/tok 3.5994 (3.4567)	LR 2.000e-03
0: TRAIN [1][430/1938]	Time 0.122 (0.163)	Data 1.05e-04 (7.06e-04)	Tok/s 82562 (87870)	Loss/tok 3.2971 (3.4567)	LR 2.000e-03
0: TRAIN [1][440/1938]	Time 0.309 (0.162)	Data 9.89e-05 (6.92e-04)	Tok/s 96183 (87849)	Loss/tok 3.8222 (3.4564)	LR 2.000e-03
0: TRAIN [1][450/1938]	Time 0.126 (0.163)	Data 1.41e-04 (6.80e-04)	Tok/s 81513 (87834)	Loss/tok 3.2623 (3.4572)	LR 2.000e-03
0: TRAIN [1][460/1938]	Time 0.242 (0.163)	Data 1.30e-04 (6.67e-04)	Tok/s 97513 (87867)	Loss/tok 3.7090 (3.4586)	LR 2.000e-03
0: TRAIN [1][470/1938]	Time 0.182 (0.163)	Data 9.99e-05 (6.55e-04)	Tok/s 92463 (87916)	Loss/tok 3.4856 (3.4603)	LR 2.000e-03
0: TRAIN [1][480/1938]	Time 0.124 (0.164)	Data 1.13e-04 (6.44e-04)	Tok/s 84562 (88004)	Loss/tok 3.1466 (3.4621)	LR 2.000e-03
0: TRAIN [1][490/1938]	Time 0.122 (0.164)	Data 1.07e-04 (6.33e-04)	Tok/s 84752 (88011)	Loss/tok 3.1562 (3.4611)	LR 2.000e-03
0: TRAIN [1][500/1938]	Time 0.180 (0.164)	Data 1.02e-04 (6.22e-04)	Tok/s 93999 (88036)	Loss/tok 3.5523 (3.4599)	LR 2.000e-03
0: TRAIN [1][510/1938]	Time 0.184 (0.163)	Data 9.30e-05 (6.12e-04)	Tok/s 89624 (87976)	Loss/tok 3.6038 (3.4578)	LR 2.000e-03
0: TRAIN [1][520/1938]	Time 0.181 (0.163)	Data 9.61e-05 (6.02e-04)	Tok/s 91508 (88009)	Loss/tok 3.4701 (3.4567)	LR 2.000e-03
0: TRAIN [1][530/1938]	Time 0.309 (0.163)	Data 9.70e-05 (5.93e-04)	Tok/s 97335 (88021)	Loss/tok 3.7856 (3.4563)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][540/1938]	Time 0.181 (0.163)	Data 1.02e-04 (5.84e-04)	Tok/s 92165 (87955)	Loss/tok 3.4768 (3.4547)	LR 2.000e-03
0: TRAIN [1][550/1938]	Time 0.316 (0.164)	Data 1.12e-04 (5.75e-04)	Tok/s 92751 (87992)	Loss/tok 3.8483 (3.4572)	LR 2.000e-03
0: TRAIN [1][560/1938]	Time 0.068 (0.163)	Data 1.01e-04 (5.67e-04)	Tok/s 76414 (87987)	Loss/tok 2.6540 (3.4555)	LR 2.000e-03
0: TRAIN [1][570/1938]	Time 0.123 (0.164)	Data 9.80e-05 (5.59e-04)	Tok/s 83970 (87999)	Loss/tok 3.0747 (3.4569)	LR 2.000e-03
0: TRAIN [1][580/1938]	Time 0.123 (0.164)	Data 1.00e-04 (5.51e-04)	Tok/s 82859 (87984)	Loss/tok 3.2633 (3.4574)	LR 2.000e-03
0: TRAIN [1][590/1938]	Time 0.181 (0.164)	Data 1.59e-04 (5.43e-04)	Tok/s 93708 (87999)	Loss/tok 3.4084 (3.4579)	LR 2.000e-03
0: TRAIN [1][600/1938]	Time 0.122 (0.164)	Data 1.05e-04 (5.36e-04)	Tok/s 84526 (87976)	Loss/tok 3.1073 (3.4585)	LR 2.000e-03
0: TRAIN [1][610/1938]	Time 0.240 (0.164)	Data 9.94e-05 (5.29e-04)	Tok/s 96356 (87992)	Loss/tok 3.6147 (3.4580)	LR 2.000e-03
0: TRAIN [1][620/1938]	Time 0.180 (0.164)	Data 1.01e-04 (5.22e-04)	Tok/s 94115 (87984)	Loss/tok 3.3796 (3.4580)	LR 2.000e-03
0: TRAIN [1][630/1938]	Time 0.180 (0.164)	Data 1.03e-04 (5.15e-04)	Tok/s 93190 (87988)	Loss/tok 3.4023 (3.4572)	LR 2.000e-03
0: TRAIN [1][640/1938]	Time 0.125 (0.164)	Data 1.11e-04 (5.09e-04)	Tok/s 82279 (87958)	Loss/tok 3.1947 (3.4574)	LR 2.000e-03
0: TRAIN [1][650/1938]	Time 0.069 (0.163)	Data 1.01e-04 (5.03e-04)	Tok/s 77513 (87907)	Loss/tok 2.7260 (3.4554)	LR 2.000e-03
0: TRAIN [1][660/1938]	Time 0.182 (0.163)	Data 1.04e-04 (4.97e-04)	Tok/s 91019 (87918)	Loss/tok 3.4886 (3.4543)	LR 2.000e-03
0: TRAIN [1][670/1938]	Time 0.123 (0.163)	Data 9.99e-05 (4.91e-04)	Tok/s 83687 (87913)	Loss/tok 3.2584 (3.4524)	LR 2.000e-03
0: TRAIN [1][680/1938]	Time 0.241 (0.163)	Data 1.04e-04 (4.85e-04)	Tok/s 97152 (87954)	Loss/tok 3.5531 (3.4532)	LR 2.000e-03
0: TRAIN [1][690/1938]	Time 0.123 (0.163)	Data 9.94e-05 (4.80e-04)	Tok/s 82232 (87933)	Loss/tok 3.1735 (3.4515)	LR 2.000e-03
0: TRAIN [1][700/1938]	Time 0.068 (0.163)	Data 9.94e-05 (4.74e-04)	Tok/s 80419 (87926)	Loss/tok 2.7171 (3.4509)	LR 2.000e-03
0: TRAIN [1][710/1938]	Time 0.179 (0.163)	Data 9.94e-05 (4.69e-04)	Tok/s 92705 (87932)	Loss/tok 3.4692 (3.4505)	LR 2.000e-03
0: TRAIN [1][720/1938]	Time 0.124 (0.163)	Data 1.04e-04 (4.64e-04)	Tok/s 83919 (87935)	Loss/tok 3.2187 (3.4498)	LR 2.000e-03
0: TRAIN [1][730/1938]	Time 0.182 (0.163)	Data 1.31e-04 (4.59e-04)	Tok/s 91501 (87937)	Loss/tok 3.4908 (3.4488)	LR 2.000e-03
0: TRAIN [1][740/1938]	Time 0.123 (0.163)	Data 1.02e-04 (4.54e-04)	Tok/s 84530 (87921)	Loss/tok 3.1652 (3.4479)	LR 2.000e-03
0: TRAIN [1][750/1938]	Time 0.069 (0.163)	Data 1.00e-04 (4.49e-04)	Tok/s 76751 (87895)	Loss/tok 2.7205 (3.4460)	LR 2.000e-03
0: TRAIN [1][760/1938]	Time 0.240 (0.163)	Data 1.03e-04 (4.45e-04)	Tok/s 96362 (87880)	Loss/tok 3.5433 (3.4448)	LR 2.000e-03
0: TRAIN [1][770/1938]	Time 0.180 (0.163)	Data 1.02e-04 (4.40e-04)	Tok/s 93588 (87912)	Loss/tok 3.4647 (3.4443)	LR 2.000e-03
0: TRAIN [1][780/1938]	Time 0.243 (0.163)	Data 1.01e-04 (4.36e-04)	Tok/s 96483 (87897)	Loss/tok 3.5000 (3.4425)	LR 2.000e-03
0: TRAIN [1][790/1938]	Time 0.124 (0.163)	Data 1.01e-04 (4.32e-04)	Tok/s 83781 (87885)	Loss/tok 3.1608 (3.4423)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][800/1938]	Time 0.125 (0.163)	Data 9.89e-05 (4.28e-04)	Tok/s 81951 (87876)	Loss/tok 3.1097 (3.4422)	LR 2.000e-03
0: TRAIN [1][810/1938]	Time 0.124 (0.163)	Data 1.03e-04 (4.24e-04)	Tok/s 84021 (87890)	Loss/tok 3.3137 (3.4424)	LR 2.000e-03
0: TRAIN [1][820/1938]	Time 0.179 (0.163)	Data 9.70e-05 (4.20e-04)	Tok/s 91963 (87909)	Loss/tok 3.5408 (3.4423)	LR 2.000e-03
0: TRAIN [1][830/1938]	Time 0.123 (0.163)	Data 1.18e-04 (4.16e-04)	Tok/s 83917 (87912)	Loss/tok 3.2366 (3.4411)	LR 2.000e-03
0: TRAIN [1][840/1938]	Time 0.180 (0.163)	Data 1.01e-04 (4.12e-04)	Tok/s 92723 (87908)	Loss/tok 3.3281 (3.4398)	LR 2.000e-03
0: TRAIN [1][850/1938]	Time 0.181 (0.163)	Data 1.05e-04 (4.09e-04)	Tok/s 92401 (87929)	Loss/tok 3.4809 (3.4397)	LR 2.000e-03
0: TRAIN [1][860/1938]	Time 0.181 (0.163)	Data 1.00e-04 (4.05e-04)	Tok/s 93653 (87970)	Loss/tok 3.3668 (3.4396)	LR 2.000e-03
0: TRAIN [1][870/1938]	Time 0.180 (0.163)	Data 1.03e-04 (4.01e-04)	Tok/s 93226 (87964)	Loss/tok 3.4364 (3.4384)	LR 2.000e-03
0: TRAIN [1][880/1938]	Time 0.241 (0.163)	Data 1.02e-04 (3.98e-04)	Tok/s 97610 (87973)	Loss/tok 3.5016 (3.4378)	LR 2.000e-03
0: TRAIN [1][890/1938]	Time 0.181 (0.163)	Data 1.03e-04 (3.95e-04)	Tok/s 92569 (88006)	Loss/tok 3.4633 (3.4384)	LR 2.000e-03
0: TRAIN [1][900/1938]	Time 0.123 (0.163)	Data 1.00e-04 (3.92e-04)	Tok/s 83719 (88004)	Loss/tok 3.1687 (3.4373)	LR 2.000e-03
0: TRAIN [1][910/1938]	Time 0.126 (0.163)	Data 1.03e-04 (3.88e-04)	Tok/s 81478 (87998)	Loss/tok 3.1560 (3.4374)	LR 2.000e-03
0: TRAIN [1][920/1938]	Time 0.124 (0.164)	Data 1.03e-04 (3.85e-04)	Tok/s 82865 (88046)	Loss/tok 3.0547 (3.4377)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][930/1938]	Time 0.181 (0.163)	Data 1.01e-04 (3.82e-04)	Tok/s 91380 (88036)	Loss/tok 3.4968 (3.4374)	LR 2.000e-03
0: TRAIN [1][940/1938]	Time 0.124 (0.164)	Data 1.16e-04 (3.79e-04)	Tok/s 84879 (88049)	Loss/tok 3.1743 (3.4378)	LR 2.000e-03
0: TRAIN [1][950/1938]	Time 0.179 (0.164)	Data 9.97e-05 (3.76e-04)	Tok/s 93550 (88058)	Loss/tok 3.3784 (3.4369)	LR 2.000e-03
0: TRAIN [1][960/1938]	Time 0.181 (0.164)	Data 1.20e-04 (3.74e-04)	Tok/s 93628 (88103)	Loss/tok 3.3534 (3.4373)	LR 2.000e-03
0: TRAIN [1][970/1938]	Time 0.183 (0.164)	Data 1.14e-04 (3.71e-04)	Tok/s 90505 (88101)	Loss/tok 3.5560 (3.4368)	LR 2.000e-03
0: TRAIN [1][980/1938]	Time 0.240 (0.164)	Data 1.04e-04 (3.68e-04)	Tok/s 96795 (88082)	Loss/tok 3.6050 (3.4361)	LR 2.000e-03
0: TRAIN [1][990/1938]	Time 0.312 (0.164)	Data 1.01e-04 (3.65e-04)	Tok/s 96662 (88080)	Loss/tok 3.7170 (3.4358)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1000/1938]	Time 0.310 (0.164)	Data 9.58e-05 (3.63e-04)	Tok/s 96661 (88067)	Loss/tok 3.8184 (3.4365)	LR 2.000e-03
0: TRAIN [1][1010/1938]	Time 0.124 (0.164)	Data 1.03e-04 (3.60e-04)	Tok/s 82748 (88053)	Loss/tok 3.1543 (3.4362)	LR 2.000e-03
0: TRAIN [1][1020/1938]	Time 0.124 (0.164)	Data 9.94e-05 (3.58e-04)	Tok/s 82704 (88053)	Loss/tok 3.1482 (3.4359)	LR 2.000e-03
0: TRAIN [1][1030/1938]	Time 0.185 (0.164)	Data 1.04e-04 (3.55e-04)	Tok/s 89961 (88047)	Loss/tok 3.3176 (3.4348)	LR 2.000e-03
0: TRAIN [1][1040/1938]	Time 0.123 (0.164)	Data 9.70e-05 (3.53e-04)	Tok/s 83994 (88025)	Loss/tok 3.1396 (3.4335)	LR 2.000e-03
0: TRAIN [1][1050/1938]	Time 0.124 (0.164)	Data 9.85e-05 (3.50e-04)	Tok/s 84469 (88042)	Loss/tok 3.2326 (3.4337)	LR 2.000e-03
0: TRAIN [1][1060/1938]	Time 0.068 (0.164)	Data 9.68e-05 (3.48e-04)	Tok/s 78000 (88042)	Loss/tok 2.6907 (3.4335)	LR 2.000e-03
0: TRAIN [1][1070/1938]	Time 0.182 (0.164)	Data 9.94e-05 (3.46e-04)	Tok/s 91812 (88070)	Loss/tok 3.4465 (3.4336)	LR 2.000e-03
0: TRAIN [1][1080/1938]	Time 0.123 (0.164)	Data 9.92e-05 (3.43e-04)	Tok/s 84119 (88087)	Loss/tok 3.0720 (3.4327)	LR 2.000e-03
0: TRAIN [1][1090/1938]	Time 0.243 (0.164)	Data 9.75e-05 (3.41e-04)	Tok/s 96782 (88094)	Loss/tok 3.5575 (3.4328)	LR 2.000e-03
0: TRAIN [1][1100/1938]	Time 0.069 (0.164)	Data 1.10e-04 (3.39e-04)	Tok/s 74712 (88090)	Loss/tok 2.6889 (3.4322)	LR 2.000e-03
0: TRAIN [1][1110/1938]	Time 0.311 (0.164)	Data 9.80e-05 (3.37e-04)	Tok/s 97320 (88091)	Loss/tok 3.5839 (3.4326)	LR 2.000e-03
0: TRAIN [1][1120/1938]	Time 0.122 (0.164)	Data 1.02e-04 (3.35e-04)	Tok/s 85830 (88067)	Loss/tok 3.1489 (3.4316)	LR 2.000e-03
0: TRAIN [1][1130/1938]	Time 0.241 (0.164)	Data 9.97e-05 (3.33e-04)	Tok/s 97227 (88067)	Loss/tok 3.5859 (3.4311)	LR 2.000e-03
0: TRAIN [1][1140/1938]	Time 0.182 (0.164)	Data 1.03e-04 (3.31e-04)	Tok/s 93804 (88071)	Loss/tok 3.2676 (3.4304)	LR 2.000e-03
0: TRAIN [1][1150/1938]	Time 0.123 (0.164)	Data 9.61e-05 (3.29e-04)	Tok/s 85076 (88086)	Loss/tok 3.0556 (3.4295)	LR 2.000e-03
0: TRAIN [1][1160/1938]	Time 0.071 (0.164)	Data 1.01e-04 (3.27e-04)	Tok/s 74788 (88099)	Loss/tok 2.7706 (3.4288)	LR 2.000e-03
0: TRAIN [1][1170/1938]	Time 0.123 (0.164)	Data 9.97e-05 (3.25e-04)	Tok/s 84514 (88110)	Loss/tok 3.2090 (3.4288)	LR 2.000e-03
0: TRAIN [1][1180/1938]	Time 0.123 (0.164)	Data 1.03e-04 (3.23e-04)	Tok/s 83870 (88137)	Loss/tok 3.1630 (3.4292)	LR 2.000e-03
0: TRAIN [1][1190/1938]	Time 0.180 (0.164)	Data 9.99e-05 (3.21e-04)	Tok/s 94018 (88118)	Loss/tok 3.4264 (3.4281)	LR 2.000e-03
0: TRAIN [1][1200/1938]	Time 0.240 (0.164)	Data 1.00e-04 (3.19e-04)	Tok/s 97283 (88082)	Loss/tok 3.6366 (3.4270)	LR 2.000e-03
0: TRAIN [1][1210/1938]	Time 0.241 (0.163)	Data 1.10e-04 (3.17e-04)	Tok/s 97947 (88071)	Loss/tok 3.5360 (3.4264)	LR 2.000e-03
0: TRAIN [1][1220/1938]	Time 0.180 (0.164)	Data 1.02e-04 (3.16e-04)	Tok/s 93856 (88093)	Loss/tok 3.2909 (3.4259)	LR 2.000e-03
0: TRAIN [1][1230/1938]	Time 0.124 (0.163)	Data 1.26e-04 (3.14e-04)	Tok/s 84567 (88096)	Loss/tok 3.0898 (3.4252)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1240/1938]	Time 0.178 (0.163)	Data 1.05e-04 (3.12e-04)	Tok/s 95408 (88081)	Loss/tok 3.3877 (3.4246)	LR 2.000e-03
0: TRAIN [1][1250/1938]	Time 0.123 (0.163)	Data 1.14e-04 (3.10e-04)	Tok/s 83533 (88079)	Loss/tok 3.2273 (3.4239)	LR 2.000e-03
0: TRAIN [1][1260/1938]	Time 0.182 (0.164)	Data 9.80e-05 (3.09e-04)	Tok/s 93322 (88107)	Loss/tok 3.3424 (3.4244)	LR 2.000e-03
0: TRAIN [1][1270/1938]	Time 0.124 (0.163)	Data 1.01e-04 (3.07e-04)	Tok/s 84188 (88094)	Loss/tok 3.1773 (3.4235)	LR 2.000e-03
0: TRAIN [1][1280/1938]	Time 0.241 (0.163)	Data 1.00e-04 (3.06e-04)	Tok/s 96556 (88099)	Loss/tok 3.5810 (3.4231)	LR 2.000e-03
0: TRAIN [1][1290/1938]	Time 0.180 (0.163)	Data 9.92e-05 (3.04e-04)	Tok/s 93658 (88084)	Loss/tok 3.2881 (3.4224)	LR 2.000e-03
0: TRAIN [1][1300/1938]	Time 0.124 (0.163)	Data 1.01e-04 (3.02e-04)	Tok/s 83800 (88079)	Loss/tok 3.1728 (3.4216)	LR 2.000e-03
0: TRAIN [1][1310/1938]	Time 0.123 (0.163)	Data 1.08e-04 (3.01e-04)	Tok/s 85199 (88078)	Loss/tok 3.0500 (3.4218)	LR 2.000e-03
0: TRAIN [1][1320/1938]	Time 0.180 (0.163)	Data 1.00e-04 (2.99e-04)	Tok/s 92099 (88091)	Loss/tok 3.3896 (3.4222)	LR 2.000e-03
0: TRAIN [1][1330/1938]	Time 0.123 (0.163)	Data 1.00e-04 (2.98e-04)	Tok/s 83687 (88084)	Loss/tok 3.1061 (3.4217)	LR 2.000e-03
0: TRAIN [1][1340/1938]	Time 0.069 (0.163)	Data 9.75e-05 (2.96e-04)	Tok/s 76031 (88071)	Loss/tok 2.6214 (3.4213)	LR 2.000e-03
0: TRAIN [1][1350/1938]	Time 0.070 (0.163)	Data 1.00e-04 (2.95e-04)	Tok/s 75034 (88077)	Loss/tok 2.7886 (3.4210)	LR 2.000e-03
0: TRAIN [1][1360/1938]	Time 0.180 (0.163)	Data 9.70e-05 (2.93e-04)	Tok/s 92149 (88068)	Loss/tok 3.3182 (3.4203)	LR 2.000e-03
0: TRAIN [1][1370/1938]	Time 0.240 (0.163)	Data 1.00e-04 (2.92e-04)	Tok/s 95738 (88056)	Loss/tok 3.6281 (3.4193)	LR 2.000e-03
0: TRAIN [1][1380/1938]	Time 0.069 (0.163)	Data 1.02e-04 (2.91e-04)	Tok/s 76563 (88072)	Loss/tok 2.6120 (3.4195)	LR 2.000e-03
0: TRAIN [1][1390/1938]	Time 0.124 (0.163)	Data 9.89e-05 (2.89e-04)	Tok/s 83703 (88065)	Loss/tok 3.1384 (3.4185)	LR 2.000e-03
0: TRAIN [1][1400/1938]	Time 0.070 (0.163)	Data 9.99e-05 (2.88e-04)	Tok/s 75242 (88033)	Loss/tok 2.6454 (3.4175)	LR 2.000e-03
0: TRAIN [1][1410/1938]	Time 0.124 (0.163)	Data 9.80e-05 (2.87e-04)	Tok/s 84855 (88037)	Loss/tok 3.0930 (3.4173)	LR 2.000e-03
0: TRAIN [1][1420/1938]	Time 0.122 (0.163)	Data 9.68e-05 (2.85e-04)	Tok/s 83899 (88039)	Loss/tok 3.0951 (3.4168)	LR 2.000e-03
0: TRAIN [1][1430/1938]	Time 0.313 (0.163)	Data 1.16e-04 (2.84e-04)	Tok/s 95452 (88056)	Loss/tok 3.7494 (3.4166)	LR 2.000e-03
0: TRAIN [1][1440/1938]	Time 0.124 (0.163)	Data 1.00e-04 (2.83e-04)	Tok/s 81812 (88041)	Loss/tok 3.1060 (3.4160)	LR 2.000e-03
0: TRAIN [1][1450/1938]	Time 0.070 (0.163)	Data 9.94e-05 (2.81e-04)	Tok/s 75814 (88044)	Loss/tok 2.6770 (3.4164)	LR 2.000e-03
0: TRAIN [1][1460/1938]	Time 0.183 (0.163)	Data 1.02e-04 (2.80e-04)	Tok/s 91576 (88059)	Loss/tok 3.2952 (3.4161)	LR 2.000e-03
0: TRAIN [1][1470/1938]	Time 0.182 (0.163)	Data 9.85e-05 (2.79e-04)	Tok/s 91565 (88065)	Loss/tok 3.4321 (3.4156)	LR 2.000e-03
0: TRAIN [1][1480/1938]	Time 0.182 (0.163)	Data 9.80e-05 (2.78e-04)	Tok/s 92530 (88066)	Loss/tok 3.4272 (3.4146)	LR 2.000e-03
0: TRAIN [1][1490/1938]	Time 0.181 (0.163)	Data 1.01e-04 (2.77e-04)	Tok/s 91800 (88071)	Loss/tok 3.4171 (3.4152)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1500/1938]	Time 0.069 (0.163)	Data 1.05e-04 (2.75e-04)	Tok/s 76555 (88072)	Loss/tok 2.6631 (3.4148)	LR 2.000e-03
0: TRAIN [1][1510/1938]	Time 0.184 (0.163)	Data 1.04e-04 (2.74e-04)	Tok/s 90694 (88074)	Loss/tok 3.3615 (3.4144)	LR 2.000e-03
0: TRAIN [1][1520/1938]	Time 0.125 (0.163)	Data 1.00e-04 (2.73e-04)	Tok/s 82287 (88075)	Loss/tok 3.1851 (3.4141)	LR 2.000e-03
0: TRAIN [1][1530/1938]	Time 0.182 (0.163)	Data 1.03e-04 (2.72e-04)	Tok/s 93177 (88078)	Loss/tok 3.2016 (3.4140)	LR 2.000e-03
0: TRAIN [1][1540/1938]	Time 0.125 (0.163)	Data 9.85e-05 (2.71e-04)	Tok/s 82504 (88092)	Loss/tok 3.1119 (3.4142)	LR 2.000e-03
0: TRAIN [1][1550/1938]	Time 0.180 (0.163)	Data 1.02e-04 (2.70e-04)	Tok/s 93124 (88101)	Loss/tok 3.3367 (3.4141)	LR 2.000e-03
0: TRAIN [1][1560/1938]	Time 0.242 (0.163)	Data 9.56e-05 (2.69e-04)	Tok/s 95223 (88112)	Loss/tok 3.6007 (3.4141)	LR 2.000e-03
0: TRAIN [1][1570/1938]	Time 0.125 (0.164)	Data 1.02e-04 (2.68e-04)	Tok/s 82054 (88120)	Loss/tok 3.1270 (3.4139)	LR 2.000e-03
0: TRAIN [1][1580/1938]	Time 0.124 (0.164)	Data 1.16e-04 (2.67e-04)	Tok/s 83523 (88124)	Loss/tok 3.0499 (3.4132)	LR 2.000e-03
0: TRAIN [1][1590/1938]	Time 0.242 (0.164)	Data 1.02e-04 (2.66e-04)	Tok/s 95689 (88129)	Loss/tok 3.5229 (3.4129)	LR 2.000e-03
0: TRAIN [1][1600/1938]	Time 0.123 (0.163)	Data 9.61e-05 (2.65e-04)	Tok/s 84520 (88122)	Loss/tok 3.1775 (3.4123)	LR 2.000e-03
0: TRAIN [1][1610/1938]	Time 0.124 (0.163)	Data 9.87e-05 (2.64e-04)	Tok/s 83133 (88104)	Loss/tok 3.1702 (3.4115)	LR 2.000e-03
0: TRAIN [1][1620/1938]	Time 0.182 (0.163)	Data 1.01e-04 (2.63e-04)	Tok/s 92912 (88106)	Loss/tok 3.2889 (3.4109)	LR 2.000e-03
0: TRAIN [1][1630/1938]	Time 0.070 (0.163)	Data 9.56e-05 (2.62e-04)	Tok/s 74476 (88106)	Loss/tok 2.6822 (3.4100)	LR 2.000e-03
0: TRAIN [1][1640/1938]	Time 0.123 (0.163)	Data 9.92e-05 (2.61e-04)	Tok/s 83665 (88091)	Loss/tok 3.1141 (3.4091)	LR 2.000e-03
0: TRAIN [1][1650/1938]	Time 0.240 (0.163)	Data 9.61e-05 (2.60e-04)	Tok/s 96912 (88099)	Loss/tok 3.4865 (3.4089)	LR 2.000e-03
0: TRAIN [1][1660/1938]	Time 0.122 (0.163)	Data 1.00e-04 (2.59e-04)	Tok/s 84379 (88102)	Loss/tok 3.1331 (3.4088)	LR 2.000e-03
0: TRAIN [1][1670/1938]	Time 0.180 (0.163)	Data 9.82e-05 (2.58e-04)	Tok/s 93773 (88124)	Loss/tok 3.3982 (3.4089)	LR 2.000e-03
0: TRAIN [1][1680/1938]	Time 0.178 (0.163)	Data 9.68e-05 (2.57e-04)	Tok/s 94956 (88095)	Loss/tok 3.3445 (3.4077)	LR 2.000e-03
0: TRAIN [1][1690/1938]	Time 0.312 (0.163)	Data 1.02e-04 (2.56e-04)	Tok/s 95018 (88096)	Loss/tok 3.6393 (3.4072)	LR 2.000e-03
0: TRAIN [1][1700/1938]	Time 0.179 (0.163)	Data 1.02e-04 (2.55e-04)	Tok/s 93699 (88085)	Loss/tok 3.2685 (3.4063)	LR 2.000e-03
0: TRAIN [1][1710/1938]	Time 0.180 (0.163)	Data 9.99e-05 (2.54e-04)	Tok/s 93970 (88098)	Loss/tok 3.3272 (3.4068)	LR 2.000e-03
0: TRAIN [1][1720/1938]	Time 0.240 (0.163)	Data 1.00e-04 (2.53e-04)	Tok/s 97224 (88098)	Loss/tok 3.4958 (3.4065)	LR 2.000e-03
0: TRAIN [1][1730/1938]	Time 0.124 (0.163)	Data 1.02e-04 (2.52e-04)	Tok/s 82489 (88077)	Loss/tok 3.0644 (3.4058)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][1740/1938]	Time 0.069 (0.163)	Data 9.87e-05 (2.52e-04)	Tok/s 75177 (88046)	Loss/tok 2.6103 (3.4055)	LR 2.000e-03
0: TRAIN [1][1750/1938]	Time 0.124 (0.163)	Data 1.06e-04 (2.51e-04)	Tok/s 85418 (88046)	Loss/tok 3.0864 (3.4050)	LR 2.000e-03
0: TRAIN [1][1760/1938]	Time 0.242 (0.163)	Data 9.92e-05 (2.50e-04)	Tok/s 95799 (88023)	Loss/tok 3.6006 (3.4040)	LR 2.000e-03
0: TRAIN [1][1770/1938]	Time 0.242 (0.163)	Data 9.89e-05 (2.49e-04)	Tok/s 95639 (88019)	Loss/tok 3.5473 (3.4035)	LR 2.000e-03
0: TRAIN [1][1780/1938]	Time 0.124 (0.163)	Data 9.68e-05 (2.48e-04)	Tok/s 83045 (88009)	Loss/tok 3.1245 (3.4029)	LR 2.000e-03
0: TRAIN [1][1790/1938]	Time 0.125 (0.163)	Data 1.13e-04 (2.47e-04)	Tok/s 83033 (88006)	Loss/tok 3.1488 (3.4022)	LR 2.000e-03
0: TRAIN [1][1800/1938]	Time 0.125 (0.163)	Data 9.94e-05 (2.47e-04)	Tok/s 81577 (88009)	Loss/tok 3.1860 (3.4020)	LR 2.000e-03
0: TRAIN [1][1810/1938]	Time 0.179 (0.162)	Data 9.99e-05 (2.46e-04)	Tok/s 92790 (87987)	Loss/tok 3.5132 (3.4013)	LR 2.000e-03
0: TRAIN [1][1820/1938]	Time 0.124 (0.162)	Data 9.94e-05 (2.45e-04)	Tok/s 83714 (87982)	Loss/tok 3.1128 (3.4010)	LR 2.000e-03
0: TRAIN [1][1830/1938]	Time 0.126 (0.162)	Data 9.97e-05 (2.44e-04)	Tok/s 82583 (87960)	Loss/tok 3.0046 (3.4005)	LR 2.000e-03
0: TRAIN [1][1840/1938]	Time 0.182 (0.162)	Data 9.92e-05 (2.43e-04)	Tok/s 92383 (87964)	Loss/tok 3.3265 (3.4000)	LR 2.000e-03
0: TRAIN [1][1850/1938]	Time 0.182 (0.162)	Data 9.99e-05 (2.43e-04)	Tok/s 93066 (87974)	Loss/tok 3.4212 (3.3999)	LR 2.000e-03
0: TRAIN [1][1860/1938]	Time 0.179 (0.162)	Data 1.06e-04 (2.42e-04)	Tok/s 93787 (87959)	Loss/tok 3.2594 (3.3992)	LR 2.000e-03
0: TRAIN [1][1870/1938]	Time 0.123 (0.162)	Data 1.15e-04 (2.41e-04)	Tok/s 82132 (87962)	Loss/tok 3.1853 (3.3991)	LR 2.000e-03
0: TRAIN [1][1880/1938]	Time 0.124 (0.162)	Data 9.82e-05 (2.40e-04)	Tok/s 83644 (87971)	Loss/tok 3.2301 (3.3989)	LR 2.000e-03
0: TRAIN [1][1890/1938]	Time 0.180 (0.162)	Data 9.94e-05 (2.40e-04)	Tok/s 94821 (87963)	Loss/tok 3.3409 (3.3982)	LR 2.000e-03
0: TRAIN [1][1900/1938]	Time 0.069 (0.162)	Data 9.99e-05 (2.39e-04)	Tok/s 77336 (87963)	Loss/tok 2.6408 (3.3979)	LR 2.000e-03
0: TRAIN [1][1910/1938]	Time 0.123 (0.162)	Data 1.02e-04 (2.38e-04)	Tok/s 83172 (87963)	Loss/tok 3.1040 (3.3977)	LR 2.000e-03
0: TRAIN [1][1920/1938]	Time 0.181 (0.162)	Data 1.01e-04 (2.38e-04)	Tok/s 92503 (87957)	Loss/tok 3.1985 (3.3971)	LR 2.000e-03
0: TRAIN [1][1930/1938]	Time 0.124 (0.162)	Data 1.14e-04 (2.37e-04)	Tok/s 82689 (87953)	Loss/tok 3.0990 (3.3965)	LR 2.000e-03
:::MLL 1575775861.033 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1575775861.034 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.633 (0.633)	Decoder iters 109.0 (109.0)	Tok/s 25941 (25941)
0: Running moses detokenizer
0: BLEU(score=21.874321686713497, counts=[36236, 17507, 9597, 5487], totals=[66399, 63396, 60393, 57395], precisions=[54.57311104082893, 27.615306959429617, 15.890914509959764, 9.560066207857828], bp=1.0, sys_len=66399, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1575775862.895 eval_accuracy: {"value": 21.87, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1575775862.895 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.3974	Test BLEU: 21.87
0: Performance: Epoch: 1	Training: 703907 Tok/s
0: Finished epoch 1
:::MLL 1575775862.895 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1575775862.896 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1575775862.896 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 655727073
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][0/1938]	Time 0.347 (0.347)	Data 2.66e-01 (2.66e-01)	Tok/s 15481 (15481)	Loss/tok 2.6063 (2.6063)	LR 2.000e-03
0: TRAIN [2][10/1938]	Time 0.179 (0.182)	Data 1.04e-04 (2.43e-02)	Tok/s 92777 (82149)	Loss/tok 3.4142 (3.2391)	LR 2.000e-03
0: TRAIN [2][20/1938]	Time 0.180 (0.168)	Data 1.17e-04 (1.28e-02)	Tok/s 93190 (84879)	Loss/tok 3.3422 (3.2124)	LR 2.000e-03
0: TRAIN [2][30/1938]	Time 0.180 (0.176)	Data 1.16e-04 (8.69e-03)	Tok/s 92917 (86964)	Loss/tok 3.2551 (3.2640)	LR 2.000e-03
0: TRAIN [2][40/1938]	Time 0.242 (0.179)	Data 1.01e-04 (6.59e-03)	Tok/s 96348 (88155)	Loss/tok 3.4262 (3.2633)	LR 2.000e-03
0: TRAIN [2][50/1938]	Time 0.180 (0.176)	Data 1.01e-04 (5.32e-03)	Tok/s 92249 (88110)	Loss/tok 3.2203 (3.2596)	LR 2.000e-03
0: TRAIN [2][60/1938]	Time 0.181 (0.175)	Data 1.00e-04 (4.46e-03)	Tok/s 93043 (88237)	Loss/tok 3.1668 (3.2690)	LR 2.000e-03
0: TRAIN [2][70/1938]	Time 0.181 (0.173)	Data 9.94e-05 (3.85e-03)	Tok/s 92851 (88468)	Loss/tok 3.4522 (3.2657)	LR 2.000e-03
0: TRAIN [2][80/1938]	Time 0.123 (0.170)	Data 9.75e-05 (3.39e-03)	Tok/s 84291 (88175)	Loss/tok 3.0724 (3.2542)	LR 2.000e-03
0: TRAIN [2][90/1938]	Time 0.242 (0.170)	Data 1.26e-04 (3.03e-03)	Tok/s 96795 (88305)	Loss/tok 3.4484 (3.2589)	LR 2.000e-03
0: TRAIN [2][100/1938]	Time 0.126 (0.169)	Data 1.04e-04 (2.74e-03)	Tok/s 82799 (88284)	Loss/tok 3.0547 (3.2535)	LR 2.000e-03
0: TRAIN [2][110/1938]	Time 0.182 (0.168)	Data 9.70e-05 (2.50e-03)	Tok/s 91663 (88167)	Loss/tok 3.3380 (3.2505)	LR 2.000e-03
0: TRAIN [2][120/1938]	Time 0.181 (0.169)	Data 9.82e-05 (2.30e-03)	Tok/s 92973 (88297)	Loss/tok 3.1825 (3.2623)	LR 2.000e-03
0: TRAIN [2][130/1938]	Time 0.124 (0.171)	Data 1.30e-04 (2.14e-03)	Tok/s 83550 (88319)	Loss/tok 3.0434 (3.2714)	LR 2.000e-03
0: TRAIN [2][140/1938]	Time 0.123 (0.170)	Data 9.99e-05 (1.99e-03)	Tok/s 83338 (88306)	Loss/tok 3.0934 (3.2705)	LR 2.000e-03
0: TRAIN [2][150/1938]	Time 0.124 (0.174)	Data 9.99e-05 (1.87e-03)	Tok/s 84920 (88585)	Loss/tok 3.0647 (3.2816)	LR 2.000e-03
0: TRAIN [2][160/1938]	Time 0.181 (0.171)	Data 1.30e-04 (1.76e-03)	Tok/s 92956 (88388)	Loss/tok 3.2212 (3.2729)	LR 2.000e-03
0: TRAIN [2][170/1938]	Time 0.183 (0.171)	Data 9.99e-05 (1.66e-03)	Tok/s 91723 (88363)	Loss/tok 3.3638 (3.2718)	LR 2.000e-03
0: TRAIN [2][180/1938]	Time 0.122 (0.170)	Data 1.15e-04 (1.58e-03)	Tok/s 84751 (88339)	Loss/tok 3.0563 (3.2699)	LR 2.000e-03
0: TRAIN [2][190/1938]	Time 0.315 (0.169)	Data 9.92e-05 (1.50e-03)	Tok/s 94816 (88236)	Loss/tok 3.5987 (3.2675)	LR 2.000e-03
0: TRAIN [2][200/1938]	Time 0.181 (0.169)	Data 1.00e-04 (1.43e-03)	Tok/s 93822 (88270)	Loss/tok 3.2368 (3.2639)	LR 2.000e-03
0: TRAIN [2][210/1938]	Time 0.184 (0.170)	Data 1.14e-04 (1.37e-03)	Tok/s 90488 (88409)	Loss/tok 3.4359 (3.2686)	LR 2.000e-03
0: TRAIN [2][220/1938]	Time 0.181 (0.170)	Data 9.97e-05 (1.31e-03)	Tok/s 94019 (88435)	Loss/tok 3.3082 (3.2702)	LR 2.000e-03
0: TRAIN [2][230/1938]	Time 0.124 (0.170)	Data 9.85e-05 (1.26e-03)	Tok/s 82771 (88412)	Loss/tok 3.0791 (3.2681)	LR 2.000e-03
0: TRAIN [2][240/1938]	Time 0.181 (0.171)	Data 1.00e-04 (1.21e-03)	Tok/s 92909 (88443)	Loss/tok 3.2915 (3.2734)	LR 2.000e-03
0: TRAIN [2][250/1938]	Time 0.313 (0.171)	Data 9.85e-05 (1.17e-03)	Tok/s 95346 (88413)	Loss/tok 3.7477 (3.2767)	LR 2.000e-03
0: TRAIN [2][260/1938]	Time 0.181 (0.171)	Data 9.87e-05 (1.13e-03)	Tok/s 91649 (88449)	Loss/tok 3.2482 (3.2768)	LR 2.000e-03
0: TRAIN [2][270/1938]	Time 0.182 (0.170)	Data 1.02e-04 (1.09e-03)	Tok/s 92776 (88437)	Loss/tok 3.3182 (3.2758)	LR 2.000e-03
0: TRAIN [2][280/1938]	Time 0.241 (0.170)	Data 1.21e-04 (1.05e-03)	Tok/s 94891 (88457)	Loss/tok 3.5782 (3.2795)	LR 2.000e-03
0: TRAIN [2][290/1938]	Time 0.122 (0.170)	Data 1.10e-04 (1.02e-03)	Tok/s 84808 (88387)	Loss/tok 3.1667 (3.2765)	LR 2.000e-03
0: TRAIN [2][300/1938]	Time 0.123 (0.169)	Data 9.99e-05 (9.90e-04)	Tok/s 83968 (88324)	Loss/tok 3.1370 (3.2723)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][310/1938]	Time 0.120 (0.169)	Data 1.04e-04 (9.61e-04)	Tok/s 88754 (88296)	Loss/tok 3.1211 (3.2736)	LR 2.000e-03
0: TRAIN [2][320/1938]	Time 0.242 (0.168)	Data 9.89e-05 (9.35e-04)	Tok/s 95768 (88278)	Loss/tok 3.6090 (3.2729)	LR 2.000e-03
0: TRAIN [2][330/1938]	Time 0.312 (0.169)	Data 9.87e-05 (9.10e-04)	Tok/s 94929 (88253)	Loss/tok 3.7949 (3.2768)	LR 2.000e-03
0: TRAIN [2][340/1938]	Time 0.241 (0.169)	Data 1.03e-04 (8.86e-04)	Tok/s 96096 (88274)	Loss/tok 3.3691 (3.2769)	LR 2.000e-03
0: TRAIN [2][350/1938]	Time 0.180 (0.168)	Data 9.87e-05 (8.64e-04)	Tok/s 93822 (88219)	Loss/tok 3.2830 (3.2751)	LR 2.000e-03
0: TRAIN [2][360/1938]	Time 0.122 (0.167)	Data 1.05e-04 (8.43e-04)	Tok/s 83824 (88159)	Loss/tok 2.9945 (3.2722)	LR 2.000e-03
0: TRAIN [2][370/1938]	Time 0.123 (0.166)	Data 9.99e-05 (8.23e-04)	Tok/s 82093 (88062)	Loss/tok 3.0365 (3.2689)	LR 2.000e-03
0: TRAIN [2][380/1938]	Time 0.180 (0.166)	Data 1.00e-04 (8.04e-04)	Tok/s 93655 (88019)	Loss/tok 3.2141 (3.2693)	LR 2.000e-03
0: TRAIN [2][390/1938]	Time 0.123 (0.165)	Data 1.20e-04 (7.86e-04)	Tok/s 83401 (88021)	Loss/tok 2.9488 (3.2673)	LR 2.000e-03
0: TRAIN [2][400/1938]	Time 0.124 (0.166)	Data 1.03e-04 (7.69e-04)	Tok/s 81482 (88065)	Loss/tok 3.0535 (3.2693)	LR 2.000e-03
0: TRAIN [2][410/1938]	Time 0.124 (0.166)	Data 1.15e-04 (7.53e-04)	Tok/s 84975 (88033)	Loss/tok 3.0957 (3.2707)	LR 2.000e-03
0: TRAIN [2][420/1938]	Time 0.181 (0.166)	Data 1.18e-04 (7.38e-04)	Tok/s 92426 (88067)	Loss/tok 3.3214 (3.2697)	LR 2.000e-03
0: TRAIN [2][430/1938]	Time 0.180 (0.166)	Data 1.02e-04 (7.23e-04)	Tok/s 92916 (88064)	Loss/tok 3.2295 (3.2692)	LR 2.000e-03
0: TRAIN [2][440/1938]	Time 0.122 (0.166)	Data 1.17e-04 (7.09e-04)	Tok/s 84573 (88088)	Loss/tok 3.0384 (3.2687)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][450/1938]	Time 0.124 (0.166)	Data 1.17e-04 (6.96e-04)	Tok/s 83526 (88146)	Loss/tok 3.1158 (3.2725)	LR 2.000e-03
0: TRAIN [2][460/1938]	Time 0.181 (0.166)	Data 1.11e-04 (6.83e-04)	Tok/s 91964 (88129)	Loss/tok 3.2472 (3.2711)	LR 2.000e-03
0: TRAIN [2][470/1938]	Time 0.123 (0.166)	Data 1.15e-04 (6.71e-04)	Tok/s 84054 (88110)	Loss/tok 3.1954 (3.2724)	LR 2.000e-03
0: TRAIN [2][480/1938]	Time 0.181 (0.166)	Data 1.03e-04 (6.59e-04)	Tok/s 92768 (88134)	Loss/tok 3.3761 (3.2707)	LR 2.000e-03
0: TRAIN [2][490/1938]	Time 0.126 (0.166)	Data 1.16e-04 (6.48e-04)	Tok/s 80512 (88127)	Loss/tok 3.0447 (3.2700)	LR 2.000e-03
0: TRAIN [2][500/1938]	Time 0.069 (0.165)	Data 9.85e-05 (6.37e-04)	Tok/s 76913 (88019)	Loss/tok 2.5723 (3.2666)	LR 2.000e-03
0: TRAIN [2][510/1938]	Time 0.125 (0.164)	Data 9.99e-05 (6.26e-04)	Tok/s 82877 (88006)	Loss/tok 3.1858 (3.2655)	LR 2.000e-03
0: TRAIN [2][520/1938]	Time 0.124 (0.164)	Data 9.92e-05 (6.16e-04)	Tok/s 84767 (88033)	Loss/tok 2.9665 (3.2650)	LR 2.000e-03
0: TRAIN [2][530/1938]	Time 0.122 (0.165)	Data 1.01e-04 (6.07e-04)	Tok/s 85834 (88077)	Loss/tok 3.0298 (3.2667)	LR 2.000e-03
0: TRAIN [2][540/1938]	Time 0.071 (0.164)	Data 1.17e-04 (5.97e-04)	Tok/s 73051 (88009)	Loss/tok 2.6937 (3.2648)	LR 2.000e-03
0: TRAIN [2][550/1938]	Time 0.069 (0.164)	Data 9.97e-05 (5.89e-04)	Tok/s 76714 (88023)	Loss/tok 2.6154 (3.2650)	LR 2.000e-03
0: TRAIN [2][560/1938]	Time 0.123 (0.163)	Data 1.15e-04 (5.80e-04)	Tok/s 84315 (87946)	Loss/tok 3.0776 (3.2634)	LR 2.000e-03
0: TRAIN [2][570/1938]	Time 0.070 (0.163)	Data 9.97e-05 (5.72e-04)	Tok/s 75319 (87943)	Loss/tok 2.5252 (3.2642)	LR 2.000e-03
0: TRAIN [2][580/1938]	Time 0.123 (0.164)	Data 1.18e-04 (5.64e-04)	Tok/s 83555 (87974)	Loss/tok 3.0116 (3.2647)	LR 2.000e-03
0: TRAIN [2][590/1938]	Time 0.123 (0.163)	Data 1.15e-04 (5.56e-04)	Tok/s 82321 (87921)	Loss/tok 3.0245 (3.2624)	LR 2.000e-03
0: TRAIN [2][600/1938]	Time 0.181 (0.163)	Data 1.03e-04 (5.48e-04)	Tok/s 92084 (87939)	Loss/tok 3.2813 (3.2628)	LR 2.000e-03
0: TRAIN [2][610/1938]	Time 0.070 (0.163)	Data 1.21e-04 (5.41e-04)	Tok/s 77138 (87922)	Loss/tok 2.6296 (3.2621)	LR 2.000e-03
0: TRAIN [2][620/1938]	Time 0.124 (0.163)	Data 1.01e-04 (5.34e-04)	Tok/s 83532 (87887)	Loss/tok 3.1215 (3.2608)	LR 2.000e-03
0: TRAIN [2][630/1938]	Time 0.122 (0.163)	Data 1.04e-04 (5.28e-04)	Tok/s 84938 (87896)	Loss/tok 2.9682 (3.2621)	LR 2.000e-03
0: TRAIN [2][640/1938]	Time 0.123 (0.162)	Data 1.36e-04 (5.21e-04)	Tok/s 84429 (87876)	Loss/tok 3.0810 (3.2623)	LR 2.000e-03
0: TRAIN [2][650/1938]	Time 0.123 (0.163)	Data 1.18e-04 (5.15e-04)	Tok/s 82427 (87869)	Loss/tok 3.0656 (3.2629)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][660/1938]	Time 0.180 (0.163)	Data 1.15e-04 (5.09e-04)	Tok/s 93805 (87885)	Loss/tok 3.2953 (3.2639)	LR 2.000e-03
0: TRAIN [2][670/1938]	Time 0.123 (0.162)	Data 1.03e-04 (5.03e-04)	Tok/s 83891 (87874)	Loss/tok 3.0287 (3.2635)	LR 2.000e-03
0: TRAIN [2][680/1938]	Time 0.242 (0.162)	Data 9.89e-05 (4.97e-04)	Tok/s 95817 (87856)	Loss/tok 3.4170 (3.2628)	LR 2.000e-03
0: TRAIN [2][690/1938]	Time 0.124 (0.162)	Data 1.01e-04 (4.92e-04)	Tok/s 85212 (87872)	Loss/tok 3.0111 (3.2632)	LR 2.000e-03
0: TRAIN [2][700/1938]	Time 0.240 (0.163)	Data 1.03e-04 (4.86e-04)	Tok/s 98055 (87885)	Loss/tok 3.4023 (3.2636)	LR 2.000e-03
0: TRAIN [2][710/1938]	Time 0.240 (0.163)	Data 1.14e-04 (4.81e-04)	Tok/s 96925 (87918)	Loss/tok 3.5171 (3.2641)	LR 2.000e-03
0: TRAIN [2][720/1938]	Time 0.180 (0.163)	Data 1.04e-04 (4.76e-04)	Tok/s 91712 (87917)	Loss/tok 3.2292 (3.2640)	LR 2.000e-03
0: TRAIN [2][730/1938]	Time 0.185 (0.163)	Data 9.92e-05 (4.71e-04)	Tok/s 91988 (87928)	Loss/tok 3.1968 (3.2634)	LR 2.000e-03
0: TRAIN [2][740/1938]	Time 0.185 (0.162)	Data 1.03e-04 (4.66e-04)	Tok/s 89330 (87894)	Loss/tok 3.3886 (3.2621)	LR 2.000e-03
0: TRAIN [2][750/1938]	Time 0.182 (0.162)	Data 1.02e-04 (4.61e-04)	Tok/s 91781 (87899)	Loss/tok 3.2657 (3.2615)	LR 2.000e-03
0: TRAIN [2][760/1938]	Time 0.181 (0.162)	Data 1.02e-04 (4.56e-04)	Tok/s 93090 (87915)	Loss/tok 3.3286 (3.2615)	LR 2.000e-03
0: TRAIN [2][770/1938]	Time 0.124 (0.163)	Data 1.00e-04 (4.52e-04)	Tok/s 83332 (87926)	Loss/tok 2.9335 (3.2620)	LR 2.000e-03
0: TRAIN [2][780/1938]	Time 0.122 (0.163)	Data 1.02e-04 (4.47e-04)	Tok/s 85152 (87949)	Loss/tok 3.0496 (3.2622)	LR 2.000e-03
0: TRAIN [2][790/1938]	Time 0.316 (0.163)	Data 1.54e-04 (4.43e-04)	Tok/s 94907 (87927)	Loss/tok 3.5695 (3.2629)	LR 2.000e-03
0: TRAIN [2][800/1938]	Time 0.123 (0.162)	Data 1.02e-04 (4.39e-04)	Tok/s 84986 (87891)	Loss/tok 3.0158 (3.2616)	LR 2.000e-03
0: TRAIN [2][810/1938]	Time 0.180 (0.162)	Data 1.01e-04 (4.35e-04)	Tok/s 94067 (87893)	Loss/tok 3.2306 (3.2623)	LR 2.000e-03
0: TRAIN [2][820/1938]	Time 0.068 (0.162)	Data 1.11e-04 (4.31e-04)	Tok/s 76331 (87898)	Loss/tok 2.6540 (3.2629)	LR 2.000e-03
0: TRAIN [2][830/1938]	Time 0.070 (0.162)	Data 1.16e-04 (4.27e-04)	Tok/s 76490 (87846)	Loss/tok 2.6134 (3.2614)	LR 2.000e-03
0: TRAIN [2][840/1938]	Time 0.181 (0.162)	Data 1.02e-04 (4.23e-04)	Tok/s 93478 (87855)	Loss/tok 3.2121 (3.2608)	LR 2.000e-03
0: TRAIN [2][850/1938]	Time 0.181 (0.161)	Data 1.01e-04 (4.19e-04)	Tok/s 92702 (87820)	Loss/tok 3.1584 (3.2593)	LR 2.000e-03
0: TRAIN [2][860/1938]	Time 0.123 (0.161)	Data 1.18e-04 (4.16e-04)	Tok/s 82486 (87807)	Loss/tok 3.0715 (3.2598)	LR 2.000e-03
0: TRAIN [2][870/1938]	Time 0.244 (0.161)	Data 1.18e-04 (4.12e-04)	Tok/s 94878 (87806)	Loss/tok 3.4177 (3.2598)	LR 2.000e-03
0: TRAIN [2][880/1938]	Time 0.124 (0.161)	Data 9.99e-05 (4.09e-04)	Tok/s 83999 (87797)	Loss/tok 3.0459 (3.2598)	LR 2.000e-03
0: TRAIN [2][890/1938]	Time 0.126 (0.161)	Data 1.18e-04 (4.05e-04)	Tok/s 83901 (87799)	Loss/tok 3.2095 (3.2593)	LR 2.000e-03
0: TRAIN [2][900/1938]	Time 0.245 (0.161)	Data 1.79e-04 (4.02e-04)	Tok/s 94249 (87789)	Loss/tok 3.4687 (3.2609)	LR 2.000e-03
0: TRAIN [2][910/1938]	Time 0.179 (0.162)	Data 1.03e-04 (3.99e-04)	Tok/s 94794 (87817)	Loss/tok 3.1823 (3.2616)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][920/1938]	Time 0.180 (0.162)	Data 1.03e-04 (3.96e-04)	Tok/s 93744 (87813)	Loss/tok 3.3035 (3.2622)	LR 2.000e-03
0: TRAIN [2][930/1938]	Time 0.069 (0.162)	Data 1.54e-04 (3.93e-04)	Tok/s 77564 (87802)	Loss/tok 2.5884 (3.2628)	LR 2.000e-03
0: TRAIN [2][940/1938]	Time 0.185 (0.162)	Data 1.03e-04 (3.90e-04)	Tok/s 90992 (87808)	Loss/tok 3.3387 (3.2624)	LR 2.000e-03
0: TRAIN [2][950/1938]	Time 0.183 (0.162)	Data 9.97e-05 (3.87e-04)	Tok/s 91647 (87846)	Loss/tok 3.2148 (3.2630)	LR 2.000e-03
0: TRAIN [2][960/1938]	Time 0.122 (0.162)	Data 1.18e-04 (3.84e-04)	Tok/s 84435 (87819)	Loss/tok 3.0872 (3.2629)	LR 2.000e-03
0: TRAIN [2][970/1938]	Time 0.124 (0.162)	Data 1.09e-04 (3.81e-04)	Tok/s 82298 (87838)	Loss/tok 3.1171 (3.2629)	LR 2.000e-03
0: TRAIN [2][980/1938]	Time 0.182 (0.162)	Data 1.04e-04 (3.78e-04)	Tok/s 93419 (87875)	Loss/tok 3.2830 (3.2642)	LR 2.000e-03
0: TRAIN [2][990/1938]	Time 0.184 (0.163)	Data 1.02e-04 (3.76e-04)	Tok/s 91072 (87900)	Loss/tok 3.3193 (3.2653)	LR 2.000e-03
0: TRAIN [2][1000/1938]	Time 0.179 (0.163)	Data 1.13e-04 (3.73e-04)	Tok/s 94228 (87903)	Loss/tok 3.2356 (3.2651)	LR 2.000e-03
0: TRAIN [2][1010/1938]	Time 0.123 (0.162)	Data 9.78e-05 (3.70e-04)	Tok/s 83211 (87898)	Loss/tok 3.0363 (3.2641)	LR 2.000e-03
0: TRAIN [2][1020/1938]	Time 0.124 (0.162)	Data 1.00e-04 (3.68e-04)	Tok/s 85077 (87890)	Loss/tok 3.0401 (3.2633)	LR 2.000e-03
0: TRAIN [2][1030/1938]	Time 0.313 (0.162)	Data 1.10e-04 (3.65e-04)	Tok/s 94643 (87897)	Loss/tok 3.6171 (3.2645)	LR 2.000e-03
0: TRAIN [2][1040/1938]	Time 0.123 (0.162)	Data 1.02e-04 (3.63e-04)	Tok/s 84696 (87915)	Loss/tok 2.9732 (3.2642)	LR 2.000e-03
0: TRAIN [2][1050/1938]	Time 0.070 (0.162)	Data 1.13e-04 (3.60e-04)	Tok/s 75986 (87904)	Loss/tok 2.7660 (3.2636)	LR 2.000e-03
0: TRAIN [2][1060/1938]	Time 0.122 (0.162)	Data 1.00e-04 (3.58e-04)	Tok/s 82409 (87886)	Loss/tok 2.9578 (3.2631)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1070/1938]	Time 0.120 (0.162)	Data 1.21e-04 (3.56e-04)	Tok/s 86793 (87892)	Loss/tok 2.9643 (3.2636)	LR 2.000e-03
0: TRAIN [2][1080/1938]	Time 0.123 (0.162)	Data 1.01e-04 (3.53e-04)	Tok/s 84398 (87906)	Loss/tok 3.1616 (3.2639)	LR 2.000e-03
0: TRAIN [2][1090/1938]	Time 0.242 (0.162)	Data 1.23e-04 (3.51e-04)	Tok/s 96216 (87904)	Loss/tok 3.4711 (3.2635)	LR 2.000e-03
0: TRAIN [2][1100/1938]	Time 0.125 (0.162)	Data 1.04e-04 (3.49e-04)	Tok/s 83490 (87919)	Loss/tok 3.0943 (3.2633)	LR 2.000e-03
0: TRAIN [2][1110/1938]	Time 0.125 (0.162)	Data 1.02e-04 (3.47e-04)	Tok/s 81017 (87934)	Loss/tok 3.0478 (3.2637)	LR 2.000e-03
0: TRAIN [2][1120/1938]	Time 0.125 (0.162)	Data 1.03e-04 (3.45e-04)	Tok/s 83224 (87940)	Loss/tok 3.0690 (3.2638)	LR 2.000e-03
0: TRAIN [2][1130/1938]	Time 0.124 (0.162)	Data 1.02e-04 (3.42e-04)	Tok/s 82234 (87929)	Loss/tok 3.0483 (3.2631)	LR 2.000e-03
0: TRAIN [2][1140/1938]	Time 0.309 (0.162)	Data 1.32e-04 (3.40e-04)	Tok/s 97243 (87941)	Loss/tok 3.5220 (3.2633)	LR 2.000e-03
0: TRAIN [2][1150/1938]	Time 0.244 (0.162)	Data 1.03e-04 (3.38e-04)	Tok/s 95792 (87960)	Loss/tok 3.3835 (3.2632)	LR 2.000e-03
0: TRAIN [2][1160/1938]	Time 0.180 (0.162)	Data 9.89e-05 (3.36e-04)	Tok/s 91267 (87953)	Loss/tok 3.3033 (3.2630)	LR 2.000e-03
0: TRAIN [2][1170/1938]	Time 0.123 (0.162)	Data 1.02e-04 (3.34e-04)	Tok/s 86210 (87933)	Loss/tok 3.1730 (3.2628)	LR 2.000e-03
0: TRAIN [2][1180/1938]	Time 0.182 (0.162)	Data 1.01e-04 (3.32e-04)	Tok/s 91963 (87950)	Loss/tok 3.1892 (3.2622)	LR 2.000e-03
0: TRAIN [2][1190/1938]	Time 0.179 (0.162)	Data 1.00e-04 (3.31e-04)	Tok/s 93023 (87941)	Loss/tok 3.2132 (3.2613)	LR 2.000e-03
0: TRAIN [2][1200/1938]	Time 0.244 (0.162)	Data 1.04e-04 (3.29e-04)	Tok/s 94771 (87960)	Loss/tok 3.5132 (3.2619)	LR 2.000e-03
0: TRAIN [2][1210/1938]	Time 0.179 (0.162)	Data 1.02e-04 (3.27e-04)	Tok/s 93886 (87972)	Loss/tok 3.2690 (3.2615)	LR 2.000e-03
0: TRAIN [2][1220/1938]	Time 0.123 (0.162)	Data 1.03e-04 (3.25e-04)	Tok/s 86413 (87984)	Loss/tok 3.1162 (3.2610)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1230/1938]	Time 0.244 (0.162)	Data 1.13e-04 (3.23e-04)	Tok/s 95574 (87997)	Loss/tok 3.3928 (3.2608)	LR 2.000e-03
0: TRAIN [2][1240/1938]	Time 0.124 (0.162)	Data 9.99e-05 (3.21e-04)	Tok/s 86317 (87996)	Loss/tok 3.0234 (3.2605)	LR 2.000e-03
0: TRAIN [2][1250/1938]	Time 0.180 (0.162)	Data 1.03e-04 (3.20e-04)	Tok/s 94158 (88010)	Loss/tok 3.2519 (3.2608)	LR 2.000e-03
0: TRAIN [2][1260/1938]	Time 0.180 (0.162)	Data 1.03e-04 (3.18e-04)	Tok/s 92062 (88035)	Loss/tok 3.3103 (3.2617)	LR 2.000e-03
0: TRAIN [2][1270/1938]	Time 0.124 (0.163)	Data 1.02e-04 (3.16e-04)	Tok/s 84849 (88040)	Loss/tok 3.0170 (3.2628)	LR 2.000e-03
0: TRAIN [2][1280/1938]	Time 0.124 (0.162)	Data 9.97e-05 (3.15e-04)	Tok/s 83919 (88018)	Loss/tok 3.0845 (3.2622)	LR 2.000e-03
0: TRAIN [2][1290/1938]	Time 0.312 (0.162)	Data 1.17e-04 (3.13e-04)	Tok/s 95896 (88022)	Loss/tok 3.5694 (3.2619)	LR 2.000e-03
0: TRAIN [2][1300/1938]	Time 0.179 (0.162)	Data 9.92e-05 (3.12e-04)	Tok/s 93808 (88011)	Loss/tok 3.2317 (3.2617)	LR 2.000e-03
0: TRAIN [2][1310/1938]	Time 0.122 (0.162)	Data 1.03e-04 (3.10e-04)	Tok/s 83987 (88014)	Loss/tok 2.9888 (3.2611)	LR 2.000e-03
0: TRAIN [2][1320/1938]	Time 0.124 (0.162)	Data 1.20e-04 (3.08e-04)	Tok/s 84832 (88014)	Loss/tok 3.0934 (3.2611)	LR 2.000e-03
0: TRAIN [2][1330/1938]	Time 0.182 (0.162)	Data 1.16e-04 (3.07e-04)	Tok/s 93043 (88030)	Loss/tok 3.3095 (3.2613)	LR 2.000e-03
0: TRAIN [2][1340/1938]	Time 0.182 (0.162)	Data 9.97e-05 (3.06e-04)	Tok/s 91221 (88022)	Loss/tok 3.2371 (3.2608)	LR 2.000e-03
0: TRAIN [2][1350/1938]	Time 0.241 (0.162)	Data 1.18e-04 (3.04e-04)	Tok/s 95901 (88032)	Loss/tok 3.4600 (3.2614)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1360/1938]	Time 0.182 (0.163)	Data 9.99e-05 (3.03e-04)	Tok/s 92630 (88044)	Loss/tok 3.2388 (3.2621)	LR 2.000e-03
0: TRAIN [2][1370/1938]	Time 0.069 (0.162)	Data 1.01e-04 (3.01e-04)	Tok/s 76473 (88024)	Loss/tok 2.5487 (3.2616)	LR 2.000e-03
0: TRAIN [2][1380/1938]	Time 0.068 (0.162)	Data 1.03e-04 (3.00e-04)	Tok/s 77657 (87999)	Loss/tok 2.6316 (3.2614)	LR 2.000e-03
0: TRAIN [2][1390/1938]	Time 0.185 (0.162)	Data 1.11e-04 (2.99e-04)	Tok/s 89262 (87991)	Loss/tok 3.1991 (3.2611)	LR 2.000e-03
0: TRAIN [2][1400/1938]	Time 0.124 (0.162)	Data 1.03e-04 (2.97e-04)	Tok/s 83241 (88011)	Loss/tok 2.8657 (3.2620)	LR 2.000e-03
0: TRAIN [2][1410/1938]	Time 0.122 (0.162)	Data 1.09e-04 (2.96e-04)	Tok/s 85951 (88021)	Loss/tok 3.1070 (3.2621)	LR 2.000e-03
0: TRAIN [2][1420/1938]	Time 0.182 (0.162)	Data 1.04e-04 (2.95e-04)	Tok/s 90654 (88020)	Loss/tok 3.2477 (3.2615)	LR 2.000e-03
0: TRAIN [2][1430/1938]	Time 0.179 (0.162)	Data 1.16e-04 (2.93e-04)	Tok/s 94361 (88033)	Loss/tok 3.2617 (3.2617)	LR 2.000e-03
0: TRAIN [2][1440/1938]	Time 0.313 (0.162)	Data 9.87e-05 (2.92e-04)	Tok/s 94986 (88026)	Loss/tok 3.6946 (3.2621)	LR 2.000e-03
0: TRAIN [2][1450/1938]	Time 0.122 (0.162)	Data 1.03e-04 (2.91e-04)	Tok/s 83922 (88024)	Loss/tok 3.0264 (3.2623)	LR 2.000e-03
0: TRAIN [2][1460/1938]	Time 0.122 (0.162)	Data 1.04e-04 (2.89e-04)	Tok/s 84115 (88001)	Loss/tok 3.0867 (3.2613)	LR 2.000e-03
0: TRAIN [2][1470/1938]	Time 0.123 (0.162)	Data 9.94e-05 (2.88e-04)	Tok/s 85522 (87993)	Loss/tok 3.1469 (3.2608)	LR 2.000e-03
0: TRAIN [2][1480/1938]	Time 0.126 (0.162)	Data 1.20e-04 (2.87e-04)	Tok/s 83470 (88010)	Loss/tok 3.0483 (3.2613)	LR 2.000e-03
0: TRAIN [2][1490/1938]	Time 0.311 (0.162)	Data 9.94e-05 (2.86e-04)	Tok/s 95230 (88003)	Loss/tok 3.6682 (3.2611)	LR 2.000e-03
0: TRAIN [2][1500/1938]	Time 0.124 (0.162)	Data 1.02e-04 (2.85e-04)	Tok/s 83298 (87990)	Loss/tok 3.0346 (3.2605)	LR 2.000e-03
0: TRAIN [2][1510/1938]	Time 0.183 (0.162)	Data 9.99e-05 (2.83e-04)	Tok/s 91062 (88003)	Loss/tok 3.1079 (3.2606)	LR 2.000e-03
0: TRAIN [2][1520/1938]	Time 0.182 (0.162)	Data 9.78e-05 (2.82e-04)	Tok/s 91016 (87995)	Loss/tok 3.3386 (3.2601)	LR 2.000e-03
0: TRAIN [2][1530/1938]	Time 0.181 (0.162)	Data 1.17e-04 (2.81e-04)	Tok/s 92831 (88014)	Loss/tok 3.2695 (3.2616)	LR 2.000e-03
0: TRAIN [2][1540/1938]	Time 0.125 (0.162)	Data 1.15e-04 (2.80e-04)	Tok/s 84657 (87995)	Loss/tok 3.0017 (3.2608)	LR 2.000e-03
0: TRAIN [2][1550/1938]	Time 0.122 (0.162)	Data 1.19e-04 (2.79e-04)	Tok/s 83606 (87976)	Loss/tok 3.0053 (3.2603)	LR 2.000e-03
0: TRAIN [2][1560/1938]	Time 0.181 (0.162)	Data 1.03e-04 (2.78e-04)	Tok/s 93249 (87984)	Loss/tok 3.2451 (3.2603)	LR 2.000e-03
0: TRAIN [2][1570/1938]	Time 0.181 (0.162)	Data 1.04e-04 (2.77e-04)	Tok/s 91763 (88002)	Loss/tok 3.2506 (3.2605)	LR 2.000e-03
0: TRAIN [2][1580/1938]	Time 0.123 (0.162)	Data 1.09e-04 (2.76e-04)	Tok/s 84228 (88002)	Loss/tok 2.9741 (3.2601)	LR 2.000e-03
0: TRAIN [2][1590/1938]	Time 0.182 (0.162)	Data 1.03e-04 (2.75e-04)	Tok/s 92642 (87994)	Loss/tok 3.1452 (3.2592)	LR 2.000e-03
0: TRAIN [2][1600/1938]	Time 0.242 (0.162)	Data 1.02e-04 (2.73e-04)	Tok/s 95668 (88012)	Loss/tok 3.3901 (3.2596)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1610/1938]	Time 0.071 (0.162)	Data 9.80e-05 (2.73e-04)	Tok/s 74905 (88002)	Loss/tok 2.4808 (3.2598)	LR 2.000e-03
0: TRAIN [2][1620/1938]	Time 0.122 (0.162)	Data 1.09e-04 (2.71e-04)	Tok/s 84831 (88014)	Loss/tok 3.0688 (3.2593)	LR 2.000e-03
0: TRAIN [2][1630/1938]	Time 0.123 (0.162)	Data 1.15e-04 (2.70e-04)	Tok/s 82004 (87997)	Loss/tok 3.0357 (3.2591)	LR 2.000e-03
0: TRAIN [2][1640/1938]	Time 0.242 (0.162)	Data 1.03e-04 (2.69e-04)	Tok/s 96558 (88001)	Loss/tok 3.5190 (3.2588)	LR 2.000e-03
0: TRAIN [2][1650/1938]	Time 0.185 (0.162)	Data 1.18e-04 (2.68e-04)	Tok/s 90878 (87999)	Loss/tok 3.1352 (3.2588)	LR 2.000e-03
0: TRAIN [2][1660/1938]	Time 0.240 (0.162)	Data 9.99e-05 (2.67e-04)	Tok/s 97010 (88016)	Loss/tok 3.4015 (3.2590)	LR 2.000e-03
0: TRAIN [2][1670/1938]	Time 0.180 (0.162)	Data 1.02e-04 (2.67e-04)	Tok/s 92578 (88031)	Loss/tok 3.1386 (3.2590)	LR 2.000e-03
0: TRAIN [2][1680/1938]	Time 0.242 (0.162)	Data 1.02e-04 (2.66e-04)	Tok/s 95145 (88014)	Loss/tok 3.4382 (3.2584)	LR 2.000e-03
0: TRAIN [2][1690/1938]	Time 0.181 (0.162)	Data 1.12e-04 (2.65e-04)	Tok/s 93226 (88033)	Loss/tok 3.2201 (3.2585)	LR 2.000e-03
0: TRAIN [2][1700/1938]	Time 0.125 (0.162)	Data 1.04e-04 (2.64e-04)	Tok/s 84692 (88031)	Loss/tok 3.0501 (3.2582)	LR 2.000e-03
0: TRAIN [2][1710/1938]	Time 0.122 (0.162)	Data 1.01e-04 (2.63e-04)	Tok/s 85110 (88017)	Loss/tok 2.8979 (3.2577)	LR 2.000e-03
0: TRAIN [2][1720/1938]	Time 0.316 (0.163)	Data 1.22e-04 (2.62e-04)	Tok/s 95192 (88037)	Loss/tok 3.5335 (3.2589)	LR 2.000e-03
0: TRAIN [2][1730/1938]	Time 0.123 (0.162)	Data 1.13e-04 (2.61e-04)	Tok/s 85073 (88031)	Loss/tok 3.0877 (3.2586)	LR 2.000e-03
0: TRAIN [2][1740/1938]	Time 0.125 (0.162)	Data 1.05e-04 (2.60e-04)	Tok/s 80827 (88027)	Loss/tok 3.1230 (3.2582)	LR 2.000e-03
0: TRAIN [2][1750/1938]	Time 0.243 (0.162)	Data 1.08e-04 (2.59e-04)	Tok/s 95808 (88031)	Loss/tok 3.4367 (3.2581)	LR 2.000e-03
0: TRAIN [2][1760/1938]	Time 0.123 (0.162)	Data 1.01e-04 (2.58e-04)	Tok/s 86321 (88041)	Loss/tok 3.0230 (3.2583)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [2][1770/1938]	Time 0.123 (0.162)	Data 1.15e-04 (2.57e-04)	Tok/s 83418 (88023)	Loss/tok 2.9860 (3.2582)	LR 2.000e-03
0: TRAIN [2][1780/1938]	Time 0.242 (0.162)	Data 1.08e-04 (2.57e-04)	Tok/s 96179 (88027)	Loss/tok 3.4102 (3.2583)	LR 2.000e-03
0: TRAIN [2][1790/1938]	Time 0.180 (0.162)	Data 1.01e-04 (2.56e-04)	Tok/s 93130 (88043)	Loss/tok 3.3719 (3.2583)	LR 2.000e-03
0: TRAIN [2][1800/1938]	Time 0.180 (0.162)	Data 1.01e-04 (2.55e-04)	Tok/s 93334 (88043)	Loss/tok 3.2558 (3.2578)	LR 2.000e-03
0: TRAIN [2][1810/1938]	Time 0.125 (0.162)	Data 1.20e-04 (2.54e-04)	Tok/s 81337 (88047)	Loss/tok 3.1102 (3.2577)	LR 2.000e-03
0: TRAIN [2][1820/1938]	Time 0.239 (0.163)	Data 1.02e-04 (2.53e-04)	Tok/s 96647 (88064)	Loss/tok 3.4101 (3.2582)	LR 2.000e-03
0: TRAIN [2][1830/1938]	Time 0.123 (0.163)	Data 1.18e-04 (2.53e-04)	Tok/s 83153 (88070)	Loss/tok 3.0746 (3.2580)	LR 2.000e-03
0: TRAIN [2][1840/1938]	Time 0.123 (0.163)	Data 1.00e-04 (2.52e-04)	Tok/s 84300 (88063)	Loss/tok 3.0630 (3.2576)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [2][1850/1938]	Time 0.182 (0.162)	Data 1.04e-04 (2.51e-04)	Tok/s 91801 (88057)	Loss/tok 3.2426 (3.2575)	LR 2.000e-03
0: TRAIN [2][1860/1938]	Time 0.122 (0.162)	Data 1.01e-04 (2.50e-04)	Tok/s 85415 (88065)	Loss/tok 3.0599 (3.2574)	LR 2.000e-03
0: TRAIN [2][1870/1938]	Time 0.240 (0.162)	Data 1.03e-04 (2.49e-04)	Tok/s 97919 (88061)	Loss/tok 3.3768 (3.2571)	LR 2.000e-03
0: TRAIN [2][1880/1938]	Time 0.123 (0.162)	Data 1.01e-04 (2.49e-04)	Tok/s 84403 (88040)	Loss/tok 2.9794 (3.2565)	LR 2.000e-03
0: TRAIN [2][1890/1938]	Time 0.180 (0.162)	Data 9.92e-05 (2.48e-04)	Tok/s 92353 (88045)	Loss/tok 3.1948 (3.2563)	LR 2.000e-03
0: TRAIN [2][1900/1938]	Time 0.180 (0.162)	Data 1.34e-04 (2.47e-04)	Tok/s 92883 (88037)	Loss/tok 3.2262 (3.2561)	LR 2.000e-03
0: TRAIN [2][1910/1938]	Time 0.069 (0.162)	Data 1.03e-04 (2.46e-04)	Tok/s 75543 (88041)	Loss/tok 2.6749 (3.2563)	LR 2.000e-03
0: TRAIN [2][1920/1938]	Time 0.124 (0.162)	Data 1.19e-04 (2.46e-04)	Tok/s 82056 (88036)	Loss/tok 3.0095 (3.2560)	LR 2.000e-03
0: TRAIN [2][1930/1938]	Time 0.126 (0.162)	Data 1.02e-04 (2.45e-04)	Tok/s 80992 (88031)	Loss/tok 2.9531 (3.2557)	LR 2.000e-03
:::MLL 1575776177.686 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1575776177.686 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.680 (0.680)	Decoder iters 118.0 (118.0)	Tok/s 24768 (24768)
0: Running moses detokenizer
0: BLEU(score=22.469581577309857, counts=[37100, 18270, 10250, 6009], totals=[68207, 65204, 62202, 59204], precisions=[54.39324409518085, 28.019753389362617, 16.47856982090608, 10.149652050537126], bp=1.0, sys_len=68207, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1575776179.698 eval_accuracy: {"value": 22.47, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1575776179.698 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.2571	Test BLEU: 22.47
0: Performance: Epoch: 2	Training: 704182 Tok/s
0: Finished epoch 2
:::MLL 1575776179.699 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1575776179.699 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1575776179.700 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1557345833
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [3][0/1938]	Time 0.517 (0.517)	Data 2.87e-01 (2.87e-01)	Tok/s 45173 (45173)	Loss/tok 3.2773 (3.2773)	LR 2.000e-03
0: TRAIN [3][10/1938]	Time 0.122 (0.207)	Data 1.05e-04 (2.62e-02)	Tok/s 83741 (85452)	Loss/tok 2.9777 (3.2133)	LR 2.000e-03
0: TRAIN [3][20/1938]	Time 0.123 (0.190)	Data 1.06e-04 (1.38e-02)	Tok/s 85026 (87291)	Loss/tok 2.9508 (3.1947)	LR 2.000e-03
0: TRAIN [3][30/1938]	Time 0.069 (0.174)	Data 1.01e-04 (9.37e-03)	Tok/s 75412 (86808)	Loss/tok 2.5781 (3.1555)	LR 2.000e-03
0: TRAIN [3][40/1938]	Time 0.239 (0.177)	Data 1.05e-04 (7.11e-03)	Tok/s 97101 (87373)	Loss/tok 3.3160 (3.1810)	LR 2.000e-03
0: TRAIN [3][50/1938]	Time 0.123 (0.175)	Data 1.32e-04 (5.74e-03)	Tok/s 82297 (87570)	Loss/tok 2.9204 (3.1668)	LR 2.000e-03
0: TRAIN [3][60/1938]	Time 0.181 (0.173)	Data 1.02e-04 (4.82e-03)	Tok/s 92766 (87723)	Loss/tok 3.1456 (3.1708)	LR 2.000e-03
0: TRAIN [3][70/1938]	Time 0.182 (0.171)	Data 1.06e-04 (4.15e-03)	Tok/s 92750 (87774)	Loss/tok 3.1391 (3.1655)	LR 2.000e-03
0: TRAIN [3][80/1938]	Time 0.122 (0.169)	Data 9.89e-05 (3.65e-03)	Tok/s 83229 (87809)	Loss/tok 2.8896 (3.1589)	LR 2.000e-03
0: TRAIN [3][90/1938]	Time 0.069 (0.166)	Data 1.04e-04 (3.26e-03)	Tok/s 78140 (87642)	Loss/tok 2.6091 (3.1527)	LR 2.000e-03
0: TRAIN [3][100/1938]	Time 0.243 (0.166)	Data 1.06e-04 (2.95e-03)	Tok/s 96302 (87729)	Loss/tok 3.2834 (3.1557)	LR 2.000e-03
0: TRAIN [3][110/1938]	Time 0.122 (0.169)	Data 1.01e-04 (2.70e-03)	Tok/s 86261 (88050)	Loss/tok 3.0043 (3.1669)	LR 2.000e-03
0: TRAIN [3][120/1938]	Time 0.126 (0.167)	Data 1.03e-04 (2.48e-03)	Tok/s 80970 (87966)	Loss/tok 2.8755 (3.1638)	LR 2.000e-03
0: TRAIN [3][130/1938]	Time 0.069 (0.167)	Data 9.82e-05 (2.30e-03)	Tok/s 78459 (88034)	Loss/tok 2.6280 (3.1638)	LR 2.000e-03
0: TRAIN [3][140/1938]	Time 0.239 (0.167)	Data 1.03e-04 (2.14e-03)	Tok/s 96095 (88023)	Loss/tok 3.4176 (3.1694)	LR 2.000e-03
0: TRAIN [3][150/1938]	Time 0.180 (0.168)	Data 1.01e-04 (2.01e-03)	Tok/s 94584 (88202)	Loss/tok 3.1574 (3.1703)	LR 2.000e-03
0: TRAIN [3][160/1938]	Time 0.242 (0.168)	Data 1.05e-04 (1.89e-03)	Tok/s 96181 (88243)	Loss/tok 3.3038 (3.1723)	LR 2.000e-03
0: TRAIN [3][170/1938]	Time 0.181 (0.168)	Data 1.02e-04 (1.79e-03)	Tok/s 94544 (88189)	Loss/tok 3.0725 (3.1719)	LR 2.000e-03
0: TRAIN [3][180/1938]	Time 0.127 (0.168)	Data 1.02e-04 (1.69e-03)	Tok/s 82509 (88230)	Loss/tok 2.9254 (3.1721)	LR 2.000e-03
0: TRAIN [3][190/1938]	Time 0.127 (0.169)	Data 1.22e-04 (1.61e-03)	Tok/s 80876 (88238)	Loss/tok 2.8751 (3.1739)	LR 2.000e-03
0: TRAIN [3][200/1938]	Time 0.185 (0.169)	Data 1.03e-04 (1.54e-03)	Tok/s 91050 (88342)	Loss/tok 3.1664 (3.1728)	LR 2.000e-03
0: TRAIN [3][210/1938]	Time 0.240 (0.169)	Data 9.87e-05 (1.47e-03)	Tok/s 97677 (88350)	Loss/tok 3.3545 (3.1720)	LR 2.000e-03
0: TRAIN [3][220/1938]	Time 0.123 (0.168)	Data 9.82e-05 (1.41e-03)	Tok/s 83361 (88335)	Loss/tok 3.0419 (3.1705)	LR 2.000e-03
0: TRAIN [3][230/1938]	Time 0.182 (0.170)	Data 1.38e-04 (1.35e-03)	Tok/s 92136 (88443)	Loss/tok 3.1059 (3.1789)	LR 2.000e-03
0: TRAIN [3][240/1938]	Time 0.181 (0.169)	Data 1.00e-04 (1.30e-03)	Tok/s 92300 (88436)	Loss/tok 3.0514 (3.1798)	LR 2.000e-03
0: TRAIN [3][250/1938]	Time 0.069 (0.169)	Data 1.49e-04 (1.25e-03)	Tok/s 76523 (88436)	Loss/tok 2.6656 (3.1815)	LR 2.000e-03
0: TRAIN [3][260/1938]	Time 0.183 (0.169)	Data 1.02e-04 (1.21e-03)	Tok/s 91613 (88421)	Loss/tok 3.2115 (3.1786)	LR 2.000e-03
0: TRAIN [3][270/1938]	Time 0.123 (0.168)	Data 9.97e-05 (1.17e-03)	Tok/s 84390 (88334)	Loss/tok 2.9699 (3.1741)	LR 2.000e-03
0: TRAIN [3][280/1938]	Time 0.181 (0.167)	Data 1.03e-04 (1.13e-03)	Tok/s 93385 (88331)	Loss/tok 3.1675 (3.1718)	LR 2.000e-03
0: TRAIN [3][290/1938]	Time 0.244 (0.168)	Data 1.08e-04 (1.09e-03)	Tok/s 95026 (88394)	Loss/tok 3.2497 (3.1731)	LR 2.000e-03
0: TRAIN [3][300/1938]	Time 0.069 (0.167)	Data 1.01e-04 (1.06e-03)	Tok/s 76622 (88314)	Loss/tok 2.5023 (3.1700)	LR 2.000e-03
0: TRAIN [3][310/1938]	Time 0.185 (0.167)	Data 1.09e-04 (1.03e-03)	Tok/s 90046 (88380)	Loss/tok 3.2716 (3.1711)	LR 2.000e-03
0: TRAIN [3][320/1938]	Time 0.070 (0.166)	Data 1.24e-04 (1.00e-03)	Tok/s 73634 (88320)	Loss/tok 2.5916 (3.1697)	LR 2.000e-03
0: TRAIN [3][330/1938]	Time 0.182 (0.165)	Data 1.01e-04 (9.74e-04)	Tok/s 92020 (88264)	Loss/tok 3.1557 (3.1669)	LR 2.000e-03
0: TRAIN [3][340/1938]	Time 0.125 (0.166)	Data 1.03e-04 (9.49e-04)	Tok/s 83052 (88299)	Loss/tok 2.9495 (3.1706)	LR 2.000e-03
0: TRAIN [3][350/1938]	Time 0.181 (0.166)	Data 1.00e-04 (9.25e-04)	Tok/s 92547 (88339)	Loss/tok 3.1754 (3.1710)	LR 2.000e-03
0: TRAIN [3][360/1938]	Time 0.183 (0.167)	Data 1.02e-04 (9.02e-04)	Tok/s 90809 (88371)	Loss/tok 3.1919 (3.1762)	LR 2.000e-03
0: TRAIN [3][370/1938]	Time 0.124 (0.167)	Data 9.89e-05 (8.81e-04)	Tok/s 83343 (88354)	Loss/tok 2.9948 (3.1761)	LR 2.000e-03
0: TRAIN [3][380/1938]	Time 0.123 (0.166)	Data 9.99e-05 (8.60e-04)	Tok/s 81722 (88263)	Loss/tok 3.0826 (3.1754)	LR 2.000e-03
0: TRAIN [3][390/1938]	Time 0.180 (0.165)	Data 9.73e-05 (8.41e-04)	Tok/s 92324 (88178)	Loss/tok 3.1567 (3.1719)	LR 2.000e-03
0: TRAIN [3][400/1938]	Time 0.127 (0.165)	Data 9.99e-05 (8.23e-04)	Tok/s 81382 (88206)	Loss/tok 2.9986 (3.1736)	LR 2.000e-03
0: TRAIN [3][410/1938]	Time 0.310 (0.165)	Data 9.94e-05 (8.05e-04)	Tok/s 97827 (88121)	Loss/tok 3.4640 (3.1719)	LR 2.000e-03
0: TRAIN [3][420/1938]	Time 0.124 (0.164)	Data 1.17e-04 (7.89e-04)	Tok/s 83075 (88026)	Loss/tok 3.0220 (3.1693)	LR 2.000e-03
0: TRAIN [3][430/1938]	Time 0.183 (0.164)	Data 1.39e-04 (7.73e-04)	Tok/s 91044 (88102)	Loss/tok 3.1347 (3.1698)	LR 2.000e-03
0: TRAIN [3][440/1938]	Time 0.123 (0.164)	Data 1.06e-04 (7.58e-04)	Tok/s 84690 (88117)	Loss/tok 2.9791 (3.1682)	LR 2.000e-03
0: TRAIN [3][450/1938]	Time 0.069 (0.164)	Data 9.89e-05 (7.43e-04)	Tok/s 77669 (88081)	Loss/tok 2.5544 (3.1680)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][460/1938]	Time 0.181 (0.164)	Data 1.01e-04 (7.30e-04)	Tok/s 92568 (88098)	Loss/tok 3.1543 (3.1699)	LR 2.000e-03
0: TRAIN [3][470/1938]	Time 0.069 (0.164)	Data 1.00e-04 (7.17e-04)	Tok/s 75708 (88080)	Loss/tok 2.6328 (3.1710)	LR 2.000e-03
0: TRAIN [3][480/1938]	Time 0.180 (0.164)	Data 1.10e-04 (7.04e-04)	Tok/s 92840 (88091)	Loss/tok 3.1358 (3.1707)	LR 2.000e-03
0: TRAIN [3][490/1938]	Time 0.124 (0.164)	Data 9.97e-05 (6.92e-04)	Tok/s 83531 (88061)	Loss/tok 2.9784 (3.1693)	LR 2.000e-03
0: TRAIN [3][500/1938]	Time 0.123 (0.163)	Data 1.02e-04 (6.80e-04)	Tok/s 83063 (88013)	Loss/tok 2.9893 (3.1679)	LR 2.000e-03
0: TRAIN [3][510/1938]	Time 0.181 (0.162)	Data 1.01e-04 (6.69e-04)	Tok/s 92992 (87965)	Loss/tok 3.1575 (3.1659)	LR 2.000e-03
0: TRAIN [3][520/1938]	Time 0.123 (0.162)	Data 1.02e-04 (6.58e-04)	Tok/s 85653 (87924)	Loss/tok 2.9967 (3.1653)	LR 2.000e-03
0: TRAIN [3][530/1938]	Time 0.181 (0.162)	Data 1.61e-04 (6.48e-04)	Tok/s 93275 (87963)	Loss/tok 3.0975 (3.1653)	LR 2.000e-03
0: TRAIN [3][540/1938]	Time 0.181 (0.163)	Data 9.73e-05 (6.38e-04)	Tok/s 91723 (88045)	Loss/tok 3.1086 (3.1665)	LR 2.000e-03
0: TRAIN [3][550/1938]	Time 0.069 (0.163)	Data 9.92e-05 (6.28e-04)	Tok/s 77248 (88027)	Loss/tok 2.6781 (3.1666)	LR 2.000e-03
0: TRAIN [3][560/1938]	Time 0.124 (0.162)	Data 1.02e-04 (6.19e-04)	Tok/s 83010 (87966)	Loss/tok 2.9656 (3.1651)	LR 2.000e-03
0: TRAIN [3][570/1938]	Time 0.181 (0.162)	Data 1.03e-04 (6.10e-04)	Tok/s 94191 (87957)	Loss/tok 3.1232 (3.1645)	LR 2.000e-03
0: TRAIN [3][580/1938]	Time 0.184 (0.161)	Data 1.01e-04 (6.01e-04)	Tok/s 91756 (87918)	Loss/tok 3.2529 (3.1646)	LR 2.000e-03
0: TRAIN [3][590/1938]	Time 0.180 (0.161)	Data 9.92e-05 (5.92e-04)	Tok/s 93535 (87889)	Loss/tok 3.1564 (3.1633)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][600/1938]	Time 0.123 (0.161)	Data 1.18e-04 (5.84e-04)	Tok/s 85008 (87887)	Loss/tok 3.0049 (3.1641)	LR 2.000e-03
0: TRAIN [3][610/1938]	Time 0.181 (0.162)	Data 9.89e-05 (5.77e-04)	Tok/s 92236 (87898)	Loss/tok 3.1966 (3.1659)	LR 2.000e-03
0: TRAIN [3][620/1938]	Time 0.181 (0.162)	Data 9.89e-05 (5.69e-04)	Tok/s 91851 (87923)	Loss/tok 3.1926 (3.1657)	LR 2.000e-03
0: TRAIN [3][630/1938]	Time 0.180 (0.162)	Data 1.00e-04 (5.62e-04)	Tok/s 93943 (87939)	Loss/tok 3.2138 (3.1664)	LR 2.000e-03
0: TRAIN [3][640/1938]	Time 0.124 (0.161)	Data 1.22e-04 (5.55e-04)	Tok/s 84130 (87927)	Loss/tok 2.9310 (3.1662)	LR 2.000e-03
0: TRAIN [3][650/1938]	Time 0.126 (0.161)	Data 1.01e-04 (5.48e-04)	Tok/s 81415 (87892)	Loss/tok 2.9141 (3.1649)	LR 2.000e-03
0: TRAIN [3][660/1938]	Time 0.182 (0.161)	Data 1.17e-04 (5.41e-04)	Tok/s 91231 (87869)	Loss/tok 3.1809 (3.1645)	LR 2.000e-03
0: TRAIN [3][670/1938]	Time 0.241 (0.161)	Data 1.69e-04 (5.35e-04)	Tok/s 96538 (87891)	Loss/tok 3.4735 (3.1663)	LR 1.000e-03
0: TRAIN [3][680/1938]	Time 0.123 (0.161)	Data 9.97e-05 (5.29e-04)	Tok/s 83862 (87872)	Loss/tok 2.9314 (3.1655)	LR 1.000e-03
0: TRAIN [3][690/1938]	Time 0.243 (0.161)	Data 9.94e-05 (5.22e-04)	Tok/s 96363 (87900)	Loss/tok 3.2701 (3.1666)	LR 1.000e-03
0: TRAIN [3][700/1938]	Time 0.123 (0.161)	Data 1.01e-04 (5.17e-04)	Tok/s 82609 (87889)	Loss/tok 2.9642 (3.1665)	LR 1.000e-03
0: TRAIN [3][710/1938]	Time 0.245 (0.162)	Data 1.03e-04 (5.11e-04)	Tok/s 95491 (87919)	Loss/tok 3.2993 (3.1662)	LR 1.000e-03
0: TRAIN [3][720/1938]	Time 0.181 (0.162)	Data 1.01e-04 (5.05e-04)	Tok/s 93083 (87960)	Loss/tok 3.1103 (3.1683)	LR 1.000e-03
0: TRAIN [3][730/1938]	Time 0.124 (0.162)	Data 1.01e-04 (5.00e-04)	Tok/s 84802 (87973)	Loss/tok 2.8079 (3.1689)	LR 1.000e-03
0: TRAIN [3][740/1938]	Time 0.185 (0.162)	Data 1.02e-04 (4.94e-04)	Tok/s 90731 (88008)	Loss/tok 3.0412 (3.1688)	LR 1.000e-03
0: TRAIN [3][750/1938]	Time 0.123 (0.162)	Data 9.80e-05 (4.89e-04)	Tok/s 83238 (87988)	Loss/tok 2.9099 (3.1677)	LR 1.000e-03
0: TRAIN [3][760/1938]	Time 0.182 (0.162)	Data 1.35e-04 (4.84e-04)	Tok/s 92596 (88005)	Loss/tok 3.2122 (3.1681)	LR 1.000e-03
0: TRAIN [3][770/1938]	Time 0.181 (0.163)	Data 1.03e-04 (4.80e-04)	Tok/s 91759 (88010)	Loss/tok 3.1292 (3.1681)	LR 1.000e-03
0: TRAIN [3][780/1938]	Time 0.124 (0.162)	Data 1.16e-04 (4.75e-04)	Tok/s 84232 (87954)	Loss/tok 2.9352 (3.1663)	LR 1.000e-03
0: TRAIN [3][790/1938]	Time 0.122 (0.162)	Data 1.27e-04 (4.70e-04)	Tok/s 83051 (87968)	Loss/tok 2.8848 (3.1656)	LR 1.000e-03
0: TRAIN [3][800/1938]	Time 0.123 (0.162)	Data 1.02e-04 (4.65e-04)	Tok/s 83719 (87978)	Loss/tok 2.7722 (3.1650)	LR 1.000e-03
0: TRAIN [3][810/1938]	Time 0.314 (0.162)	Data 1.02e-04 (4.61e-04)	Tok/s 96593 (87996)	Loss/tok 3.4841 (3.1650)	LR 1.000e-03
0: TRAIN [3][820/1938]	Time 0.122 (0.162)	Data 1.18e-04 (4.57e-04)	Tok/s 82636 (87993)	Loss/tok 2.9441 (3.1647)	LR 1.000e-03
0: TRAIN [3][830/1938]	Time 0.186 (0.162)	Data 1.01e-04 (4.53e-04)	Tok/s 90088 (88018)	Loss/tok 3.2128 (3.1642)	LR 1.000e-03
0: TRAIN [3][840/1938]	Time 0.125 (0.162)	Data 1.01e-04 (4.48e-04)	Tok/s 83155 (88040)	Loss/tok 2.9233 (3.1636)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][850/1938]	Time 0.315 (0.162)	Data 1.24e-04 (4.45e-04)	Tok/s 94342 (88036)	Loss/tok 3.4912 (3.1641)	LR 1.000e-03
0: TRAIN [3][860/1938]	Time 0.241 (0.162)	Data 9.92e-05 (4.41e-04)	Tok/s 95692 (88025)	Loss/tok 3.3726 (3.1639)	LR 1.000e-03
0: TRAIN [3][870/1938]	Time 0.181 (0.162)	Data 1.15e-04 (4.37e-04)	Tok/s 92247 (88042)	Loss/tok 3.2111 (3.1638)	LR 1.000e-03
0: TRAIN [3][880/1938]	Time 0.243 (0.162)	Data 1.01e-04 (4.33e-04)	Tok/s 95795 (88019)	Loss/tok 3.3514 (3.1631)	LR 1.000e-03
0: TRAIN [3][890/1938]	Time 0.241 (0.162)	Data 1.04e-04 (4.29e-04)	Tok/s 98002 (88036)	Loss/tok 3.3145 (3.1640)	LR 1.000e-03
0: TRAIN [3][900/1938]	Time 0.180 (0.162)	Data 9.78e-05 (4.26e-04)	Tok/s 94155 (88008)	Loss/tok 3.2057 (3.1628)	LR 1.000e-03
0: TRAIN [3][910/1938]	Time 0.123 (0.162)	Data 1.15e-04 (4.22e-04)	Tok/s 85037 (87986)	Loss/tok 2.8993 (3.1616)	LR 1.000e-03
0: TRAIN [3][920/1938]	Time 0.122 (0.162)	Data 9.94e-05 (4.19e-04)	Tok/s 84836 (87979)	Loss/tok 2.9713 (3.1608)	LR 1.000e-03
0: TRAIN [3][930/1938]	Time 0.242 (0.162)	Data 1.01e-04 (4.16e-04)	Tok/s 96140 (87984)	Loss/tok 3.3873 (3.1609)	LR 1.000e-03
0: TRAIN [3][940/1938]	Time 0.123 (0.162)	Data 9.87e-05 (4.12e-04)	Tok/s 82990 (87971)	Loss/tok 2.8365 (3.1600)	LR 1.000e-03
0: TRAIN [3][950/1938]	Time 0.181 (0.162)	Data 9.75e-05 (4.09e-04)	Tok/s 92052 (87974)	Loss/tok 3.1299 (3.1592)	LR 1.000e-03
0: TRAIN [3][960/1938]	Time 0.124 (0.162)	Data 1.01e-04 (4.06e-04)	Tok/s 83539 (87974)	Loss/tok 3.0333 (3.1583)	LR 1.000e-03
0: TRAIN [3][970/1938]	Time 0.122 (0.162)	Data 1.32e-04 (4.03e-04)	Tok/s 83671 (87975)	Loss/tok 2.9429 (3.1585)	LR 1.000e-03
0: TRAIN [3][980/1938]	Time 0.123 (0.161)	Data 1.03e-04 (4.00e-04)	Tok/s 83572 (87967)	Loss/tok 3.0597 (3.1578)	LR 1.000e-03
0: TRAIN [3][990/1938]	Time 0.124 (0.162)	Data 1.01e-04 (3.97e-04)	Tok/s 84197 (88007)	Loss/tok 2.9967 (3.1588)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][1000/1938]	Time 0.125 (0.162)	Data 1.09e-04 (3.94e-04)	Tok/s 83581 (87992)	Loss/tok 2.8921 (3.1585)	LR 1.000e-03
0: TRAIN [3][1010/1938]	Time 0.123 (0.162)	Data 1.00e-04 (3.91e-04)	Tok/s 84662 (87995)	Loss/tok 2.9680 (3.1586)	LR 1.000e-03
0: TRAIN [3][1020/1938]	Time 0.069 (0.162)	Data 1.12e-04 (3.88e-04)	Tok/s 75971 (87972)	Loss/tok 2.4897 (3.1576)	LR 1.000e-03
0: TRAIN [3][1030/1938]	Time 0.182 (0.162)	Data 1.03e-04 (3.85e-04)	Tok/s 93069 (88006)	Loss/tok 3.0962 (3.1589)	LR 1.000e-03
0: TRAIN [3][1040/1938]	Time 0.069 (0.162)	Data 1.03e-04 (3.83e-04)	Tok/s 79021 (87992)	Loss/tok 2.6805 (3.1590)	LR 1.000e-03
0: TRAIN [3][1050/1938]	Time 0.070 (0.162)	Data 1.15e-04 (3.80e-04)	Tok/s 75028 (88009)	Loss/tok 2.4639 (3.1591)	LR 1.000e-03
0: TRAIN [3][1060/1938]	Time 0.180 (0.162)	Data 9.97e-05 (3.77e-04)	Tok/s 92244 (88007)	Loss/tok 3.1741 (3.1587)	LR 1.000e-03
0: TRAIN [3][1070/1938]	Time 0.240 (0.162)	Data 9.89e-05 (3.75e-04)	Tok/s 96648 (88014)	Loss/tok 3.2504 (3.1582)	LR 1.000e-03
0: TRAIN [3][1080/1938]	Time 0.180 (0.162)	Data 1.19e-04 (3.72e-04)	Tok/s 94043 (88013)	Loss/tok 3.1396 (3.1573)	LR 1.000e-03
0: TRAIN [3][1090/1938]	Time 0.124 (0.162)	Data 1.01e-04 (3.70e-04)	Tok/s 83217 (88000)	Loss/tok 2.9549 (3.1573)	LR 1.000e-03
0: TRAIN [3][1100/1938]	Time 0.070 (0.162)	Data 1.02e-04 (3.68e-04)	Tok/s 76056 (87979)	Loss/tok 2.6688 (3.1566)	LR 1.000e-03
0: TRAIN [3][1110/1938]	Time 0.123 (0.162)	Data 9.97e-05 (3.65e-04)	Tok/s 83049 (87999)	Loss/tok 2.9073 (3.1578)	LR 1.000e-03
0: TRAIN [3][1120/1938]	Time 0.070 (0.162)	Data 9.92e-05 (3.63e-04)	Tok/s 75936 (87989)	Loss/tok 2.5274 (3.1572)	LR 1.000e-03
0: TRAIN [3][1130/1938]	Time 0.242 (0.162)	Data 1.03e-04 (3.61e-04)	Tok/s 95856 (87990)	Loss/tok 3.3913 (3.1570)	LR 1.000e-03
0: TRAIN [3][1140/1938]	Time 0.124 (0.162)	Data 1.02e-04 (3.58e-04)	Tok/s 82650 (88006)	Loss/tok 2.9411 (3.1565)	LR 1.000e-03
0: TRAIN [3][1150/1938]	Time 0.123 (0.162)	Data 9.94e-05 (3.56e-04)	Tok/s 84757 (88013)	Loss/tok 3.0004 (3.1561)	LR 1.000e-03
0: TRAIN [3][1160/1938]	Time 0.123 (0.162)	Data 1.19e-04 (3.54e-04)	Tok/s 84819 (88002)	Loss/tok 2.9471 (3.1551)	LR 1.000e-03
0: TRAIN [3][1170/1938]	Time 0.181 (0.162)	Data 1.49e-04 (3.52e-04)	Tok/s 93127 (88013)	Loss/tok 3.1014 (3.1553)	LR 1.000e-03
0: TRAIN [3][1180/1938]	Time 0.313 (0.162)	Data 1.13e-04 (3.50e-04)	Tok/s 93785 (88038)	Loss/tok 3.4830 (3.1553)	LR 1.000e-03
0: TRAIN [3][1190/1938]	Time 0.124 (0.162)	Data 9.92e-05 (3.48e-04)	Tok/s 83803 (88009)	Loss/tok 2.9989 (3.1544)	LR 1.000e-03
0: TRAIN [3][1200/1938]	Time 0.245 (0.162)	Data 1.05e-04 (3.46e-04)	Tok/s 95917 (88023)	Loss/tok 3.2622 (3.1549)	LR 1.000e-03
0: TRAIN [3][1210/1938]	Time 0.123 (0.162)	Data 1.59e-04 (3.44e-04)	Tok/s 85358 (88003)	Loss/tok 2.9110 (3.1544)	LR 1.000e-03
0: TRAIN [3][1220/1938]	Time 0.182 (0.162)	Data 1.01e-04 (3.42e-04)	Tok/s 93568 (88010)	Loss/tok 3.1377 (3.1550)	LR 1.000e-03
0: TRAIN [3][1230/1938]	Time 0.070 (0.162)	Data 1.68e-04 (3.40e-04)	Tok/s 75653 (88016)	Loss/tok 2.5704 (3.1549)	LR 1.000e-03
0: TRAIN [3][1240/1938]	Time 0.243 (0.162)	Data 1.04e-04 (3.38e-04)	Tok/s 96404 (88042)	Loss/tok 3.3572 (3.1558)	LR 1.000e-03
0: TRAIN [3][1250/1938]	Time 0.123 (0.162)	Data 9.73e-05 (3.36e-04)	Tok/s 86197 (88053)	Loss/tok 2.8862 (3.1558)	LR 1.000e-03
0: TRAIN [3][1260/1938]	Time 0.244 (0.162)	Data 9.87e-05 (3.35e-04)	Tok/s 95341 (88052)	Loss/tok 3.3696 (3.1553)	LR 1.000e-03
0: TRAIN [3][1270/1938]	Time 0.122 (0.162)	Data 1.20e-04 (3.33e-04)	Tok/s 83279 (88033)	Loss/tok 2.9769 (3.1547)	LR 1.000e-03
0: TRAIN [3][1280/1938]	Time 0.123 (0.162)	Data 9.80e-05 (3.31e-04)	Tok/s 83921 (88010)	Loss/tok 2.9902 (3.1547)	LR 1.000e-03
0: TRAIN [3][1290/1938]	Time 0.125 (0.162)	Data 1.07e-04 (3.29e-04)	Tok/s 84679 (88001)	Loss/tok 3.0052 (3.1540)	LR 1.000e-03
0: TRAIN [3][1300/1938]	Time 0.182 (0.162)	Data 1.03e-04 (3.28e-04)	Tok/s 92691 (87995)	Loss/tok 3.0886 (3.1535)	LR 1.000e-03
0: TRAIN [3][1310/1938]	Time 0.182 (0.162)	Data 1.02e-04 (3.26e-04)	Tok/s 92064 (87998)	Loss/tok 3.1760 (3.1532)	LR 1.000e-03
0: TRAIN [3][1320/1938]	Time 0.183 (0.162)	Data 1.06e-04 (3.24e-04)	Tok/s 92270 (88036)	Loss/tok 3.1119 (3.1539)	LR 1.000e-03
0: TRAIN [3][1330/1938]	Time 0.122 (0.162)	Data 1.17e-04 (3.23e-04)	Tok/s 84851 (88057)	Loss/tok 2.9678 (3.1550)	LR 1.000e-03
0: TRAIN [3][1340/1938]	Time 0.182 (0.162)	Data 1.04e-04 (3.21e-04)	Tok/s 91891 (88051)	Loss/tok 3.1503 (3.1546)	LR 1.000e-03
0: TRAIN [3][1350/1938]	Time 0.124 (0.162)	Data 1.02e-04 (3.19e-04)	Tok/s 82026 (88050)	Loss/tok 3.0164 (3.1545)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1360/1938]	Time 0.244 (0.162)	Data 1.01e-04 (3.18e-04)	Tok/s 94946 (88053)	Loss/tok 3.3101 (3.1546)	LR 1.000e-03
0: TRAIN [3][1370/1938]	Time 0.070 (0.163)	Data 1.01e-04 (3.16e-04)	Tok/s 76237 (88061)	Loss/tok 2.6280 (3.1547)	LR 1.000e-03
0: TRAIN [3][1380/1938]	Time 0.123 (0.163)	Data 1.01e-04 (3.15e-04)	Tok/s 81850 (88063)	Loss/tok 2.8886 (3.1544)	LR 1.000e-03
0: TRAIN [3][1390/1938]	Time 0.313 (0.162)	Data 1.00e-04 (3.13e-04)	Tok/s 94809 (88047)	Loss/tok 3.5949 (3.1543)	LR 1.000e-03
0: TRAIN [3][1400/1938]	Time 0.245 (0.163)	Data 9.85e-05 (3.12e-04)	Tok/s 96020 (88063)	Loss/tok 3.2231 (3.1544)	LR 1.000e-03
0: TRAIN [3][1410/1938]	Time 0.183 (0.163)	Data 1.05e-04 (3.10e-04)	Tok/s 91593 (88081)	Loss/tok 3.1290 (3.1555)	LR 1.000e-03
0: TRAIN [3][1420/1938]	Time 0.123 (0.163)	Data 1.07e-04 (3.09e-04)	Tok/s 83017 (88100)	Loss/tok 2.8457 (3.1563)	LR 1.000e-03
0: TRAIN [3][1430/1938]	Time 0.244 (0.163)	Data 9.92e-05 (3.07e-04)	Tok/s 95340 (88116)	Loss/tok 3.2669 (3.1562)	LR 1.000e-03
0: TRAIN [3][1440/1938]	Time 0.182 (0.163)	Data 1.16e-04 (3.06e-04)	Tok/s 92936 (88130)	Loss/tok 3.1574 (3.1569)	LR 1.000e-03
0: TRAIN [3][1450/1938]	Time 0.124 (0.163)	Data 1.01e-04 (3.04e-04)	Tok/s 85883 (88115)	Loss/tok 2.8796 (3.1560)	LR 1.000e-03
0: TRAIN [3][1460/1938]	Time 0.245 (0.163)	Data 1.00e-04 (3.03e-04)	Tok/s 95433 (88126)	Loss/tok 3.2479 (3.1562)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [3][1470/1938]	Time 0.123 (0.163)	Data 9.82e-05 (3.02e-04)	Tok/s 83650 (88125)	Loss/tok 2.9839 (3.1563)	LR 1.000e-03
0: TRAIN [3][1480/1938]	Time 0.182 (0.163)	Data 1.03e-04 (3.00e-04)	Tok/s 91093 (88123)	Loss/tok 3.0312 (3.1559)	LR 1.000e-03
0: TRAIN [3][1490/1938]	Time 0.124 (0.163)	Data 9.82e-05 (2.99e-04)	Tok/s 82871 (88118)	Loss/tok 2.9532 (3.1556)	LR 5.000e-04
0: TRAIN [3][1500/1938]	Time 0.124 (0.163)	Data 1.22e-04 (2.98e-04)	Tok/s 82967 (88108)	Loss/tok 2.9841 (3.1554)	LR 5.000e-04
0: TRAIN [3][1510/1938]	Time 0.242 (0.163)	Data 1.15e-04 (2.96e-04)	Tok/s 96591 (88110)	Loss/tok 3.2152 (3.1548)	LR 5.000e-04
0: TRAIN [3][1520/1938]	Time 0.180 (0.163)	Data 9.92e-05 (2.95e-04)	Tok/s 92771 (88096)	Loss/tok 3.1301 (3.1540)	LR 5.000e-04
0: TRAIN [3][1530/1938]	Time 0.185 (0.163)	Data 1.00e-04 (2.94e-04)	Tok/s 91984 (88082)	Loss/tok 3.0991 (3.1531)	LR 5.000e-04
0: TRAIN [3][1540/1938]	Time 0.125 (0.163)	Data 1.03e-04 (2.93e-04)	Tok/s 83405 (88067)	Loss/tok 2.8170 (3.1525)	LR 5.000e-04
0: TRAIN [3][1550/1938]	Time 0.069 (0.163)	Data 1.03e-04 (2.91e-04)	Tok/s 76799 (88055)	Loss/tok 2.4740 (3.1517)	LR 5.000e-04
0: TRAIN [3][1560/1938]	Time 0.070 (0.163)	Data 1.02e-04 (2.90e-04)	Tok/s 75760 (88062)	Loss/tok 2.4937 (3.1516)	LR 5.000e-04
0: TRAIN [3][1570/1938]	Time 0.124 (0.163)	Data 9.87e-05 (2.89e-04)	Tok/s 82949 (88065)	Loss/tok 3.0070 (3.1511)	LR 5.000e-04
0: TRAIN [3][1580/1938]	Time 0.124 (0.163)	Data 1.00e-04 (2.88e-04)	Tok/s 83226 (88065)	Loss/tok 3.0015 (3.1507)	LR 5.000e-04
0: TRAIN [3][1590/1938]	Time 0.241 (0.163)	Data 1.00e-04 (2.87e-04)	Tok/s 97829 (88071)	Loss/tok 3.2315 (3.1510)	LR 5.000e-04
0: TRAIN [3][1600/1938]	Time 0.069 (0.163)	Data 1.00e-04 (2.86e-04)	Tok/s 76881 (88065)	Loss/tok 2.5428 (3.1505)	LR 5.000e-04
0: TRAIN [3][1610/1938]	Time 0.314 (0.163)	Data 1.03e-04 (2.84e-04)	Tok/s 95881 (88061)	Loss/tok 3.3956 (3.1504)	LR 5.000e-04
0: TRAIN [3][1620/1938]	Time 0.181 (0.163)	Data 1.14e-04 (2.83e-04)	Tok/s 90756 (88062)	Loss/tok 3.1643 (3.1502)	LR 5.000e-04
0: TRAIN [3][1630/1938]	Time 0.182 (0.163)	Data 1.60e-04 (2.82e-04)	Tok/s 92941 (88081)	Loss/tok 3.1412 (3.1502)	LR 5.000e-04
0: TRAIN [3][1640/1938]	Time 0.180 (0.163)	Data 1.54e-04 (2.81e-04)	Tok/s 93723 (88102)	Loss/tok 3.0815 (3.1504)	LR 5.000e-04
0: TRAIN [3][1650/1938]	Time 0.123 (0.163)	Data 9.73e-05 (2.80e-04)	Tok/s 84501 (88100)	Loss/tok 2.8722 (3.1502)	LR 5.000e-04
0: TRAIN [3][1660/1938]	Time 0.180 (0.163)	Data 9.87e-05 (2.79e-04)	Tok/s 92873 (88086)	Loss/tok 3.2374 (3.1497)	LR 5.000e-04
0: TRAIN [3][1670/1938]	Time 0.123 (0.163)	Data 1.13e-04 (2.78e-04)	Tok/s 83759 (88073)	Loss/tok 2.8285 (3.1495)	LR 5.000e-04
0: TRAIN [3][1680/1938]	Time 0.122 (0.163)	Data 1.13e-04 (2.77e-04)	Tok/s 85429 (88081)	Loss/tok 2.8718 (3.1495)	LR 5.000e-04
0: TRAIN [3][1690/1938]	Time 0.069 (0.163)	Data 1.13e-04 (2.76e-04)	Tok/s 75419 (88061)	Loss/tok 2.4493 (3.1488)	LR 5.000e-04
0: TRAIN [3][1700/1938]	Time 0.069 (0.162)	Data 1.13e-04 (2.75e-04)	Tok/s 75400 (88037)	Loss/tok 2.5590 (3.1480)	LR 5.000e-04
0: TRAIN [3][1710/1938]	Time 0.314 (0.162)	Data 1.04e-04 (2.74e-04)	Tok/s 95433 (88023)	Loss/tok 3.4423 (3.1478)	LR 5.000e-04
0: TRAIN [3][1720/1938]	Time 0.240 (0.163)	Data 1.03e-04 (2.73e-04)	Tok/s 96936 (88045)	Loss/tok 3.3245 (3.1481)	LR 5.000e-04
0: TRAIN [3][1730/1938]	Time 0.071 (0.163)	Data 1.00e-04 (2.72e-04)	Tok/s 75477 (88049)	Loss/tok 2.4806 (3.1481)	LR 5.000e-04
0: TRAIN [3][1740/1938]	Time 0.245 (0.163)	Data 1.01e-04 (2.71e-04)	Tok/s 95529 (88038)	Loss/tok 3.2426 (3.1477)	LR 5.000e-04
0: TRAIN [3][1750/1938]	Time 0.124 (0.162)	Data 1.29e-04 (2.70e-04)	Tok/s 84863 (88019)	Loss/tok 2.9427 (3.1476)	LR 5.000e-04
0: TRAIN [3][1760/1938]	Time 0.124 (0.162)	Data 1.01e-04 (2.69e-04)	Tok/s 84584 (88025)	Loss/tok 2.9597 (3.1473)	LR 5.000e-04
0: TRAIN [3][1770/1938]	Time 0.244 (0.163)	Data 9.85e-05 (2.68e-04)	Tok/s 94826 (88033)	Loss/tok 3.2099 (3.1474)	LR 5.000e-04
0: TRAIN [3][1780/1938]	Time 0.124 (0.162)	Data 9.80e-05 (2.68e-04)	Tok/s 84695 (88013)	Loss/tok 2.9044 (3.1466)	LR 5.000e-04
0: TRAIN [3][1790/1938]	Time 0.242 (0.162)	Data 1.01e-04 (2.67e-04)	Tok/s 96256 (88010)	Loss/tok 3.2762 (3.1471)	LR 5.000e-04
0: TRAIN [3][1800/1938]	Time 0.124 (0.162)	Data 1.19e-04 (2.66e-04)	Tok/s 85193 (88012)	Loss/tok 2.9594 (3.1465)	LR 5.000e-04
0: TRAIN [3][1810/1938]	Time 0.241 (0.162)	Data 1.00e-04 (2.65e-04)	Tok/s 96817 (88017)	Loss/tok 3.2279 (3.1462)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][1820/1938]	Time 0.244 (0.163)	Data 1.03e-04 (2.64e-04)	Tok/s 95824 (88029)	Loss/tok 3.2227 (3.1466)	LR 5.000e-04
0: TRAIN [3][1830/1938]	Time 0.123 (0.163)	Data 1.22e-04 (2.63e-04)	Tok/s 84126 (88017)	Loss/tok 3.0053 (3.1464)	LR 5.000e-04
0: TRAIN [3][1840/1938]	Time 0.125 (0.163)	Data 1.02e-04 (2.62e-04)	Tok/s 83026 (88014)	Loss/tok 2.9530 (3.1464)	LR 5.000e-04
0: TRAIN [3][1850/1938]	Time 0.123 (0.163)	Data 9.89e-05 (2.61e-04)	Tok/s 84775 (88012)	Loss/tok 2.9923 (3.1460)	LR 5.000e-04
0: TRAIN [3][1860/1938]	Time 0.242 (0.162)	Data 1.05e-04 (2.61e-04)	Tok/s 95594 (88000)	Loss/tok 3.3966 (3.1455)	LR 5.000e-04
0: TRAIN [3][1870/1938]	Time 0.124 (0.162)	Data 1.01e-04 (2.60e-04)	Tok/s 84174 (87989)	Loss/tok 2.9030 (3.1448)	LR 5.000e-04
0: TRAIN [3][1880/1938]	Time 0.123 (0.162)	Data 9.89e-05 (2.59e-04)	Tok/s 86831 (87982)	Loss/tok 2.8123 (3.1444)	LR 5.000e-04
0: TRAIN [3][1890/1938]	Time 0.123 (0.162)	Data 9.92e-05 (2.58e-04)	Tok/s 83430 (87975)	Loss/tok 2.9190 (3.1441)	LR 5.000e-04
0: TRAIN [3][1900/1938]	Time 0.181 (0.162)	Data 1.04e-04 (2.57e-04)	Tok/s 92557 (87983)	Loss/tok 3.0351 (3.1445)	LR 5.000e-04
0: TRAIN [3][1910/1938]	Time 0.244 (0.162)	Data 1.04e-04 (2.56e-04)	Tok/s 94871 (87988)	Loss/tok 3.3825 (3.1448)	LR 5.000e-04
0: TRAIN [3][1920/1938]	Time 0.182 (0.162)	Data 9.73e-05 (2.56e-04)	Tok/s 92870 (87978)	Loss/tok 3.1653 (3.1445)	LR 5.000e-04
0: TRAIN [3][1930/1938]	Time 0.182 (0.162)	Data 1.01e-04 (2.55e-04)	Tok/s 92130 (87977)	Loss/tok 3.0770 (3.1443)	LR 5.000e-04
:::MLL 1575776494.788 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1575776494.789 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.614 (0.614)	Decoder iters 102.0 (102.0)	Tok/s 26845 (26845)
0: Running moses detokenizer
0: BLEU(score=24.026436342641613, counts=[37139, 18637, 10600, 6297], totals=[65615, 62612, 59610, 56612], precisions=[56.601386878000454, 29.765859579633297, 17.78225130011743, 11.123083445205964], bp=1.0, sys_len=65615, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1575776496.616 eval_accuracy: {"value": 24.03, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1575776496.616 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.1477	Test BLEU: 24.03
0: Performance: Epoch: 3	Training: 703568 Tok/s
0: Finished epoch 3
:::MLL 1575776496.617 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1575776496.617 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-12-08 03:41:40 AM
RESULT,RNN_TRANSLATOR,,1283,nvidia,2019-12-08 03:20:17 AM

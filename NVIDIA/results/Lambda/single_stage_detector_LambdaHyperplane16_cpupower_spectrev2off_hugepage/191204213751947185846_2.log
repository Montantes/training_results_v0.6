Beginning trial 2 of 3
Gathering sys log on 9029gp-tnvrt-0
:::MLL 1575525074.386 submission_benchmark: {"value": "ssd", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1575525074.386 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known ssd keys.
:::MLL 1575525074.387 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1575525074.387 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1575525074.387 submission_platform: {"value": "1xSYS-9029GP-TNVRT", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1575525074.388 submission_entry: {"value": "{'hardware': 'SYS-9029GP-TNVRT', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': 'InfiniBand 100 Gb/sec (4X EDR)', 'os': 'Ubuntu 18.04.3 LTS / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.7-1.0.0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '2x Intel(R) Xeon(R) Platinum 8268 CPU @ 2.90GHz', 'num_cores': '48', 'num_vcpus': '96', 'accelerator': 'Tesla V100-SXM3-32GB', 'num_accelerators': '16', 'sys_mem_size': '754 GB', 'sys_storage_type': 'NVMe SSD', 'sys_storage_size': '1x 894.3G + 1x 3.7T', 'cpu_accel_interconnect': 'UPI', 'network_card': 'Mellanox Technologies MT27800 Family [ConnectX-5]', 'num_network_cards': '8', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1575525074.388 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1575525074.389 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1575525076.548 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node 9029gp-tnvrt-0
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=LambdaHyperplane16 -e 'MULTI_NODE= --master_port=4277' -e SLURM_JOB_ID=191204213751947185846 -e SLURM_NTASKS_PER_NODE= cont_191204213751947185846 ./run_and_time.sh
Run vars: id 191204213751947185846 gpus 16 mparams  --master_port=4277
+ NUMEPOCHS=80
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
STARTING TIMING RUN AT 2019-12-05 05:51:17 AM
running benchmark
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 16 --master_port=4277 train.py --use-fp16 --nhwc --pad-input --jit --delay-allreduce --opt-loss --epochs 80 --warmup-factor 0 --no-save --threshold=0.23 --data /data/coco2017 --evaluation 120000 160000 180000 200000 220000 240000 260000 280000 --batch-size 56 --eval-batch-size 160 --warmup 650 --lr 3.2e-3 --wd 1.3e-4 --num-workers 3
Binding: ['/usr/bin/numactl', '--physcpubind=0-2,48-50', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '56', '--eval-batch-size', '160', '--warmup', '650', '--lr', '3.2e-3', '--wd', '1.3e-4', '--num-workers', '3']
Binding: ['/usr/bin/numactl', '--physcpubind=3-5,51-53', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=1', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '56', '--eval-batch-size', '160', '--warmup', '650', '--lr', '3.2e-3', '--wd', '1.3e-4', '--num-workers', '3']
Binding: ['/usr/bin/numactl', '--physcpubind=6-8,54-56', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=2', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '56', '--eval-batch-size', '160', '--warmup', '650', '--lr', '3.2e-3', '--wd', '1.3e-4', '--num-workers', '3']
Binding: ['/usr/bin/numactl', '--physcpubind=9-11,57-59', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '56', '--eval-batch-size', '160', '--warmup', '650', '--lr', '3.2e-3', '--wd', '1.3e-4', '--num-workers', '3']
Binding: ['/usr/bin/numactl', '--physcpubind=12-14,60-62', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=4', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '56', '--eval-batch-size', '160', '--warmup', '650', '--lr', '3.2e-3', '--wd', '1.3e-4', '--num-workers', '3']
Binding: ['/usr/bin/numactl', '--physcpubind=15-17,63-65', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=5', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '56', '--eval-batch-size', '160', '--warmup', '650', '--lr', '3.2e-3', '--wd', '1.3e-4', '--num-workers', '3']
Binding: ['/usr/bin/numactl', '--physcpubind=18-20,66-68', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=6', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '56', '--eval-batch-size', '160', '--warmup', '650', '--lr', '3.2e-3', '--wd', '1.3e-4', '--num-workers', '3']
Binding: ['/usr/bin/numactl', '--physcpubind=21-23,69-71', '--membind=0', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=7', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '56', '--eval-batch-size', '160', '--warmup', '650', '--lr', '3.2e-3', '--wd', '1.3e-4', '--num-workers', '3']
Binding: ['/usr/bin/numactl', '--physcpubind=24-26,72-74', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=8', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '56', '--eval-batch-size', '160', '--warmup', '650', '--lr', '3.2e-3', '--wd', '1.3e-4', '--num-workers', '3']
Binding: ['/usr/bin/numactl', '--physcpubind=27-29,75-77', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=9', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '56', '--eval-batch-size', '160', '--warmup', '650', '--lr', '3.2e-3', '--wd', '1.3e-4', '--num-workers', '3']
Binding: ['/usr/bin/numactl', '--physcpubind=30-32,78-80', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=10', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '56', '--eval-batch-size', '160', '--warmup', '650', '--lr', '3.2e-3', '--wd', '1.3e-4', '--num-workers', '3']
Binding: ['/usr/bin/numactl', '--physcpubind=33-35,81-83', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=11', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '56', '--eval-batch-size', '160', '--warmup', '650', '--lr', '3.2e-3', '--wd', '1.3e-4', '--num-workers', '3']
Binding: ['/usr/bin/numactl', '--physcpubind=36-38,84-86', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=12', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '56', '--eval-batch-size', '160', '--warmup', '650', '--lr', '3.2e-3', '--wd', '1.3e-4', '--num-workers', '3']
Binding: ['/usr/bin/numactl', '--physcpubind=39-41,87-89', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=13', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '56', '--eval-batch-size', '160', '--warmup', '650', '--lr', '3.2e-3', '--wd', '1.3e-4', '--num-workers', '3']
Binding: ['/usr/bin/numactl', '--physcpubind=42-44,90-92', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=14', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '56', '--eval-batch-size', '160', '--warmup', '650', '--lr', '3.2e-3', '--wd', '1.3e-4', '--num-workers', '3']
Binding: ['/usr/bin/numactl', '--physcpubind=45-47,93-95', '--membind=1', '/opt/conda/bin/python', '-u', 'train.py', '--local_rank=15', '--use-fp16', '--nhwc', '--pad-input', '--jit', '--delay-allreduce', '--opt-loss', '--epochs', '80', '--warmup-factor', '0', '--no-save', '--threshold=0.23', '--data', '/data/coco2017', '--evaluation', '120000', '160000', '180000', '200000', '220000', '240000', '260000', '280000', '--batch-size', '56', '--eval-batch-size', '160', '--warmup', '650', '--lr', '3.2e-3', '--wd', '1.3e-4', '--num-workers', '3']:::MLL 1575525083.311 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1575525083.373 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1575525083.427 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1575525083.434 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1575525083.452 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1575525083.465 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1575525083.482 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1575525083.503 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1575525083.509 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1575525083.523 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1575525083.529 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1575525083.532 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1575525083.533 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
:::MLL 1575525083.534 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1575525083.534 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
:::MLL 1575525083.534 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 833}}
BN group: 1
BN group: 1
BN group: 1
BN group: 1
7 Using seed = 3403338660
6 Using seed = 3403338659
8 Using seed = 3403338661
3 Using seed = 3403338656
1 Using seed = 3403338654
2 Using seed = 3403338655
5 Using seed = 3403338658
12 Using seed = 3403338665
9 Using seed = 3403338662
10 Using seed = 3403338663
13 Using seed = 3403338666
11 Using seed = 3403338664
14 Using seed = 3403338667
4 Using seed = 3403338657
0 Using seed = 3403338653
15 Using seed = 3403338668
:::MLL 1575525095.416 max_samples: {"value": 1, "metadata": {"file": "utils.py", "lineno": 465}}
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
:::MLL 1575525096.183 model_bn_span: {"value": 56, "metadata": {"file": "train.py", "lineno": 480}}
:::MLL 1575525096.183 global_batch_size: {"value": 896, "metadata": {"file": "train.py", "lineno": 481}}
:::MLL 1575525096.210 opt_base_learning_rate: {"value": 0.09, "metadata": {"file": "train.py", "lineno": 511}}
:::MLL 1575525096.210 opt_weight_decay: {"value": 0.00013, "metadata": {"file": "train.py", "lineno": 513}}
:::MLL 1575525096.210 opt_learning_rate_warmup_steps: {"value": 650, "metadata": {"file": "train.py", "lineno": 516}}
:::MLL 1575525096.211 opt_learning_rate_warmup_factor: {"value": 0, "metadata": {"file": "train.py", "lineno": 518}}
epoch nbatch loss
:::MLL 1575525101.281 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 604}}
:::MLL 1575525101.282 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 610}}
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
time_check a: 1575525103.151529312
time_check b: 1575525109.412291050
:::MLL 1575525109.872 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 32.74606450292497, "file": "train.py", "lineno": 669}}
:::MLL 1575525109.877 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 673}}
Iteration:      0, Loss function: 22.517, Average Loss: 0.023, avg. samples / sec: 62.42
Iteration:     20, Loss function: 20.833, Average Loss: 0.444, avg. samples / sec: 5404.18
Iteration:     40, Loss function: 18.715, Average Loss: 0.834, avg. samples / sec: 7421.40
Iteration:     60, Loss function: 15.251, Average Loss: 1.111, avg. samples / sec: 7277.69
Iteration:     80, Loss function: 10.403, Average Loss: 1.331, avg. samples / sec: 7619.81
Iteration:    100, Loss function: 9.170, Average Loss: 1.498, avg. samples / sec: 7796.48
Iteration:    120, Loss function: 8.921, Average Loss: 1.653, avg. samples / sec: 7810.33
:::MLL 1575525127.598 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 819}}
:::MLL 1575525127.598 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 673}}
Iteration:    140, Loss function: 8.330, Average Loss: 1.797, avg. samples / sec: 7796.37
Iteration:    160, Loss function: 8.800, Average Loss: 1.936, avg. samples / sec: 8161.61
Iteration:    180, Loss function: 8.512, Average Loss: 2.064, avg. samples / sec: 7919.42
Iteration:    200, Loss function: 8.292, Average Loss: 2.193, avg. samples / sec: 8051.66
Iteration:    220, Loss function: 7.715, Average Loss: 2.309, avg. samples / sec: 7815.39
Iteration:    240, Loss function: 7.826, Average Loss: 2.421, avg. samples / sec: 8128.97
Iteration:    260, Loss function: 7.622, Average Loss: 2.528, avg. samples / sec: 8161.81
:::MLL 1575525142.209 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 819}}
:::MLL 1575525142.209 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 673}}
Iteration:    280, Loss function: 7.699, Average Loss: 2.631, avg. samples / sec: 8051.83
Iteration:    300, Loss function: 7.217, Average Loss: 2.727, avg. samples / sec: 8197.81
Iteration:    320, Loss function: 7.613, Average Loss: 2.823, avg. samples / sec: 8199.25
Iteration:    340, Loss function: 6.991, Average Loss: 2.912, avg. samples / sec: 8277.77
Iteration:    360, Loss function: 7.086, Average Loss: 2.992, avg. samples / sec: 8263.51
Iteration:    380, Loss function: 7.324, Average Loss: 3.077, avg. samples / sec: 8308.18
:::MLL 1575525156.480 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 819}}
:::MLL 1575525156.480 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 673}}
Iteration:    400, Loss function: 7.014, Average Loss: 3.154, avg. samples / sec: 8280.97
Iteration:    420, Loss function: 6.959, Average Loss: 3.227, avg. samples / sec: 8332.19
Iteration:    440, Loss function: 6.672, Average Loss: 3.295, avg. samples / sec: 8264.53
Iteration:    460, Loss function: 6.776, Average Loss: 3.365, avg. samples / sec: 8358.06
Iteration:    480, Loss function: 6.214, Average Loss: 3.428, avg. samples / sec: 8370.82
Iteration:    500, Loss function: 6.292, Average Loss: 3.489, avg. samples / sec: 8312.12
Iteration:    520, Loss function: 6.379, Average Loss: 3.548, avg. samples / sec: 8242.55
:::MLL 1575525170.610 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 819}}
:::MLL 1575525170.610 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 673}}
Iteration:    540, Loss function: 6.597, Average Loss: 3.603, avg. samples / sec: 8309.89
Iteration:    560, Loss function: 6.298, Average Loss: 3.656, avg. samples / sec: 8375.36
Iteration:    580, Loss function: 6.306, Average Loss: 3.706, avg. samples / sec: 8330.34
Iteration:    600, Loss function: 5.972, Average Loss: 3.757, avg. samples / sec: 8333.73
Iteration:    620, Loss function: 5.925, Average Loss: 3.802, avg. samples / sec: 8347.13
Iteration:    640, Loss function: 5.621, Average Loss: 3.846, avg. samples / sec: 8301.14
:::MLL 1575525184.685 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 819}}
:::MLL 1575525184.686 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 673}}
Iteration:    660, Loss function: 5.580, Average Loss: 3.888, avg. samples / sec: 8349.39
Iteration:    680, Loss function: 6.049, Average Loss: 3.929, avg. samples / sec: 8303.76
Iteration:    700, Loss function: 5.514, Average Loss: 3.963, avg. samples / sec: 8319.95
Iteration:    720, Loss function: 5.777, Average Loss: 3.997, avg. samples / sec: 8376.59
Iteration:    740, Loss function: 5.913, Average Loss: 4.030, avg. samples / sec: 8358.70
Iteration:    760, Loss function: 5.478, Average Loss: 4.058, avg. samples / sec: 8378.19
Iteration:    780, Loss function: 5.865, Average Loss: 4.090, avg. samples / sec: 8382.51
:::MLL 1575525198.748 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 819}}
:::MLL 1575525198.749 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 673}}
Iteration:    800, Loss function: 5.407, Average Loss: 4.116, avg. samples / sec: 8298.28
Iteration:    820, Loss function: 5.387, Average Loss: 4.141, avg. samples / sec: 8350.34
Iteration:    840, Loss function: 5.439, Average Loss: 4.165, avg. samples / sec: 8390.50
Iteration:    860, Loss function: 5.414, Average Loss: 4.189, avg. samples / sec: 8378.83
Iteration:    880, Loss function: 4.819, Average Loss: 4.211, avg. samples / sec: 8358.63
Iteration:    900, Loss function: 5.598, Average Loss: 4.229, avg. samples / sec: 8388.90
:::MLL 1575525212.779 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 819}}
:::MLL 1575525212.780 epoch_start: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 673}}
Iteration:    920, Loss function: 4.591, Average Loss: 4.247, avg. samples / sec: 8383.95
Iteration:    940, Loss function: 5.200, Average Loss: 4.265, avg. samples / sec: 8344.78
Iteration:    960, Loss function: 4.674, Average Loss: 4.283, avg. samples / sec: 8380.87
Iteration:    980, Loss function: 4.804, Average Loss: 4.300, avg. samples / sec: 8349.10
Iteration:   1000, Loss function: 4.742, Average Loss: 4.318, avg. samples / sec: 8370.91
Iteration:   1020, Loss function: 5.252, Average Loss: 4.332, avg. samples / sec: 8354.25
Iteration:   1040, Loss function: 4.910, Average Loss: 4.348, avg. samples / sec: 8349.07
:::MLL 1575525226.816 epoch_stop: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 819}}
:::MLL 1575525226.816 epoch_start: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 673}}
Iteration:   1060, Loss function: 5.015, Average Loss: 4.362, avg. samples / sec: 8353.97
Iteration:   1080, Loss function: 4.866, Average Loss: 4.374, avg. samples / sec: 8387.95
Iteration:   1100, Loss function: 5.033, Average Loss: 4.386, avg. samples / sec: 8401.69
Iteration:   1120, Loss function: 4.570, Average Loss: 4.396, avg. samples / sec: 8344.40
Iteration:   1140, Loss function: 5.052, Average Loss: 4.407, avg. samples / sec: 8393.52
Iteration:   1160, Loss function: 4.957, Average Loss: 4.417, avg. samples / sec: 8388.92
:::MLL 1575525240.731 epoch_stop: {"value": null, "metadata": {"epoch_num": 9, "file": "train.py", "lineno": 819}}
:::MLL 1575525240.731 epoch_start: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 673}}
Iteration:   1180, Loss function: 4.953, Average Loss: 4.425, avg. samples / sec: 8327.33
Iteration:   1200, Loss function: 5.048, Average Loss: 4.433, avg. samples / sec: 8430.61
Iteration:   1220, Loss function: 5.004, Average Loss: 4.441, avg. samples / sec: 8393.51
Iteration:   1240, Loss function: 4.509, Average Loss: 4.447, avg. samples / sec: 8414.02
Iteration:   1260, Loss function: 4.072, Average Loss: 4.453, avg. samples / sec: 8333.12
Iteration:   1280, Loss function: 4.878, Average Loss: 4.458, avg. samples / sec: 8349.77
Iteration:   1300, Loss function: 4.629, Average Loss: 4.464, avg. samples / sec: 8398.97
:::MLL 1575525254.730 epoch_stop: {"value": null, "metadata": {"epoch_num": 10, "file": "train.py", "lineno": 819}}
:::MLL 1575525254.731 epoch_start: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 673}}
Iteration:   1320, Loss function: 4.613, Average Loss: 4.471, avg. samples / sec: 8354.10
Iteration:   1340, Loss function: 4.709, Average Loss: 4.476, avg. samples / sec: 8413.52
Iteration:   1360, Loss function: 4.802, Average Loss: 4.481, avg. samples / sec: 8397.25
Iteration:   1380, Loss function: 4.581, Average Loss: 4.486, avg. samples / sec: 8385.61
Iteration:   1400, Loss function: 4.672, Average Loss: 4.490, avg. samples / sec: 8366.97
Iteration:   1420, Loss function: 5.241, Average Loss: 4.495, avg. samples / sec: 8382.02
:::MLL 1575525268.731 epoch_stop: {"value": null, "metadata": {"epoch_num": 11, "file": "train.py", "lineno": 819}}
:::MLL 1575525268.731 epoch_start: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 673}}
Iteration:   1440, Loss function: 4.539, Average Loss: 4.496, avg. samples / sec: 8394.51
Iteration:   1460, Loss function: 4.100, Average Loss: 4.498, avg. samples / sec: 8368.70
Iteration:   1480, Loss function: 4.768, Average Loss: 4.502, avg. samples / sec: 8347.76
Iteration:   1500, Loss function: 4.600, Average Loss: 4.505, avg. samples / sec: 8395.90
Iteration:   1520, Loss function: 4.267, Average Loss: 4.506, avg. samples / sec: 8324.28
Iteration:   1540, Loss function: 4.559, Average Loss: 4.506, avg. samples / sec: 8403.15
Iteration:   1560, Loss function: 4.580, Average Loss: 4.507, avg. samples / sec: 8368.26
:::MLL 1575525282.756 epoch_stop: {"value": null, "metadata": {"epoch_num": 12, "file": "train.py", "lineno": 819}}
:::MLL 1575525282.757 epoch_start: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 673}}
Iteration:   1580, Loss function: 4.737, Average Loss: 4.507, avg. samples / sec: 8347.84
Iteration:   1600, Loss function: 5.204, Average Loss: 4.509, avg. samples / sec: 8408.74
Iteration:   1620, Loss function: 4.692, Average Loss: 4.511, avg. samples / sec: 8436.15
Iteration:   1640, Loss function: 4.539, Average Loss: 4.512, avg. samples / sec: 8363.03
Iteration:   1660, Loss function: 4.496, Average Loss: 4.511, avg. samples / sec: 8415.54
Iteration:   1680, Loss function: 4.600, Average Loss: 4.511, avg. samples / sec: 8383.42
Iteration:   1700, Loss function: 4.632, Average Loss: 4.510, avg. samples / sec: 8347.04
:::MLL 1575525296.753 epoch_stop: {"value": null, "metadata": {"epoch_num": 13, "file": "train.py", "lineno": 819}}
:::MLL 1575525296.754 epoch_start: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 673}}
Iteration:   1720, Loss function: 4.248, Average Loss: 4.508, avg. samples / sec: 8372.38
Iteration:   1740, Loss function: 4.767, Average Loss: 4.509, avg. samples / sec: 8382.63
Iteration:   1760, Loss function: 4.468, Average Loss: 4.509, avg. samples / sec: 8374.28
Iteration:   1780, Loss function: 4.462, Average Loss: 4.509, avg. samples / sec: 8304.66
Iteration:   1800, Loss function: 4.072, Average Loss: 4.507, avg. samples / sec: 8389.01
Iteration:   1820, Loss function: 4.226, Average Loss: 4.507, avg. samples / sec: 8296.02
:::MLL 1575525310.801 epoch_stop: {"value": null, "metadata": {"epoch_num": 14, "file": "train.py", "lineno": 819}}
:::MLL 1575525310.802 epoch_start: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 673}}
Iteration:   1840, Loss function: 4.719, Average Loss: 4.507, avg. samples / sec: 8319.14
Iteration:   1860, Loss function: 4.433, Average Loss: 4.506, avg. samples / sec: 8380.91
Iteration:   1880, Loss function: 3.991, Average Loss: 4.504, avg. samples / sec: 8378.05
Iteration:   1900, Loss function: 4.753, Average Loss: 4.501, avg. samples / sec: 8411.48
Iteration:   1920, Loss function: 4.723, Average Loss: 4.499, avg. samples / sec: 8392.04
Iteration:   1940, Loss function: 4.386, Average Loss: 4.497, avg. samples / sec: 8393.65
Iteration:   1960, Loss function: 4.717, Average Loss: 4.496, avg. samples / sec: 8396.20
:::MLL 1575525324.808 epoch_stop: {"value": null, "metadata": {"epoch_num": 15, "file": "train.py", "lineno": 819}}
:::MLL 1575525324.809 epoch_start: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 673}}
Iteration:   1980, Loss function: 4.437, Average Loss: 4.495, avg. samples / sec: 8330.66
Iteration:   2000, Loss function: 4.500, Average Loss: 4.493, avg. samples / sec: 8374.62
Iteration:   2020, Loss function: 4.144, Average Loss: 4.489, avg. samples / sec: 8374.28
Iteration:   2040, Loss function: 4.146, Average Loss: 4.486, avg. samples / sec: 8356.21
Iteration:   2060, Loss function: 4.529, Average Loss: 4.485, avg. samples / sec: 8344.09
Iteration:   2080, Loss function: 4.618, Average Loss: 4.484, avg. samples / sec: 8395.03
:::MLL 1575525338.844 epoch_stop: {"value": null, "metadata": {"epoch_num": 16, "file": "train.py", "lineno": 819}}
:::MLL 1575525338.844 epoch_start: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 673}}
Iteration:   2100, Loss function: 4.398, Average Loss: 4.481, avg. samples / sec: 8340.85
Iteration:   2120, Loss function: 4.540, Average Loss: 4.478, avg. samples / sec: 8390.24
Iteration:   2140, Loss function: 4.188, Average Loss: 4.475, avg. samples / sec: 8355.44
Iteration:   2160, Loss function: 4.030, Average Loss: 4.471, avg. samples / sec: 8352.69
Iteration:   2180, Loss function: 4.051, Average Loss: 4.467, avg. samples / sec: 8362.62
Iteration:   2200, Loss function: 4.074, Average Loss: 4.464, avg. samples / sec: 8384.17
Iteration:   2220, Loss function: 4.587, Average Loss: 4.461, avg. samples / sec: 8371.69
:::MLL 1575525352.768 epoch_stop: {"value": null, "metadata": {"epoch_num": 17, "file": "train.py", "lineno": 819}}
:::MLL 1575525352.768 epoch_start: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 673}}
Iteration:   2240, Loss function: 4.677, Average Loss: 4.457, avg. samples / sec: 8379.83
Iteration:   2260, Loss function: 3.971, Average Loss: 4.454, avg. samples / sec: 8363.48
Iteration:   2280, Loss function: 4.583, Average Loss: 4.451, avg. samples / sec: 8389.25
Iteration:   2300, Loss function: 4.277, Average Loss: 4.449, avg. samples / sec: 8393.47
Iteration:   2320, Loss function: 4.137, Average Loss: 4.444, avg. samples / sec: 8423.03
Iteration:   2340, Loss function: 4.173, Average Loss: 4.441, avg. samples / sec: 8376.00
:::MLL 1575525366.758 epoch_stop: {"value": null, "metadata": {"epoch_num": 18, "file": "train.py", "lineno": 819}}
:::MLL 1575525366.758 epoch_start: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 673}}
Iteration:   2360, Loss function: 4.296, Average Loss: 4.436, avg. samples / sec: 8400.45
Iteration:   2380, Loss function: 4.300, Average Loss: 4.431, avg. samples / sec: 8407.83
Iteration:   2400, Loss function: 4.709, Average Loss: 4.427, avg. samples / sec: 8347.76
Iteration:   2420, Loss function: 4.131, Average Loss: 4.422, avg. samples / sec: 8305.70
Iteration:   2440, Loss function: 4.136, Average Loss: 4.417, avg. samples / sec: 8355.54
Iteration:   2460, Loss function: 4.303, Average Loss: 4.413, avg. samples / sec: 8378.62
Iteration:   2480, Loss function: 4.271, Average Loss: 4.411, avg. samples / sec: 8391.68
:::MLL 1575525380.787 epoch_stop: {"value": null, "metadata": {"epoch_num": 19, "file": "train.py", "lineno": 819}}
:::MLL 1575525380.788 epoch_start: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 673}}
Iteration:   2500, Loss function: 4.244, Average Loss: 4.406, avg. samples / sec: 8391.94
Iteration:   2520, Loss function: 4.285, Average Loss: 4.401, avg. samples / sec: 8395.68
Iteration:   2540, Loss function: 4.096, Average Loss: 4.397, avg. samples / sec: 8365.88
Iteration:   2560, Loss function: 4.335, Average Loss: 4.395, avg. samples / sec: 8410.97
Iteration:   2580, Loss function: 4.414, Average Loss: 4.389, avg. samples / sec: 8391.63
Iteration:   2600, Loss function: 4.059, Average Loss: 4.383, avg. samples / sec: 8375.93
:::MLL 1575525394.787 epoch_stop: {"value": null, "metadata": {"epoch_num": 20, "file": "train.py", "lineno": 819}}
:::MLL 1575525394.788 epoch_start: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 673}}
Iteration:   2620, Loss function: 4.128, Average Loss: 4.379, avg. samples / sec: 8346.70
Iteration:   2640, Loss function: 4.391, Average Loss: 4.374, avg. samples / sec: 8367.05
Iteration:   2660, Loss function: 4.148, Average Loss: 4.367, avg. samples / sec: 8349.04
Iteration:   2680, Loss function: 4.390, Average Loss: 4.364, avg. samples / sec: 8416.15
Iteration:   2700, Loss function: 3.671, Average Loss: 4.360, avg. samples / sec: 8391.71
Iteration:   2720, Loss function: 3.627, Average Loss: 4.355, avg. samples / sec: 8349.90
Iteration:   2740, Loss function: 4.387, Average Loss: 4.350, avg. samples / sec: 8387.76
:::MLL 1575525408.805 epoch_stop: {"value": null, "metadata": {"epoch_num": 21, "file": "train.py", "lineno": 819}}
:::MLL 1575525408.806 epoch_start: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 673}}
Iteration:   2760, Loss function: 4.186, Average Loss: 4.345, avg. samples / sec: 8354.95
Iteration:   2780, Loss function: 3.893, Average Loss: 4.342, avg. samples / sec: 8376.32
Iteration:   2800, Loss function: 3.991, Average Loss: 4.337, avg. samples / sec: 8380.44
Iteration:   2820, Loss function: 3.755, Average Loss: 4.331, avg. samples / sec: 8377.03
Iteration:   2840, Loss function: 4.060, Average Loss: 4.325, avg. samples / sec: 8396.38
Iteration:   2860, Loss function: 4.162, Average Loss: 4.322, avg. samples / sec: 8397.71
:::MLL 1575525422.815 epoch_stop: {"value": null, "metadata": {"epoch_num": 22, "file": "train.py", "lineno": 819}}
:::MLL 1575525422.815 epoch_start: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 673}}
Iteration:   2880, Loss function: 4.076, Average Loss: 4.317, avg. samples / sec: 8358.49
Iteration:   2900, Loss function: 4.298, Average Loss: 4.314, avg. samples / sec: 8368.42
Iteration:   2920, Loss function: 4.113, Average Loss: 4.308, avg. samples / sec: 8343.75
Iteration:   2940, Loss function: 4.088, Average Loss: 4.305, avg. samples / sec: 8364.96
Iteration:   2960, Loss function: 4.490, Average Loss: 4.300, avg. samples / sec: 8372.47
Iteration:   2980, Loss function: 4.423, Average Loss: 4.299, avg. samples / sec: 8370.50
Iteration:   3000, Loss function: 4.228, Average Loss: 4.293, avg. samples / sec: 8389.62
:::MLL 1575525436.836 epoch_stop: {"value": null, "metadata": {"epoch_num": 23, "file": "train.py", "lineno": 819}}
:::MLL 1575525436.836 epoch_start: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 673}}
Iteration:   3020, Loss function: 4.159, Average Loss: 4.289, avg. samples / sec: 8372.11
Iteration:   3040, Loss function: 4.339, Average Loss: 4.284, avg. samples / sec: 8400.77
Iteration:   3060, Loss function: 3.724, Average Loss: 4.278, avg. samples / sec: 8372.97
Iteration:   3080, Loss function: 4.015, Average Loss: 4.275, avg. samples / sec: 8392.75
Iteration:   3100, Loss function: 3.956, Average Loss: 4.271, avg. samples / sec: 8403.77
Iteration:   3120, Loss function: 3.371, Average Loss: 4.265, avg. samples / sec: 8392.78
Iteration:   3140, Loss function: 3.844, Average Loss: 4.261, avg. samples / sec: 8411.46
:::MLL 1575525450.825 epoch_stop: {"value": null, "metadata": {"epoch_num": 24, "file": "train.py", "lineno": 819}}
:::MLL 1575525450.826 epoch_start: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 673}}
Iteration:   3160, Loss function: 3.739, Average Loss: 4.255, avg. samples / sec: 8359.49
Iteration:   3180, Loss function: 4.087, Average Loss: 4.251, avg. samples / sec: 8412.30
Iteration:   3200, Loss function: 4.146, Average Loss: 4.245, avg. samples / sec: 8414.68
Iteration:   3220, Loss function: 4.558, Average Loss: 4.240, avg. samples / sec: 8366.99
Iteration:   3240, Loss function: 3.802, Average Loss: 4.235, avg. samples / sec: 8399.84
Iteration:   3260, Loss function: 4.112, Average Loss: 4.230, avg. samples / sec: 8425.41
:::MLL 1575525464.703 epoch_stop: {"value": null, "metadata": {"epoch_num": 25, "file": "train.py", "lineno": 819}}
:::MLL 1575525464.704 epoch_start: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 673}}
Iteration:   3280, Loss function: 3.925, Average Loss: 4.225, avg. samples / sec: 8362.23
Iteration:   3300, Loss function: 4.380, Average Loss: 4.221, avg. samples / sec: 8402.79
Iteration:   3320, Loss function: 3.837, Average Loss: 4.215, avg. samples / sec: 8366.04
Iteration:   3340, Loss function: 3.751, Average Loss: 4.211, avg. samples / sec: 8353.68
Iteration:   3360, Loss function: 4.157, Average Loss: 4.205, avg. samples / sec: 8364.78
Iteration:   3380, Loss function: 4.392, Average Loss: 4.203, avg. samples / sec: 8373.32
Iteration:   3400, Loss function: 4.171, Average Loss: 4.199, avg. samples / sec: 8405.45
:::MLL 1575525478.712 epoch_stop: {"value": null, "metadata": {"epoch_num": 26, "file": "train.py", "lineno": 819}}
:::MLL 1575525478.712 epoch_start: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 673}}
Iteration:   3420, Loss function: 3.830, Average Loss: 4.195, avg. samples / sec: 8365.68
Iteration:   3440, Loss function: 3.650, Average Loss: 4.190, avg. samples / sec: 8435.93
Iteration:   3460, Loss function: 4.038, Average Loss: 4.188, avg. samples / sec: 8399.18
Iteration:   3480, Loss function: 4.116, Average Loss: 4.183, avg. samples / sec: 8375.37
Iteration:   3500, Loss function: 4.313, Average Loss: 4.178, avg. samples / sec: 8388.53
Iteration:   3520, Loss function: 3.735, Average Loss: 4.175, avg. samples / sec: 8397.30
:::MLL 1575525492.699 epoch_stop: {"value": null, "metadata": {"epoch_num": 27, "file": "train.py", "lineno": 819}}
:::MLL 1575525492.699 epoch_start: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 673}}
Iteration:   3540, Loss function: 3.894, Average Loss: 4.174, avg. samples / sec: 8363.62
Iteration:   3560, Loss function: 4.322, Average Loss: 4.170, avg. samples / sec: 8381.31
Iteration:   3580, Loss function: 4.002, Average Loss: 4.165, avg. samples / sec: 8381.87
Iteration:   3600, Loss function: 3.972, Average Loss: 4.162, avg. samples / sec: 8358.83
Iteration:   3620, Loss function: 3.309, Average Loss: 4.158, avg. samples / sec: 8385.23
Iteration:   3640, Loss function: 3.678, Average Loss: 4.153, avg. samples / sec: 8396.49
Iteration:   3660, Loss function: 3.555, Average Loss: 4.151, avg. samples / sec: 8367.23
:::MLL 1575525506.717 epoch_stop: {"value": null, "metadata": {"epoch_num": 28, "file": "train.py", "lineno": 819}}
:::MLL 1575525506.717 epoch_start: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 673}}
Iteration:   3680, Loss function: 3.960, Average Loss: 4.146, avg. samples / sec: 8351.50
Iteration:   3700, Loss function: 4.007, Average Loss: 4.140, avg. samples / sec: 8368.01
Iteration:   3720, Loss function: 3.851, Average Loss: 4.137, avg. samples / sec: 8390.22
Iteration:   3740, Loss function: 3.947, Average Loss: 4.133, avg. samples / sec: 8381.71
Iteration:   3760, Loss function: 3.850, Average Loss: 4.131, avg. samples / sec: 8400.03
Iteration:   3780, Loss function: 4.084, Average Loss: 4.125, avg. samples / sec: 8363.46
:::MLL 1575525520.730 epoch_stop: {"value": null, "metadata": {"epoch_num": 29, "file": "train.py", "lineno": 819}}
:::MLL 1575525520.731 epoch_start: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 673}}
Iteration:   3800, Loss function: 3.710, Average Loss: 4.121, avg. samples / sec: 8346.52
Iteration:   3820, Loss function: 3.707, Average Loss: 4.116, avg. samples / sec: 8398.74
Iteration:   3840, Loss function: 4.063, Average Loss: 4.113, avg. samples / sec: 8371.58
Iteration:   3860, Loss function: 3.831, Average Loss: 4.108, avg. samples / sec: 8402.69
Iteration:   3880, Loss function: 3.900, Average Loss: 4.105, avg. samples / sec: 8407.47
Iteration:   3900, Loss function: 3.497, Average Loss: 4.102, avg. samples / sec: 8368.35
Iteration:   3920, Loss function: 3.750, Average Loss: 4.100, avg. samples / sec: 8416.55
:::MLL 1575525534.718 epoch_stop: {"value": null, "metadata": {"epoch_num": 30, "file": "train.py", "lineno": 819}}
:::MLL 1575525534.718 epoch_start: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 673}}
Iteration:   3940, Loss function: 4.367, Average Loss: 4.097, avg. samples / sec: 8380.64
Iteration:   3960, Loss function: 4.028, Average Loss: 4.092, avg. samples / sec: 8385.69
Iteration:   3980, Loss function: 4.054, Average Loss: 4.090, avg. samples / sec: 8411.28
Iteration:   4000, Loss function: 3.512, Average Loss: 4.086, avg. samples / sec: 8314.34
Iteration:   4020, Loss function: 3.724, Average Loss: 4.084, avg. samples / sec: 8388.53
Iteration:   4040, Loss function: 3.754, Average Loss: 4.078, avg. samples / sec: 8387.34
:::MLL 1575525548.727 epoch_stop: {"value": null, "metadata": {"epoch_num": 31, "file": "train.py", "lineno": 819}}
:::MLL 1575525548.728 epoch_start: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 673}}
Iteration:   4060, Loss function: 4.041, Average Loss: 4.073, avg. samples / sec: 8391.71
Iteration:   4080, Loss function: 3.633, Average Loss: 4.070, avg. samples / sec: 8385.97
Iteration:   4100, Loss function: 3.768, Average Loss: 4.065, avg. samples / sec: 8405.80
Iteration:   4120, Loss function: 3.924, Average Loss: 4.062, avg. samples / sec: 8381.57
Iteration:   4140, Loss function: 4.169, Average Loss: 4.061, avg. samples / sec: 8378.54
Iteration:   4160, Loss function: 3.954, Average Loss: 4.056, avg. samples / sec: 8375.17
Iteration:   4180, Loss function: 3.652, Average Loss: 4.053, avg. samples / sec: 8383.58
:::MLL 1575525562.727 epoch_stop: {"value": null, "metadata": {"epoch_num": 32, "file": "train.py", "lineno": 819}}
:::MLL 1575525562.728 epoch_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 673}}
Iteration:   4200, Loss function: 3.922, Average Loss: 4.052, avg. samples / sec: 8376.91
Iteration:   4220, Loss function: 3.950, Average Loss: 4.049, avg. samples / sec: 8357.13
Iteration:   4240, Loss function: 3.717, Average Loss: 4.044, avg. samples / sec: 8399.23
Iteration:   4260, Loss function: 3.744, Average Loss: 4.041, avg. samples / sec: 8382.37
Iteration:   4280, Loss function: 4.258, Average Loss: 4.039, avg. samples / sec: 8401.03
:::MLL 1575525573.096 eval_start: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 4.65 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
DONE (t=0.35s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.37s)
DONE (t=2.69s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.16446
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.30917
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.16144
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.04331
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.17517
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.26709
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.17687
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.25925
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.27312
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.07437
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.29326
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.42033
Current AP: 0.16446 AP goal: 0.23000
:::MLL 1575525580.858 eval_accuracy: {"value": 0.16445708169249054, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 389}}
:::MLL 1575525580.892 eval_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 392}}
:::MLL 1575525580.943 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 804}}
:::MLL 1575525580.944 block_start: {"value": null, "metadata": {"first_epoch_num": 33, "epoch_count": 10.915354834308324, "file": "train.py", "lineno": 813}}
Iteration:   4300, Loss function: 3.966, Average Loss: 4.034, avg. samples / sec: 1743.90
:::MLL 1575525584.756 epoch_stop: {"value": null, "metadata": {"epoch_num": 33, "file": "train.py", "lineno": 819}}
:::MLL 1575525584.756 epoch_start: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 673}}
Iteration:   4320, Loss function: 3.890, Average Loss: 4.033, avg. samples / sec: 8387.44
Iteration:   4340, Loss function: 4.008, Average Loss: 4.031, avg. samples / sec: 8370.51
Iteration:   4360, Loss function: 3.814, Average Loss: 4.027, avg. samples / sec: 8349.19
Iteration:   4380, Loss function: 3.714, Average Loss: 4.024, avg. samples / sec: 8316.87
Iteration:   4400, Loss function: 3.680, Average Loss: 4.022, avg. samples / sec: 8321.08
Iteration:   4420, Loss function: 4.019, Average Loss: 4.018, avg. samples / sec: 8390.98
Iteration:   4440, Loss function: 3.832, Average Loss: 4.014, avg. samples / sec: 8292.41
:::MLL 1575525598.828 epoch_stop: {"value": null, "metadata": {"epoch_num": 34, "file": "train.py", "lineno": 819}}
:::MLL 1575525598.828 epoch_start: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 673}}
Iteration:   4460, Loss function: 3.826, Average Loss: 4.011, avg. samples / sec: 8357.80
Iteration:   4480, Loss function: 3.308, Average Loss: 4.006, avg. samples / sec: 8357.37
Iteration:   4500, Loss function: 3.401, Average Loss: 4.002, avg. samples / sec: 8384.02
Iteration:   4520, Loss function: 3.529, Average Loss: 3.999, avg. samples / sec: 8358.06
Iteration:   4540, Loss function: 4.036, Average Loss: 3.996, avg. samples / sec: 8358.76
Iteration:   4560, Loss function: 3.868, Average Loss: 3.994, avg. samples / sec: 8378.83
Iteration:   4580, Loss function: 3.836, Average Loss: 3.992, avg. samples / sec: 8381.92
:::MLL 1575525612.862 epoch_stop: {"value": null, "metadata": {"epoch_num": 35, "file": "train.py", "lineno": 819}}
:::MLL 1575525612.862 epoch_start: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 673}}
Iteration:   4600, Loss function: 3.918, Average Loss: 3.991, avg. samples / sec: 8330.87
Iteration:   4620, Loss function: 4.456, Average Loss: 3.988, avg. samples / sec: 8370.91
Iteration:   4640, Loss function: 3.714, Average Loss: 3.984, avg. samples / sec: 8354.15
Iteration:   4660, Loss function: 3.829, Average Loss: 3.980, avg. samples / sec: 8333.07
Iteration:   4680, Loss function: 3.926, Average Loss: 3.976, avg. samples / sec: 8367.72
Iteration:   4700, Loss function: 3.607, Average Loss: 3.973, avg. samples / sec: 8365.73
:::MLL 1575525626.913 epoch_stop: {"value": null, "metadata": {"epoch_num": 36, "file": "train.py", "lineno": 819}}
:::MLL 1575525626.914 epoch_start: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 673}}
Iteration:   4720, Loss function: 3.869, Average Loss: 3.970, avg. samples / sec: 8281.25
Iteration:   4740, Loss function: 3.564, Average Loss: 3.967, avg. samples / sec: 8354.23
Iteration:   4760, Loss function: 3.999, Average Loss: 3.965, avg. samples / sec: 8349.50
Iteration:   4780, Loss function: 3.440, Average Loss: 3.964, avg. samples / sec: 8351.95
Iteration:   4800, Loss function: 3.900, Average Loss: 3.960, avg. samples / sec: 8353.09
Iteration:   4820, Loss function: 3.433, Average Loss: 3.959, avg. samples / sec: 8331.79
Iteration:   4840, Loss function: 3.550, Average Loss: 3.956, avg. samples / sec: 8382.24
:::MLL 1575525640.978 epoch_stop: {"value": null, "metadata": {"epoch_num": 37, "file": "train.py", "lineno": 819}}
:::MLL 1575525640.979 epoch_start: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 673}}
Iteration:   4860, Loss function: 3.970, Average Loss: 3.952, avg. samples / sec: 8332.45
Iteration:   4880, Loss function: 3.802, Average Loss: 3.949, avg. samples / sec: 8357.42
Iteration:   4900, Loss function: 3.795, Average Loss: 3.947, avg. samples / sec: 8351.33
Iteration:   4920, Loss function: 3.685, Average Loss: 3.944, avg. samples / sec: 8351.17
Iteration:   4940, Loss function: 3.772, Average Loss: 3.941, avg. samples / sec: 8360.89
Iteration:   4960, Loss function: 4.029, Average Loss: 3.938, avg. samples / sec: 8369.52
:::MLL 1575525655.032 epoch_stop: {"value": null, "metadata": {"epoch_num": 38, "file": "train.py", "lineno": 819}}
:::MLL 1575525655.033 epoch_start: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 673}}
Iteration:   4980, Loss function: 3.797, Average Loss: 3.933, avg. samples / sec: 8330.22
Iteration:   5000, Loss function: 3.741, Average Loss: 3.931, avg. samples / sec: 8349.22
Iteration:   5020, Loss function: 3.656, Average Loss: 3.927, avg. samples / sec: 8377.74
Iteration:   5040, Loss function: 3.405, Average Loss: 3.925, avg. samples / sec: 8351.05
Iteration:   5060, Loss function: 3.643, Average Loss: 3.922, avg. samples / sec: 8361.16
Iteration:   5080, Loss function: 4.067, Average Loss: 3.919, avg. samples / sec: 8372.15
Iteration:   5100, Loss function: 3.815, Average Loss: 3.916, avg. samples / sec: 8336.09
:::MLL 1575525669.084 epoch_stop: {"value": null, "metadata": {"epoch_num": 39, "file": "train.py", "lineno": 819}}
:::MLL 1575525669.085 epoch_start: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 673}}
Iteration:   5120, Loss function: 3.966, Average Loss: 3.914, avg. samples / sec: 8321.87
Iteration:   5140, Loss function: 4.173, Average Loss: 3.911, avg. samples / sec: 8329.51
Iteration:   5160, Loss function: 3.802, Average Loss: 3.909, avg. samples / sec: 8356.70
Iteration:   5180, Loss function: 3.740, Average Loss: 3.907, avg. samples / sec: 8353.43
Iteration:   5200, Loss function: 3.407, Average Loss: 3.903, avg. samples / sec: 8355.53
Iteration:   5220, Loss function: 4.088, Average Loss: 3.901, avg. samples / sec: 8372.96
:::MLL 1575525683.136 epoch_stop: {"value": null, "metadata": {"epoch_num": 40, "file": "train.py", "lineno": 819}}
:::MLL 1575525683.136 epoch_start: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 673}}
Iteration:   5240, Loss function: 4.062, Average Loss: 3.898, avg. samples / sec: 8357.03
Iteration:   5260, Loss function: 3.757, Average Loss: 3.896, avg. samples / sec: 8360.73
Iteration:   5280, Loss function: 4.131, Average Loss: 3.892, avg. samples / sec: 8365.54
Iteration:   5300, Loss function: 3.809, Average Loss: 3.893, avg. samples / sec: 8349.07
Iteration:   5320, Loss function: 4.184, Average Loss: 3.889, avg. samples / sec: 8328.30
Iteration:   5340, Loss function: 3.910, Average Loss: 3.888, avg. samples / sec: 8350.19
Iteration:   5360, Loss function: 3.877, Average Loss: 3.885, avg. samples / sec: 8309.62
:::MLL 1575525697.113 epoch_stop: {"value": null, "metadata": {"epoch_num": 41, "file": "train.py", "lineno": 819}}
:::MLL 1575525697.113 epoch_start: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 673}}
Iteration:   5380, Loss function: 4.011, Average Loss: 3.881, avg. samples / sec: 8249.25
Iteration:   5400, Loss function: 3.600, Average Loss: 3.878, avg. samples / sec: 8358.02
Iteration:   5420, Loss function: 3.830, Average Loss: 3.876, avg. samples / sec: 8315.59
Iteration:   5440, Loss function: 4.335, Average Loss: 3.875, avg. samples / sec: 8355.48
Iteration:   5460, Loss function: 3.671, Average Loss: 3.873, avg. samples / sec: 8329.37
Iteration:   5480, Loss function: 3.799, Average Loss: 3.872, avg. samples / sec: 8368.73
:::MLL 1575525711.191 epoch_stop: {"value": null, "metadata": {"epoch_num": 42, "file": "train.py", "lineno": 819}}
:::MLL 1575525711.192 epoch_start: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 673}}
Iteration:   5500, Loss function: 3.919, Average Loss: 3.870, avg. samples / sec: 8312.33
Iteration:   5520, Loss function: 3.374, Average Loss: 3.867, avg. samples / sec: 8339.65
Iteration:   5540, Loss function: 4.095, Average Loss: 3.864, avg. samples / sec: 8366.87
Iteration:   5560, Loss function: 3.304, Average Loss: 3.862, avg. samples / sec: 8386.08
Iteration:   5580, Loss function: 3.785, Average Loss: 3.858, avg. samples / sec: 8399.19
Iteration:   5600, Loss function: 3.634, Average Loss: 3.858, avg. samples / sec: 8363.73
Iteration:   5620, Loss function: 3.602, Average Loss: 3.855, avg. samples / sec: 8371.25
:::MLL 1575525725.221 epoch_stop: {"value": null, "metadata": {"epoch_num": 43, "file": "train.py", "lineno": 819}}
:::MLL 1575525725.222 epoch_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 673}}
Iteration:   5640, Loss function: 3.523, Average Loss: 3.853, avg. samples / sec: 8331.93
Iteration:   5660, Loss function: 3.468, Average Loss: 3.850, avg. samples / sec: 8342.64
Iteration:   5680, Loss function: 3.913, Average Loss: 3.847, avg. samples / sec: 8319.59
Iteration:   5700, Loss function: 3.699, Average Loss: 3.846, avg. samples / sec: 8361.78
lr decay step #1
:::MLL 1575525734.571 eval_start: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 3.45 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.47s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.48s)
DONE (t=0.49s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.51s)
DONE (t=2.82s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.18378
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.33447
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.18205
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.04572
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.20006
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.29299
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.19099
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.28441
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.29916
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.07920
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.32449
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.46210
Current AP: 0.18378 AP goal: 0.23000
:::MLL 1575525741.385 eval_accuracy: {"value": 0.18377625060120895, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 389}}
:::MLL 1575525741.386 eval_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 392}}
:::MLL 1575525741.438 block_stop: {"value": null, "metadata": {"first_epoch_num": 33, "file": "train.py", "lineno": 804}}
:::MLL 1575525741.438 block_start: {"value": null, "metadata": {"first_epoch_num": 44, "epoch_count": 5.457677417154162, "file": "train.py", "lineno": 813}}
Iteration:   5720, Loss function: 4.058, Average Loss: 3.845, avg. samples / sec: 1985.12
Iteration:   5740, Loss function: 3.273, Average Loss: 3.840, avg. samples / sec: 8379.13
:::MLL 1575525746.161 epoch_stop: {"value": null, "metadata": {"epoch_num": 44, "file": "train.py", "lineno": 819}}
:::MLL 1575525746.162 epoch_start: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 673}}
Iteration:   5760, Loss function: 3.409, Average Loss: 3.834, avg. samples / sec: 8340.09
Iteration:   5780, Loss function: 3.383, Average Loss: 3.827, avg. samples / sec: 8369.13
Iteration:   5800, Loss function: 3.682, Average Loss: 3.821, avg. samples / sec: 8371.27
Iteration:   5820, Loss function: 3.203, Average Loss: 3.812, avg. samples / sec: 8360.79
Iteration:   5840, Loss function: 3.492, Average Loss: 3.804, avg. samples / sec: 8384.69
Iteration:   5860, Loss function: 3.349, Average Loss: 3.796, avg. samples / sec: 8378.94
Iteration:   5880, Loss function: 3.596, Average Loss: 3.787, avg. samples / sec: 8323.52
:::MLL 1575525760.206 epoch_stop: {"value": null, "metadata": {"epoch_num": 45, "file": "train.py", "lineno": 819}}
:::MLL 1575525760.206 epoch_start: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 673}}
Iteration:   5900, Loss function: 3.388, Average Loss: 3.778, avg. samples / sec: 8310.04
Iteration:   5920, Loss function: 3.221, Average Loss: 3.770, avg. samples / sec: 8346.76
Iteration:   5940, Loss function: 3.039, Average Loss: 3.762, avg. samples / sec: 8377.96
Iteration:   5960, Loss function: 3.154, Average Loss: 3.754, avg. samples / sec: 8359.40
Iteration:   5980, Loss function: 3.465, Average Loss: 3.747, avg. samples / sec: 8303.49
Iteration:   6000, Loss function: 3.329, Average Loss: 3.738, avg. samples / sec: 8382.48
Iteration:   6020, Loss function: 3.304, Average Loss: 3.731, avg. samples / sec: 8330.44
:::MLL 1575525774.272 epoch_stop: {"value": null, "metadata": {"epoch_num": 46, "file": "train.py", "lineno": 819}}
:::MLL 1575525774.272 epoch_start: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 673}}
Iteration:   6040, Loss function: 3.548, Average Loss: 3.721, avg. samples / sec: 8357.22
Iteration:   6060, Loss function: 3.639, Average Loss: 3.716, avg. samples / sec: 8368.53
Iteration:   6080, Loss function: 3.598, Average Loss: 3.708, avg. samples / sec: 8337.72
Iteration:   6100, Loss function: 3.548, Average Loss: 3.702, avg. samples / sec: 8352.61
Iteration:   6120, Loss function: 3.861, Average Loss: 3.696, avg. samples / sec: 8354.53
Iteration:   6140, Loss function: 3.406, Average Loss: 3.689, avg. samples / sec: 8371.52
:::MLL 1575525788.319 epoch_stop: {"value": null, "metadata": {"epoch_num": 47, "file": "train.py", "lineno": 819}}
:::MLL 1575525788.319 epoch_start: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 673}}
Iteration:   6160, Loss function: 3.473, Average Loss: 3.682, avg. samples / sec: 8326.45
Iteration:   6180, Loss function: 3.308, Average Loss: 3.673, avg. samples / sec: 8367.36
Iteration:   6200, Loss function: 2.990, Average Loss: 3.665, avg. samples / sec: 8363.29
Iteration:   6220, Loss function: 3.364, Average Loss: 3.659, avg. samples / sec: 8341.16
Iteration:   6240, Loss function: 3.308, Average Loss: 3.653, avg. samples / sec: 8338.91
Iteration:   6260, Loss function: 3.557, Average Loss: 3.646, avg. samples / sec: 8354.40
Iteration:   6280, Loss function: 3.508, Average Loss: 3.641, avg. samples / sec: 8360.28
:::MLL 1575525802.374 epoch_stop: {"value": null, "metadata": {"epoch_num": 48, "file": "train.py", "lineno": 819}}
:::MLL 1575525802.374 epoch_start: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 673}}
Iteration:   6300, Loss function: 2.768, Average Loss: 3.633, avg. samples / sec: 8320.08
Iteration:   6320, Loss function: 3.390, Average Loss: 3.626, avg. samples / sec: 8356.34
Iteration:   6340, Loss function: 3.152, Average Loss: 3.619, avg. samples / sec: 8364.65
Iteration:   6360, Loss function: 3.073, Average Loss: 3.615, avg. samples / sec: 8367.33
Iteration:   6380, Loss function: 3.067, Average Loss: 3.608, avg. samples / sec: 8352.38
Iteration:   6400, Loss function: 3.386, Average Loss: 3.601, avg. samples / sec: 8355.18
:::MLL 1575525816.321 epoch_stop: {"value": null, "metadata": {"epoch_num": 49, "file": "train.py", "lineno": 819}}
:::MLL 1575525816.321 epoch_start: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 673}}
Iteration:   6420, Loss function: 3.399, Average Loss: 3.596, avg. samples / sec: 8335.73
:::MLL 1575525818.043 eval_start: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 276}}
Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 0/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Parsing batch: 1/2Predicting Ended, total time: 3.64 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
DONE (t=0.52s)
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.53s)
DONE (t=0.55s)
DONE (t=2.78s).
Accumulating evaluation results...
DONE (t=0.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.23094
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.39539
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.23564
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.05964
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24517
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37778
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.22333
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.32551
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.34148
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.10236
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36976
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.53396
Current AP: 0.23094 AP goal: 0.23000
:::MLL 1575525825.049 eval_accuracy: {"value": 0.23094495238678095, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 389}}
:::MLL 1575525825.102 eval_stop: {"value": null, "metadata": {"epoch_num": 50, "file": "train.py", "lineno": 392}}
:::MLL 1575525825.154 block_stop: {"value": null, "metadata": {"first_epoch_num": 44, "file": "train.py", "lineno": 804}}
:::MLL 1575525826.166 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 849}}

+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-12-05 06:03:53 AM
RESULT,SINGLE_STAGE_DETECTOR,,756,nvidia,2019-12-05 05:51:17 AM

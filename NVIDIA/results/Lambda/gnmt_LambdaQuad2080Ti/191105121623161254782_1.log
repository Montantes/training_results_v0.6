Beginning trial 1 of 1
Gathering sys log on lambda-quad
:::MLL 1572985053.730 submission_benchmark: {"value": "gnmt", "metadata": {"file": "mlperf_log_utils.py", "lineno": 176}}
:::MLL 1572985053.731 submission_org: {"value": "NVIDIA", "metadata": {"file": "mlperf_log_utils.py", "lineno": 181}}
WARNING: Log validation: Key "submission_division" is not in known gnmt keys.
:::MLL 1572985053.732 submission_division: {"value": "closed", "metadata": {"file": "mlperf_log_utils.py", "lineno": 185}}
:::MLL 1572985053.732 submission_status: {"value": "onprem", "metadata": {"file": "mlperf_log_utils.py", "lineno": 189}}
:::MLL 1572985053.733 submission_platform: {"value": "1xSystem Product Name", "metadata": {"file": "mlperf_log_utils.py", "lineno": 193}}
:::MLL 1572985053.734 submission_entry: {"value": "{'hardware': 'System Product Name', 'framework': 'PyTorch NVIDIA Release 19.05', 'power': 'N/A', 'notes': 'N/A', 'interconnect': ' ', 'os': 'Ubuntu 18.04.3 LTS / ', 'libraries': \"{'container_base': 'Ubuntu-16.04', 'openmpi_version': '3.1.3', 'mofed_version': '4.7-1.0.0', 'cuda_version': '10.1.163', 'cuda_driver_version': '418.67', 'nccl_version': '2.4.6', 'cudnn_version': '7.6.0.64', 'cublas_version': '10.2.0.163', 'trt_version': '5.1.5.0', 'dali_version': '0.9.1'}\", 'compilers': 'gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609', 'nodes': \"{'num_nodes': '1', 'cpu': '1x Intel(R) Core(TM) i9-9820X CPU @ 3.30GHz', 'num_cores': '10', 'num_vcpus': '20', 'accelerator': 'GeForce RTX 2080 Ti', 'num_accelerators': '4', 'sys_mem_size': '62 GB', 'sys_storage_type': '<unknown bus> SSD', 'sys_storage_size': '1x 2.3M + 1x 1.8T + 1x 956K + 1x 14.8M + 1x 44.2M + 1x 4.2M + 1x 14.5M + 2x 3.7M + 2x 54.5M + 1x 156M + 1x 91M + 1x 34.6M + 2x 140.7M + 1x 13M + 1x 89.1M', 'cpu_accel_interconnect': 'UPI', 'network_card': '', 'num_network_cards': '0', 'notes': ''}\"}", "metadata": {"file": "mlperf_log_utils.py", "lineno": 197}}
:::MLL 1572985053.735 submission_poc_name: {"value": "Paulius Micikevicius", "metadata": {"file": "mlperf_log_utils.py", "lineno": 201}}
:::MLL 1572985053.736 submission_poc_email: {"value": "pauliusm@nvidia.com", "metadata": {"file": "mlperf_log_utils.py", "lineno": 205}}
Clearing caches
:::MLL 1572985054.751 cache_clear: {"value": true, "metadata": {"file": "<string>", "lineno": 1}}
Launching on node lambda-quad
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=LambdaQuad2080Ti -e 'MULTI_NODE= --master_port=5020' -e LR=2.0e-3 -e TRAIN_BATCH_SIZE=256 -e TEST_BATCH_SIZE=128 -e WARMUP_STEPS=200 -e REMAIN_STEPS=6453 -e DECAY_INTERVAL=809 -e TARGET=24.0 -e NUMEPOCHS=8 -e MAX_SEQ_LEN=75 -e 'EXTRA_OPTS=   --fused-attention    --fused-xentropy    --no-log-all-ranks    ' -e SLURM_JOB_ID=191105121623161254782 -e SLURM_NTASKS_PER_NODE= -e SLURM_NNODES=1 cont_191105121623161254782 ./run_and_time.sh
Run vars: id 191105121623161254782 gpus 4 mparams  --master_port=5020
STARTING TIMING RUN AT 2019-11-05 08:17:35 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 1  --ncores_per_socket 10 --nproc_per_node 4  --master_port=5020'
+ echo 'running benchmark'
running benchmark
+ python -m bind_launch --nsockets_per_node 1 --ncores_per_socket 10 --nproc_per_node 4 --master_port=5020 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1572985057.128 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1572985057.132 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1572985057.133 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1572985057.149 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 367134696
0: Worker 0 is using worker seed: 3508004280
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1572985065.707 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1572985066.794 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1572985066.795 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1572985066.795 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1572985067.103 global_batch_size: {"value": 1024, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1572985067.106 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1572985067.107 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1572985067.107 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1572985067.107 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1572985067.107 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1572985067.107 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1572985067.108 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1572985067.108 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1572985067.108 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 1422642089
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/3880]	Time 0.750 (0.750)	Data 4.02e-01 (4.02e-01)	Tok/s 22591 (22591)	Loss/tok 10.6826 (10.6826)	LR 2.000e-05
0: TRAIN [0][10/3880]	Time 0.266 (0.379)	Data 1.04e-04 (3.66e-02)	Tok/s 39186 (41698)	Loss/tok 9.6874 (10.1864)	LR 2.518e-05
0: TRAIN [0][20/3880]	Time 0.353 (0.359)	Data 1.03e-04 (1.92e-02)	Tok/s 47436 (43239)	Loss/tok 9.4018 (9.8732)	LR 3.170e-05
0: TRAIN [0][30/3880]	Time 0.266 (0.335)	Data 1.08e-04 (1.31e-02)	Tok/s 39101 (42304)	Loss/tok 9.0222 (9.6843)	LR 3.991e-05
0: TRAIN [0][40/3880]	Time 0.272 (0.336)	Data 1.05e-04 (9.90e-03)	Tok/s 37933 (42751)	Loss/tok 8.7989 (9.5109)	LR 5.024e-05
0: TRAIN [0][50/3880]	Time 0.187 (0.336)	Data 1.12e-04 (7.98e-03)	Tok/s 27967 (42762)	Loss/tok 8.3569 (9.3590)	LR 6.325e-05
0: TRAIN [0][60/3880]	Time 0.271 (0.330)	Data 1.05e-04 (6.69e-03)	Tok/s 37438 (42254)	Loss/tok 8.3429 (9.2342)	LR 7.962e-05
0: TRAIN [0][70/3880]	Time 0.190 (0.324)	Data 1.07e-04 (5.76e-03)	Tok/s 28757 (41826)	Loss/tok 7.6783 (9.1171)	LR 1.002e-04
0: TRAIN [0][80/3880]	Time 0.272 (0.322)	Data 1.06e-04 (5.06e-03)	Tok/s 37737 (41740)	Loss/tok 7.9187 (9.0049)	LR 1.262e-04
0: TRAIN [0][90/3880]	Time 0.447 (0.323)	Data 1.03e-04 (4.52e-03)	Tok/s 51683 (41919)	Loss/tok 8.1755 (8.8926)	LR 1.589e-04
0: TRAIN [0][100/3880]	Time 0.274 (0.326)	Data 1.09e-04 (4.08e-03)	Tok/s 36960 (42061)	Loss/tok 7.8178 (8.7936)	LR 2.000e-04
0: TRAIN [0][110/3880]	Time 0.280 (0.331)	Data 1.20e-04 (3.72e-03)	Tok/s 36895 (42303)	Loss/tok 7.8134 (8.7038)	LR 2.518e-04
0: TRAIN [0][120/3880]	Time 0.468 (0.335)	Data 1.09e-04 (3.42e-03)	Tok/s 50295 (42365)	Loss/tok 8.1240 (8.6374)	LR 3.170e-04
0: TRAIN [0][130/3880]	Time 0.467 (0.332)	Data 1.02e-04 (3.17e-03)	Tok/s 49724 (42018)	Loss/tok 8.0431 (8.5891)	LR 3.991e-04
0: TRAIN [0][140/3880]	Time 0.289 (0.330)	Data 1.05e-04 (2.95e-03)	Tok/s 35022 (41735)	Loss/tok 7.5600 (8.5400)	LR 5.024e-04
0: TRAIN [0][150/3880]	Time 0.390 (0.330)	Data 1.05e-04 (2.77e-03)	Tok/s 43344 (41497)	Loss/tok 7.8146 (8.4936)	LR 6.325e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][160/3880]	Time 0.287 (0.329)	Data 1.08e-04 (2.60e-03)	Tok/s 36187 (41234)	Loss/tok 7.4382 (8.4494)	LR 7.781e-04
0: TRAIN [0][170/3880]	Time 0.371 (0.329)	Data 1.05e-04 (2.45e-03)	Tok/s 45490 (41084)	Loss/tok 7.6311 (8.4056)	LR 9.796e-04
0: TRAIN [0][180/3880]	Time 0.502 (0.329)	Data 1.08e-04 (2.32e-03)	Tok/s 46555 (40866)	Loss/tok 7.7538 (8.3616)	LR 1.233e-03
0: TRAIN [0][190/3880]	Time 0.386 (0.331)	Data 1.08e-04 (2.21e-03)	Tok/s 42845 (40843)	Loss/tok 7.5139 (8.3126)	LR 1.552e-03
0: TRAIN [0][200/3880]	Time 0.398 (0.332)	Data 1.04e-04 (2.10e-03)	Tok/s 42015 (40681)	Loss/tok 7.3169 (8.2645)	LR 1.954e-03
0: TRAIN [0][210/3880]	Time 0.298 (0.332)	Data 1.07e-04 (2.01e-03)	Tok/s 34944 (40603)	Loss/tok 6.9403 (8.2118)	LR 2.000e-03
0: TRAIN [0][220/3880]	Time 0.301 (0.333)	Data 1.02e-04 (1.92e-03)	Tok/s 35335 (40429)	Loss/tok 6.7647 (8.1565)	LR 2.000e-03
0: TRAIN [0][230/3880]	Time 0.395 (0.334)	Data 1.08e-04 (1.84e-03)	Tok/s 42511 (40249)	Loss/tok 6.8618 (8.1021)	LR 2.000e-03
0: TRAIN [0][240/3880]	Time 0.299 (0.336)	Data 1.09e-04 (1.77e-03)	Tok/s 34800 (40219)	Loss/tok 6.4558 (8.0396)	LR 2.000e-03
0: TRAIN [0][250/3880]	Time 0.501 (0.336)	Data 1.07e-04 (1.71e-03)	Tok/s 46380 (40102)	Loss/tok 6.8008 (7.9847)	LR 2.000e-03
0: TRAIN [0][260/3880]	Time 0.303 (0.335)	Data 1.08e-04 (1.64e-03)	Tok/s 34302 (39883)	Loss/tok 6.2823 (7.9358)	LR 2.000e-03
0: TRAIN [0][270/3880]	Time 0.302 (0.337)	Data 1.08e-04 (1.59e-03)	Tok/s 34092 (39842)	Loss/tok 6.1237 (7.8744)	LR 2.000e-03
0: TRAIN [0][280/3880]	Time 0.205 (0.338)	Data 1.06e-04 (1.54e-03)	Tok/s 25612 (39608)	Loss/tok 5.2373 (7.8234)	LR 2.000e-03
0: TRAIN [0][290/3880]	Time 0.301 (0.338)	Data 1.10e-04 (1.49e-03)	Tok/s 33654 (39541)	Loss/tok 5.9517 (7.7697)	LR 2.000e-03
0: TRAIN [0][300/3880]	Time 0.669 (0.339)	Data 1.10e-04 (1.44e-03)	Tok/s 34559 (39360)	Loss/tok 6.2959 (7.7196)	LR 2.000e-03
0: TRAIN [0][310/3880]	Time 0.301 (0.342)	Data 1.12e-04 (1.40e-03)	Tok/s 34066 (39340)	Loss/tok 5.8020 (7.6581)	LR 2.000e-03
0: TRAIN [0][320/3880]	Time 0.204 (0.342)	Data 1.03e-04 (1.36e-03)	Tok/s 25880 (39238)	Loss/tok 4.8392 (7.6084)	LR 2.000e-03
0: TRAIN [0][330/3880]	Time 0.299 (0.342)	Data 1.03e-04 (1.32e-03)	Tok/s 34285 (39179)	Loss/tok 5.5454 (7.5575)	LR 2.000e-03
0: TRAIN [0][340/3880]	Time 0.388 (0.345)	Data 1.05e-04 (1.28e-03)	Tok/s 43338 (39186)	Loss/tok 5.8619 (7.4985)	LR 2.000e-03
0: TRAIN [0][350/3880]	Time 0.631 (0.348)	Data 1.09e-04 (1.25e-03)	Tok/s 36771 (39112)	Loss/tok 5.9960 (7.4427)	LR 2.000e-03
0: TRAIN [0][360/3880]	Time 0.520 (0.350)	Data 1.03e-04 (1.22e-03)	Tok/s 44864 (38994)	Loss/tok 5.9885 (7.3919)	LR 2.000e-03
0: TRAIN [0][370/3880]	Time 0.212 (0.348)	Data 1.05e-04 (1.19e-03)	Tok/s 24578 (38804)	Loss/tok 4.5227 (7.3564)	LR 2.000e-03
0: TRAIN [0][380/3880]	Time 0.301 (0.349)	Data 1.17e-04 (1.16e-03)	Tok/s 33472 (38764)	Loss/tok 5.3422 (7.3071)	LR 2.000e-03
0: TRAIN [0][390/3880]	Time 0.512 (0.350)	Data 1.08e-04 (1.13e-03)	Tok/s 45831 (38734)	Loss/tok 5.5431 (7.2559)	LR 2.000e-03
0: TRAIN [0][400/3880]	Time 0.395 (0.351)	Data 1.07e-04 (1.11e-03)	Tok/s 42752 (38726)	Loss/tok 5.5948 (7.2085)	LR 2.000e-03
0: TRAIN [0][410/3880]	Time 0.325 (0.354)	Data 1.12e-04 (1.08e-03)	Tok/s 32053 (38589)	Loss/tok 5.1165 (7.1570)	LR 2.000e-03
0: TRAIN [0][420/3880]	Time 0.311 (0.354)	Data 1.07e-04 (1.06e-03)	Tok/s 32535 (38544)	Loss/tok 5.1149 (7.1141)	LR 2.000e-03
0: TRAIN [0][430/3880]	Time 0.208 (0.355)	Data 1.07e-04 (1.04e-03)	Tok/s 25888 (38556)	Loss/tok 4.0958 (7.0637)	LR 2.000e-03
0: TRAIN [0][440/3880]	Time 0.537 (0.358)	Data 1.08e-04 (1.02e-03)	Tok/s 31054 (38468)	Loss/tok 5.2495 (7.0147)	LR 2.000e-03
0: TRAIN [0][450/3880]	Time 0.341 (0.361)	Data 1.12e-04 (9.97e-04)	Tok/s 30122 (38331)	Loss/tok 4.8364 (6.9667)	LR 2.000e-03
0: TRAIN [0][460/3880]	Time 0.422 (0.361)	Data 1.10e-04 (9.78e-04)	Tok/s 40252 (38256)	Loss/tok 4.9851 (6.9239)	LR 2.000e-03
0: TRAIN [0][470/3880]	Time 0.204 (0.362)	Data 1.09e-04 (9.59e-04)	Tok/s 26216 (38215)	Loss/tok 3.8729 (6.8823)	LR 2.000e-03
0: TRAIN [0][480/3880]	Time 0.423 (0.362)	Data 1.03e-04 (9.42e-04)	Tok/s 39842 (38195)	Loss/tok 5.0099 (6.8435)	LR 2.000e-03
0: TRAIN [0][490/3880]	Time 0.645 (0.362)	Data 1.09e-04 (9.25e-04)	Tok/s 46524 (38198)	Loss/tok 5.1639 (6.8004)	LR 2.000e-03
0: TRAIN [0][500/3880]	Time 0.318 (0.363)	Data 1.08e-04 (9.08e-04)	Tok/s 33130 (38145)	Loss/tok 4.4717 (6.7604)	LR 2.000e-03
0: TRAIN [0][510/3880]	Time 0.303 (0.362)	Data 1.05e-04 (8.93e-04)	Tok/s 33732 (38087)	Loss/tok 4.5279 (6.7261)	LR 2.000e-03
0: TRAIN [0][520/3880]	Time 0.716 (0.362)	Data 1.06e-04 (8.78e-04)	Tok/s 41904 (37986)	Loss/tok 5.3978 (6.6944)	LR 2.000e-03
0: TRAIN [0][530/3880]	Time 0.299 (0.362)	Data 1.11e-04 (8.63e-04)	Tok/s 34505 (37957)	Loss/tok 4.4231 (6.6579)	LR 2.000e-03
0: TRAIN [0][540/3880]	Time 0.642 (0.362)	Data 1.04e-04 (8.49e-04)	Tok/s 46073 (37960)	Loss/tok 5.2263 (6.6204)	LR 2.000e-03
0: TRAIN [0][550/3880]	Time 0.407 (0.363)	Data 1.10e-04 (8.36e-04)	Tok/s 41621 (38011)	Loss/tok 4.6554 (6.5803)	LR 2.000e-03
0: TRAIN [0][560/3880]	Time 0.298 (0.362)	Data 1.03e-04 (8.23e-04)	Tok/s 33855 (37976)	Loss/tok 4.2679 (6.5473)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][570/3880]	Time 0.329 (0.364)	Data 1.08e-04 (8.10e-04)	Tok/s 30992 (37910)	Loss/tok 4.1946 (6.5086)	LR 2.000e-03
0: TRAIN [0][580/3880]	Time 0.407 (0.365)	Data 1.10e-04 (7.98e-04)	Tok/s 41309 (37884)	Loss/tok 4.6161 (6.4760)	LR 2.000e-03
0: TRAIN [0][590/3880]	Time 0.402 (0.365)	Data 1.09e-04 (7.87e-04)	Tok/s 41891 (37930)	Loss/tok 4.6087 (6.4397)	LR 2.000e-03
0: TRAIN [0][600/3880]	Time 0.285 (0.365)	Data 1.11e-04 (7.75e-04)	Tok/s 36574 (37882)	Loss/tok 4.2217 (6.4111)	LR 2.000e-03
0: TRAIN [0][610/3880]	Time 0.527 (0.365)	Data 1.11e-04 (7.64e-04)	Tok/s 44236 (37879)	Loss/tok 4.6782 (6.3792)	LR 2.000e-03
0: TRAIN [0][620/3880]	Time 0.512 (0.366)	Data 1.08e-04 (7.54e-04)	Tok/s 46685 (37913)	Loss/tok 4.6659 (6.3448)	LR 2.000e-03
0: TRAIN [0][630/3880]	Time 0.564 (0.367)	Data 1.07e-04 (7.44e-04)	Tok/s 41518 (37924)	Loss/tok 4.7485 (6.3111)	LR 2.000e-03
0: TRAIN [0][640/3880]	Time 0.298 (0.367)	Data 1.08e-04 (7.34e-04)	Tok/s 34982 (37904)	Loss/tok 4.1439 (6.2810)	LR 2.000e-03
0: TRAIN [0][650/3880]	Time 0.490 (0.368)	Data 1.07e-04 (7.24e-04)	Tok/s 47386 (37941)	Loss/tok 4.6267 (6.2478)	LR 2.000e-03
0: TRAIN [0][660/3880]	Time 0.211 (0.369)	Data 1.08e-04 (7.15e-04)	Tok/s 25362 (37867)	Loss/tok 3.6001 (6.2198)	LR 2.000e-03
0: TRAIN [0][670/3880]	Time 0.297 (0.369)	Data 1.10e-04 (7.06e-04)	Tok/s 34934 (37892)	Loss/tok 4.1568 (6.1875)	LR 2.000e-03
0: TRAIN [0][680/3880]	Time 0.402 (0.369)	Data 1.08e-04 (6.97e-04)	Tok/s 41991 (37883)	Loss/tok 4.3080 (6.1614)	LR 2.000e-03
0: TRAIN [0][690/3880]	Time 0.224 (0.369)	Data 1.08e-04 (6.88e-04)	Tok/s 24082 (37845)	Loss/tok 3.3855 (6.1361)	LR 2.000e-03
0: TRAIN [0][700/3880]	Time 0.316 (0.370)	Data 2.44e-04 (6.80e-04)	Tok/s 33200 (37802)	Loss/tok 3.9493 (6.1095)	LR 2.000e-03
0: TRAIN [0][710/3880]	Time 0.293 (0.370)	Data 1.11e-04 (6.72e-04)	Tok/s 34809 (37805)	Loss/tok 4.0434 (6.0839)	LR 2.000e-03
0: TRAIN [0][720/3880]	Time 0.294 (0.369)	Data 1.08e-04 (6.64e-04)	Tok/s 35020 (37775)	Loss/tok 3.9797 (6.0613)	LR 2.000e-03
0: TRAIN [0][730/3880]	Time 0.210 (0.369)	Data 1.08e-04 (6.57e-04)	Tok/s 25289 (37704)	Loss/tok 3.4110 (6.0396)	LR 2.000e-03
0: TRAIN [0][740/3880]	Time 0.533 (0.369)	Data 1.06e-04 (6.49e-04)	Tok/s 43636 (37670)	Loss/tok 4.6306 (6.0181)	LR 2.000e-03
0: TRAIN [0][750/3880]	Time 0.204 (0.368)	Data 1.12e-04 (6.42e-04)	Tok/s 25204 (37657)	Loss/tok 3.3269 (5.9954)	LR 2.000e-03
0: TRAIN [0][760/3880]	Time 0.770 (0.369)	Data 1.08e-04 (6.35e-04)	Tok/s 38919 (37664)	Loss/tok 4.6102 (5.9697)	LR 2.000e-03
0: TRAIN [0][770/3880]	Time 0.522 (0.370)	Data 1.13e-04 (6.28e-04)	Tok/s 44695 (37684)	Loss/tok 4.4866 (5.9431)	LR 2.000e-03
0: TRAIN [0][780/3880]	Time 0.299 (0.370)	Data 1.07e-04 (6.22e-04)	Tok/s 34753 (37667)	Loss/tok 3.9545 (5.9226)	LR 2.000e-03
0: TRAIN [0][790/3880]	Time 0.360 (0.370)	Data 1.39e-04 (6.15e-04)	Tok/s 29492 (37641)	Loss/tok 3.8672 (5.9000)	LR 2.000e-03
0: TRAIN [0][800/3880]	Time 0.304 (0.371)	Data 1.08e-04 (6.09e-04)	Tok/s 34555 (37596)	Loss/tok 3.9379 (5.8795)	LR 2.000e-03
0: TRAIN [0][810/3880]	Time 0.207 (0.370)	Data 1.10e-04 (6.03e-04)	Tok/s 25093 (37556)	Loss/tok 3.2786 (5.8616)	LR 2.000e-03
0: TRAIN [0][820/3880]	Time 0.297 (0.370)	Data 1.10e-04 (5.97e-04)	Tok/s 34950 (37574)	Loss/tok 4.0884 (5.8408)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][830/3880]	Time 0.652 (0.370)	Data 1.08e-04 (5.91e-04)	Tok/s 45979 (37559)	Loss/tok 4.5464 (5.8212)	LR 2.000e-03
0: TRAIN [0][840/3880]	Time 0.299 (0.370)	Data 1.06e-04 (5.85e-04)	Tok/s 34421 (37564)	Loss/tok 3.9634 (5.8013)	LR 2.000e-03
0: TRAIN [0][850/3880]	Time 0.314 (0.370)	Data 1.09e-04 (5.80e-04)	Tok/s 32089 (37560)	Loss/tok 3.9166 (5.7815)	LR 2.000e-03
0: TRAIN [0][860/3880]	Time 0.537 (0.371)	Data 1.08e-04 (5.74e-04)	Tok/s 43559 (37567)	Loss/tok 4.4668 (5.7596)	LR 2.000e-03
0: TRAIN [0][870/3880]	Time 0.407 (0.371)	Data 1.20e-04 (5.69e-04)	Tok/s 41023 (37542)	Loss/tok 4.2098 (5.7417)	LR 2.000e-03
0: TRAIN [0][880/3880]	Time 0.414 (0.371)	Data 1.24e-04 (5.64e-04)	Tok/s 40061 (37559)	Loss/tok 4.1929 (5.7227)	LR 2.000e-03
0: TRAIN [0][890/3880]	Time 0.204 (0.371)	Data 1.10e-04 (5.59e-04)	Tok/s 25874 (37554)	Loss/tok 3.3181 (5.7049)	LR 2.000e-03
0: TRAIN [0][900/3880]	Time 0.887 (0.372)	Data 1.14e-04 (5.53e-04)	Tok/s 33762 (37518)	Loss/tok 4.5772 (5.6875)	LR 2.000e-03
0: TRAIN [0][910/3880]	Time 0.306 (0.371)	Data 1.12e-04 (5.49e-04)	Tok/s 33655 (37471)	Loss/tok 3.7041 (5.6715)	LR 2.000e-03
0: TRAIN [0][920/3880]	Time 0.631 (0.372)	Data 1.03e-04 (5.44e-04)	Tok/s 47323 (37516)	Loss/tok 4.5762 (5.6510)	LR 2.000e-03
0: TRAIN [0][930/3880]	Time 0.316 (0.372)	Data 1.09e-04 (5.39e-04)	Tok/s 32718 (37492)	Loss/tok 3.9706 (5.6360)	LR 2.000e-03
0: TRAIN [0][940/3880]	Time 0.221 (0.372)	Data 1.03e-04 (5.35e-04)	Tok/s 24093 (37481)	Loss/tok 3.3020 (5.6197)	LR 2.000e-03
0: TRAIN [0][950/3880]	Time 0.396 (0.372)	Data 1.11e-04 (5.30e-04)	Tok/s 43017 (37467)	Loss/tok 4.0271 (5.6028)	LR 2.000e-03
0: TRAIN [0][960/3880]	Time 0.329 (0.372)	Data 1.05e-04 (5.26e-04)	Tok/s 30336 (37423)	Loss/tok 3.6942 (5.5864)	LR 2.000e-03
0: TRAIN [0][970/3880]	Time 0.296 (0.372)	Data 1.07e-04 (5.21e-04)	Tok/s 34152 (37413)	Loss/tok 3.7956 (5.5706)	LR 2.000e-03
0: TRAIN [0][980/3880]	Time 0.311 (0.372)	Data 1.14e-04 (5.17e-04)	Tok/s 33700 (37428)	Loss/tok 3.8558 (5.5544)	LR 2.000e-03
0: TRAIN [0][990/3880]	Time 0.303 (0.372)	Data 1.06e-04 (5.13e-04)	Tok/s 34121 (37431)	Loss/tok 3.7060 (5.5387)	LR 2.000e-03
0: TRAIN [0][1000/3880]	Time 0.208 (0.372)	Data 1.09e-04 (5.09e-04)	Tok/s 26196 (37402)	Loss/tok 3.3214 (5.5256)	LR 2.000e-03
0: TRAIN [0][1010/3880]	Time 0.302 (0.372)	Data 1.07e-04 (5.05e-04)	Tok/s 33783 (37395)	Loss/tok 3.8089 (5.5107)	LR 2.000e-03
0: TRAIN [0][1020/3880]	Time 0.298 (0.372)	Data 1.07e-04 (5.01e-04)	Tok/s 35055 (37405)	Loss/tok 3.6713 (5.4961)	LR 2.000e-03
0: TRAIN [0][1030/3880]	Time 0.304 (0.372)	Data 1.08e-04 (4.98e-04)	Tok/s 34239 (37415)	Loss/tok 3.7520 (5.4808)	LR 2.000e-03
0: TRAIN [0][1040/3880]	Time 0.302 (0.372)	Data 1.08e-04 (4.94e-04)	Tok/s 33635 (37399)	Loss/tok 3.7472 (5.4678)	LR 2.000e-03
0: TRAIN [0][1050/3880]	Time 0.296 (0.371)	Data 1.01e-04 (4.90e-04)	Tok/s 34793 (37385)	Loss/tok 3.6354 (5.4551)	LR 2.000e-03
0: TRAIN [0][1060/3880]	Time 0.554 (0.372)	Data 1.14e-04 (4.87e-04)	Tok/s 41183 (37359)	Loss/tok 4.2015 (5.4396)	LR 2.000e-03
0: TRAIN [0][1070/3880]	Time 0.299 (0.373)	Data 1.09e-04 (4.83e-04)	Tok/s 34597 (37387)	Loss/tok 3.7483 (5.4241)	LR 2.000e-03
0: TRAIN [0][1080/3880]	Time 0.403 (0.373)	Data 1.08e-04 (4.80e-04)	Tok/s 41573 (37407)	Loss/tok 3.9853 (5.4100)	LR 2.000e-03
0: TRAIN [0][1090/3880]	Time 0.430 (0.373)	Data 1.10e-04 (4.76e-04)	Tok/s 39598 (37430)	Loss/tok 3.9614 (5.3960)	LR 2.000e-03
0: TRAIN [0][1100/3880]	Time 0.652 (0.373)	Data 1.05e-04 (4.73e-04)	Tok/s 45615 (37422)	Loss/tok 4.5553 (5.3839)	LR 2.000e-03
0: TRAIN [0][1110/3880]	Time 0.430 (0.372)	Data 1.06e-04 (4.70e-04)	Tok/s 39894 (37394)	Loss/tok 4.0351 (5.3728)	LR 2.000e-03
0: TRAIN [0][1120/3880]	Time 0.672 (0.373)	Data 1.09e-04 (4.66e-04)	Tok/s 43951 (37377)	Loss/tok 4.4901 (5.3605)	LR 2.000e-03
0: TRAIN [0][1130/3880]	Time 0.534 (0.373)	Data 1.07e-04 (4.63e-04)	Tok/s 43927 (37387)	Loss/tok 4.2381 (5.3479)	LR 2.000e-03
0: TRAIN [0][1140/3880]	Time 0.208 (0.373)	Data 1.13e-04 (4.60e-04)	Tok/s 25797 (37402)	Loss/tok 3.1887 (5.3349)	LR 2.000e-03
0: TRAIN [0][1150/3880]	Time 0.208 (0.373)	Data 1.06e-04 (4.57e-04)	Tok/s 24425 (37382)	Loss/tok 3.0757 (5.3232)	LR 2.000e-03
0: TRAIN [0][1160/3880]	Time 0.301 (0.373)	Data 1.10e-04 (4.54e-04)	Tok/s 34144 (37393)	Loss/tok 3.6131 (5.3105)	LR 2.000e-03
0: TRAIN [0][1170/3880]	Time 0.318 (0.373)	Data 1.08e-04 (4.51e-04)	Tok/s 32587 (37364)	Loss/tok 3.7686 (5.2994)	LR 2.000e-03
0: TRAIN [0][1180/3880]	Time 0.570 (0.373)	Data 1.07e-04 (4.48e-04)	Tok/s 40643 (37369)	Loss/tok 4.1994 (5.2869)	LR 2.000e-03
0: TRAIN [0][1190/3880]	Time 0.289 (0.373)	Data 1.12e-04 (4.45e-04)	Tok/s 36120 (37361)	Loss/tok 3.6196 (5.2753)	LR 2.000e-03
0: TRAIN [0][1200/3880]	Time 0.300 (0.373)	Data 1.05e-04 (4.43e-04)	Tok/s 34867 (37366)	Loss/tok 3.6301 (5.2637)	LR 2.000e-03
0: TRAIN [0][1210/3880]	Time 0.288 (0.373)	Data 1.06e-04 (4.40e-04)	Tok/s 35631 (37364)	Loss/tok 3.5770 (5.2525)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1220/3880]	Time 0.686 (0.373)	Data 1.04e-04 (4.37e-04)	Tok/s 43752 (37364)	Loss/tok 4.2220 (5.2410)	LR 2.000e-03
0: TRAIN [0][1230/3880]	Time 0.335 (0.374)	Data 1.12e-04 (4.34e-04)	Tok/s 30805 (37370)	Loss/tok 3.6924 (5.2300)	LR 2.000e-03
0: TRAIN [0][1240/3880]	Time 0.454 (0.374)	Data 1.02e-04 (4.32e-04)	Tok/s 37113 (37386)	Loss/tok 3.7633 (5.2176)	LR 2.000e-03
0: TRAIN [0][1250/3880]	Time 0.315 (0.374)	Data 1.06e-04 (4.29e-04)	Tok/s 32360 (37355)	Loss/tok 3.6757 (5.2075)	LR 2.000e-03
0: TRAIN [0][1260/3880]	Time 0.289 (0.374)	Data 1.11e-04 (4.27e-04)	Tok/s 35452 (37348)	Loss/tok 3.5816 (5.1969)	LR 2.000e-03
0: TRAIN [0][1270/3880]	Time 0.299 (0.374)	Data 1.09e-04 (4.24e-04)	Tok/s 34820 (37350)	Loss/tok 3.6973 (5.1865)	LR 2.000e-03
0: TRAIN [0][1280/3880]	Time 0.400 (0.374)	Data 1.45e-04 (4.22e-04)	Tok/s 41286 (37340)	Loss/tok 3.8008 (5.1768)	LR 2.000e-03
0: TRAIN [0][1290/3880]	Time 0.667 (0.374)	Data 1.08e-04 (4.19e-04)	Tok/s 45002 (37367)	Loss/tok 4.2429 (5.1646)	LR 2.000e-03
0: TRAIN [0][1300/3880]	Time 0.343 (0.374)	Data 1.06e-04 (4.17e-04)	Tok/s 29985 (37319)	Loss/tok 3.6934 (5.1553)	LR 2.000e-03
0: TRAIN [0][1310/3880]	Time 0.505 (0.375)	Data 1.09e-04 (4.15e-04)	Tok/s 46095 (37335)	Loss/tok 4.1688 (5.1446)	LR 2.000e-03
0: TRAIN [0][1320/3880]	Time 0.401 (0.375)	Data 1.08e-04 (4.12e-04)	Tok/s 42359 (37341)	Loss/tok 3.8132 (5.1349)	LR 2.000e-03
0: TRAIN [0][1330/3880]	Time 0.403 (0.375)	Data 1.11e-04 (4.10e-04)	Tok/s 42010 (37334)	Loss/tok 3.8635 (5.1260)	LR 2.000e-03
0: TRAIN [0][1340/3880]	Time 0.298 (0.374)	Data 1.03e-04 (4.08e-04)	Tok/s 33703 (37333)	Loss/tok 3.5004 (5.1168)	LR 2.000e-03
0: TRAIN [0][1350/3880]	Time 0.646 (0.374)	Data 1.09e-04 (4.05e-04)	Tok/s 46007 (37339)	Loss/tok 4.2756 (5.1073)	LR 2.000e-03
0: TRAIN [0][1360/3880]	Time 0.542 (0.375)	Data 1.35e-04 (4.03e-04)	Tok/s 43335 (37332)	Loss/tok 4.0007 (5.0980)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1370/3880]	Time 0.526 (0.375)	Data 1.09e-04 (4.01e-04)	Tok/s 44652 (37335)	Loss/tok 4.0464 (5.0883)	LR 2.000e-03
0: TRAIN [0][1380/3880]	Time 0.411 (0.375)	Data 1.09e-04 (3.99e-04)	Tok/s 41457 (37338)	Loss/tok 3.8531 (5.0793)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1390/3880]	Time 0.298 (0.375)	Data 1.11e-04 (3.97e-04)	Tok/s 35144 (37339)	Loss/tok 3.6112 (5.0706)	LR 2.000e-03
0: TRAIN [0][1400/3880]	Time 0.421 (0.375)	Data 1.10e-04 (3.95e-04)	Tok/s 39970 (37342)	Loss/tok 3.7626 (5.0617)	LR 2.000e-03
0: TRAIN [0][1410/3880]	Time 0.208 (0.375)	Data 1.06e-04 (3.93e-04)	Tok/s 25132 (37322)	Loss/tok 3.0045 (5.0537)	LR 2.000e-03
0: TRAIN [0][1420/3880]	Time 0.320 (0.375)	Data 1.08e-04 (3.91e-04)	Tok/s 32875 (37322)	Loss/tok 3.5374 (5.0452)	LR 2.000e-03
0: TRAIN [0][1430/3880]	Time 0.308 (0.375)	Data 1.09e-04 (3.89e-04)	Tok/s 33504 (37310)	Loss/tok 3.4543 (5.0370)	LR 2.000e-03
0: TRAIN [0][1440/3880]	Time 0.290 (0.374)	Data 1.08e-04 (3.87e-04)	Tok/s 34486 (37306)	Loss/tok 3.5149 (5.0292)	LR 2.000e-03
0: TRAIN [0][1450/3880]	Time 0.428 (0.375)	Data 1.06e-04 (3.85e-04)	Tok/s 39365 (37287)	Loss/tok 3.8096 (5.0205)	LR 2.000e-03
0: TRAIN [0][1460/3880]	Time 0.421 (0.375)	Data 1.05e-04 (3.83e-04)	Tok/s 39766 (37284)	Loss/tok 3.8271 (5.0121)	LR 2.000e-03
0: TRAIN [0][1470/3880]	Time 0.338 (0.375)	Data 1.11e-04 (3.81e-04)	Tok/s 30287 (37273)	Loss/tok 3.5294 (5.0038)	LR 2.000e-03
0: TRAIN [0][1480/3880]	Time 0.293 (0.375)	Data 1.05e-04 (3.79e-04)	Tok/s 34805 (37272)	Loss/tok 3.6280 (4.9956)	LR 2.000e-03
0: TRAIN [0][1490/3880]	Time 0.294 (0.375)	Data 1.08e-04 (3.77e-04)	Tok/s 34706 (37277)	Loss/tok 3.6075 (4.9878)	LR 2.000e-03
0: TRAIN [0][1500/3880]	Time 0.309 (0.375)	Data 1.16e-04 (3.76e-04)	Tok/s 33467 (37257)	Loss/tok 3.4482 (4.9810)	LR 2.000e-03
0: TRAIN [0][1510/3880]	Time 0.405 (0.375)	Data 1.09e-04 (3.74e-04)	Tok/s 41585 (37269)	Loss/tok 3.7065 (4.9723)	LR 2.000e-03
0: TRAIN [0][1520/3880]	Time 0.292 (0.375)	Data 1.15e-04 (3.72e-04)	Tok/s 34781 (37274)	Loss/tok 3.5328 (4.9645)	LR 2.000e-03
0: TRAIN [0][1530/3880]	Time 0.421 (0.375)	Data 1.07e-04 (3.70e-04)	Tok/s 39444 (37253)	Loss/tok 3.8865 (4.9575)	LR 2.000e-03
0: TRAIN [0][1540/3880]	Time 0.298 (0.375)	Data 1.07e-04 (3.69e-04)	Tok/s 34003 (37255)	Loss/tok 3.5744 (4.9497)	LR 2.000e-03
0: TRAIN [0][1550/3880]	Time 0.416 (0.375)	Data 1.10e-04 (3.67e-04)	Tok/s 40332 (37268)	Loss/tok 3.6783 (4.9418)	LR 2.000e-03
0: TRAIN [0][1560/3880]	Time 0.298 (0.375)	Data 1.01e-04 (3.65e-04)	Tok/s 34931 (37276)	Loss/tok 3.4786 (4.9339)	LR 2.000e-03
0: TRAIN [0][1570/3880]	Time 0.304 (0.375)	Data 1.07e-04 (3.64e-04)	Tok/s 33832 (37290)	Loss/tok 3.5581 (4.9261)	LR 2.000e-03
0: TRAIN [0][1580/3880]	Time 0.469 (0.375)	Data 1.07e-04 (3.62e-04)	Tok/s 35758 (37281)	Loss/tok 3.7490 (4.9187)	LR 2.000e-03
0: TRAIN [0][1590/3880]	Time 0.295 (0.375)	Data 1.08e-04 (3.61e-04)	Tok/s 35367 (37262)	Loss/tok 3.5797 (4.9119)	LR 2.000e-03
0: TRAIN [0][1600/3880]	Time 0.527 (0.375)	Data 1.05e-04 (3.59e-04)	Tok/s 43955 (37273)	Loss/tok 3.9146 (4.9042)	LR 2.000e-03
0: TRAIN [0][1610/3880]	Time 0.334 (0.375)	Data 1.08e-04 (3.57e-04)	Tok/s 30475 (37251)	Loss/tok 3.4030 (4.8979)	LR 2.000e-03
0: TRAIN [0][1620/3880]	Time 0.320 (0.375)	Data 1.04e-04 (3.56e-04)	Tok/s 31979 (37245)	Loss/tok 3.4613 (4.8913)	LR 2.000e-03
0: TRAIN [0][1630/3880]	Time 0.657 (0.375)	Data 1.08e-04 (3.54e-04)	Tok/s 45831 (37233)	Loss/tok 4.1116 (4.8843)	LR 2.000e-03
0: TRAIN [0][1640/3880]	Time 0.296 (0.375)	Data 1.05e-04 (3.53e-04)	Tok/s 34552 (37206)	Loss/tok 3.5604 (4.8786)	LR 2.000e-03
0: TRAIN [0][1650/3880]	Time 0.292 (0.375)	Data 1.08e-04 (3.51e-04)	Tok/s 35213 (37226)	Loss/tok 3.6066 (4.8710)	LR 2.000e-03
0: TRAIN [0][1660/3880]	Time 0.302 (0.375)	Data 1.07e-04 (3.50e-04)	Tok/s 33980 (37234)	Loss/tok 3.5189 (4.8640)	LR 2.000e-03
0: TRAIN [0][1670/3880]	Time 0.309 (0.375)	Data 1.09e-04 (3.48e-04)	Tok/s 32650 (37238)	Loss/tok 3.4533 (4.8572)	LR 2.000e-03
0: TRAIN [0][1680/3880]	Time 0.291 (0.375)	Data 1.09e-04 (3.47e-04)	Tok/s 35635 (37241)	Loss/tok 3.4421 (4.8504)	LR 2.000e-03
0: TRAIN [0][1690/3880]	Time 0.394 (0.375)	Data 1.10e-04 (3.45e-04)	Tok/s 42400 (37251)	Loss/tok 3.6949 (4.8434)	LR 2.000e-03
0: TRAIN [0][1700/3880]	Time 0.313 (0.375)	Data 1.08e-04 (3.44e-04)	Tok/s 32843 (37234)	Loss/tok 3.6108 (4.8371)	LR 2.000e-03
0: TRAIN [0][1710/3880]	Time 0.400 (0.375)	Data 1.09e-04 (3.43e-04)	Tok/s 41933 (37233)	Loss/tok 3.7423 (4.8306)	LR 2.000e-03
0: TRAIN [0][1720/3880]	Time 0.298 (0.375)	Data 1.51e-04 (3.41e-04)	Tok/s 34991 (37241)	Loss/tok 3.4000 (4.8240)	LR 2.000e-03
0: TRAIN [0][1730/3880]	Time 0.474 (0.376)	Data 1.10e-04 (3.40e-04)	Tok/s 35455 (37238)	Loss/tok 3.7315 (4.8166)	LR 2.000e-03
0: TRAIN [0][1740/3880]	Time 0.301 (0.375)	Data 1.10e-04 (3.39e-04)	Tok/s 34579 (37222)	Loss/tok 3.4039 (4.8108)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1750/3880]	Time 0.206 (0.375)	Data 1.08e-04 (3.37e-04)	Tok/s 26229 (37215)	Loss/tok 2.9294 (4.8053)	LR 2.000e-03
0: TRAIN [0][1760/3880]	Time 0.310 (0.375)	Data 1.09e-04 (3.36e-04)	Tok/s 33078 (37228)	Loss/tok 3.4131 (4.7987)	LR 2.000e-03
0: TRAIN [0][1770/3880]	Time 0.291 (0.375)	Data 1.11e-04 (3.35e-04)	Tok/s 35205 (37230)	Loss/tok 3.4507 (4.7928)	LR 2.000e-03
0: TRAIN [0][1780/3880]	Time 0.654 (0.375)	Data 1.09e-04 (3.34e-04)	Tok/s 45454 (37223)	Loss/tok 4.1424 (4.7867)	LR 2.000e-03
0: TRAIN [0][1790/3880]	Time 0.310 (0.376)	Data 1.12e-04 (3.32e-04)	Tok/s 32528 (37225)	Loss/tok 3.5744 (4.7806)	LR 2.000e-03
0: TRAIN [0][1800/3880]	Time 0.300 (0.376)	Data 1.08e-04 (3.31e-04)	Tok/s 34481 (37229)	Loss/tok 3.4059 (4.7747)	LR 2.000e-03
0: TRAIN [0][1810/3880]	Time 0.341 (0.375)	Data 1.11e-04 (3.30e-04)	Tok/s 30627 (37214)	Loss/tok 3.4082 (4.7693)	LR 2.000e-03
0: TRAIN [0][1820/3880]	Time 0.434 (0.376)	Data 1.11e-04 (3.29e-04)	Tok/s 38275 (37200)	Loss/tok 3.8040 (4.7631)	LR 2.000e-03
0: TRAIN [0][1830/3880]	Time 0.299 (0.375)	Data 1.08e-04 (3.27e-04)	Tok/s 35263 (37187)	Loss/tok 3.4027 (4.7579)	LR 2.000e-03
0: TRAIN [0][1840/3880]	Time 0.292 (0.375)	Data 1.07e-04 (3.26e-04)	Tok/s 35214 (37186)	Loss/tok 3.4608 (4.7523)	LR 2.000e-03
0: TRAIN [0][1850/3880]	Time 0.387 (0.375)	Data 1.07e-04 (3.25e-04)	Tok/s 43498 (37199)	Loss/tok 3.7688 (4.7463)	LR 2.000e-03
0: TRAIN [0][1860/3880]	Time 0.546 (0.376)	Data 1.11e-04 (3.24e-04)	Tok/s 42296 (37195)	Loss/tok 3.9955 (4.7402)	LR 2.000e-03
0: TRAIN [0][1870/3880]	Time 0.299 (0.376)	Data 1.00e-04 (3.23e-04)	Tok/s 33935 (37195)	Loss/tok 3.6440 (4.7348)	LR 2.000e-03
0: TRAIN [0][1880/3880]	Time 0.518 (0.376)	Data 1.07e-04 (3.22e-04)	Tok/s 45286 (37211)	Loss/tok 3.9699 (4.7289)	LR 2.000e-03
0: TRAIN [0][1890/3880]	Time 0.310 (0.376)	Data 1.11e-04 (3.20e-04)	Tok/s 32889 (37199)	Loss/tok 3.5031 (4.7238)	LR 2.000e-03
0: TRAIN [0][1900/3880]	Time 0.507 (0.376)	Data 1.07e-04 (3.19e-04)	Tok/s 46141 (37197)	Loss/tok 3.9064 (4.7186)	LR 2.000e-03
0: TRAIN [0][1910/3880]	Time 0.415 (0.376)	Data 1.08e-04 (3.18e-04)	Tok/s 40572 (37197)	Loss/tok 3.8200 (4.7134)	LR 2.000e-03
0: TRAIN [0][1920/3880]	Time 0.414 (0.376)	Data 1.09e-04 (3.17e-04)	Tok/s 40974 (37195)	Loss/tok 3.8504 (4.7084)	LR 2.000e-03
0: TRAIN [0][1930/3880]	Time 0.430 (0.376)	Data 1.02e-04 (3.16e-04)	Tok/s 38728 (37213)	Loss/tok 3.7039 (4.7024)	LR 2.000e-03
0: TRAIN [0][1940/3880]	Time 0.311 (0.376)	Data 1.09e-04 (3.15e-04)	Tok/s 33103 (37198)	Loss/tok 3.4971 (4.6979)	LR 2.000e-03
0: TRAIN [0][1950/3880]	Time 0.319 (0.376)	Data 1.06e-04 (3.14e-04)	Tok/s 32377 (37184)	Loss/tok 3.5414 (4.6927)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1960/3880]	Time 0.505 (0.376)	Data 1.16e-04 (3.13e-04)	Tok/s 46794 (37192)	Loss/tok 3.7862 (4.6870)	LR 2.000e-03
0: TRAIN [0][1970/3880]	Time 0.349 (0.376)	Data 1.08e-04 (3.12e-04)	Tok/s 28538 (37200)	Loss/tok 3.6083 (4.6815)	LR 2.000e-03
0: TRAIN [0][1980/3880]	Time 0.424 (0.376)	Data 1.06e-04 (3.11e-04)	Tok/s 39718 (37188)	Loss/tok 3.7031 (4.6760)	LR 2.000e-03
0: TRAIN [0][1990/3880]	Time 0.301 (0.377)	Data 1.10e-04 (3.10e-04)	Tok/s 34087 (37204)	Loss/tok 3.3704 (4.6702)	LR 2.000e-03
0: TRAIN [0][2000/3880]	Time 0.410 (0.377)	Data 1.11e-04 (3.09e-04)	Tok/s 41011 (37207)	Loss/tok 3.7181 (4.6653)	LR 2.000e-03
0: TRAIN [0][2010/3880]	Time 0.299 (0.377)	Data 1.09e-04 (3.08e-04)	Tok/s 34184 (37215)	Loss/tok 3.2736 (4.6600)	LR 2.000e-03
0: TRAIN [0][2020/3880]	Time 0.406 (0.377)	Data 1.09e-04 (3.07e-04)	Tok/s 41506 (37227)	Loss/tok 3.6719 (4.6549)	LR 2.000e-03
0: TRAIN [0][2030/3880]	Time 0.730 (0.377)	Data 1.02e-04 (3.06e-04)	Tok/s 41036 (37210)	Loss/tok 4.0631 (4.6502)	LR 2.000e-03
0: TRAIN [0][2040/3880]	Time 0.311 (0.377)	Data 1.08e-04 (3.05e-04)	Tok/s 33799 (37212)	Loss/tok 3.3962 (4.6452)	LR 2.000e-03
0: TRAIN [0][2050/3880]	Time 0.397 (0.377)	Data 1.08e-04 (3.04e-04)	Tok/s 41907 (37205)	Loss/tok 3.7204 (4.6408)	LR 2.000e-03
0: TRAIN [0][2060/3880]	Time 0.342 (0.377)	Data 1.13e-04 (3.03e-04)	Tok/s 29926 (37209)	Loss/tok 3.5371 (4.6359)	LR 2.000e-03
0: TRAIN [0][2070/3880]	Time 0.306 (0.377)	Data 1.06e-04 (3.02e-04)	Tok/s 33187 (37209)	Loss/tok 3.3635 (4.6307)	LR 2.000e-03
0: TRAIN [0][2080/3880]	Time 0.403 (0.378)	Data 9.97e-05 (3.01e-04)	Tok/s 41541 (37222)	Loss/tok 3.6151 (4.6255)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2090/3880]	Time 0.301 (0.378)	Data 1.07e-04 (3.00e-04)	Tok/s 33980 (37233)	Loss/tok 3.5191 (4.6210)	LR 2.000e-03
0: TRAIN [0][2100/3880]	Time 0.294 (0.377)	Data 1.22e-04 (2.99e-04)	Tok/s 35192 (37226)	Loss/tok 3.3836 (4.6168)	LR 2.000e-03
0: TRAIN [0][2110/3880]	Time 0.533 (0.377)	Data 1.25e-04 (2.98e-04)	Tok/s 43596 (37215)	Loss/tok 3.7796 (4.6121)	LR 2.000e-03
0: TRAIN [0][2120/3880]	Time 0.504 (0.377)	Data 1.12e-04 (2.97e-04)	Tok/s 46308 (37223)	Loss/tok 3.8306 (4.6074)	LR 2.000e-03
0: TRAIN [0][2130/3880]	Time 0.226 (0.377)	Data 1.07e-04 (2.97e-04)	Tok/s 23326 (37215)	Loss/tok 2.9192 (4.6033)	LR 2.000e-03
0: TRAIN [0][2140/3880]	Time 0.303 (0.377)	Data 1.10e-04 (2.96e-04)	Tok/s 34083 (37203)	Loss/tok 3.5680 (4.5988)	LR 2.000e-03
0: TRAIN [0][2150/3880]	Time 0.297 (0.377)	Data 1.10e-04 (2.95e-04)	Tok/s 34032 (37208)	Loss/tok 3.3693 (4.5944)	LR 2.000e-03
0: TRAIN [0][2160/3880]	Time 0.298 (0.377)	Data 1.04e-04 (2.94e-04)	Tok/s 35068 (37210)	Loss/tok 3.3505 (4.5900)	LR 2.000e-03
0: TRAIN [0][2170/3880]	Time 0.504 (0.377)	Data 1.09e-04 (2.93e-04)	Tok/s 32929 (37209)	Loss/tok 3.6037 (4.5855)	LR 2.000e-03
0: TRAIN [0][2180/3880]	Time 0.290 (0.378)	Data 1.11e-04 (2.92e-04)	Tok/s 35107 (37215)	Loss/tok 3.2786 (4.5808)	LR 2.000e-03
0: TRAIN [0][2190/3880]	Time 0.410 (0.378)	Data 1.07e-04 (2.91e-04)	Tok/s 40829 (37217)	Loss/tok 3.6509 (4.5765)	LR 2.000e-03
0: TRAIN [0][2200/3880]	Time 0.291 (0.378)	Data 1.08e-04 (2.91e-04)	Tok/s 35965 (37229)	Loss/tok 3.4543 (4.5722)	LR 2.000e-03
0: TRAIN [0][2210/3880]	Time 0.298 (0.378)	Data 1.07e-04 (2.90e-04)	Tok/s 34691 (37215)	Loss/tok 3.4592 (4.5682)	LR 2.000e-03
0: TRAIN [0][2220/3880]	Time 0.297 (0.377)	Data 1.08e-04 (2.89e-04)	Tok/s 35043 (37203)	Loss/tok 3.4002 (4.5645)	LR 2.000e-03
0: TRAIN [0][2230/3880]	Time 0.200 (0.377)	Data 1.10e-04 (2.88e-04)	Tok/s 25769 (37205)	Loss/tok 2.9255 (4.5603)	LR 2.000e-03
0: TRAIN [0][2240/3880]	Time 0.400 (0.378)	Data 1.06e-04 (2.87e-04)	Tok/s 41964 (37206)	Loss/tok 3.6932 (4.5561)	LR 2.000e-03
0: TRAIN [0][2250/3880]	Time 0.438 (0.378)	Data 1.05e-04 (2.86e-04)	Tok/s 38784 (37216)	Loss/tok 3.7693 (4.5517)	LR 2.000e-03
0: TRAIN [0][2260/3880]	Time 0.298 (0.378)	Data 1.05e-04 (2.86e-04)	Tok/s 34299 (37204)	Loss/tok 3.4281 (4.5479)	LR 2.000e-03
0: TRAIN [0][2270/3880]	Time 0.319 (0.378)	Data 1.07e-04 (2.85e-04)	Tok/s 31903 (37207)	Loss/tok 3.3526 (4.5436)	LR 2.000e-03
0: TRAIN [0][2280/3880]	Time 0.510 (0.378)	Data 1.06e-04 (2.84e-04)	Tok/s 45538 (37198)	Loss/tok 3.7335 (4.5397)	LR 2.000e-03
0: TRAIN [0][2290/3880]	Time 0.419 (0.378)	Data 1.11e-04 (2.83e-04)	Tok/s 40185 (37202)	Loss/tok 3.5601 (4.5355)	LR 2.000e-03
0: TRAIN [0][2300/3880]	Time 0.402 (0.378)	Data 1.09e-04 (2.83e-04)	Tok/s 42044 (37204)	Loss/tok 3.6846 (4.5314)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2310/3880]	Time 0.291 (0.378)	Data 1.07e-04 (2.82e-04)	Tok/s 35100 (37218)	Loss/tok 3.4092 (4.5272)	LR 2.000e-03
0: TRAIN [0][2320/3880]	Time 0.291 (0.378)	Data 1.11e-04 (2.81e-04)	Tok/s 35069 (37207)	Loss/tok 3.3914 (4.5234)	LR 2.000e-03
0: TRAIN [0][2330/3880]	Time 0.387 (0.378)	Data 1.10e-04 (2.80e-04)	Tok/s 43979 (37221)	Loss/tok 3.8408 (4.5194)	LR 2.000e-03
0: TRAIN [0][2340/3880]	Time 0.301 (0.378)	Data 1.15e-04 (2.80e-04)	Tok/s 34009 (37227)	Loss/tok 3.4038 (4.5157)	LR 2.000e-03
0: TRAIN [0][2350/3880]	Time 0.422 (0.378)	Data 1.05e-04 (2.79e-04)	Tok/s 39855 (37219)	Loss/tok 3.7660 (4.5122)	LR 2.000e-03
0: TRAIN [0][2360/3880]	Time 0.207 (0.378)	Data 1.06e-04 (2.78e-04)	Tok/s 25198 (37208)	Loss/tok 2.8758 (4.5086)	LR 2.000e-03
0: TRAIN [0][2370/3880]	Time 0.299 (0.378)	Data 1.10e-04 (2.78e-04)	Tok/s 34390 (37217)	Loss/tok 3.5063 (4.5047)	LR 2.000e-03
0: TRAIN [0][2380/3880]	Time 0.309 (0.378)	Data 1.06e-04 (2.77e-04)	Tok/s 33002 (37210)	Loss/tok 3.3824 (4.5012)	LR 2.000e-03
0: TRAIN [0][2390/3880]	Time 0.303 (0.378)	Data 1.08e-04 (2.76e-04)	Tok/s 34446 (37221)	Loss/tok 3.3307 (4.4973)	LR 2.000e-03
0: TRAIN [0][2400/3880]	Time 0.616 (0.378)	Data 1.08e-04 (2.75e-04)	Tok/s 37722 (37214)	Loss/tok 3.8673 (4.4935)	LR 2.000e-03
0: TRAIN [0][2410/3880]	Time 0.509 (0.378)	Data 1.04e-04 (2.75e-04)	Tok/s 46630 (37212)	Loss/tok 3.7814 (4.4899)	LR 2.000e-03
0: TRAIN [0][2420/3880]	Time 0.306 (0.378)	Data 1.06e-04 (2.74e-04)	Tok/s 33415 (37210)	Loss/tok 3.4432 (4.4865)	LR 2.000e-03
0: TRAIN [0][2430/3880]	Time 0.401 (0.378)	Data 1.08e-04 (2.73e-04)	Tok/s 42019 (37219)	Loss/tok 3.6626 (4.4826)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2440/3880]	Time 0.388 (0.378)	Data 1.06e-04 (2.73e-04)	Tok/s 43535 (37219)	Loss/tok 3.6295 (4.4788)	LR 2.000e-03
0: TRAIN [0][2450/3880]	Time 0.391 (0.378)	Data 1.05e-04 (2.72e-04)	Tok/s 42913 (37227)	Loss/tok 3.6091 (4.4750)	LR 2.000e-03
0: TRAIN [0][2460/3880]	Time 0.314 (0.378)	Data 1.08e-04 (2.71e-04)	Tok/s 32466 (37216)	Loss/tok 3.3671 (4.4717)	LR 2.000e-03
0: TRAIN [0][2470/3880]	Time 0.301 (0.378)	Data 1.11e-04 (2.71e-04)	Tok/s 34127 (37219)	Loss/tok 3.3212 (4.4680)	LR 2.000e-03
0: TRAIN [0][2480/3880]	Time 0.301 (0.379)	Data 1.17e-04 (2.70e-04)	Tok/s 34185 (37230)	Loss/tok 3.3424 (4.4641)	LR 2.000e-03
0: TRAIN [0][2490/3880]	Time 0.207 (0.378)	Data 1.04e-04 (2.69e-04)	Tok/s 25607 (37211)	Loss/tok 2.7847 (4.4612)	LR 2.000e-03
0: TRAIN [0][2500/3880]	Time 0.206 (0.378)	Data 1.08e-04 (2.69e-04)	Tok/s 25759 (37214)	Loss/tok 2.9597 (4.4579)	LR 2.000e-03
0: TRAIN [0][2510/3880]	Time 0.304 (0.378)	Data 1.07e-04 (2.68e-04)	Tok/s 34238 (37211)	Loss/tok 3.3501 (4.4546)	LR 2.000e-03
0: TRAIN [0][2520/3880]	Time 0.299 (0.378)	Data 1.09e-04 (2.67e-04)	Tok/s 34735 (37215)	Loss/tok 3.3702 (4.4510)	LR 2.000e-03
0: TRAIN [0][2530/3880]	Time 0.413 (0.378)	Data 1.11e-04 (2.67e-04)	Tok/s 40558 (37212)	Loss/tok 3.6563 (4.4478)	LR 2.000e-03
0: TRAIN [0][2540/3880]	Time 0.396 (0.378)	Data 1.07e-04 (2.66e-04)	Tok/s 42350 (37214)	Loss/tok 3.5801 (4.4444)	LR 2.000e-03
0: TRAIN [0][2550/3880]	Time 0.528 (0.378)	Data 1.08e-04 (2.65e-04)	Tok/s 44069 (37209)	Loss/tok 3.8443 (4.4412)	LR 2.000e-03
0: TRAIN [0][2560/3880]	Time 0.303 (0.378)	Data 1.09e-04 (2.65e-04)	Tok/s 34033 (37215)	Loss/tok 3.4282 (4.4379)	LR 2.000e-03
0: TRAIN [0][2570/3880]	Time 0.332 (0.378)	Data 1.09e-04 (2.64e-04)	Tok/s 31688 (37209)	Loss/tok 3.3431 (4.4348)	LR 2.000e-03
0: TRAIN [0][2580/3880]	Time 0.310 (0.378)	Data 1.10e-04 (2.64e-04)	Tok/s 33776 (37203)	Loss/tok 3.3526 (4.4314)	LR 2.000e-03
0: TRAIN [0][2590/3880]	Time 0.295 (0.378)	Data 1.18e-04 (2.63e-04)	Tok/s 35398 (37206)	Loss/tok 3.4489 (4.4283)	LR 2.000e-03
0: TRAIN [0][2600/3880]	Time 0.300 (0.378)	Data 1.02e-04 (2.62e-04)	Tok/s 34256 (37194)	Loss/tok 3.3941 (4.4255)	LR 2.000e-03
0: TRAIN [0][2610/3880]	Time 0.302 (0.378)	Data 1.03e-04 (2.62e-04)	Tok/s 34173 (37189)	Loss/tok 3.3214 (4.4225)	LR 2.000e-03
0: TRAIN [0][2620/3880]	Time 0.307 (0.377)	Data 1.07e-04 (2.61e-04)	Tok/s 34215 (37176)	Loss/tok 3.3198 (4.4197)	LR 2.000e-03
0: TRAIN [0][2630/3880]	Time 0.515 (0.377)	Data 1.07e-04 (2.61e-04)	Tok/s 44990 (37176)	Loss/tok 3.8129 (4.4166)	LR 2.000e-03
0: TRAIN [0][2640/3880]	Time 0.298 (0.377)	Data 1.03e-04 (2.60e-04)	Tok/s 34253 (37172)	Loss/tok 3.3035 (4.4135)	LR 2.000e-03
0: TRAIN [0][2650/3880]	Time 0.314 (0.377)	Data 1.03e-04 (2.60e-04)	Tok/s 33200 (37154)	Loss/tok 3.3307 (4.4104)	LR 2.000e-03
0: TRAIN [0][2660/3880]	Time 0.508 (0.377)	Data 1.09e-04 (2.59e-04)	Tok/s 46297 (37151)	Loss/tok 3.8215 (4.4074)	LR 2.000e-03
0: TRAIN [0][2670/3880]	Time 0.320 (0.377)	Data 1.09e-04 (2.58e-04)	Tok/s 32823 (37151)	Loss/tok 3.3150 (4.4043)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2680/3880]	Time 0.507 (0.377)	Data 1.10e-04 (2.58e-04)	Tok/s 45934 (37166)	Loss/tok 3.7146 (4.4009)	LR 2.000e-03
0: TRAIN [0][2690/3880]	Time 0.398 (0.377)	Data 1.11e-04 (2.57e-04)	Tok/s 42077 (37180)	Loss/tok 3.4969 (4.3976)	LR 2.000e-03
0: TRAIN [0][2700/3880]	Time 0.305 (0.377)	Data 1.09e-04 (2.57e-04)	Tok/s 33423 (37177)	Loss/tok 3.3088 (4.3946)	LR 2.000e-03
0: TRAIN [0][2710/3880]	Time 0.357 (0.377)	Data 1.09e-04 (2.56e-04)	Tok/s 29008 (37164)	Loss/tok 3.3662 (4.3918)	LR 2.000e-03
0: TRAIN [0][2720/3880]	Time 0.306 (0.377)	Data 1.11e-04 (2.56e-04)	Tok/s 33870 (37162)	Loss/tok 3.2816 (4.3886)	LR 2.000e-03
0: TRAIN [0][2730/3880]	Time 0.298 (0.377)	Data 1.08e-04 (2.55e-04)	Tok/s 35375 (37165)	Loss/tok 3.3295 (4.3855)	LR 2.000e-03
0: TRAIN [0][2740/3880]	Time 0.199 (0.377)	Data 1.04e-04 (2.55e-04)	Tok/s 26509 (37154)	Loss/tok 2.8530 (4.3830)	LR 2.000e-03
0: TRAIN [0][2750/3880]	Time 0.322 (0.377)	Data 1.08e-04 (2.54e-04)	Tok/s 32762 (37146)	Loss/tok 3.3292 (4.3802)	LR 2.000e-03
0: TRAIN [0][2760/3880]	Time 0.675 (0.378)	Data 1.11e-04 (2.53e-04)	Tok/s 43639 (37151)	Loss/tok 3.9920 (4.3769)	LR 2.000e-03
0: TRAIN [0][2770/3880]	Time 0.514 (0.378)	Data 1.12e-04 (2.53e-04)	Tok/s 45040 (37159)	Loss/tok 3.7261 (4.3739)	LR 2.000e-03
0: TRAIN [0][2780/3880]	Time 0.568 (0.378)	Data 1.05e-04 (2.52e-04)	Tok/s 41443 (37163)	Loss/tok 3.8293 (4.3709)	LR 2.000e-03
0: TRAIN [0][2790/3880]	Time 0.288 (0.378)	Data 1.02e-04 (2.52e-04)	Tok/s 35644 (37172)	Loss/tok 3.3894 (4.3674)	LR 2.000e-03
0: TRAIN [0][2800/3880]	Time 0.495 (0.378)	Data 1.03e-04 (2.51e-04)	Tok/s 46709 (37172)	Loss/tok 3.7109 (4.3645)	LR 2.000e-03
0: TRAIN [0][2810/3880]	Time 0.312 (0.378)	Data 1.13e-04 (2.51e-04)	Tok/s 32992 (37165)	Loss/tok 3.5528 (4.3618)	LR 2.000e-03
0: TRAIN [0][2820/3880]	Time 0.315 (0.378)	Data 1.09e-04 (2.50e-04)	Tok/s 32792 (37159)	Loss/tok 3.2984 (4.3591)	LR 2.000e-03
0: TRAIN [0][2830/3880]	Time 0.375 (0.378)	Data 1.10e-04 (2.50e-04)	Tok/s 26851 (37152)	Loss/tok 3.3219 (4.3561)	LR 2.000e-03
0: TRAIN [0][2840/3880]	Time 0.309 (0.378)	Data 1.09e-04 (2.49e-04)	Tok/s 33378 (37135)	Loss/tok 3.3186 (4.3534)	LR 2.000e-03
0: TRAIN [0][2850/3880]	Time 0.510 (0.378)	Data 1.06e-04 (2.49e-04)	Tok/s 45856 (37139)	Loss/tok 3.7606 (4.3506)	LR 2.000e-03
0: TRAIN [0][2860/3880]	Time 0.304 (0.378)	Data 1.06e-04 (2.48e-04)	Tok/s 34010 (37150)	Loss/tok 3.3180 (4.3476)	LR 2.000e-03
0: TRAIN [0][2870/3880]	Time 0.431 (0.378)	Data 1.05e-04 (2.48e-04)	Tok/s 38922 (37147)	Loss/tok 3.5383 (4.3446)	LR 2.000e-03
0: TRAIN [0][2880/3880]	Time 0.206 (0.378)	Data 1.10e-04 (2.47e-04)	Tok/s 25794 (37130)	Loss/tok 2.8483 (4.3422)	LR 2.000e-03
0: TRAIN [0][2890/3880]	Time 0.205 (0.378)	Data 1.09e-04 (2.47e-04)	Tok/s 25392 (37138)	Loss/tok 2.8115 (4.3395)	LR 2.000e-03
0: TRAIN [0][2900/3880]	Time 0.690 (0.378)	Data 1.08e-04 (2.46e-04)	Tok/s 33521 (37138)	Loss/tok 3.9136 (4.3367)	LR 2.000e-03
0: TRAIN [0][2910/3880]	Time 0.408 (0.378)	Data 1.11e-04 (2.46e-04)	Tok/s 40725 (37128)	Loss/tok 3.4924 (4.3341)	LR 2.000e-03
0: TRAIN [0][2920/3880]	Time 0.330 (0.378)	Data 1.06e-04 (2.45e-04)	Tok/s 31680 (37117)	Loss/tok 3.3029 (4.3315)	LR 2.000e-03
0: TRAIN [0][2930/3880]	Time 0.715 (0.379)	Data 1.09e-04 (2.45e-04)	Tok/s 41896 (37111)	Loss/tok 3.8777 (4.3287)	LR 2.000e-03
0: TRAIN [0][2940/3880]	Time 0.209 (0.378)	Data 1.06e-04 (2.45e-04)	Tok/s 25473 (37104)	Loss/tok 2.9240 (4.3260)	LR 2.000e-03
0: TRAIN [0][2950/3880]	Time 0.297 (0.378)	Data 1.08e-04 (2.44e-04)	Tok/s 34324 (37112)	Loss/tok 3.3689 (4.3233)	LR 2.000e-03
0: TRAIN [0][2960/3880]	Time 0.305 (0.378)	Data 1.10e-04 (2.44e-04)	Tok/s 34478 (37120)	Loss/tok 3.4436 (4.3205)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][2970/3880]	Time 0.296 (0.378)	Data 1.07e-04 (2.43e-04)	Tok/s 35252 (37116)	Loss/tok 3.3377 (4.3181)	LR 2.000e-03
0: TRAIN [0][2980/3880]	Time 0.441 (0.378)	Data 1.10e-04 (2.43e-04)	Tok/s 37939 (37100)	Loss/tok 3.6073 (4.3155)	LR 2.000e-03
0: TRAIN [0][2990/3880]	Time 0.502 (0.378)	Data 1.11e-04 (2.42e-04)	Tok/s 45930 (37094)	Loss/tok 3.8779 (4.3131)	LR 2.000e-03
0: TRAIN [0][3000/3880]	Time 0.514 (0.378)	Data 1.03e-04 (2.42e-04)	Tok/s 44990 (37096)	Loss/tok 3.6745 (4.3105)	LR 2.000e-03
0: TRAIN [0][3010/3880]	Time 0.402 (0.378)	Data 1.06e-04 (2.41e-04)	Tok/s 41783 (37104)	Loss/tok 3.5148 (4.3079)	LR 2.000e-03
0: TRAIN [0][3020/3880]	Time 0.431 (0.378)	Data 1.06e-04 (2.41e-04)	Tok/s 39075 (37112)	Loss/tok 3.4943 (4.3051)	LR 2.000e-03
0: TRAIN [0][3030/3880]	Time 0.511 (0.378)	Data 1.10e-04 (2.40e-04)	Tok/s 32773 (37115)	Loss/tok 3.5311 (4.3025)	LR 2.000e-03
0: TRAIN [0][3040/3880]	Time 0.428 (0.379)	Data 1.10e-04 (2.40e-04)	Tok/s 38800 (37106)	Loss/tok 3.6015 (4.3000)	LR 2.000e-03
0: TRAIN [0][3050/3880]	Time 0.398 (0.378)	Data 1.06e-04 (2.40e-04)	Tok/s 42518 (37107)	Loss/tok 3.5710 (4.2975)	LR 2.000e-03
0: TRAIN [0][3060/3880]	Time 0.505 (0.378)	Data 1.08e-04 (2.39e-04)	Tok/s 45950 (37116)	Loss/tok 3.7053 (4.2950)	LR 2.000e-03
0: TRAIN [0][3070/3880]	Time 0.400 (0.378)	Data 1.08e-04 (2.39e-04)	Tok/s 41601 (37118)	Loss/tok 3.5667 (4.2927)	LR 2.000e-03
0: TRAIN [0][3080/3880]	Time 0.510 (0.379)	Data 1.08e-04 (2.38e-04)	Tok/s 45369 (37131)	Loss/tok 3.7539 (4.2902)	LR 2.000e-03
0: TRAIN [0][3090/3880]	Time 0.308 (0.378)	Data 1.03e-04 (2.38e-04)	Tok/s 32858 (37125)	Loss/tok 3.2586 (4.2878)	LR 2.000e-03
0: TRAIN [0][3100/3880]	Time 0.420 (0.379)	Data 1.09e-04 (2.37e-04)	Tok/s 39186 (37133)	Loss/tok 3.5758 (4.2851)	LR 2.000e-03
0: TRAIN [0][3110/3880]	Time 0.301 (0.378)	Data 1.12e-04 (2.37e-04)	Tok/s 34441 (37128)	Loss/tok 3.1714 (4.2830)	LR 2.000e-03
0: TRAIN [0][3120/3880]	Time 0.319 (0.379)	Data 1.08e-04 (2.37e-04)	Tok/s 31901 (37115)	Loss/tok 3.2743 (4.2807)	LR 2.000e-03
0: TRAIN [0][3130/3880]	Time 0.408 (0.379)	Data 2.40e-04 (2.36e-04)	Tok/s 41418 (37119)	Loss/tok 3.6193 (4.2781)	LR 2.000e-03
0: TRAIN [0][3140/3880]	Time 0.302 (0.379)	Data 1.08e-04 (2.36e-04)	Tok/s 34091 (37115)	Loss/tok 3.3673 (4.2758)	LR 2.000e-03
0: TRAIN [0][3150/3880]	Time 0.304 (0.379)	Data 1.04e-04 (2.35e-04)	Tok/s 33893 (37124)	Loss/tok 3.3852 (4.2732)	LR 2.000e-03
0: TRAIN [0][3160/3880]	Time 0.396 (0.379)	Data 1.05e-04 (2.35e-04)	Tok/s 42194 (37122)	Loss/tok 3.4981 (4.2709)	LR 2.000e-03
0: TRAIN [0][3170/3880]	Time 0.419 (0.379)	Data 1.10e-04 (2.35e-04)	Tok/s 39688 (37122)	Loss/tok 3.6724 (4.2686)	LR 2.000e-03
0: TRAIN [0][3180/3880]	Time 0.388 (0.378)	Data 1.06e-04 (2.34e-04)	Tok/s 42972 (37119)	Loss/tok 3.4753 (4.2664)	LR 2.000e-03
0: TRAIN [0][3190/3880]	Time 0.405 (0.378)	Data 1.03e-04 (2.34e-04)	Tok/s 41355 (37125)	Loss/tok 3.4771 (4.2639)	LR 2.000e-03
0: TRAIN [0][3200/3880]	Time 0.217 (0.379)	Data 1.08e-04 (2.33e-04)	Tok/s 23836 (37128)	Loss/tok 2.7098 (4.2615)	LR 2.000e-03
0: TRAIN [0][3210/3880]	Time 0.309 (0.379)	Data 1.04e-04 (2.33e-04)	Tok/s 32883 (37129)	Loss/tok 3.3027 (4.2591)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [0][3220/3880]	Time 0.515 (0.379)	Data 1.12e-04 (2.33e-04)	Tok/s 45235 (37133)	Loss/tok 3.7352 (4.2568)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3230/3880]	Time 0.296 (0.379)	Data 1.06e-04 (2.32e-04)	Tok/s 35345 (37145)	Loss/tok 3.2142 (4.2543)	LR 2.000e-03
0: TRAIN [0][3240/3880]	Time 0.390 (0.379)	Data 1.10e-04 (2.32e-04)	Tok/s 43112 (37146)	Loss/tok 3.4263 (4.2518)	LR 2.000e-03
0: TRAIN [0][3250/3880]	Time 0.307 (0.379)	Data 1.19e-04 (2.31e-04)	Tok/s 33671 (37141)	Loss/tok 3.3267 (4.2495)	LR 2.000e-03
0: TRAIN [0][3260/3880]	Time 0.202 (0.379)	Data 1.08e-04 (2.31e-04)	Tok/s 26606 (37136)	Loss/tok 2.7497 (4.2474)	LR 2.000e-03
0: TRAIN [0][3270/3880]	Time 0.647 (0.379)	Data 1.08e-04 (2.31e-04)	Tok/s 45915 (37139)	Loss/tok 3.9355 (4.2449)	LR 2.000e-03
0: TRAIN [0][3280/3880]	Time 0.387 (0.379)	Data 1.07e-04 (2.30e-04)	Tok/s 43779 (37146)	Loss/tok 3.5729 (4.2426)	LR 2.000e-03
0: TRAIN [0][3290/3880]	Time 0.301 (0.379)	Data 1.19e-04 (2.30e-04)	Tok/s 34318 (37140)	Loss/tok 3.2377 (4.2404)	LR 2.000e-03
0: TRAIN [0][3300/3880]	Time 0.307 (0.379)	Data 1.06e-04 (2.30e-04)	Tok/s 33496 (37132)	Loss/tok 3.3172 (4.2383)	LR 2.000e-03
0: TRAIN [0][3310/3880]	Time 0.396 (0.379)	Data 1.09e-04 (2.29e-04)	Tok/s 42390 (37139)	Loss/tok 3.5799 (4.2361)	LR 2.000e-03
0: TRAIN [0][3320/3880]	Time 0.665 (0.379)	Data 1.14e-04 (2.29e-04)	Tok/s 43895 (37138)	Loss/tok 3.9581 (4.2340)	LR 2.000e-03
0: TRAIN [0][3330/3880]	Time 0.411 (0.379)	Data 1.06e-04 (2.29e-04)	Tok/s 40901 (37137)	Loss/tok 3.3984 (4.2318)	LR 2.000e-03
0: TRAIN [0][3340/3880]	Time 0.308 (0.379)	Data 1.07e-04 (2.28e-04)	Tok/s 34568 (37133)	Loss/tok 3.3062 (4.2296)	LR 2.000e-03
0: TRAIN [0][3350/3880]	Time 0.299 (0.379)	Data 1.07e-04 (2.28e-04)	Tok/s 34608 (37137)	Loss/tok 3.3094 (4.2274)	LR 2.000e-03
0: TRAIN [0][3360/3880]	Time 0.288 (0.378)	Data 1.08e-04 (2.27e-04)	Tok/s 35216 (37134)	Loss/tok 3.2982 (4.2253)	LR 2.000e-03
0: TRAIN [0][3370/3880]	Time 0.206 (0.378)	Data 1.10e-04 (2.27e-04)	Tok/s 25852 (37130)	Loss/tok 2.7927 (4.2232)	LR 2.000e-03
0: TRAIN [0][3380/3880]	Time 0.404 (0.378)	Data 1.04e-04 (2.27e-04)	Tok/s 41887 (37137)	Loss/tok 3.5927 (4.2210)	LR 2.000e-03
0: TRAIN [0][3390/3880]	Time 0.587 (0.378)	Data 1.05e-04 (2.26e-04)	Tok/s 39326 (37135)	Loss/tok 3.7320 (4.2191)	LR 2.000e-03
0: TRAIN [0][3400/3880]	Time 0.216 (0.378)	Data 1.10e-04 (2.26e-04)	Tok/s 24299 (37128)	Loss/tok 2.8475 (4.2170)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3410/3880]	Time 0.206 (0.378)	Data 1.08e-04 (2.26e-04)	Tok/s 25477 (37123)	Loss/tok 2.6971 (4.2150)	LR 2.000e-03
0: TRAIN [0][3420/3880]	Time 0.299 (0.378)	Data 1.08e-04 (2.25e-04)	Tok/s 35186 (37125)	Loss/tok 3.2489 (4.2128)	LR 2.000e-03
0: TRAIN [0][3430/3880]	Time 0.437 (0.378)	Data 1.13e-04 (2.25e-04)	Tok/s 38656 (37129)	Loss/tok 3.4935 (4.2108)	LR 2.000e-03
0: TRAIN [0][3440/3880]	Time 0.301 (0.378)	Data 1.12e-04 (2.25e-04)	Tok/s 34078 (37138)	Loss/tok 3.1844 (4.2085)	LR 2.000e-03
0: TRAIN [0][3450/3880]	Time 0.298 (0.379)	Data 1.10e-04 (2.24e-04)	Tok/s 34211 (37147)	Loss/tok 3.2947 (4.2063)	LR 2.000e-03
0: TRAIN [0][3460/3880]	Time 0.544 (0.379)	Data 1.08e-04 (2.24e-04)	Tok/s 42652 (37144)	Loss/tok 3.7507 (4.2043)	LR 2.000e-03
0: TRAIN [0][3470/3880]	Time 0.302 (0.379)	Data 1.11e-04 (2.24e-04)	Tok/s 33883 (37147)	Loss/tok 3.3343 (4.2024)	LR 2.000e-03
0: TRAIN [0][3480/3880]	Time 0.520 (0.379)	Data 1.12e-04 (2.23e-04)	Tok/s 44605 (37149)	Loss/tok 3.7218 (4.2003)	LR 2.000e-03
0: TRAIN [0][3490/3880]	Time 0.644 (0.379)	Data 1.10e-04 (2.23e-04)	Tok/s 45840 (37159)	Loss/tok 3.8477 (4.1982)	LR 2.000e-03
0: TRAIN [0][3500/3880]	Time 0.746 (0.379)	Data 1.07e-04 (2.23e-04)	Tok/s 39865 (37162)	Loss/tok 3.8355 (4.1962)	LR 2.000e-03
0: TRAIN [0][3510/3880]	Time 0.292 (0.379)	Data 1.07e-04 (2.22e-04)	Tok/s 35794 (37161)	Loss/tok 3.2382 (4.1942)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3520/3880]	Time 0.318 (0.379)	Data 1.07e-04 (2.22e-04)	Tok/s 32707 (37154)	Loss/tok 3.2200 (4.1924)	LR 2.000e-03
0: TRAIN [0][3530/3880]	Time 0.533 (0.379)	Data 1.10e-04 (2.22e-04)	Tok/s 43754 (37160)	Loss/tok 3.6548 (4.1901)	LR 2.000e-03
0: TRAIN [0][3540/3880]	Time 0.315 (0.379)	Data 1.22e-04 (2.21e-04)	Tok/s 34109 (37147)	Loss/tok 3.3143 (4.1883)	LR 2.000e-03
0: TRAIN [0][3550/3880]	Time 0.521 (0.379)	Data 1.11e-04 (2.21e-04)	Tok/s 44793 (37153)	Loss/tok 3.7207 (4.1864)	LR 2.000e-03
0: TRAIN [0][3560/3880]	Time 0.412 (0.379)	Data 1.20e-04 (2.21e-04)	Tok/s 41499 (37163)	Loss/tok 3.4495 (4.1843)	LR 2.000e-03
0: TRAIN [0][3570/3880]	Time 0.407 (0.379)	Data 1.10e-04 (2.20e-04)	Tok/s 40805 (37160)	Loss/tok 3.5184 (4.1824)	LR 2.000e-03
0: TRAIN [0][3580/3880]	Time 0.371 (0.379)	Data 1.08e-04 (2.20e-04)	Tok/s 27988 (37162)	Loss/tok 3.1958 (4.1804)	LR 2.000e-03
0: TRAIN [0][3590/3880]	Time 0.536 (0.380)	Data 1.10e-04 (2.20e-04)	Tok/s 43155 (37153)	Loss/tok 3.6712 (4.1782)	LR 2.000e-03
0: TRAIN [0][3600/3880]	Time 0.300 (0.379)	Data 1.00e-04 (2.19e-04)	Tok/s 34321 (37141)	Loss/tok 3.2770 (4.1765)	LR 2.000e-03
0: TRAIN [0][3610/3880]	Time 0.297 (0.379)	Data 1.06e-04 (2.19e-04)	Tok/s 34425 (37136)	Loss/tok 3.3225 (4.1747)	LR 2.000e-03
0: TRAIN [0][3620/3880]	Time 0.529 (0.380)	Data 1.11e-04 (2.19e-04)	Tok/s 43608 (37147)	Loss/tok 3.6290 (4.1728)	LR 2.000e-03
0: TRAIN [0][3630/3880]	Time 0.295 (0.380)	Data 1.09e-04 (2.19e-04)	Tok/s 35085 (37150)	Loss/tok 3.2766 (4.1709)	LR 2.000e-03
0: TRAIN [0][3640/3880]	Time 0.298 (0.380)	Data 1.06e-04 (2.18e-04)	Tok/s 34398 (37153)	Loss/tok 3.2027 (4.1689)	LR 2.000e-03
0: TRAIN [0][3650/3880]	Time 0.306 (0.379)	Data 1.06e-04 (2.18e-04)	Tok/s 33983 (37150)	Loss/tok 3.2487 (4.1670)	LR 2.000e-03
0: TRAIN [0][3660/3880]	Time 0.203 (0.379)	Data 1.13e-04 (2.18e-04)	Tok/s 25252 (37152)	Loss/tok 2.7646 (4.1653)	LR 2.000e-03
0: TRAIN [0][3670/3880]	Time 0.224 (0.379)	Data 1.10e-04 (2.17e-04)	Tok/s 23492 (37144)	Loss/tok 2.7539 (4.1635)	LR 2.000e-03
0: TRAIN [0][3680/3880]	Time 0.211 (0.379)	Data 1.08e-04 (2.17e-04)	Tok/s 25120 (37134)	Loss/tok 2.9498 (4.1618)	LR 2.000e-03
0: TRAIN [0][3690/3880]	Time 0.309 (0.379)	Data 1.02e-04 (2.17e-04)	Tok/s 33432 (37128)	Loss/tok 3.3173 (4.1601)	LR 2.000e-03
0: TRAIN [0][3700/3880]	Time 0.311 (0.379)	Data 1.07e-04 (2.16e-04)	Tok/s 33062 (37131)	Loss/tok 3.3571 (4.1582)	LR 2.000e-03
0: TRAIN [0][3710/3880]	Time 0.345 (0.380)	Data 1.06e-04 (2.16e-04)	Tok/s 30239 (37130)	Loss/tok 3.3120 (4.1565)	LR 2.000e-03
0: TRAIN [0][3720/3880]	Time 0.412 (0.379)	Data 1.03e-04 (2.16e-04)	Tok/s 40764 (37125)	Loss/tok 3.5666 (4.1547)	LR 2.000e-03
0: TRAIN [0][3730/3880]	Time 0.314 (0.380)	Data 1.18e-04 (2.16e-04)	Tok/s 33334 (37131)	Loss/tok 3.3040 (4.1529)	LR 2.000e-03
0: TRAIN [0][3740/3880]	Time 0.303 (0.380)	Data 1.11e-04 (2.15e-04)	Tok/s 33621 (37125)	Loss/tok 3.2412 (4.1512)	LR 2.000e-03
0: TRAIN [0][3750/3880]	Time 0.296 (0.379)	Data 1.03e-04 (2.15e-04)	Tok/s 35661 (37129)	Loss/tok 3.2750 (4.1492)	LR 2.000e-03
0: TRAIN [0][3760/3880]	Time 0.427 (0.380)	Data 1.08e-04 (2.15e-04)	Tok/s 39214 (37129)	Loss/tok 3.5366 (4.1475)	LR 2.000e-03
0: TRAIN [0][3770/3880]	Time 0.384 (0.380)	Data 1.04e-04 (2.14e-04)	Tok/s 43779 (37125)	Loss/tok 3.4556 (4.1458)	LR 2.000e-03
0: TRAIN [0][3780/3880]	Time 0.298 (0.380)	Data 1.07e-04 (2.14e-04)	Tok/s 34654 (37129)	Loss/tok 3.1701 (4.1439)	LR 2.000e-03
0: TRAIN [0][3790/3880]	Time 0.649 (0.380)	Data 1.08e-04 (2.14e-04)	Tok/s 46067 (37122)	Loss/tok 3.8122 (4.1423)	LR 2.000e-03
0: TRAIN [0][3800/3880]	Time 0.288 (0.380)	Data 1.06e-04 (2.13e-04)	Tok/s 35551 (37113)	Loss/tok 3.3162 (4.1406)	LR 2.000e-03
0: TRAIN [0][3810/3880]	Time 0.470 (0.380)	Data 1.09e-04 (2.13e-04)	Tok/s 35649 (37110)	Loss/tok 3.3658 (4.1388)	LR 2.000e-03
0: TRAIN [0][3820/3880]	Time 0.403 (0.380)	Data 1.07e-04 (2.13e-04)	Tok/s 41314 (37107)	Loss/tok 3.5501 (4.1371)	LR 2.000e-03
0: TRAIN [0][3830/3880]	Time 0.401 (0.380)	Data 1.06e-04 (2.13e-04)	Tok/s 41965 (37117)	Loss/tok 3.4776 (4.1353)	LR 2.000e-03
0: TRAIN [0][3840/3880]	Time 0.416 (0.380)	Data 1.09e-04 (2.12e-04)	Tok/s 40891 (37117)	Loss/tok 3.4465 (4.1336)	LR 2.000e-03
0: TRAIN [0][3850/3880]	Time 0.310 (0.380)	Data 1.10e-04 (2.12e-04)	Tok/s 33443 (37115)	Loss/tok 3.3052 (4.1319)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3860/3880]	Time 0.304 (0.380)	Data 1.08e-04 (2.12e-04)	Tok/s 34155 (37115)	Loss/tok 3.2545 (4.1302)	LR 2.000e-03
0: TRAIN [0][3870/3880]	Time 0.302 (0.380)	Data 1.06e-04 (2.12e-04)	Tok/s 34029 (37120)	Loss/tok 3.2940 (4.1284)	LR 2.000e-03
:::MLL 1572986540.445 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1572986540.446 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/6]	Time 0.893 (0.893)	Decoder iters 149.0 (149.0)	Tok/s 18222 (18222)
0: Running moses detokenizer
0: BLEU(score=21.16500339475243, counts=[35233, 16593, 8926, 5004], totals=[64433, 61430, 58427, 55427], precisions=[54.68160725094284, 27.01123229692333, 15.277183493932599, 9.02809100257997], bp=0.9962357432475866, sys_len=64433, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1572986543.696 eval_accuracy: {"value": 21.17, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1572986543.697 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.1256	Test BLEU: 21.17
0: Performance: Epoch: 0	Training: 148484 Tok/s
0: Finished epoch 0
:::MLL 1572986543.697 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1572986543.697 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1572986543.698 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 152066638
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][0/3880]	Time 0.629 (0.629)	Data 2.98e-01 (2.98e-01)	Tok/s 16658 (16658)	Loss/tok 3.2138 (3.2138)	LR 2.000e-03
0: TRAIN [1][10/3880]	Time 0.297 (0.423)	Data 1.24e-04 (2.72e-02)	Tok/s 34804 (38759)	Loss/tok 3.1478 (3.4294)	LR 2.000e-03
0: TRAIN [1][20/3880]	Time 0.658 (0.416)	Data 1.08e-04 (1.43e-02)	Tok/s 45341 (38279)	Loss/tok 3.8157 (3.4696)	LR 2.000e-03
0: TRAIN [1][30/3880]	Time 0.520 (0.398)	Data 1.08e-04 (9.72e-03)	Tok/s 44959 (38487)	Loss/tok 3.6012 (3.4274)	LR 2.000e-03
0: TRAIN [1][40/3880]	Time 0.334 (0.393)	Data 1.14e-04 (7.37e-03)	Tok/s 30783 (37347)	Loss/tok 3.1253 (3.4013)	LR 2.000e-03
0: TRAIN [1][50/3880]	Time 0.394 (0.395)	Data 1.11e-04 (5.95e-03)	Tok/s 42930 (37730)	Loss/tok 3.2541 (3.4005)	LR 2.000e-03
0: TRAIN [1][60/3880]	Time 0.585 (0.408)	Data 1.27e-04 (4.99e-03)	Tok/s 40078 (37601)	Loss/tok 3.4962 (3.4292)	LR 2.000e-03
0: TRAIN [1][70/3880]	Time 0.201 (0.394)	Data 1.11e-04 (4.30e-03)	Tok/s 26382 (36880)	Loss/tok 2.6640 (3.4116)	LR 2.000e-03
0: TRAIN [1][80/3880]	Time 0.204 (0.387)	Data 1.08e-04 (3.79e-03)	Tok/s 26128 (36937)	Loss/tok 2.6727 (3.4045)	LR 2.000e-03
0: TRAIN [1][90/3880]	Time 0.660 (0.393)	Data 1.06e-04 (3.38e-03)	Tok/s 45114 (37426)	Loss/tok 3.8553 (3.4222)	LR 2.000e-03
0: TRAIN [1][100/3880]	Time 0.301 (0.392)	Data 1.05e-04 (3.06e-03)	Tok/s 34745 (37613)	Loss/tok 3.2436 (3.4187)	LR 2.000e-03
0: TRAIN [1][110/3880]	Time 0.303 (0.391)	Data 1.12e-04 (2.79e-03)	Tok/s 34845 (37681)	Loss/tok 3.1937 (3.4182)	LR 2.000e-03
0: TRAIN [1][120/3880]	Time 0.203 (0.386)	Data 1.12e-04 (2.57e-03)	Tok/s 26298 (37509)	Loss/tok 2.7338 (3.4114)	LR 2.000e-03
0: TRAIN [1][130/3880]	Time 0.203 (0.382)	Data 1.11e-04 (2.38e-03)	Tok/s 26213 (37250)	Loss/tok 2.7150 (3.4059)	LR 2.000e-03
0: TRAIN [1][140/3880]	Time 0.421 (0.381)	Data 1.08e-04 (2.22e-03)	Tok/s 39450 (37364)	Loss/tok 3.3531 (3.4066)	LR 2.000e-03
0: TRAIN [1][150/3880]	Time 0.481 (0.377)	Data 1.10e-04 (2.08e-03)	Tok/s 34506 (37030)	Loss/tok 3.4146 (3.3972)	LR 2.000e-03
0: TRAIN [1][160/3880]	Time 0.312 (0.378)	Data 1.12e-04 (1.96e-03)	Tok/s 32409 (36961)	Loss/tok 3.1401 (3.3952)	LR 2.000e-03
0: TRAIN [1][170/3880]	Time 0.535 (0.383)	Data 1.24e-04 (1.85e-03)	Tok/s 30940 (36924)	Loss/tok 3.5192 (3.4004)	LR 2.000e-03
0: TRAIN [1][180/3880]	Time 0.209 (0.380)	Data 1.09e-04 (1.75e-03)	Tok/s 24715 (36639)	Loss/tok 2.7743 (3.3920)	LR 2.000e-03
0: TRAIN [1][190/3880]	Time 0.409 (0.379)	Data 1.10e-04 (1.67e-03)	Tok/s 41206 (36767)	Loss/tok 3.4725 (3.3900)	LR 2.000e-03
0: TRAIN [1][200/3880]	Time 0.630 (0.380)	Data 1.09e-04 (1.59e-03)	Tok/s 47294 (36841)	Loss/tok 3.7218 (3.3936)	LR 2.000e-03
0: TRAIN [1][210/3880]	Time 0.588 (0.382)	Data 1.12e-04 (1.52e-03)	Tok/s 39791 (36888)	Loss/tok 3.6056 (3.3985)	LR 2.000e-03
0: TRAIN [1][220/3880]	Time 0.399 (0.384)	Data 1.06e-04 (1.46e-03)	Tok/s 42170 (37034)	Loss/tok 3.3928 (3.4029)	LR 2.000e-03
0: TRAIN [1][230/3880]	Time 0.306 (0.385)	Data 1.07e-04 (1.40e-03)	Tok/s 33280 (37074)	Loss/tok 3.1301 (3.4045)	LR 2.000e-03
0: TRAIN [1][240/3880]	Time 0.303 (0.384)	Data 1.19e-04 (1.34e-03)	Tok/s 33827 (37079)	Loss/tok 3.1827 (3.4046)	LR 2.000e-03
0: TRAIN [1][250/3880]	Time 0.312 (0.386)	Data 1.09e-04 (1.30e-03)	Tok/s 33398 (37098)	Loss/tok 3.1594 (3.4099)	LR 2.000e-03
0: TRAIN [1][260/3880]	Time 0.304 (0.384)	Data 1.06e-04 (1.25e-03)	Tok/s 34260 (37059)	Loss/tok 3.1278 (3.4067)	LR 2.000e-03
0: TRAIN [1][270/3880]	Time 0.303 (0.383)	Data 1.08e-04 (1.21e-03)	Tok/s 33239 (37046)	Loss/tok 3.1857 (3.4045)	LR 2.000e-03
0: TRAIN [1][280/3880]	Time 0.431 (0.382)	Data 1.11e-04 (1.17e-03)	Tok/s 38993 (37083)	Loss/tok 3.3461 (3.4034)	LR 2.000e-03
0: TRAIN [1][290/3880]	Time 0.509 (0.383)	Data 1.09e-04 (1.13e-03)	Tok/s 45665 (37191)	Loss/tok 3.5812 (3.4048)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][300/3880]	Time 0.290 (0.383)	Data 1.08e-04 (1.10e-03)	Tok/s 35999 (37247)	Loss/tok 3.3072 (3.4052)	LR 2.000e-03
0: TRAIN [1][310/3880]	Time 0.518 (0.381)	Data 1.08e-04 (1.07e-03)	Tok/s 45062 (37124)	Loss/tok 3.6136 (3.4032)	LR 2.000e-03
0: TRAIN [1][320/3880]	Time 0.569 (0.381)	Data 1.10e-04 (1.04e-03)	Tok/s 41720 (36995)	Loss/tok 3.4941 (3.4010)	LR 2.000e-03
0: TRAIN [1][330/3880]	Time 0.513 (0.381)	Data 1.14e-04 (1.01e-03)	Tok/s 45383 (37037)	Loss/tok 3.5875 (3.3998)	LR 2.000e-03
0: TRAIN [1][340/3880]	Time 0.301 (0.380)	Data 1.10e-04 (9.82e-04)	Tok/s 34744 (37015)	Loss/tok 3.2319 (3.3991)	LR 2.000e-03
0: TRAIN [1][350/3880]	Time 0.424 (0.382)	Data 1.10e-04 (9.57e-04)	Tok/s 39832 (37016)	Loss/tok 3.2402 (3.4003)	LR 2.000e-03
0: TRAIN [1][360/3880]	Time 0.522 (0.382)	Data 1.10e-04 (9.34e-04)	Tok/s 44643 (37045)	Loss/tok 3.6088 (3.4014)	LR 2.000e-03
0: TRAIN [1][370/3880]	Time 0.407 (0.382)	Data 1.12e-04 (9.11e-04)	Tok/s 41340 (37057)	Loss/tok 3.3714 (3.4000)	LR 2.000e-03
0: TRAIN [1][380/3880]	Time 0.516 (0.381)	Data 1.10e-04 (8.90e-04)	Tok/s 44875 (37066)	Loss/tok 3.6054 (3.3978)	LR 2.000e-03
0: TRAIN [1][390/3880]	Time 0.301 (0.381)	Data 1.11e-04 (8.70e-04)	Tok/s 35001 (37080)	Loss/tok 3.1975 (3.3973)	LR 2.000e-03
0: TRAIN [1][400/3880]	Time 0.308 (0.380)	Data 1.17e-04 (8.51e-04)	Tok/s 33478 (37067)	Loss/tok 3.1984 (3.3957)	LR 2.000e-03
0: TRAIN [1][410/3880]	Time 0.406 (0.380)	Data 1.11e-04 (8.33e-04)	Tok/s 41522 (37091)	Loss/tok 3.3295 (3.3954)	LR 2.000e-03
0: TRAIN [1][420/3880]	Time 0.312 (0.379)	Data 1.09e-04 (8.16e-04)	Tok/s 32849 (37088)	Loss/tok 3.0931 (3.3929)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][430/3880]	Time 0.290 (0.379)	Data 1.09e-04 (8.00e-04)	Tok/s 35592 (37082)	Loss/tok 3.3529 (3.3943)	LR 2.000e-03
0: TRAIN [1][440/3880]	Time 0.307 (0.378)	Data 1.05e-04 (7.84e-04)	Tok/s 33816 (37061)	Loss/tok 3.1755 (3.3925)	LR 2.000e-03
0: TRAIN [1][450/3880]	Time 0.415 (0.377)	Data 1.13e-04 (7.69e-04)	Tok/s 40306 (37035)	Loss/tok 3.4235 (3.3920)	LR 2.000e-03
0: TRAIN [1][460/3880]	Time 0.526 (0.378)	Data 1.08e-04 (7.55e-04)	Tok/s 44000 (37072)	Loss/tok 3.6254 (3.3940)	LR 2.000e-03
0: TRAIN [1][470/3880]	Time 0.449 (0.379)	Data 1.08e-04 (7.41e-04)	Tok/s 37615 (37070)	Loss/tok 3.3614 (3.3944)	LR 2.000e-03
0: TRAIN [1][480/3880]	Time 0.345 (0.379)	Data 1.08e-04 (7.28e-04)	Tok/s 29737 (36959)	Loss/tok 3.1117 (3.3927)	LR 2.000e-03
0: TRAIN [1][490/3880]	Time 0.290 (0.379)	Data 1.10e-04 (7.15e-04)	Tok/s 35436 (36930)	Loss/tok 3.2021 (3.3929)	LR 2.000e-03
0: TRAIN [1][500/3880]	Time 0.297 (0.379)	Data 1.14e-04 (7.03e-04)	Tok/s 35029 (36953)	Loss/tok 3.2358 (3.3927)	LR 2.000e-03
0: TRAIN [1][510/3880]	Time 0.299 (0.379)	Data 1.13e-04 (6.91e-04)	Tok/s 35122 (37009)	Loss/tok 3.2419 (3.3935)	LR 2.000e-03
0: TRAIN [1][520/3880]	Time 0.364 (0.378)	Data 1.06e-04 (6.80e-04)	Tok/s 28338 (36918)	Loss/tok 3.1524 (3.3907)	LR 2.000e-03
0: TRAIN [1][530/3880]	Time 0.517 (0.378)	Data 1.07e-04 (6.69e-04)	Tok/s 44901 (36914)	Loss/tok 3.5295 (3.3905)	LR 2.000e-03
0: TRAIN [1][540/3880]	Time 0.299 (0.378)	Data 1.09e-04 (6.59e-04)	Tok/s 35276 (36960)	Loss/tok 3.1673 (3.3915)	LR 2.000e-03
0: TRAIN [1][550/3880]	Time 0.483 (0.379)	Data 1.10e-04 (6.50e-04)	Tok/s 34572 (36960)	Loss/tok 3.2799 (3.3927)	LR 2.000e-03
0: TRAIN [1][560/3880]	Time 0.417 (0.379)	Data 1.09e-04 (6.40e-04)	Tok/s 40753 (36928)	Loss/tok 3.4413 (3.3909)	LR 2.000e-03
0: TRAIN [1][570/3880]	Time 0.307 (0.379)	Data 1.07e-04 (6.31e-04)	Tok/s 33883 (36891)	Loss/tok 3.1754 (3.3905)	LR 2.000e-03
0: TRAIN [1][580/3880]	Time 0.670 (0.381)	Data 1.08e-04 (6.22e-04)	Tok/s 45006 (36986)	Loss/tok 3.7092 (3.3961)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][590/3880]	Time 0.458 (0.381)	Data 1.10e-04 (6.13e-04)	Tok/s 36597 (36910)	Loss/tok 3.5097 (3.3960)	LR 2.000e-03
0: TRAIN [1][600/3880]	Time 0.511 (0.381)	Data 1.04e-04 (6.05e-04)	Tok/s 45055 (36921)	Loss/tok 3.6109 (3.3972)	LR 2.000e-03
0: TRAIN [1][610/3880]	Time 0.205 (0.381)	Data 1.08e-04 (5.96e-04)	Tok/s 25822 (36928)	Loss/tok 2.7570 (3.3964)	LR 2.000e-03
0: TRAIN [1][620/3880]	Time 0.524 (0.381)	Data 1.16e-04 (5.89e-04)	Tok/s 44377 (36938)	Loss/tok 3.7467 (3.3958)	LR 2.000e-03
0: TRAIN [1][630/3880]	Time 0.517 (0.381)	Data 1.08e-04 (5.81e-04)	Tok/s 44885 (36931)	Loss/tok 3.6066 (3.3965)	LR 2.000e-03
0: TRAIN [1][640/3880]	Time 0.298 (0.381)	Data 1.40e-04 (5.74e-04)	Tok/s 34732 (36932)	Loss/tok 3.2654 (3.3979)	LR 2.000e-03
0: TRAIN [1][650/3880]	Time 0.330 (0.381)	Data 1.09e-04 (5.67e-04)	Tok/s 31613 (36960)	Loss/tok 3.1730 (3.3979)	LR 2.000e-03
0: TRAIN [1][660/3880]	Time 0.306 (0.381)	Data 1.08e-04 (5.60e-04)	Tok/s 32632 (36969)	Loss/tok 3.1577 (3.3980)	LR 2.000e-03
0: TRAIN [1][670/3880]	Time 0.303 (0.382)	Data 1.09e-04 (5.53e-04)	Tok/s 34786 (36996)	Loss/tok 3.1198 (3.3992)	LR 2.000e-03
0: TRAIN [1][680/3880]	Time 0.483 (0.382)	Data 1.08e-04 (5.46e-04)	Tok/s 34816 (36988)	Loss/tok 3.4395 (3.3998)	LR 2.000e-03
0: TRAIN [1][690/3880]	Time 0.322 (0.383)	Data 1.07e-04 (5.40e-04)	Tok/s 31648 (36948)	Loss/tok 3.1110 (3.3986)	LR 2.000e-03
0: TRAIN [1][700/3880]	Time 0.507 (0.382)	Data 1.08e-04 (5.34e-04)	Tok/s 45777 (36930)	Loss/tok 3.5058 (3.3973)	LR 2.000e-03
0: TRAIN [1][710/3880]	Time 0.317 (0.383)	Data 1.07e-04 (5.28e-04)	Tok/s 33178 (36938)	Loss/tok 3.1603 (3.3972)	LR 2.000e-03
0: TRAIN [1][720/3880]	Time 0.302 (0.382)	Data 1.09e-04 (5.22e-04)	Tok/s 34055 (36940)	Loss/tok 3.0102 (3.3964)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][730/3880]	Time 0.399 (0.383)	Data 1.07e-04 (5.16e-04)	Tok/s 41954 (36987)	Loss/tok 3.4919 (3.3972)	LR 2.000e-03
0: TRAIN [1][740/3880]	Time 0.636 (0.383)	Data 1.08e-04 (5.11e-04)	Tok/s 46997 (36967)	Loss/tok 3.6881 (3.3976)	LR 2.000e-03
0: TRAIN [1][750/3880]	Time 0.453 (0.384)	Data 1.07e-04 (5.06e-04)	Tok/s 36184 (36936)	Loss/tok 3.3878 (3.3979)	LR 2.000e-03
0: TRAIN [1][760/3880]	Time 0.430 (0.383)	Data 1.10e-04 (5.00e-04)	Tok/s 39305 (36923)	Loss/tok 3.4669 (3.3970)	LR 2.000e-03
0: TRAIN [1][770/3880]	Time 0.301 (0.383)	Data 1.05e-04 (4.95e-04)	Tok/s 34750 (36920)	Loss/tok 3.0783 (3.3970)	LR 2.000e-03
0: TRAIN [1][780/3880]	Time 0.440 (0.383)	Data 1.09e-04 (4.90e-04)	Tok/s 38333 (36899)	Loss/tok 3.3245 (3.3965)	LR 2.000e-03
0: TRAIN [1][790/3880]	Time 0.416 (0.382)	Data 1.17e-04 (4.85e-04)	Tok/s 40522 (36862)	Loss/tok 3.2996 (3.3957)	LR 2.000e-03
0: TRAIN [1][800/3880]	Time 0.204 (0.382)	Data 1.05e-04 (4.81e-04)	Tok/s 26118 (36841)	Loss/tok 2.6492 (3.3941)	LR 2.000e-03
0: TRAIN [1][810/3880]	Time 0.458 (0.381)	Data 1.11e-04 (4.76e-04)	Tok/s 36885 (36834)	Loss/tok 3.4007 (3.3944)	LR 2.000e-03
0: TRAIN [1][820/3880]	Time 0.298 (0.380)	Data 1.08e-04 (4.72e-04)	Tok/s 34425 (36789)	Loss/tok 3.2750 (3.3929)	LR 2.000e-03
0: TRAIN [1][830/3880]	Time 0.302 (0.381)	Data 1.15e-04 (4.67e-04)	Tok/s 34890 (36811)	Loss/tok 3.1189 (3.3945)	LR 2.000e-03
0: TRAIN [1][840/3880]	Time 0.368 (0.381)	Data 1.11e-04 (4.63e-04)	Tok/s 28380 (36846)	Loss/tok 3.2626 (3.3968)	LR 2.000e-03
0: TRAIN [1][850/3880]	Time 0.315 (0.383)	Data 1.11e-04 (4.59e-04)	Tok/s 32735 (36836)	Loss/tok 3.2616 (3.3984)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][860/3880]	Time 0.501 (0.383)	Data 1.08e-04 (4.55e-04)	Tok/s 46257 (36859)	Loss/tok 3.5695 (3.3991)	LR 2.000e-03
0: TRAIN [1][870/3880]	Time 0.648 (0.383)	Data 1.09e-04 (4.51e-04)	Tok/s 45499 (36860)	Loss/tok 3.7524 (3.3993)	LR 2.000e-03
0: TRAIN [1][880/3880]	Time 0.293 (0.382)	Data 1.04e-04 (4.47e-04)	Tok/s 35494 (36824)	Loss/tok 3.1864 (3.3981)	LR 2.000e-03
0: TRAIN [1][890/3880]	Time 0.300 (0.382)	Data 1.06e-04 (4.43e-04)	Tok/s 35018 (36815)	Loss/tok 3.2466 (3.3974)	LR 2.000e-03
0: TRAIN [1][900/3880]	Time 0.204 (0.381)	Data 1.15e-04 (4.39e-04)	Tok/s 25694 (36779)	Loss/tok 2.7520 (3.3961)	LR 2.000e-03
0: TRAIN [1][910/3880]	Time 0.305 (0.380)	Data 1.08e-04 (4.36e-04)	Tok/s 33656 (36761)	Loss/tok 3.2622 (3.3950)	LR 2.000e-03
0: TRAIN [1][920/3880]	Time 0.303 (0.380)	Data 1.08e-04 (4.32e-04)	Tok/s 33970 (36746)	Loss/tok 3.1052 (3.3943)	LR 2.000e-03
0: TRAIN [1][930/3880]	Time 0.401 (0.380)	Data 1.09e-04 (4.29e-04)	Tok/s 41197 (36768)	Loss/tok 3.4757 (3.3946)	LR 2.000e-03
0: TRAIN [1][940/3880]	Time 0.485 (0.381)	Data 1.11e-04 (4.26e-04)	Tok/s 35064 (36783)	Loss/tok 3.4003 (3.3963)	LR 2.000e-03
0: TRAIN [1][950/3880]	Time 0.307 (0.380)	Data 1.06e-04 (4.22e-04)	Tok/s 34000 (36736)	Loss/tok 3.0381 (3.3951)	LR 2.000e-03
0: TRAIN [1][960/3880]	Time 0.303 (0.380)	Data 1.09e-04 (4.19e-04)	Tok/s 33957 (36730)	Loss/tok 3.1335 (3.3944)	LR 2.000e-03
0: TRAIN [1][970/3880]	Time 0.519 (0.380)	Data 1.08e-04 (4.16e-04)	Tok/s 44889 (36760)	Loss/tok 3.5348 (3.3947)	LR 2.000e-03
0: TRAIN [1][980/3880]	Time 0.333 (0.380)	Data 1.11e-04 (4.13e-04)	Tok/s 29898 (36729)	Loss/tok 3.1953 (3.3943)	LR 2.000e-03
0: TRAIN [1][990/3880]	Time 0.298 (0.381)	Data 1.09e-04 (4.10e-04)	Tok/s 34312 (36730)	Loss/tok 3.1748 (3.3948)	LR 2.000e-03
0: TRAIN [1][1000/3880]	Time 0.425 (0.380)	Data 1.07e-04 (4.06e-04)	Tok/s 39492 (36713)	Loss/tok 3.4022 (3.3944)	LR 2.000e-03
0: TRAIN [1][1010/3880]	Time 0.304 (0.380)	Data 1.07e-04 (4.04e-04)	Tok/s 33892 (36718)	Loss/tok 3.2667 (3.3940)	LR 2.000e-03
0: TRAIN [1][1020/3880]	Time 0.318 (0.380)	Data 1.12e-04 (4.01e-04)	Tok/s 32569 (36701)	Loss/tok 3.1411 (3.3931)	LR 2.000e-03
0: TRAIN [1][1030/3880]	Time 0.300 (0.380)	Data 1.06e-04 (3.98e-04)	Tok/s 35185 (36692)	Loss/tok 3.1287 (3.3930)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1040/3880]	Time 0.662 (0.380)	Data 1.07e-04 (3.95e-04)	Tok/s 45124 (36693)	Loss/tok 3.7572 (3.3931)	LR 2.000e-03
0: TRAIN [1][1050/3880]	Time 0.411 (0.380)	Data 1.11e-04 (3.92e-04)	Tok/s 40886 (36710)	Loss/tok 3.3535 (3.3931)	LR 2.000e-03
0: TRAIN [1][1060/3880]	Time 0.494 (0.380)	Data 1.07e-04 (3.90e-04)	Tok/s 48220 (36733)	Loss/tok 3.4378 (3.3929)	LR 2.000e-03
0: TRAIN [1][1070/3880]	Time 0.314 (0.380)	Data 1.08e-04 (3.87e-04)	Tok/s 32482 (36747)	Loss/tok 3.0479 (3.3924)	LR 2.000e-03
0: TRAIN [1][1080/3880]	Time 0.315 (0.380)	Data 1.11e-04 (3.84e-04)	Tok/s 32806 (36764)	Loss/tok 3.1966 (3.3921)	LR 2.000e-03
0: TRAIN [1][1090/3880]	Time 0.317 (0.380)	Data 1.08e-04 (3.82e-04)	Tok/s 32559 (36762)	Loss/tok 3.1400 (3.3924)	LR 2.000e-03
0: TRAIN [1][1100/3880]	Time 0.288 (0.381)	Data 1.08e-04 (3.79e-04)	Tok/s 35593 (36782)	Loss/tok 3.2351 (3.3935)	LR 2.000e-03
0: TRAIN [1][1110/3880]	Time 0.427 (0.381)	Data 1.31e-04 (3.77e-04)	Tok/s 39391 (36746)	Loss/tok 3.3588 (3.3931)	LR 2.000e-03
0: TRAIN [1][1120/3880]	Time 0.400 (0.381)	Data 1.13e-04 (3.75e-04)	Tok/s 42029 (36757)	Loss/tok 3.3126 (3.3925)	LR 2.000e-03
0: TRAIN [1][1130/3880]	Time 0.412 (0.381)	Data 1.11e-04 (3.72e-04)	Tok/s 40801 (36776)	Loss/tok 3.3168 (3.3925)	LR 2.000e-03
0: TRAIN [1][1140/3880]	Time 0.301 (0.380)	Data 1.07e-04 (3.70e-04)	Tok/s 34191 (36762)	Loss/tok 3.2706 (3.3915)	LR 2.000e-03
0: TRAIN [1][1150/3880]	Time 0.435 (0.380)	Data 1.11e-04 (3.68e-04)	Tok/s 38001 (36770)	Loss/tok 3.4378 (3.3916)	LR 2.000e-03
0: TRAIN [1][1160/3880]	Time 0.208 (0.380)	Data 1.07e-04 (3.65e-04)	Tok/s 25372 (36743)	Loss/tok 2.7919 (3.3909)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1170/3880]	Time 0.403 (0.380)	Data 1.03e-04 (3.63e-04)	Tok/s 42351 (36743)	Loss/tok 3.4101 (3.3905)	LR 2.000e-03
0: TRAIN [1][1180/3880]	Time 0.394 (0.380)	Data 1.09e-04 (3.61e-04)	Tok/s 42308 (36756)	Loss/tok 3.3525 (3.3903)	LR 2.000e-03
0: TRAIN [1][1190/3880]	Time 0.212 (0.380)	Data 1.14e-04 (3.59e-04)	Tok/s 24703 (36745)	Loss/tok 2.7254 (3.3904)	LR 2.000e-03
0: TRAIN [1][1200/3880]	Time 0.351 (0.380)	Data 1.09e-04 (3.57e-04)	Tok/s 29854 (36742)	Loss/tok 3.0610 (3.3901)	LR 2.000e-03
0: TRAIN [1][1210/3880]	Time 0.302 (0.380)	Data 1.09e-04 (3.55e-04)	Tok/s 33166 (36732)	Loss/tok 3.1592 (3.3899)	LR 2.000e-03
0: TRAIN [1][1220/3880]	Time 0.299 (0.380)	Data 1.10e-04 (3.53e-04)	Tok/s 34365 (36753)	Loss/tok 3.1478 (3.3895)	LR 2.000e-03
0: TRAIN [1][1230/3880]	Time 0.302 (0.379)	Data 1.07e-04 (3.51e-04)	Tok/s 34019 (36735)	Loss/tok 3.1203 (3.3886)	LR 2.000e-03
0: TRAIN [1][1240/3880]	Time 0.304 (0.379)	Data 1.14e-04 (3.49e-04)	Tok/s 34415 (36743)	Loss/tok 3.2352 (3.3882)	LR 2.000e-03
0: TRAIN [1][1250/3880]	Time 0.529 (0.379)	Data 1.08e-04 (3.47e-04)	Tok/s 44147 (36743)	Loss/tok 3.5641 (3.3877)	LR 2.000e-03
0: TRAIN [1][1260/3880]	Time 0.204 (0.379)	Data 1.09e-04 (3.45e-04)	Tok/s 25831 (36746)	Loss/tok 2.7239 (3.3880)	LR 2.000e-03
0: TRAIN [1][1270/3880]	Time 0.346 (0.379)	Data 1.05e-04 (3.43e-04)	Tok/s 30203 (36726)	Loss/tok 3.1973 (3.3878)	LR 2.000e-03
0: TRAIN [1][1280/3880]	Time 0.300 (0.378)	Data 1.08e-04 (3.41e-04)	Tok/s 34108 (36689)	Loss/tok 3.2346 (3.3866)	LR 2.000e-03
0: TRAIN [1][1290/3880]	Time 0.396 (0.379)	Data 1.11e-04 (3.40e-04)	Tok/s 42319 (36728)	Loss/tok 3.3922 (3.3882)	LR 2.000e-03
0: TRAIN [1][1300/3880]	Time 0.326 (0.379)	Data 1.09e-04 (3.38e-04)	Tok/s 31709 (36693)	Loss/tok 3.0983 (3.3875)	LR 2.000e-03
0: TRAIN [1][1310/3880]	Time 0.404 (0.379)	Data 1.07e-04 (3.36e-04)	Tok/s 41464 (36702)	Loss/tok 3.4026 (3.3869)	LR 2.000e-03
0: TRAIN [1][1320/3880]	Time 0.290 (0.379)	Data 1.05e-04 (3.34e-04)	Tok/s 35302 (36695)	Loss/tok 3.1076 (3.3867)	LR 2.000e-03
0: TRAIN [1][1330/3880]	Time 0.209 (0.379)	Data 1.08e-04 (3.33e-04)	Tok/s 25074 (36688)	Loss/tok 2.6377 (3.3861)	LR 2.000e-03
0: TRAIN [1][1340/3880]	Time 0.625 (0.379)	Data 1.09e-04 (3.31e-04)	Tok/s 47519 (36687)	Loss/tok 3.8830 (3.3862)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1350/3880]	Time 0.299 (0.379)	Data 1.11e-04 (3.29e-04)	Tok/s 34389 (36680)	Loss/tok 3.1088 (3.3865)	LR 2.000e-03
0: TRAIN [1][1360/3880]	Time 0.405 (0.379)	Data 1.32e-04 (3.28e-04)	Tok/s 41959 (36708)	Loss/tok 3.3892 (3.3873)	LR 2.000e-03
0: TRAIN [1][1370/3880]	Time 0.578 (0.380)	Data 1.06e-04 (3.26e-04)	Tok/s 28662 (36720)	Loss/tok 3.3665 (3.3879)	LR 2.000e-03
0: TRAIN [1][1380/3880]	Time 0.298 (0.380)	Data 1.07e-04 (3.25e-04)	Tok/s 34919 (36699)	Loss/tok 3.1906 (3.3872)	LR 2.000e-03
0: TRAIN [1][1390/3880]	Time 0.464 (0.379)	Data 1.03e-04 (3.23e-04)	Tok/s 36084 (36683)	Loss/tok 3.3365 (3.3865)	LR 2.000e-03
0: TRAIN [1][1400/3880]	Time 0.293 (0.379)	Data 1.09e-04 (3.22e-04)	Tok/s 34993 (36671)	Loss/tok 3.0324 (3.3862)	LR 2.000e-03
0: TRAIN [1][1410/3880]	Time 0.647 (0.380)	Data 1.07e-04 (3.20e-04)	Tok/s 36665 (36679)	Loss/tok 3.4492 (3.3860)	LR 2.000e-03
0: TRAIN [1][1420/3880]	Time 0.411 (0.380)	Data 1.10e-04 (3.19e-04)	Tok/s 41177 (36679)	Loss/tok 3.2380 (3.3855)	LR 2.000e-03
0: TRAIN [1][1430/3880]	Time 0.296 (0.379)	Data 1.11e-04 (3.17e-04)	Tok/s 34168 (36671)	Loss/tok 3.1917 (3.3849)	LR 2.000e-03
0: TRAIN [1][1440/3880]	Time 0.303 (0.379)	Data 1.10e-04 (3.16e-04)	Tok/s 34462 (36677)	Loss/tok 3.0728 (3.3843)	LR 2.000e-03
0: TRAIN [1][1450/3880]	Time 0.407 (0.379)	Data 1.07e-04 (3.14e-04)	Tok/s 41197 (36686)	Loss/tok 3.3944 (3.3837)	LR 2.000e-03
0: TRAIN [1][1460/3880]	Time 0.205 (0.379)	Data 1.07e-04 (3.13e-04)	Tok/s 25554 (36669)	Loss/tok 2.6984 (3.3830)	LR 2.000e-03
0: TRAIN [1][1470/3880]	Time 0.292 (0.379)	Data 1.08e-04 (3.11e-04)	Tok/s 35791 (36680)	Loss/tok 3.3070 (3.3827)	LR 2.000e-03
0: TRAIN [1][1480/3880]	Time 0.335 (0.379)	Data 1.09e-04 (3.10e-04)	Tok/s 30989 (36664)	Loss/tok 3.2582 (3.3823)	LR 2.000e-03
0: TRAIN [1][1490/3880]	Time 0.299 (0.378)	Data 1.07e-04 (3.09e-04)	Tok/s 33603 (36655)	Loss/tok 3.2612 (3.3820)	LR 2.000e-03
0: TRAIN [1][1500/3880]	Time 0.305 (0.379)	Data 1.09e-04 (3.07e-04)	Tok/s 34756 (36659)	Loss/tok 3.0994 (3.3817)	LR 2.000e-03
0: TRAIN [1][1510/3880]	Time 0.426 (0.378)	Data 1.12e-04 (3.06e-04)	Tok/s 39213 (36663)	Loss/tok 3.3885 (3.3816)	LR 2.000e-03
0: TRAIN [1][1520/3880]	Time 0.307 (0.378)	Data 1.18e-04 (3.05e-04)	Tok/s 33839 (36668)	Loss/tok 3.1255 (3.3812)	LR 2.000e-03
0: TRAIN [1][1530/3880]	Time 0.542 (0.378)	Data 1.11e-04 (3.03e-04)	Tok/s 31139 (36660)	Loss/tok 3.2517 (3.3807)	LR 2.000e-03
0: TRAIN [1][1540/3880]	Time 0.312 (0.378)	Data 1.07e-04 (3.02e-04)	Tok/s 33023 (36634)	Loss/tok 3.2099 (3.3803)	LR 2.000e-03
0: TRAIN [1][1550/3880]	Time 0.412 (0.379)	Data 1.09e-04 (3.01e-04)	Tok/s 40505 (36647)	Loss/tok 3.4396 (3.3809)	LR 2.000e-03
0: TRAIN [1][1560/3880]	Time 0.296 (0.378)	Data 1.06e-04 (3.00e-04)	Tok/s 34334 (36657)	Loss/tok 3.2285 (3.3802)	LR 2.000e-03
0: TRAIN [1][1570/3880]	Time 0.423 (0.378)	Data 1.13e-04 (2.98e-04)	Tok/s 39400 (36672)	Loss/tok 3.3390 (3.3800)	LR 2.000e-03
0: TRAIN [1][1580/3880]	Time 0.204 (0.378)	Data 1.09e-04 (2.97e-04)	Tok/s 26044 (36669)	Loss/tok 2.7789 (3.3799)	LR 2.000e-03
0: TRAIN [1][1590/3880]	Time 0.425 (0.378)	Data 1.10e-04 (2.96e-04)	Tok/s 39745 (36662)	Loss/tok 3.3800 (3.3794)	LR 2.000e-03
0: TRAIN [1][1600/3880]	Time 0.206 (0.378)	Data 1.11e-04 (2.95e-04)	Tok/s 25643 (36669)	Loss/tok 2.6664 (3.3796)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1610/3880]	Time 0.425 (0.378)	Data 1.12e-04 (2.94e-04)	Tok/s 39918 (36684)	Loss/tok 3.2628 (3.3800)	LR 2.000e-03
0: TRAIN [1][1620/3880]	Time 0.543 (0.379)	Data 1.08e-04 (2.93e-04)	Tok/s 43010 (36698)	Loss/tok 3.6206 (3.3798)	LR 2.000e-03
0: TRAIN [1][1630/3880]	Time 0.501 (0.379)	Data 1.06e-04 (2.91e-04)	Tok/s 45715 (36719)	Loss/tok 3.6403 (3.3801)	LR 2.000e-03
0: TRAIN [1][1640/3880]	Time 0.308 (0.379)	Data 1.08e-04 (2.90e-04)	Tok/s 33451 (36719)	Loss/tok 3.1794 (3.3801)	LR 2.000e-03
0: TRAIN [1][1650/3880]	Time 0.395 (0.379)	Data 1.06e-04 (2.89e-04)	Tok/s 42616 (36726)	Loss/tok 3.3191 (3.3800)	LR 2.000e-03
0: TRAIN [1][1660/3880]	Time 0.300 (0.379)	Data 1.10e-04 (2.88e-04)	Tok/s 33615 (36735)	Loss/tok 3.0805 (3.3799)	LR 2.000e-03
0: TRAIN [1][1670/3880]	Time 0.312 (0.379)	Data 1.10e-04 (2.87e-04)	Tok/s 32867 (36729)	Loss/tok 3.2706 (3.3803)	LR 2.000e-03
0: TRAIN [1][1680/3880]	Time 0.301 (0.379)	Data 1.09e-04 (2.86e-04)	Tok/s 34061 (36713)	Loss/tok 3.1772 (3.3795)	LR 2.000e-03
0: TRAIN [1][1690/3880]	Time 0.211 (0.379)	Data 1.06e-04 (2.85e-04)	Tok/s 25375 (36707)	Loss/tok 2.7298 (3.3795)	LR 2.000e-03
0: TRAIN [1][1700/3880]	Time 0.503 (0.379)	Data 1.08e-04 (2.84e-04)	Tok/s 46249 (36726)	Loss/tok 3.6722 (3.3801)	LR 2.000e-03
0: TRAIN [1][1710/3880]	Time 0.289 (0.379)	Data 1.05e-04 (2.83e-04)	Tok/s 34913 (36738)	Loss/tok 3.1408 (3.3801)	LR 2.000e-03
0: TRAIN [1][1720/3880]	Time 0.317 (0.379)	Data 1.11e-04 (2.82e-04)	Tok/s 32607 (36725)	Loss/tok 3.0985 (3.3798)	LR 2.000e-03
0: TRAIN [1][1730/3880]	Time 0.638 (0.379)	Data 1.03e-04 (2.81e-04)	Tok/s 46608 (36746)	Loss/tok 3.7136 (3.3809)	LR 2.000e-03
0: TRAIN [1][1740/3880]	Time 0.403 (0.379)	Data 1.10e-04 (2.80e-04)	Tok/s 41815 (36743)	Loss/tok 3.3655 (3.3805)	LR 2.000e-03
0: TRAIN [1][1750/3880]	Time 0.300 (0.379)	Data 1.08e-04 (2.79e-04)	Tok/s 34271 (36724)	Loss/tok 3.1251 (3.3796)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1760/3880]	Time 0.306 (0.379)	Data 1.11e-04 (2.78e-04)	Tok/s 33698 (36736)	Loss/tok 3.1041 (3.3802)	LR 2.000e-03
0: TRAIN [1][1770/3880]	Time 0.339 (0.379)	Data 1.11e-04 (2.77e-04)	Tok/s 31250 (36732)	Loss/tok 3.1801 (3.3800)	LR 2.000e-03
0: TRAIN [1][1780/3880]	Time 0.314 (0.379)	Data 1.05e-04 (2.76e-04)	Tok/s 32326 (36722)	Loss/tok 3.1533 (3.3795)	LR 2.000e-03
0: TRAIN [1][1790/3880]	Time 0.308 (0.379)	Data 1.10e-04 (2.75e-04)	Tok/s 33691 (36727)	Loss/tok 3.0814 (3.3790)	LR 2.000e-03
0: TRAIN [1][1800/3880]	Time 0.212 (0.379)	Data 1.07e-04 (2.74e-04)	Tok/s 24658 (36728)	Loss/tok 2.6863 (3.3786)	LR 2.000e-03
0: TRAIN [1][1810/3880]	Time 0.350 (0.379)	Data 1.07e-04 (2.73e-04)	Tok/s 30172 (36725)	Loss/tok 3.1082 (3.3787)	LR 2.000e-03
0: TRAIN [1][1820/3880]	Time 0.428 (0.379)	Data 1.09e-04 (2.72e-04)	Tok/s 38768 (36724)	Loss/tok 3.2766 (3.3786)	LR 2.000e-03
0: TRAIN [1][1830/3880]	Time 0.297 (0.379)	Data 1.10e-04 (2.71e-04)	Tok/s 34786 (36731)	Loss/tok 3.1218 (3.3785)	LR 2.000e-03
0: TRAIN [1][1840/3880]	Time 0.467 (0.379)	Data 1.09e-04 (2.71e-04)	Tok/s 36048 (36727)	Loss/tok 3.3257 (3.3783)	LR 2.000e-03
0: TRAIN [1][1850/3880]	Time 0.419 (0.379)	Data 1.05e-04 (2.70e-04)	Tok/s 39434 (36738)	Loss/tok 3.3388 (3.3784)	LR 2.000e-03
0: TRAIN [1][1860/3880]	Time 0.302 (0.380)	Data 1.08e-04 (2.69e-04)	Tok/s 34395 (36758)	Loss/tok 3.1219 (3.3791)	LR 2.000e-03
0: TRAIN [1][1870/3880]	Time 0.298 (0.380)	Data 1.08e-04 (2.68e-04)	Tok/s 35088 (36768)	Loss/tok 3.1334 (3.3795)	LR 2.000e-03
0: TRAIN [1][1880/3880]	Time 0.338 (0.380)	Data 1.10e-04 (2.67e-04)	Tok/s 30968 (36768)	Loss/tok 3.0942 (3.3789)	LR 2.000e-03
0: TRAIN [1][1890/3880]	Time 0.291 (0.380)	Data 1.09e-04 (2.66e-04)	Tok/s 34973 (36773)	Loss/tok 3.2535 (3.3789)	LR 2.000e-03
0: TRAIN [1][1900/3880]	Time 0.319 (0.380)	Data 1.09e-04 (2.65e-04)	Tok/s 32896 (36771)	Loss/tok 3.1646 (3.3787)	LR 2.000e-03
0: TRAIN [1][1910/3880]	Time 0.611 (0.380)	Data 1.05e-04 (2.65e-04)	Tok/s 49144 (36766)	Loss/tok 3.6861 (3.3791)	LR 2.000e-03
0: TRAIN [1][1920/3880]	Time 0.313 (0.380)	Data 1.14e-04 (2.64e-04)	Tok/s 33607 (36773)	Loss/tok 3.2794 (3.3791)	LR 2.000e-03
0: TRAIN [1][1930/3880]	Time 0.411 (0.380)	Data 1.07e-04 (2.63e-04)	Tok/s 40967 (36776)	Loss/tok 3.3337 (3.3788)	LR 2.000e-03
0: TRAIN [1][1940/3880]	Time 0.501 (0.380)	Data 1.10e-04 (2.62e-04)	Tok/s 46254 (36789)	Loss/tok 3.5985 (3.3791)	LR 2.000e-03
0: TRAIN [1][1950/3880]	Time 0.205 (0.380)	Data 1.03e-04 (2.61e-04)	Tok/s 25348 (36784)	Loss/tok 2.6116 (3.3789)	LR 2.000e-03
0: TRAIN [1][1960/3880]	Time 0.524 (0.380)	Data 1.08e-04 (2.61e-04)	Tok/s 44960 (36779)	Loss/tok 3.4547 (3.3786)	LR 2.000e-03
0: TRAIN [1][1970/3880]	Time 0.520 (0.379)	Data 1.08e-04 (2.60e-04)	Tok/s 44757 (36764)	Loss/tok 3.5904 (3.3780)	LR 2.000e-03
0: TRAIN [1][1980/3880]	Time 0.514 (0.379)	Data 1.06e-04 (2.59e-04)	Tok/s 45414 (36755)	Loss/tok 3.4587 (3.3773)	LR 2.000e-03
0: TRAIN [1][1990/3880]	Time 0.452 (0.379)	Data 1.08e-04 (2.58e-04)	Tok/s 37453 (36757)	Loss/tok 3.3366 (3.3769)	LR 2.000e-03
0: TRAIN [1][2000/3880]	Time 0.616 (0.379)	Data 1.12e-04 (2.58e-04)	Tok/s 48512 (36756)	Loss/tok 3.6389 (3.3771)	LR 2.000e-03
0: TRAIN [1][2010/3880]	Time 0.431 (0.379)	Data 1.27e-04 (2.57e-04)	Tok/s 38955 (36732)	Loss/tok 3.3802 (3.3766)	LR 2.000e-03
0: TRAIN [1][2020/3880]	Time 0.291 (0.379)	Data 1.08e-04 (2.56e-04)	Tok/s 35319 (36718)	Loss/tok 3.0733 (3.3760)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][2030/3880]	Time 0.294 (0.379)	Data 1.14e-04 (2.55e-04)	Tok/s 35667 (36725)	Loss/tok 3.1377 (3.3757)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2040/3880]	Time 0.855 (0.379)	Data 1.11e-04 (2.55e-04)	Tok/s 35120 (36720)	Loss/tok 3.7075 (3.3758)	LR 2.000e-03
0: TRAIN [1][2050/3880]	Time 0.660 (0.379)	Data 1.06e-04 (2.54e-04)	Tok/s 44747 (36722)	Loss/tok 3.8062 (3.3761)	LR 2.000e-03
0: TRAIN [1][2060/3880]	Time 0.507 (0.379)	Data 1.15e-04 (2.53e-04)	Tok/s 45399 (36728)	Loss/tok 3.5102 (3.3762)	LR 2.000e-03
0: TRAIN [1][2070/3880]	Time 0.385 (0.379)	Data 1.07e-04 (2.53e-04)	Tok/s 43155 (36740)	Loss/tok 3.4009 (3.3761)	LR 2.000e-03
0: TRAIN [1][2080/3880]	Time 0.573 (0.379)	Data 1.10e-04 (2.52e-04)	Tok/s 40880 (36732)	Loss/tok 3.5422 (3.3762)	LR 2.000e-03
0: TRAIN [1][2090/3880]	Time 0.359 (0.379)	Data 1.07e-04 (2.51e-04)	Tok/s 29012 (36726)	Loss/tok 3.1024 (3.3758)	LR 2.000e-03
0: TRAIN [1][2100/3880]	Time 0.307 (0.379)	Data 1.06e-04 (2.51e-04)	Tok/s 32484 (36699)	Loss/tok 3.2408 (3.3750)	LR 2.000e-03
0: TRAIN [1][2110/3880]	Time 0.513 (0.379)	Data 1.09e-04 (2.50e-04)	Tok/s 45507 (36710)	Loss/tok 3.5401 (3.3748)	LR 2.000e-03
0: TRAIN [1][2120/3880]	Time 0.303 (0.379)	Data 1.09e-04 (2.49e-04)	Tok/s 34221 (36713)	Loss/tok 3.1066 (3.3748)	LR 2.000e-03
0: TRAIN [1][2130/3880]	Time 0.406 (0.379)	Data 1.10e-04 (2.49e-04)	Tok/s 41851 (36718)	Loss/tok 3.4168 (3.3746)	LR 2.000e-03
0: TRAIN [1][2140/3880]	Time 0.483 (0.379)	Data 1.07e-04 (2.48e-04)	Tok/s 34809 (36708)	Loss/tok 3.4780 (3.3741)	LR 2.000e-03
0: TRAIN [1][2150/3880]	Time 0.423 (0.379)	Data 1.07e-04 (2.47e-04)	Tok/s 39706 (36708)	Loss/tok 3.2185 (3.3738)	LR 2.000e-03
0: TRAIN [1][2160/3880]	Time 0.300 (0.379)	Data 1.10e-04 (2.47e-04)	Tok/s 34260 (36702)	Loss/tok 3.1082 (3.3731)	LR 2.000e-03
0: TRAIN [1][2170/3880]	Time 0.639 (0.378)	Data 1.08e-04 (2.46e-04)	Tok/s 46845 (36702)	Loss/tok 3.6298 (3.3728)	LR 2.000e-03
0: TRAIN [1][2180/3880]	Time 0.513 (0.378)	Data 1.11e-04 (2.45e-04)	Tok/s 45153 (36701)	Loss/tok 3.4820 (3.3724)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2190/3880]	Time 0.403 (0.379)	Data 1.09e-04 (2.45e-04)	Tok/s 41661 (36721)	Loss/tok 3.3531 (3.3725)	LR 2.000e-03
0: TRAIN [1][2200/3880]	Time 0.310 (0.379)	Data 1.07e-04 (2.44e-04)	Tok/s 33377 (36722)	Loss/tok 3.2451 (3.3726)	LR 2.000e-03
0: TRAIN [1][2210/3880]	Time 0.332 (0.379)	Data 1.10e-04 (2.43e-04)	Tok/s 31417 (36712)	Loss/tok 3.0646 (3.3729)	LR 2.000e-03
0: TRAIN [1][2220/3880]	Time 0.403 (0.379)	Data 1.13e-04 (2.43e-04)	Tok/s 41546 (36727)	Loss/tok 3.2918 (3.3735)	LR 2.000e-03
0: TRAIN [1][2230/3880]	Time 0.661 (0.379)	Data 1.08e-04 (2.42e-04)	Tok/s 45464 (36734)	Loss/tok 3.6024 (3.3739)	LR 2.000e-03
0: TRAIN [1][2240/3880]	Time 0.306 (0.380)	Data 1.11e-04 (2.42e-04)	Tok/s 34068 (36746)	Loss/tok 2.9793 (3.3737)	LR 2.000e-03
0: TRAIN [1][2250/3880]	Time 0.211 (0.380)	Data 1.09e-04 (2.41e-04)	Tok/s 25322 (36753)	Loss/tok 2.6504 (3.3737)	LR 2.000e-03
0: TRAIN [1][2260/3880]	Time 0.329 (0.380)	Data 1.10e-04 (2.41e-04)	Tok/s 30563 (36734)	Loss/tok 3.1360 (3.3736)	LR 2.000e-03
0: TRAIN [1][2270/3880]	Time 0.297 (0.380)	Data 1.09e-04 (2.40e-04)	Tok/s 35176 (36739)	Loss/tok 3.2184 (3.3738)	LR 2.000e-03
0: TRAIN [1][2280/3880]	Time 0.303 (0.380)	Data 1.28e-04 (2.39e-04)	Tok/s 33917 (36741)	Loss/tok 3.0484 (3.3735)	LR 2.000e-03
0: TRAIN [1][2290/3880]	Time 0.410 (0.380)	Data 1.12e-04 (2.39e-04)	Tok/s 41560 (36749)	Loss/tok 3.4032 (3.3732)	LR 2.000e-03
0: TRAIN [1][2300/3880]	Time 0.296 (0.380)	Data 1.05e-04 (2.38e-04)	Tok/s 35102 (36744)	Loss/tok 3.1295 (3.3727)	LR 2.000e-03
0: TRAIN [1][2310/3880]	Time 0.304 (0.380)	Data 1.14e-04 (2.38e-04)	Tok/s 33615 (36762)	Loss/tok 3.1039 (3.3731)	LR 2.000e-03
0: TRAIN [1][2320/3880]	Time 0.421 (0.380)	Data 1.09e-04 (2.37e-04)	Tok/s 39922 (36769)	Loss/tok 3.3829 (3.3735)	LR 2.000e-03
0: TRAIN [1][2330/3880]	Time 0.209 (0.380)	Data 1.08e-04 (2.37e-04)	Tok/s 24828 (36757)	Loss/tok 2.6855 (3.3729)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2340/3880]	Time 0.212 (0.380)	Data 1.10e-04 (2.36e-04)	Tok/s 24972 (36755)	Loss/tok 2.7866 (3.3731)	LR 2.000e-03
0: TRAIN [1][2350/3880]	Time 0.403 (0.380)	Data 1.09e-04 (2.35e-04)	Tok/s 41575 (36767)	Loss/tok 3.3389 (3.3734)	LR 2.000e-03
0: TRAIN [1][2360/3880]	Time 0.300 (0.380)	Data 1.11e-04 (2.35e-04)	Tok/s 34232 (36764)	Loss/tok 3.2400 (3.3729)	LR 2.000e-03
0: TRAIN [1][2370/3880]	Time 0.453 (0.380)	Data 1.12e-04 (2.34e-04)	Tok/s 36747 (36774)	Loss/tok 3.3176 (3.3733)	LR 2.000e-03
0: TRAIN [1][2380/3880]	Time 0.410 (0.380)	Data 1.08e-04 (2.34e-04)	Tok/s 41140 (36778)	Loss/tok 3.3731 (3.3732)	LR 2.000e-03
0: TRAIN [1][2390/3880]	Time 0.303 (0.380)	Data 1.04e-04 (2.33e-04)	Tok/s 34360 (36773)	Loss/tok 3.0645 (3.3726)	LR 2.000e-03
0: TRAIN [1][2400/3880]	Time 0.382 (0.380)	Data 1.16e-04 (2.33e-04)	Tok/s 27141 (36772)	Loss/tok 3.1559 (3.3729)	LR 2.000e-03
0: TRAIN [1][2410/3880]	Time 0.506 (0.380)	Data 1.12e-04 (2.32e-04)	Tok/s 45975 (36773)	Loss/tok 3.5661 (3.3733)	LR 2.000e-03
0: TRAIN [1][2420/3880]	Time 0.318 (0.381)	Data 1.09e-04 (2.32e-04)	Tok/s 32083 (36782)	Loss/tok 3.1008 (3.3735)	LR 2.000e-03
0: TRAIN [1][2430/3880]	Time 0.506 (0.381)	Data 1.30e-04 (2.31e-04)	Tok/s 46538 (36792)	Loss/tok 3.5477 (3.3733)	LR 2.000e-03
0: TRAIN [1][2440/3880]	Time 0.409 (0.381)	Data 1.12e-04 (2.31e-04)	Tok/s 41435 (36794)	Loss/tok 3.3633 (3.3732)	LR 2.000e-03
0: TRAIN [1][2450/3880]	Time 0.332 (0.381)	Data 1.10e-04 (2.30e-04)	Tok/s 31803 (36791)	Loss/tok 3.0908 (3.3730)	LR 2.000e-03
0: TRAIN [1][2460/3880]	Time 0.502 (0.381)	Data 1.10e-04 (2.30e-04)	Tok/s 46666 (36793)	Loss/tok 3.5292 (3.3730)	LR 2.000e-03
0: TRAIN [1][2470/3880]	Time 0.300 (0.381)	Data 1.11e-04 (2.29e-04)	Tok/s 34661 (36797)	Loss/tok 3.1964 (3.3730)	LR 2.000e-03
0: TRAIN [1][2480/3880]	Time 0.211 (0.381)	Data 1.13e-04 (2.29e-04)	Tok/s 25145 (36785)	Loss/tok 2.6540 (3.3733)	LR 2.000e-03
0: TRAIN [1][2490/3880]	Time 0.200 (0.381)	Data 1.08e-04 (2.28e-04)	Tok/s 25660 (36771)	Loss/tok 2.6712 (3.3727)	LR 2.000e-03
0: TRAIN [1][2500/3880]	Time 0.530 (0.381)	Data 1.07e-04 (2.28e-04)	Tok/s 43849 (36782)	Loss/tok 3.5324 (3.3729)	LR 2.000e-03
0: TRAIN [1][2510/3880]	Time 0.298 (0.381)	Data 1.12e-04 (2.27e-04)	Tok/s 33838 (36786)	Loss/tok 3.1782 (3.3730)	LR 2.000e-03
0: TRAIN [1][2520/3880]	Time 0.402 (0.381)	Data 1.11e-04 (2.27e-04)	Tok/s 41631 (36797)	Loss/tok 3.2643 (3.3728)	LR 2.000e-03
0: TRAIN [1][2530/3880]	Time 0.313 (0.381)	Data 1.06e-04 (2.26e-04)	Tok/s 33241 (36784)	Loss/tok 3.1910 (3.3723)	LR 2.000e-03
0: TRAIN [1][2540/3880]	Time 0.402 (0.381)	Data 1.13e-04 (2.26e-04)	Tok/s 42329 (36789)	Loss/tok 3.3203 (3.3719)	LR 2.000e-03
0: TRAIN [1][2550/3880]	Time 0.300 (0.381)	Data 1.06e-04 (2.26e-04)	Tok/s 34224 (36786)	Loss/tok 3.0830 (3.3718)	LR 2.000e-03
0: TRAIN [1][2560/3880]	Time 0.410 (0.381)	Data 1.09e-04 (2.25e-04)	Tok/s 40558 (36792)	Loss/tok 3.3698 (3.3717)	LR 2.000e-03
0: TRAIN [1][2570/3880]	Time 0.300 (0.381)	Data 1.07e-04 (2.25e-04)	Tok/s 34826 (36788)	Loss/tok 3.2051 (3.3712)	LR 2.000e-03
0: TRAIN [1][2580/3880]	Time 0.417 (0.381)	Data 1.10e-04 (2.24e-04)	Tok/s 40630 (36798)	Loss/tok 3.2174 (3.3711)	LR 2.000e-03
0: TRAIN [1][2590/3880]	Time 0.391 (0.381)	Data 1.09e-04 (2.24e-04)	Tok/s 43391 (36800)	Loss/tok 3.3672 (3.3708)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2600/3880]	Time 0.511 (0.380)	Data 1.03e-04 (2.23e-04)	Tok/s 45672 (36786)	Loss/tok 3.4957 (3.3703)	LR 2.000e-03
0: TRAIN [1][2610/3880]	Time 0.300 (0.380)	Data 1.07e-04 (2.23e-04)	Tok/s 34232 (36782)	Loss/tok 3.1246 (3.3700)	LR 1.000e-03
0: TRAIN [1][2620/3880]	Time 0.415 (0.380)	Data 1.31e-04 (2.22e-04)	Tok/s 40479 (36774)	Loss/tok 3.3508 (3.3695)	LR 1.000e-03
0: TRAIN [1][2630/3880]	Time 0.219 (0.380)	Data 1.09e-04 (2.22e-04)	Tok/s 23746 (36768)	Loss/tok 2.5320 (3.3694)	LR 1.000e-03
0: TRAIN [1][2640/3880]	Time 0.325 (0.380)	Data 1.12e-04 (2.22e-04)	Tok/s 31255 (36765)	Loss/tok 3.1207 (3.3693)	LR 1.000e-03
0: TRAIN [1][2650/3880]	Time 0.426 (0.380)	Data 1.07e-04 (2.21e-04)	Tok/s 39851 (36759)	Loss/tok 3.3486 (3.3690)	LR 1.000e-03
0: TRAIN [1][2660/3880]	Time 0.614 (0.380)	Data 1.07e-04 (2.21e-04)	Tok/s 48078 (36767)	Loss/tok 3.6782 (3.3694)	LR 1.000e-03
0: TRAIN [1][2670/3880]	Time 0.536 (0.380)	Data 1.06e-04 (2.20e-04)	Tok/s 42688 (36760)	Loss/tok 3.5730 (3.3692)	LR 1.000e-03
0: TRAIN [1][2680/3880]	Time 0.422 (0.381)	Data 1.07e-04 (2.20e-04)	Tok/s 40021 (36763)	Loss/tok 3.3623 (3.3691)	LR 1.000e-03
0: TRAIN [1][2690/3880]	Time 0.305 (0.381)	Data 1.10e-04 (2.20e-04)	Tok/s 33474 (36763)	Loss/tok 3.1036 (3.3688)	LR 1.000e-03
0: TRAIN [1][2700/3880]	Time 0.289 (0.381)	Data 1.15e-04 (2.19e-04)	Tok/s 35655 (36770)	Loss/tok 3.1100 (3.3688)	LR 1.000e-03
0: TRAIN [1][2710/3880]	Time 0.304 (0.381)	Data 1.08e-04 (2.19e-04)	Tok/s 33792 (36769)	Loss/tok 3.1619 (3.3686)	LR 1.000e-03
0: TRAIN [1][2720/3880]	Time 0.315 (0.381)	Data 1.08e-04 (2.18e-04)	Tok/s 32270 (36781)	Loss/tok 2.9773 (3.3688)	LR 1.000e-03
0: TRAIN [1][2730/3880]	Time 0.390 (0.381)	Data 1.08e-04 (2.18e-04)	Tok/s 42794 (36783)	Loss/tok 3.3983 (3.3685)	LR 1.000e-03
0: TRAIN [1][2740/3880]	Time 0.414 (0.381)	Data 1.06e-04 (2.18e-04)	Tok/s 40945 (36782)	Loss/tok 3.2592 (3.3684)	LR 1.000e-03
0: TRAIN [1][2750/3880]	Time 0.516 (0.381)	Data 1.08e-04 (2.17e-04)	Tok/s 44970 (36783)	Loss/tok 3.4453 (3.3683)	LR 1.000e-03
0: TRAIN [1][2760/3880]	Time 0.586 (0.381)	Data 1.12e-04 (2.17e-04)	Tok/s 40087 (36783)	Loss/tok 3.5201 (3.3685)	LR 1.000e-03
0: TRAIN [1][2770/3880]	Time 0.285 (0.381)	Data 1.09e-04 (2.16e-04)	Tok/s 36746 (36779)	Loss/tok 3.2521 (3.3681)	LR 1.000e-03
0: TRAIN [1][2780/3880]	Time 0.316 (0.381)	Data 1.08e-04 (2.16e-04)	Tok/s 32839 (36764)	Loss/tok 3.2216 (3.3677)	LR 1.000e-03
0: TRAIN [1][2790/3880]	Time 0.308 (0.381)	Data 1.08e-04 (2.16e-04)	Tok/s 33024 (36764)	Loss/tok 3.0742 (3.3674)	LR 1.000e-03
0: TRAIN [1][2800/3880]	Time 0.299 (0.381)	Data 1.10e-04 (2.15e-04)	Tok/s 35045 (36773)	Loss/tok 3.1250 (3.3677)	LR 1.000e-03
0: TRAIN [1][2810/3880]	Time 0.309 (0.381)	Data 1.12e-04 (2.15e-04)	Tok/s 32723 (36772)	Loss/tok 3.1347 (3.3676)	LR 1.000e-03
0: TRAIN [1][2820/3880]	Time 0.204 (0.381)	Data 1.07e-04 (2.14e-04)	Tok/s 25287 (36767)	Loss/tok 2.5887 (3.3671)	LR 1.000e-03
0: TRAIN [1][2830/3880]	Time 0.294 (0.381)	Data 1.10e-04 (2.14e-04)	Tok/s 34761 (36776)	Loss/tok 3.0694 (3.3670)	LR 1.000e-03
0: TRAIN [1][2840/3880]	Time 0.324 (0.381)	Data 1.10e-04 (2.14e-04)	Tok/s 32166 (36773)	Loss/tok 3.0197 (3.3670)	LR 1.000e-03
0: TRAIN [1][2850/3880]	Time 0.214 (0.381)	Data 1.19e-04 (2.13e-04)	Tok/s 24554 (36769)	Loss/tok 2.6613 (3.3664)	LR 1.000e-03
0: TRAIN [1][2860/3880]	Time 0.298 (0.381)	Data 1.09e-04 (2.13e-04)	Tok/s 35498 (36778)	Loss/tok 3.1476 (3.3664)	LR 1.000e-03
0: TRAIN [1][2870/3880]	Time 0.304 (0.381)	Data 1.08e-04 (2.13e-04)	Tok/s 34516 (36769)	Loss/tok 3.1379 (3.3657)	LR 1.000e-03
0: TRAIN [1][2880/3880]	Time 0.306 (0.381)	Data 1.05e-04 (2.12e-04)	Tok/s 34159 (36770)	Loss/tok 3.0435 (3.3654)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][2890/3880]	Time 0.290 (0.381)	Data 1.11e-04 (2.12e-04)	Tok/s 34888 (36773)	Loss/tok 3.1065 (3.3650)	LR 1.000e-03
0: TRAIN [1][2900/3880]	Time 0.313 (0.381)	Data 1.07e-04 (2.12e-04)	Tok/s 33178 (36774)	Loss/tok 3.2458 (3.3649)	LR 1.000e-03
0: TRAIN [1][2910/3880]	Time 0.398 (0.381)	Data 1.10e-04 (2.11e-04)	Tok/s 42356 (36779)	Loss/tok 3.3058 (3.3646)	LR 1.000e-03
0: TRAIN [1][2920/3880]	Time 0.520 (0.381)	Data 1.12e-04 (2.11e-04)	Tok/s 32267 (36783)	Loss/tok 3.3098 (3.3642)	LR 1.000e-03
0: TRAIN [1][2930/3880]	Time 0.430 (0.381)	Data 1.06e-04 (2.10e-04)	Tok/s 38657 (36779)	Loss/tok 3.4415 (3.3643)	LR 1.000e-03
0: TRAIN [1][2940/3880]	Time 0.302 (0.381)	Data 1.07e-04 (2.10e-04)	Tok/s 34031 (36776)	Loss/tok 3.1219 (3.3639)	LR 1.000e-03
0: TRAIN [1][2950/3880]	Time 0.290 (0.381)	Data 1.15e-04 (2.10e-04)	Tok/s 36645 (36784)	Loss/tok 3.1157 (3.3634)	LR 1.000e-03
0: TRAIN [1][2960/3880]	Time 0.412 (0.381)	Data 1.07e-04 (2.09e-04)	Tok/s 40251 (36780)	Loss/tok 3.2498 (3.3629)	LR 1.000e-03
0: TRAIN [1][2970/3880]	Time 0.506 (0.381)	Data 1.08e-04 (2.09e-04)	Tok/s 46707 (36795)	Loss/tok 3.5142 (3.3630)	LR 1.000e-03
0: TRAIN [1][2980/3880]	Time 0.400 (0.381)	Data 1.08e-04 (2.09e-04)	Tok/s 41758 (36793)	Loss/tok 3.3116 (3.3624)	LR 1.000e-03
0: TRAIN [1][2990/3880]	Time 0.576 (0.381)	Data 1.09e-04 (2.08e-04)	Tok/s 40716 (36788)	Loss/tok 3.4772 (3.3623)	LR 1.000e-03
0: TRAIN [1][3000/3880]	Time 0.200 (0.381)	Data 1.07e-04 (2.08e-04)	Tok/s 26143 (36786)	Loss/tok 2.6344 (3.3620)	LR 1.000e-03
0: TRAIN [1][3010/3880]	Time 0.536 (0.381)	Data 1.08e-04 (2.08e-04)	Tok/s 43229 (36797)	Loss/tok 3.5591 (3.3618)	LR 1.000e-03
0: TRAIN [1][3020/3880]	Time 0.398 (0.381)	Data 1.07e-04 (2.07e-04)	Tok/s 42549 (36802)	Loss/tok 3.1851 (3.3615)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][3030/3880]	Time 0.314 (0.381)	Data 1.08e-04 (2.07e-04)	Tok/s 32967 (36789)	Loss/tok 3.0923 (3.3611)	LR 1.000e-03
0: TRAIN [1][3040/3880]	Time 0.291 (0.381)	Data 1.24e-04 (2.07e-04)	Tok/s 34667 (36796)	Loss/tok 2.9513 (3.3610)	LR 1.000e-03
0: TRAIN [1][3050/3880]	Time 0.206 (0.381)	Data 1.04e-04 (2.06e-04)	Tok/s 25480 (36796)	Loss/tok 2.5400 (3.3606)	LR 1.000e-03
0: TRAIN [1][3060/3880]	Time 0.422 (0.381)	Data 1.07e-04 (2.06e-04)	Tok/s 40228 (36804)	Loss/tok 3.3130 (3.3606)	LR 1.000e-03
0: TRAIN [1][3070/3880]	Time 0.435 (0.381)	Data 1.17e-04 (2.06e-04)	Tok/s 38349 (36806)	Loss/tok 3.3872 (3.3604)	LR 1.000e-03
0: TRAIN [1][3080/3880]	Time 0.507 (0.381)	Data 1.10e-04 (2.06e-04)	Tok/s 46434 (36815)	Loss/tok 3.3459 (3.3602)	LR 1.000e-03
0: TRAIN [1][3090/3880]	Time 0.483 (0.382)	Data 1.11e-04 (2.05e-04)	Tok/s 35343 (36822)	Loss/tok 3.2575 (3.3603)	LR 1.000e-03
0: TRAIN [1][3100/3880]	Time 0.206 (0.382)	Data 1.13e-04 (2.05e-04)	Tok/s 26058 (36813)	Loss/tok 2.6909 (3.3603)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3110/3880]	Time 0.404 (0.382)	Data 1.09e-04 (2.05e-04)	Tok/s 42301 (36825)	Loss/tok 3.2474 (3.3604)	LR 1.000e-03
0: TRAIN [1][3120/3880]	Time 0.293 (0.382)	Data 1.08e-04 (2.04e-04)	Tok/s 34982 (36819)	Loss/tok 3.0894 (3.3598)	LR 1.000e-03
0: TRAIN [1][3130/3880]	Time 0.591 (0.382)	Data 1.05e-04 (2.04e-04)	Tok/s 50395 (36815)	Loss/tok 3.6278 (3.3596)	LR 1.000e-03
0: TRAIN [1][3140/3880]	Time 0.443 (0.382)	Data 1.25e-04 (2.04e-04)	Tok/s 38057 (36801)	Loss/tok 3.1582 (3.3591)	LR 1.000e-03
0: TRAIN [1][3150/3880]	Time 0.314 (0.381)	Data 1.06e-04 (2.03e-04)	Tok/s 32603 (36797)	Loss/tok 2.9624 (3.3587)	LR 1.000e-03
0: TRAIN [1][3160/3880]	Time 0.506 (0.382)	Data 1.07e-04 (2.03e-04)	Tok/s 46005 (36806)	Loss/tok 3.4668 (3.3587)	LR 1.000e-03
0: TRAIN [1][3170/3880]	Time 0.664 (0.382)	Data 1.08e-04 (2.03e-04)	Tok/s 43986 (36813)	Loss/tok 3.6115 (3.3586)	LR 1.000e-03
0: TRAIN [1][3180/3880]	Time 0.307 (0.381)	Data 1.07e-04 (2.02e-04)	Tok/s 34100 (36800)	Loss/tok 3.0489 (3.3579)	LR 1.000e-03
0: TRAIN [1][3190/3880]	Time 0.308 (0.381)	Data 1.09e-04 (2.02e-04)	Tok/s 33686 (36803)	Loss/tok 3.1341 (3.3577)	LR 1.000e-03
0: TRAIN [1][3200/3880]	Time 0.315 (0.381)	Data 1.07e-04 (2.02e-04)	Tok/s 33078 (36800)	Loss/tok 3.0406 (3.3574)	LR 1.000e-03
0: TRAIN [1][3210/3880]	Time 0.509 (0.381)	Data 1.13e-04 (2.02e-04)	Tok/s 45874 (36805)	Loss/tok 3.4857 (3.3572)	LR 1.000e-03
0: TRAIN [1][3220/3880]	Time 0.392 (0.381)	Data 1.07e-04 (2.01e-04)	Tok/s 42352 (36803)	Loss/tok 3.3503 (3.3568)	LR 1.000e-03
0: TRAIN [1][3230/3880]	Time 0.299 (0.381)	Data 1.07e-04 (2.01e-04)	Tok/s 34396 (36805)	Loss/tok 3.0288 (3.3566)	LR 1.000e-03
0: TRAIN [1][3240/3880]	Time 0.402 (0.381)	Data 1.18e-04 (2.01e-04)	Tok/s 41472 (36803)	Loss/tok 3.3192 (3.3566)	LR 1.000e-03
0: TRAIN [1][3250/3880]	Time 0.333 (0.381)	Data 1.11e-04 (2.00e-04)	Tok/s 31507 (36800)	Loss/tok 3.1028 (3.3564)	LR 1.000e-03
0: TRAIN [1][3260/3880]	Time 0.312 (0.381)	Data 1.10e-04 (2.00e-04)	Tok/s 32844 (36799)	Loss/tok 3.1496 (3.3565)	LR 1.000e-03
0: TRAIN [1][3270/3880]	Time 0.298 (0.381)	Data 1.08e-04 (2.00e-04)	Tok/s 34213 (36792)	Loss/tok 2.9842 (3.3563)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3280/3880]	Time 0.851 (0.381)	Data 1.10e-04 (2.00e-04)	Tok/s 35043 (36790)	Loss/tok 3.6768 (3.3563)	LR 1.000e-03
0: TRAIN [1][3290/3880]	Time 0.302 (0.381)	Data 1.06e-04 (1.99e-04)	Tok/s 33999 (36785)	Loss/tok 2.9893 (3.3562)	LR 1.000e-03
0: TRAIN [1][3300/3880]	Time 0.400 (0.381)	Data 1.07e-04 (1.99e-04)	Tok/s 42100 (36794)	Loss/tok 3.2904 (3.3562)	LR 1.000e-03
0: TRAIN [1][3310/3880]	Time 0.335 (0.381)	Data 1.13e-04 (1.99e-04)	Tok/s 30797 (36795)	Loss/tok 3.1559 (3.3559)	LR 1.000e-03
0: TRAIN [1][3320/3880]	Time 0.205 (0.381)	Data 1.18e-04 (1.99e-04)	Tok/s 25992 (36788)	Loss/tok 2.5998 (3.3554)	LR 1.000e-03
0: TRAIN [1][3330/3880]	Time 0.335 (0.381)	Data 1.13e-04 (1.98e-04)	Tok/s 31123 (36792)	Loss/tok 2.9670 (3.3551)	LR 1.000e-03
0: TRAIN [1][3340/3880]	Time 0.521 (0.381)	Data 1.08e-04 (1.98e-04)	Tok/s 45254 (36793)	Loss/tok 3.4538 (3.3548)	LR 1.000e-03
0: TRAIN [1][3350/3880]	Time 0.405 (0.381)	Data 1.07e-04 (1.98e-04)	Tok/s 41211 (36796)	Loss/tok 3.3218 (3.3546)	LR 1.000e-03
0: TRAIN [1][3360/3880]	Time 0.650 (0.381)	Data 1.11e-04 (1.97e-04)	Tok/s 45380 (36800)	Loss/tok 3.6450 (3.3545)	LR 1.000e-03
0: TRAIN [1][3370/3880]	Time 0.538 (0.381)	Data 1.07e-04 (1.97e-04)	Tok/s 43033 (36801)	Loss/tok 3.4741 (3.3542)	LR 1.000e-03
0: TRAIN [1][3380/3880]	Time 0.332 (0.382)	Data 1.12e-04 (1.97e-04)	Tok/s 31463 (36797)	Loss/tok 3.0937 (3.3540)	LR 1.000e-03
0: TRAIN [1][3390/3880]	Time 0.295 (0.382)	Data 1.13e-04 (1.97e-04)	Tok/s 35922 (36792)	Loss/tok 3.0137 (3.3536)	LR 1.000e-03
0: TRAIN [1][3400/3880]	Time 0.498 (0.382)	Data 1.13e-04 (1.96e-04)	Tok/s 47128 (36805)	Loss/tok 3.4042 (3.3536)	LR 1.000e-03
0: TRAIN [1][3410/3880]	Time 0.297 (0.382)	Data 1.13e-04 (1.96e-04)	Tok/s 34957 (36809)	Loss/tok 3.0839 (3.3533)	LR 1.000e-03
0: TRAIN [1][3420/3880]	Time 0.580 (0.382)	Data 1.09e-04 (1.96e-04)	Tok/s 40656 (36803)	Loss/tok 3.4953 (3.3531)	LR 1.000e-03
0: TRAIN [1][3430/3880]	Time 0.524 (0.382)	Data 1.07e-04 (1.96e-04)	Tok/s 44096 (36806)	Loss/tok 3.4942 (3.3530)	LR 5.000e-04
0: TRAIN [1][3440/3880]	Time 0.398 (0.382)	Data 1.12e-04 (1.95e-04)	Tok/s 42716 (36806)	Loss/tok 3.3822 (3.3527)	LR 5.000e-04
0: TRAIN [1][3450/3880]	Time 0.518 (0.382)	Data 1.11e-04 (1.95e-04)	Tok/s 44483 (36810)	Loss/tok 3.5067 (3.3525)	LR 5.000e-04
0: TRAIN [1][3460/3880]	Time 0.564 (0.382)	Data 1.08e-04 (1.95e-04)	Tok/s 41144 (36801)	Loss/tok 3.5699 (3.3522)	LR 5.000e-04
0: TRAIN [1][3470/3880]	Time 0.298 (0.382)	Data 1.11e-04 (1.95e-04)	Tok/s 34681 (36795)	Loss/tok 3.1087 (3.3517)	LR 5.000e-04
0: TRAIN [1][3480/3880]	Time 0.300 (0.382)	Data 1.09e-04 (1.94e-04)	Tok/s 34225 (36801)	Loss/tok 2.9918 (3.3514)	LR 5.000e-04
0: TRAIN [1][3490/3880]	Time 0.500 (0.382)	Data 1.12e-04 (1.94e-04)	Tok/s 46352 (36812)	Loss/tok 3.4955 (3.3516)	LR 5.000e-04
0: TRAIN [1][3500/3880]	Time 0.315 (0.382)	Data 1.11e-04 (1.94e-04)	Tok/s 32096 (36819)	Loss/tok 3.0653 (3.3515)	LR 5.000e-04
0: TRAIN [1][3510/3880]	Time 0.407 (0.382)	Data 1.11e-04 (1.94e-04)	Tok/s 41813 (36821)	Loss/tok 3.2380 (3.3510)	LR 5.000e-04
0: TRAIN [1][3520/3880]	Time 0.308 (0.382)	Data 1.08e-04 (1.93e-04)	Tok/s 33110 (36823)	Loss/tok 3.0116 (3.3507)	LR 5.000e-04
0: TRAIN [1][3530/3880]	Time 0.201 (0.382)	Data 1.08e-04 (1.93e-04)	Tok/s 25403 (36822)	Loss/tok 2.5289 (3.3505)	LR 5.000e-04
0: TRAIN [1][3540/3880]	Time 0.462 (0.382)	Data 1.07e-04 (1.93e-04)	Tok/s 36413 (36807)	Loss/tok 3.2504 (3.3500)	LR 5.000e-04
0: TRAIN [1][3550/3880]	Time 0.351 (0.382)	Data 1.11e-04 (1.93e-04)	Tok/s 29680 (36798)	Loss/tok 3.0214 (3.3496)	LR 5.000e-04
0: TRAIN [1][3560/3880]	Time 0.416 (0.382)	Data 1.07e-04 (1.92e-04)	Tok/s 39953 (36804)	Loss/tok 3.3035 (3.3496)	LR 5.000e-04
0: TRAIN [1][3570/3880]	Time 0.309 (0.382)	Data 1.10e-04 (1.92e-04)	Tok/s 33532 (36803)	Loss/tok 2.9216 (3.3492)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][3580/3880]	Time 0.658 (0.382)	Data 1.11e-04 (1.92e-04)	Tok/s 45261 (36812)	Loss/tok 3.6118 (3.3493)	LR 5.000e-04
0: TRAIN [1][3590/3880]	Time 0.356 (0.382)	Data 1.07e-04 (1.92e-04)	Tok/s 29225 (36818)	Loss/tok 3.0929 (3.3492)	LR 5.000e-04
0: TRAIN [1][3600/3880]	Time 0.431 (0.382)	Data 1.11e-04 (1.92e-04)	Tok/s 38858 (36817)	Loss/tok 3.1968 (3.3491)	LR 5.000e-04
0: TRAIN [1][3610/3880]	Time 0.406 (0.382)	Data 1.10e-04 (1.91e-04)	Tok/s 41422 (36828)	Loss/tok 3.1725 (3.3489)	LR 5.000e-04
0: TRAIN [1][3620/3880]	Time 0.406 (0.382)	Data 1.08e-04 (1.91e-04)	Tok/s 41540 (36831)	Loss/tok 3.1912 (3.3485)	LR 5.000e-04
0: TRAIN [1][3630/3880]	Time 0.505 (0.382)	Data 1.08e-04 (1.91e-04)	Tok/s 33346 (36825)	Loss/tok 3.2669 (3.3483)	LR 5.000e-04
0: TRAIN [1][3640/3880]	Time 0.316 (0.382)	Data 1.10e-04 (1.91e-04)	Tok/s 32258 (36821)	Loss/tok 3.0095 (3.3480)	LR 5.000e-04
0: TRAIN [1][3650/3880]	Time 0.296 (0.382)	Data 1.09e-04 (1.90e-04)	Tok/s 34459 (36824)	Loss/tok 2.8920 (3.3476)	LR 5.000e-04
0: TRAIN [1][3660/3880]	Time 0.291 (0.382)	Data 1.10e-04 (1.90e-04)	Tok/s 36539 (36824)	Loss/tok 2.9849 (3.3475)	LR 5.000e-04
0: TRAIN [1][3670/3880]	Time 0.414 (0.382)	Data 1.91e-04 (1.90e-04)	Tok/s 41027 (36817)	Loss/tok 3.2245 (3.3471)	LR 5.000e-04
0: TRAIN [1][3680/3880]	Time 0.503 (0.382)	Data 1.05e-04 (1.90e-04)	Tok/s 46217 (36817)	Loss/tok 3.5380 (3.3467)	LR 5.000e-04
0: TRAIN [1][3690/3880]	Time 0.208 (0.382)	Data 1.10e-04 (1.90e-04)	Tok/s 25713 (36815)	Loss/tok 2.7295 (3.3465)	LR 5.000e-04
0: TRAIN [1][3700/3880]	Time 0.504 (0.382)	Data 1.07e-04 (1.89e-04)	Tok/s 46246 (36822)	Loss/tok 3.5099 (3.3462)	LR 5.000e-04
0: TRAIN [1][3710/3880]	Time 0.460 (0.382)	Data 1.11e-04 (1.89e-04)	Tok/s 36983 (36824)	Loss/tok 3.2985 (3.3459)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][3720/3880]	Time 0.401 (0.382)	Data 1.08e-04 (1.89e-04)	Tok/s 41644 (36825)	Loss/tok 3.3482 (3.3458)	LR 5.000e-04
0: TRAIN [1][3730/3880]	Time 0.639 (0.382)	Data 1.10e-04 (1.89e-04)	Tok/s 45779 (36835)	Loss/tok 3.6837 (3.3458)	LR 5.000e-04
0: TRAIN [1][3740/3880]	Time 0.410 (0.382)	Data 1.10e-04 (1.88e-04)	Tok/s 40674 (36832)	Loss/tok 3.3584 (3.3453)	LR 5.000e-04
0: TRAIN [1][3750/3880]	Time 0.449 (0.382)	Data 1.07e-04 (1.88e-04)	Tok/s 36996 (36827)	Loss/tok 3.2016 (3.3451)	LR 5.000e-04
0: TRAIN [1][3760/3880]	Time 0.546 (0.382)	Data 1.08e-04 (1.88e-04)	Tok/s 43073 (36832)	Loss/tok 3.3628 (3.3448)	LR 5.000e-04
0: TRAIN [1][3770/3880]	Time 0.496 (0.382)	Data 1.11e-04 (1.88e-04)	Tok/s 46252 (36839)	Loss/tok 3.5141 (3.3448)	LR 5.000e-04
0: TRAIN [1][3780/3880]	Time 0.351 (0.383)	Data 1.12e-04 (1.88e-04)	Tok/s 29392 (36835)	Loss/tok 2.9381 (3.3447)	LR 5.000e-04
0: TRAIN [1][3790/3880]	Time 0.313 (0.382)	Data 1.04e-04 (1.87e-04)	Tok/s 33493 (36826)	Loss/tok 2.9424 (3.3442)	LR 5.000e-04
0: TRAIN [1][3800/3880]	Time 0.302 (0.382)	Data 1.10e-04 (1.87e-04)	Tok/s 34603 (36824)	Loss/tok 2.9890 (3.3440)	LR 5.000e-04
0: TRAIN [1][3810/3880]	Time 0.823 (0.382)	Data 1.10e-04 (1.87e-04)	Tok/s 36346 (36830)	Loss/tok 3.4346 (3.3437)	LR 5.000e-04
0: TRAIN [1][3820/3880]	Time 0.618 (0.382)	Data 1.06e-04 (1.87e-04)	Tok/s 47719 (36826)	Loss/tok 3.5507 (3.3435)	LR 5.000e-04
0: TRAIN [1][3830/3880]	Time 0.548 (0.382)	Data 1.08e-04 (1.87e-04)	Tok/s 42856 (36817)	Loss/tok 3.3897 (3.3433)	LR 5.000e-04
0: TRAIN [1][3840/3880]	Time 0.507 (0.382)	Data 1.11e-04 (1.86e-04)	Tok/s 45965 (36819)	Loss/tok 3.4822 (3.3430)	LR 5.000e-04
0: TRAIN [1][3850/3880]	Time 0.306 (0.382)	Data 1.09e-04 (1.86e-04)	Tok/s 33097 (36825)	Loss/tok 3.0089 (3.3427)	LR 5.000e-04
0: TRAIN [1][3860/3880]	Time 0.301 (0.382)	Data 1.11e-04 (1.86e-04)	Tok/s 34081 (36822)	Loss/tok 3.0494 (3.3423)	LR 5.000e-04
0: TRAIN [1][3870/3880]	Time 0.429 (0.382)	Data 1.10e-04 (1.86e-04)	Tok/s 39543 (36822)	Loss/tok 3.2276 (3.3420)	LR 5.000e-04
:::MLL 1572988027.937 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1572988027.938 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/6]	Time 0.798 (0.798)	Decoder iters 100.0 (100.0)	Tok/s 20587 (20587)
0: Running moses detokenizer
0: BLEU(score=23.40472102011252, counts=[36711, 18141, 10235, 6017], totals=[65399, 62396, 59393, 56396], precisions=[56.13388583923302, 29.073979101224438, 17.23267051672756, 10.669196396907582], bp=1.0, sys_len=65399, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1572988031.164 eval_accuracy: {"value": 23.4, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1572988031.164 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.3439	Test BLEU: 23.40
0: Performance: Epoch: 1	Training: 147295 Tok/s
0: Finished epoch 1
:::MLL 1572988031.164 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1572988031.165 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1572988031.165 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 993265470
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][0/3880]	Time 0.650 (0.650)	Data 3.09e-01 (3.09e-01)	Tok/s 15993 (15993)	Loss/tok 2.9593 (2.9593)	LR 5.000e-04
0: TRAIN [2][10/3880]	Time 0.501 (0.414)	Data 1.08e-04 (2.82e-02)	Tok/s 46159 (37485)	Loss/tok 3.4280 (3.2293)	LR 5.000e-04
0: TRAIN [2][20/3880]	Time 0.442 (0.374)	Data 1.04e-04 (1.48e-02)	Tok/s 38073 (35419)	Loss/tok 3.1256 (3.1459)	LR 5.000e-04
0: TRAIN [2][30/3880]	Time 0.297 (0.385)	Data 1.09e-04 (1.01e-02)	Tok/s 34522 (36208)	Loss/tok 2.8384 (3.1641)	LR 5.000e-04
0: TRAIN [2][40/3880]	Time 0.204 (0.380)	Data 1.10e-04 (7.65e-03)	Tok/s 26085 (36486)	Loss/tok 2.5212 (3.1707)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][50/3880]	Time 0.408 (0.384)	Data 1.09e-04 (6.17e-03)	Tok/s 41288 (37066)	Loss/tok 3.1657 (3.1883)	LR 5.000e-04
0: TRAIN [2][60/3880]	Time 0.634 (0.389)	Data 1.09e-04 (5.18e-03)	Tok/s 45696 (37616)	Loss/tok 3.6577 (3.2029)	LR 5.000e-04
0: TRAIN [2][70/3880]	Time 0.402 (0.386)	Data 1.10e-04 (4.46e-03)	Tok/s 41752 (37695)	Loss/tok 3.1789 (3.1928)	LR 5.000e-04
0: TRAIN [2][80/3880]	Time 0.497 (0.392)	Data 1.08e-04 (3.93e-03)	Tok/s 33079 (37718)	Loss/tok 3.1938 (3.2080)	LR 5.000e-04
0: TRAIN [2][90/3880]	Time 0.634 (0.394)	Data 1.04e-04 (3.51e-03)	Tok/s 47484 (37451)	Loss/tok 3.3563 (3.2016)	LR 5.000e-04
0: TRAIN [2][100/3880]	Time 0.322 (0.395)	Data 1.11e-04 (3.17e-03)	Tok/s 32314 (37456)	Loss/tok 3.0627 (3.2006)	LR 5.000e-04
0: TRAIN [2][110/3880]	Time 0.311 (0.394)	Data 1.09e-04 (2.89e-03)	Tok/s 33702 (37465)	Loss/tok 2.9741 (3.1956)	LR 5.000e-04
0: TRAIN [2][120/3880]	Time 0.306 (0.388)	Data 1.18e-04 (2.66e-03)	Tok/s 33467 (37347)	Loss/tok 2.8360 (3.1816)	LR 5.000e-04
0: TRAIN [2][130/3880]	Time 0.206 (0.384)	Data 1.08e-04 (2.47e-03)	Tok/s 25986 (37196)	Loss/tok 2.5985 (3.1766)	LR 5.000e-04
0: TRAIN [2][140/3880]	Time 0.304 (0.382)	Data 1.15e-04 (2.30e-03)	Tok/s 33670 (37118)	Loss/tok 2.8151 (3.1736)	LR 5.000e-04
0: TRAIN [2][150/3880]	Time 0.306 (0.383)	Data 1.09e-04 (2.16e-03)	Tok/s 34066 (37254)	Loss/tok 2.7966 (3.1705)	LR 5.000e-04
0: TRAIN [2][160/3880]	Time 0.349 (0.380)	Data 1.09e-04 (2.03e-03)	Tok/s 29547 (37094)	Loss/tok 2.9481 (3.1631)	LR 5.000e-04
0: TRAIN [2][170/3880]	Time 0.666 (0.382)	Data 1.09e-04 (1.92e-03)	Tok/s 45023 (37022)	Loss/tok 3.3400 (3.1635)	LR 5.000e-04
0: TRAIN [2][180/3880]	Time 0.306 (0.380)	Data 1.04e-04 (1.82e-03)	Tok/s 34017 (36913)	Loss/tok 2.9374 (3.1606)	LR 5.000e-04
0: TRAIN [2][190/3880]	Time 0.510 (0.379)	Data 1.09e-04 (1.73e-03)	Tok/s 45272 (36942)	Loss/tok 3.4921 (3.1608)	LR 5.000e-04
0: TRAIN [2][200/3880]	Time 0.306 (0.380)	Data 1.09e-04 (1.65e-03)	Tok/s 34064 (37012)	Loss/tok 2.9182 (3.1630)	LR 5.000e-04
0: TRAIN [2][210/3880]	Time 0.528 (0.379)	Data 1.08e-04 (1.57e-03)	Tok/s 44512 (36986)	Loss/tok 3.2936 (3.1597)	LR 5.000e-04
0: TRAIN [2][220/3880]	Time 0.308 (0.378)	Data 1.10e-04 (1.51e-03)	Tok/s 33460 (36955)	Loss/tok 2.8572 (3.1607)	LR 5.000e-04
0: TRAIN [2][230/3880]	Time 0.304 (0.377)	Data 1.07e-04 (1.45e-03)	Tok/s 33683 (36940)	Loss/tok 2.9390 (3.1595)	LR 5.000e-04
0: TRAIN [2][240/3880]	Time 0.203 (0.376)	Data 1.17e-04 (1.39e-03)	Tok/s 26313 (36831)	Loss/tok 2.6499 (3.1579)	LR 5.000e-04
0: TRAIN [2][250/3880]	Time 0.525 (0.377)	Data 1.11e-04 (1.34e-03)	Tok/s 44307 (36940)	Loss/tok 3.1924 (3.1564)	LR 5.000e-04
0: TRAIN [2][260/3880]	Time 0.289 (0.376)	Data 1.10e-04 (1.29e-03)	Tok/s 35746 (36931)	Loss/tok 2.9553 (3.1540)	LR 5.000e-04
0: TRAIN [2][270/3880]	Time 0.413 (0.377)	Data 1.09e-04 (1.25e-03)	Tok/s 40309 (36986)	Loss/tok 3.1958 (3.1567)	LR 5.000e-04
0: TRAIN [2][280/3880]	Time 0.295 (0.376)	Data 1.11e-04 (1.21e-03)	Tok/s 35654 (36950)	Loss/tok 2.9502 (3.1559)	LR 5.000e-04
0: TRAIN [2][290/3880]	Time 0.392 (0.380)	Data 1.09e-04 (1.17e-03)	Tok/s 42832 (36981)	Loss/tok 3.1255 (3.1620)	LR 5.000e-04
0: TRAIN [2][300/3880]	Time 0.318 (0.381)	Data 1.13e-04 (1.14e-03)	Tok/s 32409 (37051)	Loss/tok 2.9661 (3.1665)	LR 5.000e-04
0: TRAIN [2][310/3880]	Time 0.320 (0.382)	Data 1.14e-04 (1.10e-03)	Tok/s 31693 (37054)	Loss/tok 2.8741 (3.1656)	LR 5.000e-04
0: TRAIN [2][320/3880]	Time 0.422 (0.381)	Data 1.09e-04 (1.07e-03)	Tok/s 40198 (37027)	Loss/tok 3.2409 (3.1633)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][330/3880]	Time 0.449 (0.381)	Data 1.11e-04 (1.04e-03)	Tok/s 37770 (37012)	Loss/tok 3.1158 (3.1641)	LR 5.000e-04
0: TRAIN [2][340/3880]	Time 0.209 (0.380)	Data 1.07e-04 (1.02e-03)	Tok/s 25003 (36955)	Loss/tok 2.6809 (3.1608)	LR 5.000e-04
0: TRAIN [2][350/3880]	Time 0.355 (0.381)	Data 1.06e-04 (9.91e-04)	Tok/s 29643 (36952)	Loss/tok 2.8788 (3.1609)	LR 5.000e-04
0: TRAIN [2][360/3880]	Time 0.319 (0.382)	Data 1.07e-04 (9.66e-04)	Tok/s 32815 (36844)	Loss/tok 2.9655 (3.1617)	LR 2.500e-04
0: TRAIN [2][370/3880]	Time 0.387 (0.384)	Data 1.08e-04 (9.43e-04)	Tok/s 43194 (36912)	Loss/tok 3.1706 (3.1634)	LR 2.500e-04
0: TRAIN [2][380/3880]	Time 0.307 (0.382)	Data 1.10e-04 (9.21e-04)	Tok/s 33652 (36820)	Loss/tok 2.9324 (3.1598)	LR 2.500e-04
0: TRAIN [2][390/3880]	Time 0.299 (0.382)	Data 1.07e-04 (9.00e-04)	Tok/s 33934 (36837)	Loss/tok 3.0300 (3.1598)	LR 2.500e-04
0: TRAIN [2][400/3880]	Time 0.314 (0.381)	Data 1.15e-04 (8.81e-04)	Tok/s 33019 (36807)	Loss/tok 2.9255 (3.1594)	LR 2.500e-04
0: TRAIN [2][410/3880]	Time 0.291 (0.381)	Data 1.08e-04 (8.62e-04)	Tok/s 36305 (36822)	Loss/tok 2.9574 (3.1591)	LR 2.500e-04
0: TRAIN [2][420/3880]	Time 0.410 (0.381)	Data 1.10e-04 (8.44e-04)	Tok/s 41369 (36853)	Loss/tok 3.1688 (3.1576)	LR 2.500e-04
0: TRAIN [2][430/3880]	Time 0.668 (0.382)	Data 1.10e-04 (8.27e-04)	Tok/s 45293 (36847)	Loss/tok 3.4981 (3.1581)	LR 2.500e-04
0: TRAIN [2][440/3880]	Time 0.411 (0.380)	Data 1.08e-04 (8.11e-04)	Tok/s 40463 (36784)	Loss/tok 3.1556 (3.1556)	LR 2.500e-04
0: TRAIN [2][450/3880]	Time 0.208 (0.381)	Data 1.13e-04 (7.95e-04)	Tok/s 25015 (36854)	Loss/tok 2.5386 (3.1577)	LR 2.500e-04
0: TRAIN [2][460/3880]	Time 0.644 (0.381)	Data 1.09e-04 (7.80e-04)	Tok/s 46169 (36856)	Loss/tok 3.5010 (3.1574)	LR 2.500e-04
0: TRAIN [2][470/3880]	Time 0.804 (0.381)	Data 1.07e-04 (7.66e-04)	Tok/s 36797 (36844)	Loss/tok 3.5483 (3.1592)	LR 2.500e-04
0: TRAIN [2][480/3880]	Time 0.504 (0.382)	Data 1.07e-04 (7.52e-04)	Tok/s 46173 (36866)	Loss/tok 3.2748 (3.1596)	LR 2.500e-04
0: TRAIN [2][490/3880]	Time 0.294 (0.384)	Data 1.05e-04 (7.39e-04)	Tok/s 35322 (36869)	Loss/tok 2.9184 (3.1618)	LR 2.500e-04
0: TRAIN [2][500/3880]	Time 0.302 (0.383)	Data 1.07e-04 (7.26e-04)	Tok/s 33950 (36828)	Loss/tok 2.9689 (3.1601)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][510/3880]	Time 0.295 (0.382)	Data 1.09e-04 (7.14e-04)	Tok/s 35092 (36854)	Loss/tok 3.0415 (3.1600)	LR 2.500e-04
0: TRAIN [2][520/3880]	Time 0.399 (0.383)	Data 1.07e-04 (7.03e-04)	Tok/s 42210 (36813)	Loss/tok 3.0956 (3.1599)	LR 2.500e-04
0: TRAIN [2][530/3880]	Time 0.305 (0.382)	Data 1.05e-04 (6.91e-04)	Tok/s 33690 (36767)	Loss/tok 2.9788 (3.1575)	LR 2.500e-04
0: TRAIN [2][540/3880]	Time 0.314 (0.382)	Data 1.11e-04 (6.81e-04)	Tok/s 32695 (36792)	Loss/tok 2.9076 (3.1566)	LR 2.500e-04
0: TRAIN [2][550/3880]	Time 0.516 (0.383)	Data 1.12e-04 (6.70e-04)	Tok/s 45492 (36883)	Loss/tok 3.3319 (3.1595)	LR 2.500e-04
0: TRAIN [2][560/3880]	Time 0.301 (0.383)	Data 1.12e-04 (6.60e-04)	Tok/s 33923 (36886)	Loss/tok 2.9644 (3.1593)	LR 2.500e-04
0: TRAIN [2][570/3880]	Time 0.338 (0.384)	Data 1.10e-04 (6.51e-04)	Tok/s 30441 (36887)	Loss/tok 2.9094 (3.1607)	LR 2.500e-04
0: TRAIN [2][580/3880]	Time 0.208 (0.384)	Data 1.12e-04 (6.41e-04)	Tok/s 25322 (36862)	Loss/tok 2.5030 (3.1593)	LR 2.500e-04
0: TRAIN [2][590/3880]	Time 0.396 (0.383)	Data 1.11e-04 (6.32e-04)	Tok/s 41967 (36873)	Loss/tok 2.9585 (3.1580)	LR 2.500e-04
0: TRAIN [2][600/3880]	Time 0.314 (0.384)	Data 1.10e-04 (6.24e-04)	Tok/s 33330 (36817)	Loss/tok 2.9815 (3.1571)	LR 2.500e-04
0: TRAIN [2][610/3880]	Time 0.303 (0.383)	Data 1.08e-04 (6.15e-04)	Tok/s 34167 (36814)	Loss/tok 2.9415 (3.1562)	LR 2.500e-04
0: TRAIN [2][620/3880]	Time 0.508 (0.383)	Data 1.11e-04 (6.07e-04)	Tok/s 46191 (36845)	Loss/tok 3.2584 (3.1582)	LR 2.500e-04
0: TRAIN [2][630/3880]	Time 0.543 (0.383)	Data 1.09e-04 (5.99e-04)	Tok/s 42849 (36856)	Loss/tok 3.2424 (3.1583)	LR 2.500e-04
0: TRAIN [2][640/3880]	Time 0.419 (0.383)	Data 1.12e-04 (5.92e-04)	Tok/s 40391 (36858)	Loss/tok 3.1189 (3.1586)	LR 2.500e-04
0: TRAIN [2][650/3880]	Time 0.495 (0.384)	Data 1.14e-04 (5.84e-04)	Tok/s 34040 (36871)	Loss/tok 3.1768 (3.1589)	LR 2.500e-04
0: TRAIN [2][660/3880]	Time 0.418 (0.384)	Data 1.05e-04 (5.77e-04)	Tok/s 40371 (36834)	Loss/tok 3.0999 (3.1583)	LR 2.500e-04
0: TRAIN [2][670/3880]	Time 0.297 (0.383)	Data 1.05e-04 (5.70e-04)	Tok/s 34880 (36802)	Loss/tok 2.9796 (3.1571)	LR 2.500e-04
0: TRAIN [2][680/3880]	Time 0.421 (0.384)	Data 1.09e-04 (5.63e-04)	Tok/s 40142 (36835)	Loss/tok 3.1729 (3.1580)	LR 2.500e-04
0: TRAIN [2][690/3880]	Time 0.299 (0.384)	Data 1.16e-04 (5.57e-04)	Tok/s 34936 (36861)	Loss/tok 3.0231 (3.1574)	LR 2.500e-04
0: TRAIN [2][700/3880]	Time 0.301 (0.384)	Data 1.06e-04 (5.50e-04)	Tok/s 34313 (36873)	Loss/tok 2.9693 (3.1575)	LR 2.500e-04
0: TRAIN [2][710/3880]	Time 0.313 (0.384)	Data 1.06e-04 (5.44e-04)	Tok/s 33423 (36889)	Loss/tok 2.9691 (3.1565)	LR 2.500e-04
0: TRAIN [2][720/3880]	Time 0.350 (0.385)	Data 1.10e-04 (5.38e-04)	Tok/s 29727 (36868)	Loss/tok 2.8966 (3.1574)	LR 2.500e-04
0: TRAIN [2][730/3880]	Time 0.546 (0.385)	Data 1.11e-04 (5.32e-04)	Tok/s 42782 (36846)	Loss/tok 3.2995 (3.1570)	LR 2.500e-04
0: TRAIN [2][740/3880]	Time 0.304 (0.385)	Data 1.09e-04 (5.27e-04)	Tok/s 34286 (36882)	Loss/tok 2.9267 (3.1574)	LR 2.500e-04
0: TRAIN [2][750/3880]	Time 0.298 (0.385)	Data 1.11e-04 (5.21e-04)	Tok/s 34402 (36882)	Loss/tok 3.0999 (3.1564)	LR 2.500e-04
0: TRAIN [2][760/3880]	Time 0.315 (0.384)	Data 1.07e-04 (5.16e-04)	Tok/s 32783 (36879)	Loss/tok 2.8966 (3.1555)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][770/3880]	Time 0.410 (0.385)	Data 1.10e-04 (5.10e-04)	Tok/s 41201 (36913)	Loss/tok 3.1409 (3.1574)	LR 2.500e-04
0: TRAIN [2][780/3880]	Time 0.402 (0.385)	Data 1.11e-04 (5.05e-04)	Tok/s 41391 (36930)	Loss/tok 3.0840 (3.1575)	LR 2.500e-04
0: TRAIN [2][790/3880]	Time 0.410 (0.386)	Data 1.12e-04 (5.00e-04)	Tok/s 41313 (36962)	Loss/tok 3.1387 (3.1588)	LR 2.500e-04
0: TRAIN [2][800/3880]	Time 0.512 (0.385)	Data 1.08e-04 (4.95e-04)	Tok/s 45330 (36952)	Loss/tok 3.2229 (3.1580)	LR 2.500e-04
0: TRAIN [2][810/3880]	Time 0.605 (0.385)	Data 1.07e-04 (4.90e-04)	Tok/s 38893 (36936)	Loss/tok 3.2401 (3.1574)	LR 2.500e-04
0: TRAIN [2][820/3880]	Time 0.295 (0.386)	Data 1.10e-04 (4.86e-04)	Tok/s 35723 (36925)	Loss/tok 2.9291 (3.1577)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][830/3880]	Time 0.207 (0.385)	Data 1.04e-04 (4.81e-04)	Tok/s 25843 (36926)	Loss/tok 2.5684 (3.1577)	LR 2.500e-04
0: TRAIN [2][840/3880]	Time 0.423 (0.385)	Data 1.07e-04 (4.77e-04)	Tok/s 39820 (36903)	Loss/tok 3.0831 (3.1566)	LR 2.500e-04
0: TRAIN [2][850/3880]	Time 0.298 (0.385)	Data 1.03e-04 (4.72e-04)	Tok/s 34925 (36885)	Loss/tok 3.0153 (3.1552)	LR 2.500e-04
0: TRAIN [2][860/3880]	Time 0.202 (0.384)	Data 1.12e-04 (4.68e-04)	Tok/s 25925 (36876)	Loss/tok 2.5413 (3.1541)	LR 2.500e-04
0: TRAIN [2][870/3880]	Time 0.434 (0.384)	Data 1.09e-04 (4.64e-04)	Tok/s 39074 (36842)	Loss/tok 3.1661 (3.1534)	LR 2.500e-04
0: TRAIN [2][880/3880]	Time 0.296 (0.384)	Data 1.08e-04 (4.60e-04)	Tok/s 34335 (36837)	Loss/tok 2.9453 (3.1524)	LR 2.500e-04
0: TRAIN [2][890/3880]	Time 0.287 (0.383)	Data 1.05e-04 (4.56e-04)	Tok/s 35858 (36830)	Loss/tok 2.9349 (3.1518)	LR 2.500e-04
0: TRAIN [2][900/3880]	Time 0.414 (0.383)	Data 1.22e-04 (4.52e-04)	Tok/s 40675 (36865)	Loss/tok 3.1285 (3.1522)	LR 2.500e-04
0: TRAIN [2][910/3880]	Time 0.302 (0.383)	Data 1.06e-04 (4.48e-04)	Tok/s 34523 (36858)	Loss/tok 2.8589 (3.1512)	LR 2.500e-04
0: TRAIN [2][920/3880]	Time 0.300 (0.382)	Data 1.05e-04 (4.45e-04)	Tok/s 34445 (36835)	Loss/tok 2.8868 (3.1499)	LR 2.500e-04
0: TRAIN [2][930/3880]	Time 0.206 (0.382)	Data 1.11e-04 (4.41e-04)	Tok/s 25504 (36832)	Loss/tok 2.5420 (3.1491)	LR 2.500e-04
0: TRAIN [2][940/3880]	Time 0.322 (0.382)	Data 1.08e-04 (4.38e-04)	Tok/s 32040 (36817)	Loss/tok 3.0149 (3.1487)	LR 2.500e-04
0: TRAIN [2][950/3880]	Time 0.292 (0.382)	Data 1.09e-04 (4.34e-04)	Tok/s 36369 (36797)	Loss/tok 2.9439 (3.1486)	LR 2.500e-04
0: TRAIN [2][960/3880]	Time 0.215 (0.382)	Data 1.09e-04 (4.31e-04)	Tok/s 24672 (36759)	Loss/tok 2.6196 (3.1480)	LR 2.500e-04
0: TRAIN [2][970/3880]	Time 0.402 (0.381)	Data 1.08e-04 (4.27e-04)	Tok/s 41886 (36762)	Loss/tok 3.0797 (3.1479)	LR 2.500e-04
0: TRAIN [2][980/3880]	Time 0.304 (0.381)	Data 1.10e-04 (4.24e-04)	Tok/s 33570 (36734)	Loss/tok 2.9344 (3.1470)	LR 2.500e-04
0: TRAIN [2][990/3880]	Time 0.506 (0.380)	Data 1.05e-04 (4.21e-04)	Tok/s 46030 (36724)	Loss/tok 3.2308 (3.1464)	LR 2.500e-04
0: TRAIN [2][1000/3880]	Time 0.304 (0.380)	Data 1.13e-04 (4.18e-04)	Tok/s 33970 (36743)	Loss/tok 2.9377 (3.1466)	LR 2.500e-04
0: TRAIN [2][1010/3880]	Time 0.402 (0.381)	Data 1.06e-04 (4.15e-04)	Tok/s 41676 (36765)	Loss/tok 3.1164 (3.1470)	LR 2.500e-04
0: TRAIN [2][1020/3880]	Time 0.294 (0.381)	Data 1.05e-04 (4.12e-04)	Tok/s 35751 (36782)	Loss/tok 2.9381 (3.1487)	LR 2.500e-04
0: TRAIN [2][1030/3880]	Time 0.316 (0.381)	Data 1.11e-04 (4.09e-04)	Tok/s 31801 (36752)	Loss/tok 3.0169 (3.1479)	LR 2.500e-04
0: TRAIN [2][1040/3880]	Time 0.338 (0.381)	Data 1.14e-04 (4.06e-04)	Tok/s 30771 (36726)	Loss/tok 2.9132 (3.1472)	LR 2.500e-04
0: TRAIN [2][1050/3880]	Time 0.670 (0.381)	Data 1.11e-04 (4.03e-04)	Tok/s 44200 (36725)	Loss/tok 3.4642 (3.1475)	LR 2.500e-04
0: TRAIN [2][1060/3880]	Time 0.215 (0.382)	Data 1.28e-04 (4.01e-04)	Tok/s 24239 (36744)	Loss/tok 2.6034 (3.1481)	LR 2.500e-04
0: TRAIN [2][1070/3880]	Time 0.434 (0.382)	Data 1.10e-04 (3.98e-04)	Tok/s 37560 (36770)	Loss/tok 3.2093 (3.1486)	LR 2.500e-04
0: TRAIN [2][1080/3880]	Time 0.319 (0.382)	Data 1.06e-04 (3.95e-04)	Tok/s 32471 (36772)	Loss/tok 2.9520 (3.1487)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1090/3880]	Time 0.396 (0.381)	Data 1.09e-04 (3.92e-04)	Tok/s 42777 (36751)	Loss/tok 3.2284 (3.1479)	LR 2.500e-04
0: TRAIN [2][1100/3880]	Time 0.207 (0.381)	Data 1.23e-04 (3.90e-04)	Tok/s 25419 (36752)	Loss/tok 2.5434 (3.1475)	LR 2.500e-04
0: TRAIN [2][1110/3880]	Time 0.338 (0.382)	Data 1.10e-04 (3.87e-04)	Tok/s 30157 (36761)	Loss/tok 2.9050 (3.1477)	LR 2.500e-04
0: TRAIN [2][1120/3880]	Time 0.209 (0.381)	Data 1.14e-04 (3.85e-04)	Tok/s 25383 (36741)	Loss/tok 2.5527 (3.1473)	LR 2.500e-04
0: TRAIN [2][1130/3880]	Time 0.488 (0.381)	Data 1.08e-04 (3.82e-04)	Tok/s 48473 (36765)	Loss/tok 3.1540 (3.1473)	LR 2.500e-04
0: TRAIN [2][1140/3880]	Time 0.505 (0.381)	Data 1.09e-04 (3.80e-04)	Tok/s 46125 (36770)	Loss/tok 3.2320 (3.1468)	LR 2.500e-04
0: TRAIN [2][1150/3880]	Time 0.527 (0.382)	Data 1.10e-04 (3.78e-04)	Tok/s 44550 (36816)	Loss/tok 3.3793 (3.1476)	LR 2.500e-04
0: TRAIN [2][1160/3880]	Time 0.223 (0.383)	Data 1.11e-04 (3.75e-04)	Tok/s 23364 (36774)	Loss/tok 2.6431 (3.1486)	LR 2.500e-04
0: TRAIN [2][1170/3880]	Time 0.385 (0.383)	Data 1.07e-04 (3.73e-04)	Tok/s 43546 (36808)	Loss/tok 3.1254 (3.1488)	LR 1.250e-04
0: TRAIN [2][1180/3880]	Time 0.709 (0.383)	Data 1.08e-04 (3.71e-04)	Tok/s 42441 (36779)	Loss/tok 3.4133 (3.1489)	LR 1.250e-04
0: TRAIN [2][1190/3880]	Time 0.654 (0.383)	Data 1.15e-04 (3.69e-04)	Tok/s 45027 (36761)	Loss/tok 3.4000 (3.1491)	LR 1.250e-04
0: TRAIN [2][1200/3880]	Time 0.304 (0.383)	Data 1.14e-04 (3.67e-04)	Tok/s 33855 (36760)	Loss/tok 2.8245 (3.1482)	LR 1.250e-04
0: TRAIN [2][1210/3880]	Time 0.400 (0.382)	Data 1.10e-04 (3.64e-04)	Tok/s 42254 (36761)	Loss/tok 3.1376 (3.1475)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1220/3880]	Time 0.354 (0.383)	Data 1.07e-04 (3.62e-04)	Tok/s 28934 (36784)	Loss/tok 2.9568 (3.1483)	LR 1.250e-04
0: TRAIN [2][1230/3880]	Time 0.391 (0.383)	Data 1.10e-04 (3.60e-04)	Tok/s 43567 (36789)	Loss/tok 3.1615 (3.1482)	LR 1.250e-04
0: TRAIN [2][1240/3880]	Time 0.301 (0.383)	Data 1.12e-04 (3.58e-04)	Tok/s 34882 (36783)	Loss/tok 3.0397 (3.1486)	LR 1.250e-04
0: TRAIN [2][1250/3880]	Time 0.293 (0.383)	Data 1.09e-04 (3.56e-04)	Tok/s 35836 (36791)	Loss/tok 2.8567 (3.1484)	LR 1.250e-04
0: TRAIN [2][1260/3880]	Time 0.306 (0.383)	Data 1.08e-04 (3.54e-04)	Tok/s 33374 (36811)	Loss/tok 2.9214 (3.1488)	LR 1.250e-04
0: TRAIN [2][1270/3880]	Time 0.308 (0.383)	Data 1.07e-04 (3.52e-04)	Tok/s 33728 (36802)	Loss/tok 2.9513 (3.1483)	LR 1.250e-04
0: TRAIN [2][1280/3880]	Time 0.311 (0.383)	Data 1.08e-04 (3.50e-04)	Tok/s 33053 (36803)	Loss/tok 2.9178 (3.1478)	LR 1.250e-04
0: TRAIN [2][1290/3880]	Time 0.295 (0.383)	Data 1.04e-04 (3.49e-04)	Tok/s 35253 (36796)	Loss/tok 2.8256 (3.1472)	LR 1.250e-04
0: TRAIN [2][1300/3880]	Time 0.231 (0.383)	Data 1.10e-04 (3.47e-04)	Tok/s 23332 (36795)	Loss/tok 2.5992 (3.1478)	LR 1.250e-04
0: TRAIN [2][1310/3880]	Time 0.209 (0.383)	Data 1.12e-04 (3.45e-04)	Tok/s 25181 (36772)	Loss/tok 2.5683 (3.1477)	LR 1.250e-04
0: TRAIN [2][1320/3880]	Time 0.306 (0.383)	Data 1.08e-04 (3.43e-04)	Tok/s 33587 (36769)	Loss/tok 2.9408 (3.1472)	LR 1.250e-04
0: TRAIN [2][1330/3880]	Time 0.301 (0.383)	Data 1.11e-04 (3.41e-04)	Tok/s 34803 (36773)	Loss/tok 2.9114 (3.1472)	LR 1.250e-04
0: TRAIN [2][1340/3880]	Time 0.406 (0.382)	Data 1.18e-04 (3.40e-04)	Tok/s 41333 (36768)	Loss/tok 3.1481 (3.1468)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1350/3880]	Time 0.492 (0.383)	Data 1.09e-04 (3.38e-04)	Tok/s 47385 (36786)	Loss/tok 3.2719 (3.1469)	LR 1.250e-04
0: TRAIN [2][1360/3880]	Time 0.440 (0.383)	Data 1.12e-04 (3.36e-04)	Tok/s 37975 (36776)	Loss/tok 3.1614 (3.1470)	LR 1.250e-04
0: TRAIN [2][1370/3880]	Time 0.419 (0.383)	Data 1.11e-04 (3.35e-04)	Tok/s 40880 (36786)	Loss/tok 3.1354 (3.1475)	LR 1.250e-04
0: TRAIN [2][1380/3880]	Time 0.206 (0.383)	Data 1.10e-04 (3.33e-04)	Tok/s 25181 (36766)	Loss/tok 2.5686 (3.1466)	LR 1.250e-04
0: TRAIN [2][1390/3880]	Time 0.515 (0.383)	Data 1.13e-04 (3.32e-04)	Tok/s 45469 (36778)	Loss/tok 3.3096 (3.1464)	LR 1.250e-04
0: TRAIN [2][1400/3880]	Time 0.311 (0.382)	Data 1.07e-04 (3.30e-04)	Tok/s 32916 (36742)	Loss/tok 2.9315 (3.1456)	LR 1.250e-04
0: TRAIN [2][1410/3880]	Time 0.300 (0.383)	Data 1.12e-04 (3.28e-04)	Tok/s 33903 (36768)	Loss/tok 2.9057 (3.1469)	LR 1.250e-04
0: TRAIN [2][1420/3880]	Time 0.405 (0.383)	Data 1.10e-04 (3.27e-04)	Tok/s 41924 (36771)	Loss/tok 3.0331 (3.1465)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1430/3880]	Time 0.208 (0.383)	Data 1.11e-04 (3.25e-04)	Tok/s 25742 (36758)	Loss/tok 2.6144 (3.1470)	LR 1.250e-04
0: TRAIN [2][1440/3880]	Time 0.302 (0.383)	Data 1.08e-04 (3.24e-04)	Tok/s 33727 (36766)	Loss/tok 2.8497 (3.1465)	LR 1.250e-04
0: TRAIN [2][1450/3880]	Time 0.298 (0.383)	Data 1.07e-04 (3.22e-04)	Tok/s 34438 (36779)	Loss/tok 2.9383 (3.1467)	LR 1.250e-04
0: TRAIN [2][1460/3880]	Time 0.532 (0.383)	Data 1.11e-04 (3.21e-04)	Tok/s 44471 (36787)	Loss/tok 3.3459 (3.1464)	LR 1.250e-04
0: TRAIN [2][1470/3880]	Time 0.396 (0.383)	Data 1.07e-04 (3.19e-04)	Tok/s 42172 (36775)	Loss/tok 3.1954 (3.1461)	LR 1.250e-04
0: TRAIN [2][1480/3880]	Time 0.336 (0.382)	Data 1.05e-04 (3.18e-04)	Tok/s 30321 (36760)	Loss/tok 2.9667 (3.1453)	LR 1.250e-04
0: TRAIN [2][1490/3880]	Time 0.434 (0.382)	Data 1.13e-04 (3.17e-04)	Tok/s 38379 (36747)	Loss/tok 3.1550 (3.1450)	LR 1.250e-04
0: TRAIN [2][1500/3880]	Time 0.310 (0.382)	Data 1.11e-04 (3.15e-04)	Tok/s 33040 (36740)	Loss/tok 2.9041 (3.1445)	LR 1.250e-04
0: TRAIN [2][1510/3880]	Time 0.316 (0.382)	Data 1.14e-04 (3.14e-04)	Tok/s 32546 (36741)	Loss/tok 2.9551 (3.1444)	LR 1.250e-04
0: TRAIN [2][1520/3880]	Time 0.520 (0.382)	Data 1.10e-04 (3.12e-04)	Tok/s 45202 (36729)	Loss/tok 3.3022 (3.1441)	LR 1.250e-04
0: TRAIN [2][1530/3880]	Time 0.296 (0.382)	Data 1.04e-04 (3.11e-04)	Tok/s 34358 (36742)	Loss/tok 2.8704 (3.1445)	LR 1.250e-04
0: TRAIN [2][1540/3880]	Time 0.304 (0.382)	Data 1.07e-04 (3.10e-04)	Tok/s 34009 (36753)	Loss/tok 2.9878 (3.1446)	LR 1.250e-04
0: TRAIN [2][1550/3880]	Time 0.304 (0.383)	Data 1.11e-04 (3.09e-04)	Tok/s 33718 (36790)	Loss/tok 3.0364 (3.1461)	LR 1.250e-04
0: TRAIN [2][1560/3880]	Time 0.404 (0.383)	Data 1.10e-04 (3.07e-04)	Tok/s 41723 (36805)	Loss/tok 3.0988 (3.1463)	LR 1.250e-04
0: TRAIN [2][1570/3880]	Time 0.523 (0.383)	Data 1.10e-04 (3.06e-04)	Tok/s 44932 (36814)	Loss/tok 3.3471 (3.1463)	LR 1.250e-04
0: TRAIN [2][1580/3880]	Time 0.312 (0.383)	Data 1.12e-04 (3.05e-04)	Tok/s 33039 (36799)	Loss/tok 2.9314 (3.1456)	LR 1.250e-04
0: TRAIN [2][1590/3880]	Time 0.325 (0.383)	Data 1.09e-04 (3.04e-04)	Tok/s 31588 (36785)	Loss/tok 2.9374 (3.1457)	LR 1.250e-04
0: TRAIN [2][1600/3880]	Time 0.303 (0.383)	Data 1.10e-04 (3.02e-04)	Tok/s 33961 (36794)	Loss/tok 3.0218 (3.1459)	LR 1.250e-04
0: TRAIN [2][1610/3880]	Time 0.422 (0.383)	Data 1.09e-04 (3.01e-04)	Tok/s 39394 (36809)	Loss/tok 3.1565 (3.1464)	LR 1.250e-04
0: TRAIN [2][1620/3880]	Time 0.299 (0.383)	Data 1.24e-04 (3.00e-04)	Tok/s 35086 (36803)	Loss/tok 2.9406 (3.1465)	LR 1.250e-04
0: TRAIN [2][1630/3880]	Time 0.302 (0.383)	Data 1.09e-04 (2.99e-04)	Tok/s 34341 (36822)	Loss/tok 3.0058 (3.1469)	LR 1.250e-04
0: TRAIN [2][1640/3880]	Time 0.408 (0.383)	Data 1.11e-04 (2.98e-04)	Tok/s 41648 (36830)	Loss/tok 3.0642 (3.1467)	LR 1.250e-04
0: TRAIN [2][1650/3880]	Time 0.345 (0.383)	Data 1.10e-04 (2.96e-04)	Tok/s 30070 (36830)	Loss/tok 2.9928 (3.1469)	LR 1.250e-04
0: TRAIN [2][1660/3880]	Time 0.548 (0.383)	Data 1.10e-04 (2.95e-04)	Tok/s 42946 (36815)	Loss/tok 3.2364 (3.1466)	LR 1.250e-04
0: TRAIN [2][1670/3880]	Time 0.309 (0.384)	Data 1.11e-04 (2.94e-04)	Tok/s 32916 (36816)	Loss/tok 2.9748 (3.1473)	LR 1.250e-04
0: TRAIN [2][1680/3880]	Time 0.292 (0.384)	Data 1.35e-04 (2.93e-04)	Tok/s 35708 (36823)	Loss/tok 2.9520 (3.1473)	LR 1.250e-04
0: TRAIN [2][1690/3880]	Time 0.303 (0.383)	Data 1.07e-04 (2.92e-04)	Tok/s 33911 (36820)	Loss/tok 2.9301 (3.1468)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1700/3880]	Time 0.295 (0.383)	Data 1.11e-04 (2.91e-04)	Tok/s 34467 (36809)	Loss/tok 3.0219 (3.1463)	LR 1.250e-04
0: TRAIN [2][1710/3880]	Time 0.402 (0.383)	Data 1.06e-04 (2.90e-04)	Tok/s 42062 (36797)	Loss/tok 3.0703 (3.1457)	LR 1.250e-04
0: TRAIN [2][1720/3880]	Time 0.523 (0.383)	Data 1.13e-04 (2.89e-04)	Tok/s 44721 (36810)	Loss/tok 3.2993 (3.1458)	LR 1.250e-04
0: TRAIN [2][1730/3880]	Time 0.462 (0.383)	Data 1.11e-04 (2.88e-04)	Tok/s 36096 (36810)	Loss/tok 3.0836 (3.1463)	LR 1.250e-04
0: TRAIN [2][1740/3880]	Time 0.514 (0.383)	Data 1.10e-04 (2.87e-04)	Tok/s 45604 (36808)	Loss/tok 3.3384 (3.1461)	LR 1.250e-04
0: TRAIN [2][1750/3880]	Time 0.623 (0.383)	Data 1.12e-04 (2.86e-04)	Tok/s 37194 (36810)	Loss/tok 3.2872 (3.1460)	LR 1.250e-04
0: TRAIN [2][1760/3880]	Time 0.408 (0.383)	Data 1.10e-04 (2.85e-04)	Tok/s 40908 (36797)	Loss/tok 3.1890 (3.1457)	LR 1.250e-04
0: TRAIN [2][1770/3880]	Time 0.509 (0.384)	Data 1.16e-04 (2.84e-04)	Tok/s 46289 (36827)	Loss/tok 3.3968 (3.1461)	LR 1.250e-04
0: TRAIN [2][1780/3880]	Time 0.421 (0.383)	Data 1.11e-04 (2.83e-04)	Tok/s 39911 (36820)	Loss/tok 3.1071 (3.1456)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1790/3880]	Time 0.389 (0.383)	Data 1.10e-04 (2.82e-04)	Tok/s 43039 (36837)	Loss/tok 3.0603 (3.1456)	LR 1.250e-04
0: TRAIN [2][1800/3880]	Time 0.434 (0.383)	Data 1.11e-04 (2.81e-04)	Tok/s 38994 (36811)	Loss/tok 3.0847 (3.1454)	LR 1.250e-04
0: TRAIN [2][1810/3880]	Time 0.400 (0.383)	Data 1.09e-04 (2.80e-04)	Tok/s 41937 (36801)	Loss/tok 3.1144 (3.1446)	LR 1.250e-04
0: TRAIN [2][1820/3880]	Time 0.505 (0.383)	Data 1.09e-04 (2.79e-04)	Tok/s 45979 (36799)	Loss/tok 3.3186 (3.1446)	LR 1.250e-04
0: TRAIN [2][1830/3880]	Time 0.314 (0.383)	Data 1.12e-04 (2.78e-04)	Tok/s 32294 (36811)	Loss/tok 2.8424 (3.1446)	LR 1.250e-04
0: TRAIN [2][1840/3880]	Time 0.304 (0.383)	Data 1.10e-04 (2.77e-04)	Tok/s 34288 (36809)	Loss/tok 2.9713 (3.1456)	LR 1.250e-04
0: TRAIN [2][1850/3880]	Time 0.402 (0.383)	Data 1.10e-04 (2.76e-04)	Tok/s 41341 (36828)	Loss/tok 3.2212 (3.1461)	LR 1.250e-04
0: TRAIN [2][1860/3880]	Time 0.356 (0.383)	Data 1.10e-04 (2.75e-04)	Tok/s 29151 (36826)	Loss/tok 2.8876 (3.1461)	LR 1.250e-04
0: TRAIN [2][1870/3880]	Time 0.293 (0.384)	Data 1.14e-04 (2.74e-04)	Tok/s 34515 (36817)	Loss/tok 2.9301 (3.1466)	LR 1.250e-04
0: TRAIN [2][1880/3880]	Time 0.320 (0.384)	Data 1.15e-04 (2.74e-04)	Tok/s 32318 (36815)	Loss/tok 3.0250 (3.1467)	LR 1.250e-04
0: TRAIN [2][1890/3880]	Time 0.503 (0.384)	Data 1.08e-04 (2.73e-04)	Tok/s 47036 (36819)	Loss/tok 3.2074 (3.1467)	LR 1.250e-04
0: TRAIN [2][1900/3880]	Time 0.300 (0.384)	Data 1.12e-04 (2.72e-04)	Tok/s 34998 (36842)	Loss/tok 2.9948 (3.1474)	LR 1.250e-04
0: TRAIN [2][1910/3880]	Time 0.461 (0.385)	Data 1.11e-04 (2.71e-04)	Tok/s 36703 (36844)	Loss/tok 3.0507 (3.1477)	LR 1.250e-04
0: TRAIN [2][1920/3880]	Time 0.219 (0.385)	Data 1.11e-04 (2.70e-04)	Tok/s 24393 (36835)	Loss/tok 2.5055 (3.1477)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1930/3880]	Time 0.327 (0.385)	Data 1.11e-04 (2.69e-04)	Tok/s 32019 (36835)	Loss/tok 3.1643 (3.1485)	LR 1.250e-04
0: TRAIN [2][1940/3880]	Time 0.386 (0.385)	Data 1.10e-04 (2.68e-04)	Tok/s 43844 (36847)	Loss/tok 3.1640 (3.1487)	LR 1.250e-04
0: TRAIN [2][1950/3880]	Time 0.289 (0.385)	Data 1.07e-04 (2.68e-04)	Tok/s 36974 (36849)	Loss/tok 2.8893 (3.1490)	LR 1.250e-04
0: TRAIN [2][1960/3880]	Time 0.397 (0.385)	Data 1.11e-04 (2.67e-04)	Tok/s 42437 (36858)	Loss/tok 3.0733 (3.1491)	LR 1.250e-04
0: TRAIN [2][1970/3880]	Time 0.442 (0.386)	Data 1.11e-04 (2.66e-04)	Tok/s 37592 (36833)	Loss/tok 3.1745 (3.1487)	LR 1.250e-04
0: TRAIN [2][1980/3880]	Time 0.656 (0.386)	Data 1.09e-04 (2.65e-04)	Tok/s 44741 (36840)	Loss/tok 3.6536 (3.1489)	LR 1.250e-04
0: TRAIN [2][1990/3880]	Time 0.301 (0.386)	Data 1.04e-04 (2.64e-04)	Tok/s 33980 (36844)	Loss/tok 2.9205 (3.1490)	LR 1.250e-04
0: TRAIN [2][2000/3880]	Time 0.547 (0.386)	Data 1.08e-04 (2.64e-04)	Tok/s 42684 (36841)	Loss/tok 3.1902 (3.1491)	LR 1.250e-04
0: TRAIN [2][2010/3880]	Time 0.407 (0.386)	Data 1.08e-04 (2.63e-04)	Tok/s 40698 (36838)	Loss/tok 3.1153 (3.1487)	LR 1.250e-04
0: TRAIN [2][2020/3880]	Time 0.301 (0.386)	Data 1.12e-04 (2.62e-04)	Tok/s 33875 (36836)	Loss/tok 2.9063 (3.1484)	LR 1.250e-04
0: TRAIN [2][2030/3880]	Time 0.394 (0.386)	Data 1.11e-04 (2.61e-04)	Tok/s 42314 (36855)	Loss/tok 3.1508 (3.1486)	LR 1.250e-04
0: TRAIN [2][2040/3880]	Time 0.447 (0.386)	Data 1.09e-04 (2.61e-04)	Tok/s 37212 (36842)	Loss/tok 3.0809 (3.1482)	LR 1.250e-04
0: TRAIN [2][2050/3880]	Time 0.311 (0.386)	Data 1.06e-04 (2.60e-04)	Tok/s 33405 (36833)	Loss/tok 2.8374 (3.1476)	LR 1.250e-04
0: TRAIN [2][2060/3880]	Time 0.401 (0.386)	Data 1.09e-04 (2.59e-04)	Tok/s 42331 (36833)	Loss/tok 3.1932 (3.1475)	LR 1.250e-04
0: TRAIN [2][2070/3880]	Time 0.325 (0.386)	Data 1.12e-04 (2.58e-04)	Tok/s 31636 (36831)	Loss/tok 2.8270 (3.1480)	LR 1.250e-04
0: TRAIN [2][2080/3880]	Time 0.310 (0.386)	Data 1.11e-04 (2.58e-04)	Tok/s 32722 (36823)	Loss/tok 2.8359 (3.1478)	LR 1.250e-04
0: TRAIN [2][2090/3880]	Time 0.300 (0.386)	Data 1.12e-04 (2.57e-04)	Tok/s 34374 (36820)	Loss/tok 2.9844 (3.1475)	LR 1.250e-04
0: TRAIN [2][2100/3880]	Time 0.414 (0.386)	Data 1.13e-04 (2.56e-04)	Tok/s 40235 (36837)	Loss/tok 3.1423 (3.1474)	LR 1.250e-04
0: TRAIN [2][2110/3880]	Time 0.397 (0.385)	Data 1.07e-04 (2.56e-04)	Tok/s 41941 (36821)	Loss/tok 3.0816 (3.1470)	LR 1.250e-04
0: TRAIN [2][2120/3880]	Time 0.296 (0.385)	Data 1.08e-04 (2.55e-04)	Tok/s 35181 (36812)	Loss/tok 2.9821 (3.1466)	LR 1.250e-04
0: TRAIN [2][2130/3880]	Time 0.666 (0.385)	Data 1.16e-04 (2.54e-04)	Tok/s 44426 (36830)	Loss/tok 3.4519 (3.1476)	LR 1.250e-04
0: TRAIN [2][2140/3880]	Time 0.579 (0.385)	Data 1.11e-04 (2.54e-04)	Tok/s 40139 (36799)	Loss/tok 3.3685 (3.1473)	LR 1.250e-04
0: TRAIN [2][2150/3880]	Time 0.206 (0.385)	Data 1.10e-04 (2.53e-04)	Tok/s 25319 (36787)	Loss/tok 2.5828 (3.1472)	LR 1.250e-04
0: TRAIN [2][2160/3880]	Time 0.305 (0.385)	Data 1.09e-04 (2.52e-04)	Tok/s 34512 (36786)	Loss/tok 2.9630 (3.1473)	LR 1.250e-04
0: TRAIN [2][2170/3880]	Time 0.295 (0.385)	Data 1.08e-04 (2.52e-04)	Tok/s 35004 (36797)	Loss/tok 3.0675 (3.1476)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][2180/3880]	Time 0.501 (0.385)	Data 1.11e-04 (2.51e-04)	Tok/s 46111 (36808)	Loss/tok 3.2416 (3.1473)	LR 1.250e-04
0: TRAIN [2][2190/3880]	Time 0.414 (0.385)	Data 1.14e-04 (2.50e-04)	Tok/s 40958 (36811)	Loss/tok 3.1683 (3.1469)	LR 1.250e-04
0: TRAIN [2][2200/3880]	Time 0.538 (0.385)	Data 1.15e-04 (2.50e-04)	Tok/s 43105 (36820)	Loss/tok 3.4090 (3.1474)	LR 1.250e-04
0: TRAIN [2][2210/3880]	Time 0.300 (0.386)	Data 1.13e-04 (2.49e-04)	Tok/s 34518 (36834)	Loss/tok 2.8928 (3.1477)	LR 1.250e-04
0: TRAIN [2][2220/3880]	Time 0.301 (0.385)	Data 1.11e-04 (2.48e-04)	Tok/s 34202 (36827)	Loss/tok 2.9169 (3.1471)	LR 1.250e-04
0: TRAIN [2][2230/3880]	Time 0.298 (0.385)	Data 1.10e-04 (2.48e-04)	Tok/s 34466 (36843)	Loss/tok 2.9339 (3.1476)	LR 1.250e-04
0: TRAIN [2][2240/3880]	Time 0.320 (0.385)	Data 1.07e-04 (2.47e-04)	Tok/s 32563 (36848)	Loss/tok 2.9789 (3.1475)	LR 1.250e-04
0: TRAIN [2][2250/3880]	Time 0.313 (0.385)	Data 1.08e-04 (2.47e-04)	Tok/s 33173 (36834)	Loss/tok 2.8412 (3.1473)	LR 1.250e-04
0: TRAIN [2][2260/3880]	Time 0.415 (0.385)	Data 1.11e-04 (2.46e-04)	Tok/s 40280 (36831)	Loss/tok 3.1147 (3.1469)	LR 1.250e-04
0: TRAIN [2][2270/3880]	Time 0.406 (0.385)	Data 1.06e-04 (2.45e-04)	Tok/s 41129 (36840)	Loss/tok 3.1002 (3.1473)	LR 1.250e-04
0: TRAIN [2][2280/3880]	Time 0.303 (0.385)	Data 1.09e-04 (2.45e-04)	Tok/s 33582 (36852)	Loss/tok 2.8949 (3.1474)	LR 1.250e-04
0: TRAIN [2][2290/3880]	Time 0.311 (0.385)	Data 1.14e-04 (2.44e-04)	Tok/s 33273 (36845)	Loss/tok 2.9904 (3.1473)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2300/3880]	Time 0.523 (0.385)	Data 1.24e-04 (2.44e-04)	Tok/s 45172 (36840)	Loss/tok 3.2139 (3.1474)	LR 1.250e-04
0: TRAIN [2][2310/3880]	Time 0.521 (0.385)	Data 1.09e-04 (2.43e-04)	Tok/s 45054 (36840)	Loss/tok 3.3634 (3.1471)	LR 1.250e-04
0: TRAIN [2][2320/3880]	Time 0.513 (0.385)	Data 1.11e-04 (2.42e-04)	Tok/s 45753 (36846)	Loss/tok 3.2403 (3.1470)	LR 1.250e-04
0: TRAIN [2][2330/3880]	Time 0.310 (0.385)	Data 1.10e-04 (2.42e-04)	Tok/s 33427 (36839)	Loss/tok 2.9087 (3.1473)	LR 1.250e-04
0: TRAIN [2][2340/3880]	Time 0.205 (0.385)	Data 1.12e-04 (2.41e-04)	Tok/s 26130 (36850)	Loss/tok 2.5654 (3.1474)	LR 1.250e-04
0: TRAIN [2][2350/3880]	Time 0.298 (0.385)	Data 1.11e-04 (2.41e-04)	Tok/s 34763 (36865)	Loss/tok 2.8914 (3.1477)	LR 1.250e-04
0: TRAIN [2][2360/3880]	Time 0.209 (0.385)	Data 1.10e-04 (2.40e-04)	Tok/s 25424 (36872)	Loss/tok 2.6321 (3.1476)	LR 1.250e-04
0: TRAIN [2][2370/3880]	Time 0.300 (0.385)	Data 1.05e-04 (2.40e-04)	Tok/s 34625 (36863)	Loss/tok 2.9496 (3.1472)	LR 1.250e-04
0: TRAIN [2][2380/3880]	Time 0.502 (0.385)	Data 1.09e-04 (2.39e-04)	Tok/s 45625 (36883)	Loss/tok 3.3913 (3.1477)	LR 1.250e-04
0: TRAIN [2][2390/3880]	Time 0.303 (0.385)	Data 1.08e-04 (2.38e-04)	Tok/s 33741 (36887)	Loss/tok 2.8326 (3.1476)	LR 1.250e-04
0: TRAIN [2][2400/3880]	Time 0.295 (0.385)	Data 1.09e-04 (2.38e-04)	Tok/s 34787 (36884)	Loss/tok 2.9377 (3.1476)	LR 1.250e-04
0: TRAIN [2][2410/3880]	Time 0.680 (0.385)	Data 1.13e-04 (2.37e-04)	Tok/s 43733 (36885)	Loss/tok 3.5099 (3.1480)	LR 1.250e-04
0: TRAIN [2][2420/3880]	Time 0.298 (0.385)	Data 1.09e-04 (2.37e-04)	Tok/s 35036 (36894)	Loss/tok 2.9858 (3.1484)	LR 1.250e-04
0: TRAIN [2][2430/3880]	Time 0.203 (0.385)	Data 1.07e-04 (2.36e-04)	Tok/s 25720 (36880)	Loss/tok 2.5345 (3.1480)	LR 1.250e-04
0: TRAIN [2][2440/3880]	Time 0.357 (0.385)	Data 1.07e-04 (2.36e-04)	Tok/s 28800 (36875)	Loss/tok 2.9788 (3.1478)	LR 1.250e-04
0: TRAIN [2][2450/3880]	Time 0.313 (0.385)	Data 1.09e-04 (2.35e-04)	Tok/s 32427 (36861)	Loss/tok 2.8716 (3.1474)	LR 1.250e-04
0: TRAIN [2][2460/3880]	Time 0.403 (0.385)	Data 1.08e-04 (2.35e-04)	Tok/s 41032 (36868)	Loss/tok 3.2210 (3.1475)	LR 1.250e-04
0: TRAIN [2][2470/3880]	Time 0.401 (0.385)	Data 1.11e-04 (2.34e-04)	Tok/s 42120 (36873)	Loss/tok 3.1871 (3.1474)	LR 1.250e-04
0: TRAIN [2][2480/3880]	Time 0.295 (0.385)	Data 1.09e-04 (2.34e-04)	Tok/s 35296 (36879)	Loss/tok 3.0497 (3.1473)	LR 1.250e-04
0: TRAIN [2][2490/3880]	Time 0.369 (0.385)	Data 1.13e-04 (2.33e-04)	Tok/s 28194 (36871)	Loss/tok 2.9419 (3.1470)	LR 1.250e-04
0: TRAIN [2][2500/3880]	Time 0.314 (0.385)	Data 1.06e-04 (2.33e-04)	Tok/s 32344 (36860)	Loss/tok 2.9696 (3.1468)	LR 1.250e-04
0: TRAIN [2][2510/3880]	Time 0.500 (0.385)	Data 1.09e-04 (2.32e-04)	Tok/s 47200 (36868)	Loss/tok 3.2523 (3.1469)	LR 1.250e-04
0: TRAIN [2][2520/3880]	Time 0.528 (0.385)	Data 1.08e-04 (2.32e-04)	Tok/s 44329 (36858)	Loss/tok 3.2419 (3.1465)	LR 1.250e-04
0: TRAIN [2][2530/3880]	Time 0.523 (0.385)	Data 1.06e-04 (2.31e-04)	Tok/s 45165 (36864)	Loss/tok 3.1074 (3.1462)	LR 1.250e-04
0: TRAIN [2][2540/3880]	Time 0.521 (0.385)	Data 1.07e-04 (2.31e-04)	Tok/s 44529 (36859)	Loss/tok 3.3080 (3.1461)	LR 1.250e-04
0: TRAIN [2][2550/3880]	Time 0.518 (0.385)	Data 1.06e-04 (2.30e-04)	Tok/s 45072 (36861)	Loss/tok 3.3769 (3.1461)	LR 1.250e-04
0: TRAIN [2][2560/3880]	Time 0.463 (0.385)	Data 1.08e-04 (2.30e-04)	Tok/s 36570 (36862)	Loss/tok 3.2222 (3.1463)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][2570/3880]	Time 0.421 (0.385)	Data 1.09e-04 (2.29e-04)	Tok/s 39480 (36864)	Loss/tok 3.1077 (3.1462)	LR 1.250e-04
0: TRAIN [2][2580/3880]	Time 0.403 (0.385)	Data 1.11e-04 (2.29e-04)	Tok/s 41324 (36869)	Loss/tok 3.1560 (3.1465)	LR 1.250e-04
0: TRAIN [2][2590/3880]	Time 0.461 (0.385)	Data 1.10e-04 (2.29e-04)	Tok/s 36176 (36870)	Loss/tok 3.1212 (3.1465)	LR 1.250e-04
0: TRAIN [2][2600/3880]	Time 0.208 (0.385)	Data 1.07e-04 (2.28e-04)	Tok/s 25134 (36855)	Loss/tok 2.5494 (3.1460)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2610/3880]	Time 0.391 (0.385)	Data 1.10e-04 (2.28e-04)	Tok/s 42699 (36860)	Loss/tok 3.1284 (3.1460)	LR 1.250e-04
0: TRAIN [2][2620/3880]	Time 0.301 (0.385)	Data 1.10e-04 (2.27e-04)	Tok/s 35049 (36871)	Loss/tok 3.0070 (3.1460)	LR 1.250e-04
0: TRAIN [2][2630/3880]	Time 0.399 (0.385)	Data 1.10e-04 (2.27e-04)	Tok/s 41866 (36884)	Loss/tok 3.0832 (3.1461)	LR 1.250e-04
0: TRAIN [2][2640/3880]	Time 0.342 (0.385)	Data 1.10e-04 (2.26e-04)	Tok/s 29848 (36859)	Loss/tok 2.9341 (3.1460)	LR 1.250e-04
0: TRAIN [2][2650/3880]	Time 0.413 (0.385)	Data 1.06e-04 (2.26e-04)	Tok/s 40856 (36847)	Loss/tok 3.1594 (3.1456)	LR 1.250e-04
0: TRAIN [2][2660/3880]	Time 0.203 (0.385)	Data 1.12e-04 (2.25e-04)	Tok/s 25959 (36836)	Loss/tok 2.5292 (3.1450)	LR 1.250e-04
0: TRAIN [2][2670/3880]	Time 0.407 (0.385)	Data 1.10e-04 (2.25e-04)	Tok/s 40896 (36839)	Loss/tok 3.1490 (3.1449)	LR 1.250e-04
0: TRAIN [2][2680/3880]	Time 0.414 (0.385)	Data 1.10e-04 (2.24e-04)	Tok/s 40945 (36844)	Loss/tok 3.1261 (3.1447)	LR 1.250e-04
0: TRAIN [2][2690/3880]	Time 0.406 (0.385)	Data 1.06e-04 (2.24e-04)	Tok/s 41313 (36843)	Loss/tok 3.1221 (3.1446)	LR 1.250e-04
0: TRAIN [2][2700/3880]	Time 0.292 (0.384)	Data 1.13e-04 (2.24e-04)	Tok/s 35432 (36845)	Loss/tok 2.8366 (3.1443)	LR 1.250e-04
0: TRAIN [2][2710/3880]	Time 0.309 (0.385)	Data 1.04e-04 (2.23e-04)	Tok/s 33158 (36833)	Loss/tok 2.9439 (3.1443)	LR 1.250e-04
0: TRAIN [2][2720/3880]	Time 0.207 (0.385)	Data 1.04e-04 (2.23e-04)	Tok/s 24923 (36832)	Loss/tok 2.4302 (3.1440)	LR 1.250e-04
0: TRAIN [2][2730/3880]	Time 0.308 (0.384)	Data 1.08e-04 (2.22e-04)	Tok/s 33631 (36826)	Loss/tok 2.8638 (3.1436)	LR 1.250e-04
0: TRAIN [2][2740/3880]	Time 0.299 (0.384)	Data 1.10e-04 (2.22e-04)	Tok/s 33927 (36834)	Loss/tok 3.0315 (3.1435)	LR 1.250e-04
0: TRAIN [2][2750/3880]	Time 0.299 (0.384)	Data 1.08e-04 (2.22e-04)	Tok/s 34654 (36839)	Loss/tok 3.0677 (3.1434)	LR 1.250e-04
0: TRAIN [2][2760/3880]	Time 0.304 (0.384)	Data 1.11e-04 (2.21e-04)	Tok/s 34389 (36836)	Loss/tok 2.8941 (3.1434)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2770/3880]	Time 0.305 (0.384)	Data 1.06e-04 (2.21e-04)	Tok/s 33307 (36838)	Loss/tok 3.0345 (3.1434)	LR 1.250e-04
0: TRAIN [2][2780/3880]	Time 0.656 (0.384)	Data 1.09e-04 (2.20e-04)	Tok/s 45588 (36844)	Loss/tok 3.4870 (3.1434)	LR 1.250e-04
0: TRAIN [2][2790/3880]	Time 0.510 (0.384)	Data 1.12e-04 (2.20e-04)	Tok/s 45797 (36847)	Loss/tok 3.3132 (3.1432)	LR 1.250e-04
0: TRAIN [2][2800/3880]	Time 0.208 (0.384)	Data 1.08e-04 (2.20e-04)	Tok/s 25250 (36838)	Loss/tok 2.5250 (3.1429)	LR 1.250e-04
0: TRAIN [2][2810/3880]	Time 0.444 (0.384)	Data 1.09e-04 (2.19e-04)	Tok/s 37414 (36838)	Loss/tok 3.1497 (3.1427)	LR 1.250e-04
0: TRAIN [2][2820/3880]	Time 0.312 (0.384)	Data 1.10e-04 (2.19e-04)	Tok/s 33154 (36842)	Loss/tok 2.9045 (3.1428)	LR 1.250e-04
0: TRAIN [2][2830/3880]	Time 0.401 (0.384)	Data 1.07e-04 (2.18e-04)	Tok/s 41666 (36847)	Loss/tok 3.2080 (3.1434)	LR 1.250e-04
0: TRAIN [2][2840/3880]	Time 0.499 (0.384)	Data 1.14e-04 (2.18e-04)	Tok/s 46507 (36858)	Loss/tok 3.3919 (3.1435)	LR 1.250e-04
0: TRAIN [2][2850/3880]	Time 0.197 (0.384)	Data 1.11e-04 (2.18e-04)	Tok/s 26629 (36853)	Loss/tok 2.6504 (3.1434)	LR 1.250e-04
0: TRAIN [2][2860/3880]	Time 0.514 (0.384)	Data 1.13e-04 (2.17e-04)	Tok/s 32532 (36850)	Loss/tok 3.1026 (3.1438)	LR 1.250e-04
0: TRAIN [2][2870/3880]	Time 0.312 (0.385)	Data 1.09e-04 (2.17e-04)	Tok/s 33878 (36842)	Loss/tok 2.9354 (3.1438)	LR 1.250e-04
0: TRAIN [2][2880/3880]	Time 0.417 (0.385)	Data 1.09e-04 (2.16e-04)	Tok/s 40190 (36842)	Loss/tok 3.0799 (3.1436)	LR 1.250e-04
0: TRAIN [2][2890/3880]	Time 0.293 (0.384)	Data 1.08e-04 (2.16e-04)	Tok/s 35012 (36843)	Loss/tok 2.9874 (3.1436)	LR 1.250e-04
0: TRAIN [2][2900/3880]	Time 0.624 (0.385)	Data 1.11e-04 (2.16e-04)	Tok/s 37268 (36845)	Loss/tok 3.3442 (3.1442)	LR 1.250e-04
0: TRAIN [2][2910/3880]	Time 0.644 (0.385)	Data 1.08e-04 (2.15e-04)	Tok/s 45465 (36845)	Loss/tok 3.5366 (3.1445)	LR 1.250e-04
0: TRAIN [2][2920/3880]	Time 0.626 (0.385)	Data 1.10e-04 (2.15e-04)	Tok/s 48377 (36852)	Loss/tok 3.4044 (3.1447)	LR 1.250e-04
0: TRAIN [2][2930/3880]	Time 0.298 (0.385)	Data 1.08e-04 (2.15e-04)	Tok/s 35174 (36847)	Loss/tok 2.9525 (3.1446)	LR 1.250e-04
0: TRAIN [2][2940/3880]	Time 0.302 (0.385)	Data 1.01e-04 (2.14e-04)	Tok/s 34167 (36847)	Loss/tok 2.9111 (3.1444)	LR 1.250e-04
0: TRAIN [2][2950/3880]	Time 0.405 (0.385)	Data 1.09e-04 (2.14e-04)	Tok/s 41843 (36844)	Loss/tok 3.0492 (3.1440)	LR 1.250e-04
0: TRAIN [2][2960/3880]	Time 0.302 (0.384)	Data 1.08e-04 (2.13e-04)	Tok/s 33853 (36842)	Loss/tok 3.1542 (3.1439)	LR 1.250e-04
0: TRAIN [2][2970/3880]	Time 0.409 (0.384)	Data 1.06e-04 (2.13e-04)	Tok/s 41425 (36840)	Loss/tok 3.0964 (3.1436)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2980/3880]	Time 0.211 (0.384)	Data 1.09e-04 (2.13e-04)	Tok/s 25046 (36833)	Loss/tok 2.4700 (3.1437)	LR 1.250e-04
0: TRAIN [2][2990/3880]	Time 0.332 (0.384)	Data 1.14e-04 (2.12e-04)	Tok/s 31546 (36816)	Loss/tok 3.0284 (3.1433)	LR 1.250e-04
0: TRAIN [2][3000/3880]	Time 0.411 (0.384)	Data 1.07e-04 (2.12e-04)	Tok/s 40604 (36807)	Loss/tok 3.1899 (3.1431)	LR 1.250e-04
0: TRAIN [2][3010/3880]	Time 0.505 (0.384)	Data 1.13e-04 (2.12e-04)	Tok/s 46354 (36817)	Loss/tok 3.3183 (3.1432)	LR 1.250e-04
0: TRAIN [2][3020/3880]	Time 0.415 (0.384)	Data 1.08e-04 (2.11e-04)	Tok/s 40110 (36814)	Loss/tok 3.1739 (3.1428)	LR 1.250e-04
0: TRAIN [2][3030/3880]	Time 0.308 (0.384)	Data 1.13e-04 (2.11e-04)	Tok/s 34297 (36812)	Loss/tok 2.8211 (3.1425)	LR 1.250e-04
0: TRAIN [2][3040/3880]	Time 0.534 (0.384)	Data 1.09e-04 (2.11e-04)	Tok/s 43991 (36818)	Loss/tok 3.3379 (3.1426)	LR 1.250e-04
0: TRAIN [2][3050/3880]	Time 0.418 (0.384)	Data 1.09e-04 (2.10e-04)	Tok/s 40460 (36810)	Loss/tok 3.0861 (3.1424)	LR 1.250e-04
0: TRAIN [2][3060/3880]	Time 0.325 (0.384)	Data 1.12e-04 (2.10e-04)	Tok/s 32059 (36806)	Loss/tok 2.9925 (3.1422)	LR 1.250e-04
0: TRAIN [2][3070/3880]	Time 0.302 (0.384)	Data 1.10e-04 (2.10e-04)	Tok/s 34956 (36808)	Loss/tok 2.8566 (3.1425)	LR 1.250e-04
0: TRAIN [2][3080/3880]	Time 0.315 (0.384)	Data 1.10e-04 (2.09e-04)	Tok/s 32028 (36812)	Loss/tok 3.0512 (3.1424)	LR 1.250e-04
0: TRAIN [2][3090/3880]	Time 0.725 (0.384)	Data 1.09e-04 (2.09e-04)	Tok/s 41030 (36803)	Loss/tok 3.4249 (3.1425)	LR 1.250e-04
0: TRAIN [2][3100/3880]	Time 0.502 (0.384)	Data 1.12e-04 (2.09e-04)	Tok/s 45572 (36811)	Loss/tok 3.3350 (3.1423)	LR 1.250e-04
0: TRAIN [2][3110/3880]	Time 0.299 (0.384)	Data 1.08e-04 (2.08e-04)	Tok/s 34085 (36812)	Loss/tok 2.8834 (3.1422)	LR 1.250e-04
0: TRAIN [2][3120/3880]	Time 0.390 (0.383)	Data 1.05e-04 (2.08e-04)	Tok/s 43156 (36809)	Loss/tok 3.1787 (3.1419)	LR 1.250e-04
0: TRAIN [2][3130/3880]	Time 0.579 (0.384)	Data 1.04e-04 (2.08e-04)	Tok/s 39722 (36809)	Loss/tok 3.3559 (3.1423)	LR 1.250e-04
0: TRAIN [2][3140/3880]	Time 0.640 (0.384)	Data 1.13e-04 (2.07e-04)	Tok/s 45780 (36812)	Loss/tok 3.5205 (3.1424)	LR 1.250e-04
0: TRAIN [2][3150/3880]	Time 0.441 (0.384)	Data 1.08e-04 (2.07e-04)	Tok/s 38168 (36811)	Loss/tok 3.0258 (3.1424)	LR 1.250e-04
0: TRAIN [2][3160/3880]	Time 0.411 (0.384)	Data 1.08e-04 (2.07e-04)	Tok/s 41062 (36811)	Loss/tok 3.1348 (3.1423)	LR 1.250e-04
0: TRAIN [2][3170/3880]	Time 0.308 (0.384)	Data 1.13e-04 (2.07e-04)	Tok/s 33597 (36821)	Loss/tok 2.8702 (3.1424)	LR 1.250e-04
0: TRAIN [2][3180/3880]	Time 0.206 (0.384)	Data 1.13e-04 (2.06e-04)	Tok/s 25070 (36814)	Loss/tok 2.4088 (3.1423)	LR 1.250e-04
0: TRAIN [2][3190/3880]	Time 0.327 (0.384)	Data 1.10e-04 (2.06e-04)	Tok/s 31352 (36808)	Loss/tok 2.9596 (3.1421)	LR 1.250e-04
0: TRAIN [2][3200/3880]	Time 0.409 (0.383)	Data 1.08e-04 (2.06e-04)	Tok/s 40843 (36812)	Loss/tok 3.1868 (3.1421)	LR 1.250e-04
0: TRAIN [2][3210/3880]	Time 0.541 (0.384)	Data 1.07e-04 (2.05e-04)	Tok/s 42848 (36814)	Loss/tok 3.3898 (3.1421)	LR 1.250e-04
0: TRAIN [2][3220/3880]	Time 0.204 (0.383)	Data 1.10e-04 (2.05e-04)	Tok/s 25383 (36805)	Loss/tok 2.5775 (3.1419)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3230/3880]	Time 0.196 (0.383)	Data 1.13e-04 (2.05e-04)	Tok/s 26685 (36806)	Loss/tok 2.5318 (3.1418)	LR 1.250e-04
0: TRAIN [2][3240/3880]	Time 0.304 (0.383)	Data 1.08e-04 (2.04e-04)	Tok/s 33681 (36800)	Loss/tok 2.9521 (3.1416)	LR 1.250e-04
0: TRAIN [2][3250/3880]	Time 0.394 (0.383)	Data 1.06e-04 (2.04e-04)	Tok/s 42600 (36814)	Loss/tok 3.2176 (3.1419)	LR 1.250e-04
0: TRAIN [2][3260/3880]	Time 0.215 (0.383)	Data 1.11e-04 (2.04e-04)	Tok/s 25036 (36812)	Loss/tok 2.5457 (3.1418)	LR 1.250e-04
0: TRAIN [2][3270/3880]	Time 0.306 (0.383)	Data 1.08e-04 (2.04e-04)	Tok/s 34217 (36812)	Loss/tok 2.9160 (3.1415)	LR 1.250e-04
0: TRAIN [2][3280/3880]	Time 0.209 (0.383)	Data 1.06e-04 (2.03e-04)	Tok/s 24759 (36810)	Loss/tok 2.4043 (3.1413)	LR 1.250e-04
0: TRAIN [2][3290/3880]	Time 0.435 (0.383)	Data 1.11e-04 (2.03e-04)	Tok/s 38364 (36817)	Loss/tok 3.1965 (3.1417)	LR 1.250e-04
0: TRAIN [2][3300/3880]	Time 0.418 (0.383)	Data 1.09e-04 (2.03e-04)	Tok/s 40290 (36816)	Loss/tok 3.0177 (3.1415)	LR 1.250e-04
0: TRAIN [2][3310/3880]	Time 0.421 (0.383)	Data 1.13e-04 (2.02e-04)	Tok/s 39835 (36813)	Loss/tok 3.1149 (3.1412)	LR 1.250e-04
0: TRAIN [2][3320/3880]	Time 0.392 (0.383)	Data 1.12e-04 (2.02e-04)	Tok/s 41787 (36816)	Loss/tok 3.1672 (3.1413)	LR 1.250e-04
0: TRAIN [2][3330/3880]	Time 0.296 (0.383)	Data 1.07e-04 (2.02e-04)	Tok/s 35371 (36815)	Loss/tok 2.9815 (3.1410)	LR 1.250e-04
0: TRAIN [2][3340/3880]	Time 0.548 (0.383)	Data 1.11e-04 (2.02e-04)	Tok/s 42626 (36806)	Loss/tok 3.2962 (3.1409)	LR 1.250e-04
0: TRAIN [2][3350/3880]	Time 0.305 (0.383)	Data 1.09e-04 (2.01e-04)	Tok/s 33866 (36800)	Loss/tok 2.9364 (3.1406)	LR 1.250e-04
0: TRAIN [2][3360/3880]	Time 0.409 (0.383)	Data 1.08e-04 (2.01e-04)	Tok/s 41267 (36808)	Loss/tok 3.1454 (3.1406)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3370/3880]	Time 0.500 (0.383)	Data 1.07e-04 (2.01e-04)	Tok/s 46486 (36812)	Loss/tok 3.3274 (3.1406)	LR 1.250e-04
0: TRAIN [2][3380/3880]	Time 0.392 (0.383)	Data 1.10e-04 (2.00e-04)	Tok/s 43211 (36819)	Loss/tok 3.1761 (3.1408)	LR 1.250e-04
0: TRAIN [2][3390/3880]	Time 0.532 (0.383)	Data 1.09e-04 (2.00e-04)	Tok/s 43912 (36823)	Loss/tok 3.3063 (3.1408)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3400/3880]	Time 0.300 (0.383)	Data 1.11e-04 (2.00e-04)	Tok/s 34240 (36833)	Loss/tok 3.0046 (3.1411)	LR 1.250e-04
0: TRAIN [2][3410/3880]	Time 0.320 (0.383)	Data 1.09e-04 (2.00e-04)	Tok/s 31777 (36811)	Loss/tok 2.9591 (3.1407)	LR 1.250e-04
0: TRAIN [2][3420/3880]	Time 0.207 (0.383)	Data 1.12e-04 (1.99e-04)	Tok/s 25786 (36813)	Loss/tok 2.6103 (3.1408)	LR 1.250e-04
0: TRAIN [2][3430/3880]	Time 0.310 (0.383)	Data 1.07e-04 (1.99e-04)	Tok/s 33981 (36812)	Loss/tok 2.9210 (3.1406)	LR 1.250e-04
0: TRAIN [2][3440/3880]	Time 0.301 (0.383)	Data 1.10e-04 (1.99e-04)	Tok/s 34108 (36807)	Loss/tok 2.9265 (3.1404)	LR 1.250e-04
0: TRAIN [2][3450/3880]	Time 0.202 (0.383)	Data 1.06e-04 (1.99e-04)	Tok/s 27017 (36806)	Loss/tok 2.6486 (3.1403)	LR 1.250e-04
0: TRAIN [2][3460/3880]	Time 0.389 (0.383)	Data 1.06e-04 (1.98e-04)	Tok/s 43105 (36805)	Loss/tok 3.0732 (3.1401)	LR 1.250e-04
0: TRAIN [2][3470/3880]	Time 0.222 (0.383)	Data 1.15e-04 (1.98e-04)	Tok/s 23487 (36802)	Loss/tok 2.5716 (3.1401)	LR 1.250e-04
0: TRAIN [2][3480/3880]	Time 0.684 (0.383)	Data 1.07e-04 (1.98e-04)	Tok/s 43883 (36797)	Loss/tok 3.4347 (3.1400)	LR 1.250e-04
0: TRAIN [2][3490/3880]	Time 0.729 (0.383)	Data 1.18e-04 (1.98e-04)	Tok/s 40869 (36798)	Loss/tok 3.4602 (3.1400)	LR 1.250e-04
0: TRAIN [2][3500/3880]	Time 0.300 (0.382)	Data 1.09e-04 (1.97e-04)	Tok/s 34043 (36793)	Loss/tok 2.8965 (3.1397)	LR 1.250e-04
0: TRAIN [2][3510/3880]	Time 0.398 (0.382)	Data 1.11e-04 (1.97e-04)	Tok/s 41663 (36792)	Loss/tok 3.2065 (3.1394)	LR 1.250e-04
0: TRAIN [2][3520/3880]	Time 0.322 (0.382)	Data 1.07e-04 (1.97e-04)	Tok/s 31387 (36794)	Loss/tok 3.0077 (3.1395)	LR 1.250e-04
0: TRAIN [2][3530/3880]	Time 0.535 (0.383)	Data 1.09e-04 (1.97e-04)	Tok/s 44131 (36805)	Loss/tok 3.3195 (3.1399)	LR 1.250e-04
0: TRAIN [2][3540/3880]	Time 0.290 (0.383)	Data 1.10e-04 (1.96e-04)	Tok/s 35675 (36810)	Loss/tok 2.9556 (3.1404)	LR 1.250e-04
0: TRAIN [2][3550/3880]	Time 0.305 (0.383)	Data 1.08e-04 (1.96e-04)	Tok/s 32709 (36805)	Loss/tok 2.9408 (3.1402)	LR 1.250e-04
0: TRAIN [2][3560/3880]	Time 0.304 (0.383)	Data 1.09e-04 (1.96e-04)	Tok/s 34619 (36800)	Loss/tok 2.8846 (3.1399)	LR 1.250e-04
0: TRAIN [2][3570/3880]	Time 0.520 (0.383)	Data 1.10e-04 (1.96e-04)	Tok/s 45483 (36798)	Loss/tok 3.3010 (3.1397)	LR 1.250e-04
0: TRAIN [2][3580/3880]	Time 0.391 (0.383)	Data 1.11e-04 (1.95e-04)	Tok/s 42031 (36810)	Loss/tok 3.1731 (3.1400)	LR 1.250e-04
0: TRAIN [2][3590/3880]	Time 0.404 (0.383)	Data 1.08e-04 (1.95e-04)	Tok/s 41553 (36804)	Loss/tok 3.1861 (3.1399)	LR 1.250e-04
0: TRAIN [2][3600/3880]	Time 0.496 (0.383)	Data 1.08e-04 (1.95e-04)	Tok/s 34393 (36814)	Loss/tok 3.0712 (3.1403)	LR 1.250e-04
0: TRAIN [2][3610/3880]	Time 0.290 (0.383)	Data 1.08e-04 (1.95e-04)	Tok/s 35149 (36814)	Loss/tok 2.9726 (3.1401)	LR 1.250e-04
0: TRAIN [2][3620/3880]	Time 0.313 (0.383)	Data 1.09e-04 (1.94e-04)	Tok/s 32487 (36807)	Loss/tok 2.9981 (3.1399)	LR 1.250e-04
0: TRAIN [2][3630/3880]	Time 0.299 (0.383)	Data 1.05e-04 (1.94e-04)	Tok/s 34311 (36806)	Loss/tok 3.0644 (3.1398)	LR 1.250e-04
0: TRAIN [2][3640/3880]	Time 0.425 (0.383)	Data 1.04e-04 (1.94e-04)	Tok/s 39166 (36794)	Loss/tok 3.2087 (3.1397)	LR 1.250e-04
0: TRAIN [2][3650/3880]	Time 0.676 (0.383)	Data 1.09e-04 (1.94e-04)	Tok/s 43727 (36793)	Loss/tok 3.5254 (3.1399)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3660/3880]	Time 0.520 (0.383)	Data 1.11e-04 (1.93e-04)	Tok/s 44882 (36793)	Loss/tok 3.3125 (3.1398)	LR 1.250e-04
0: TRAIN [2][3670/3880]	Time 0.505 (0.383)	Data 1.11e-04 (1.93e-04)	Tok/s 46465 (36802)	Loss/tok 3.2852 (3.1400)	LR 1.250e-04
0: TRAIN [2][3680/3880]	Time 0.297 (0.382)	Data 1.09e-04 (1.93e-04)	Tok/s 34981 (36798)	Loss/tok 2.8973 (3.1397)	LR 1.250e-04
0: TRAIN [2][3690/3880]	Time 0.521 (0.382)	Data 1.11e-04 (1.93e-04)	Tok/s 45090 (36803)	Loss/tok 3.2530 (3.1398)	LR 1.250e-04
0: TRAIN [2][3700/3880]	Time 0.318 (0.382)	Data 1.12e-04 (1.93e-04)	Tok/s 32856 (36801)	Loss/tok 2.8531 (3.1396)	LR 1.250e-04
0: TRAIN [2][3710/3880]	Time 0.311 (0.382)	Data 1.08e-04 (1.92e-04)	Tok/s 32497 (36798)	Loss/tok 2.8073 (3.1395)	LR 1.250e-04
0: TRAIN [2][3720/3880]	Time 0.298 (0.383)	Data 1.11e-04 (1.92e-04)	Tok/s 34497 (36807)	Loss/tok 2.9125 (3.1398)	LR 1.250e-04
0: TRAIN [2][3730/3880]	Time 0.768 (0.383)	Data 1.11e-04 (1.92e-04)	Tok/s 38862 (36809)	Loss/tok 3.5255 (3.1400)	LR 1.250e-04
0: TRAIN [2][3740/3880]	Time 0.303 (0.383)	Data 1.08e-04 (1.92e-04)	Tok/s 32796 (36811)	Loss/tok 3.0390 (3.1402)	LR 1.250e-04
0: TRAIN [2][3750/3880]	Time 0.298 (0.383)	Data 1.10e-04 (1.91e-04)	Tok/s 34822 (36820)	Loss/tok 2.9995 (3.1404)	LR 1.250e-04
0: TRAIN [2][3760/3880]	Time 0.296 (0.383)	Data 1.08e-04 (1.91e-04)	Tok/s 35403 (36818)	Loss/tok 2.7614 (3.1402)	LR 1.250e-04
0: TRAIN [2][3770/3880]	Time 0.400 (0.383)	Data 1.07e-04 (1.91e-04)	Tok/s 41822 (36820)	Loss/tok 3.1138 (3.1400)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3780/3880]	Time 0.311 (0.383)	Data 1.07e-04 (1.91e-04)	Tok/s 33189 (36829)	Loss/tok 2.9093 (3.1403)	LR 1.250e-04
0: TRAIN [2][3790/3880]	Time 0.307 (0.383)	Data 1.07e-04 (1.91e-04)	Tok/s 33719 (36820)	Loss/tok 2.8816 (3.1400)	LR 1.250e-04
0: TRAIN [2][3800/3880]	Time 0.526 (0.382)	Data 1.15e-04 (1.90e-04)	Tok/s 44329 (36819)	Loss/tok 3.3111 (3.1398)	LR 1.250e-04
0: TRAIN [2][3810/3880]	Time 0.300 (0.382)	Data 1.04e-04 (1.90e-04)	Tok/s 34575 (36814)	Loss/tok 2.8850 (3.1397)	LR 1.250e-04
0: TRAIN [2][3820/3880]	Time 0.410 (0.382)	Data 1.10e-04 (1.90e-04)	Tok/s 40633 (36819)	Loss/tok 3.1751 (3.1397)	LR 1.250e-04
0: TRAIN [2][3830/3880]	Time 0.452 (0.382)	Data 1.07e-04 (1.90e-04)	Tok/s 36708 (36816)	Loss/tok 3.1604 (3.1399)	LR 1.250e-04
0: TRAIN [2][3840/3880]	Time 0.501 (0.382)	Data 1.12e-04 (1.89e-04)	Tok/s 46551 (36812)	Loss/tok 3.3600 (3.1397)	LR 1.250e-04
0: TRAIN [2][3850/3880]	Time 0.299 (0.382)	Data 1.10e-04 (1.89e-04)	Tok/s 34485 (36810)	Loss/tok 2.9551 (3.1396)	LR 1.250e-04
0: TRAIN [2][3860/3880]	Time 0.301 (0.382)	Data 1.14e-04 (1.89e-04)	Tok/s 33368 (36816)	Loss/tok 2.9711 (3.1396)	LR 1.250e-04
0: TRAIN [2][3870/3880]	Time 0.209 (0.383)	Data 1.02e-04 (1.89e-04)	Tok/s 24975 (36809)	Loss/tok 2.5949 (3.1399)	LR 1.250e-04
:::MLL 1572989516.251 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1572989516.252 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/6]	Time 0.798 (0.798)	Decoder iters 100.0 (100.0)	Tok/s 20632 (20632)
0: Running moses detokenizer
0: BLEU(score=23.662648221663567, counts=[37017, 18423, 10390, 6120], totals=[65580, 62577, 59574, 56576], precisions=[56.445562671546206, 29.440529267941894, 17.44049417531138, 10.817307692307692], bp=1.0, sys_len=65580, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1572989519.417 eval_accuracy: {"value": 23.66, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1572989519.417 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.1387	Test BLEU: 23.66
0: Performance: Epoch: 2	Training: 147229 Tok/s
0: Finished epoch 2
:::MLL 1572989519.418 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1572989519.418 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1572989519.418 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 1974706011
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [3][0/3880]	Time 0.525 (0.525)	Data 3.22e-01 (3.22e-01)	Tok/s 10354 (10354)	Loss/tok 2.5062 (2.5062)	LR 1.250e-04
0: TRAIN [3][10/3880]	Time 0.299 (0.359)	Data 1.10e-04 (2.94e-02)	Tok/s 34997 (33613)	Loss/tok 2.8997 (3.1347)	LR 1.250e-04
0: TRAIN [3][20/3880]	Time 0.518 (0.380)	Data 1.07e-04 (1.54e-02)	Tok/s 44998 (36949)	Loss/tok 3.2330 (3.1127)	LR 1.250e-04
0: TRAIN [3][30/3880]	Time 0.402 (0.385)	Data 1.08e-04 (1.05e-02)	Tok/s 42252 (37659)	Loss/tok 3.0462 (3.1438)	LR 1.250e-04
0: TRAIN [3][40/3880]	Time 0.292 (0.380)	Data 1.07e-04 (7.96e-03)	Tok/s 35898 (37690)	Loss/tok 2.8368 (3.1380)	LR 1.250e-04
0: TRAIN [3][50/3880]	Time 0.303 (0.375)	Data 1.07e-04 (6.42e-03)	Tok/s 34100 (37573)	Loss/tok 2.8843 (3.1253)	LR 1.250e-04
0: TRAIN [3][60/3880]	Time 0.448 (0.381)	Data 1.08e-04 (5.39e-03)	Tok/s 37634 (37056)	Loss/tok 3.1224 (3.1133)	LR 1.250e-04
0: TRAIN [3][70/3880]	Time 0.308 (0.381)	Data 1.14e-04 (4.64e-03)	Tok/s 33497 (36932)	Loss/tok 2.7989 (3.1062)	LR 1.250e-04
0: TRAIN [3][80/3880]	Time 0.534 (0.378)	Data 1.07e-04 (4.08e-03)	Tok/s 44215 (36911)	Loss/tok 3.1844 (3.0978)	LR 1.250e-04
0: TRAIN [3][90/3880]	Time 0.402 (0.376)	Data 1.11e-04 (3.65e-03)	Tok/s 42136 (37081)	Loss/tok 3.0889 (3.0943)	LR 1.250e-04
0: TRAIN [3][100/3880]	Time 0.318 (0.374)	Data 1.09e-04 (3.30e-03)	Tok/s 32731 (37040)	Loss/tok 2.8279 (3.0851)	LR 1.250e-04
0: TRAIN [3][110/3880]	Time 0.400 (0.373)	Data 1.08e-04 (3.01e-03)	Tok/s 41754 (36983)	Loss/tok 3.0458 (3.0835)	LR 1.250e-04
0: TRAIN [3][120/3880]	Time 0.306 (0.366)	Data 1.09e-04 (2.77e-03)	Tok/s 33652 (36631)	Loss/tok 2.8967 (3.0745)	LR 1.250e-04
0: TRAIN [3][130/3880]	Time 0.304 (0.365)	Data 1.07e-04 (2.57e-03)	Tok/s 33930 (36657)	Loss/tok 2.8215 (3.0702)	LR 1.250e-04
0: TRAIN [3][140/3880]	Time 0.401 (0.368)	Data 1.07e-04 (2.39e-03)	Tok/s 42356 (36878)	Loss/tok 3.2000 (3.0784)	LR 1.250e-04
0: TRAIN [3][150/3880]	Time 0.641 (0.369)	Data 1.11e-04 (2.24e-03)	Tok/s 46533 (36937)	Loss/tok 3.4932 (3.0871)	LR 1.250e-04
0: TRAIN [3][160/3880]	Time 0.399 (0.370)	Data 1.24e-04 (2.11e-03)	Tok/s 41823 (37061)	Loss/tok 3.0686 (3.0870)	LR 1.250e-04
0: TRAIN [3][170/3880]	Time 0.300 (0.373)	Data 1.06e-04 (1.99e-03)	Tok/s 34656 (37097)	Loss/tok 2.8763 (3.0935)	LR 1.250e-04
0: TRAIN [3][180/3880]	Time 0.307 (0.373)	Data 1.09e-04 (1.89e-03)	Tok/s 33282 (37170)	Loss/tok 3.0303 (3.0948)	LR 1.250e-04
0: TRAIN [3][190/3880]	Time 0.311 (0.374)	Data 1.08e-04 (1.79e-03)	Tok/s 33060 (37220)	Loss/tok 2.9296 (3.0970)	LR 1.250e-04
0: TRAIN [3][200/3880]	Time 0.410 (0.373)	Data 1.13e-04 (1.71e-03)	Tok/s 40906 (37188)	Loss/tok 3.0436 (3.0936)	LR 1.250e-04
0: TRAIN [3][210/3880]	Time 0.222 (0.372)	Data 1.15e-04 (1.63e-03)	Tok/s 24257 (37008)	Loss/tok 2.6257 (3.0916)	LR 1.250e-04
0: TRAIN [3][220/3880]	Time 0.300 (0.370)	Data 1.08e-04 (1.57e-03)	Tok/s 34565 (36768)	Loss/tok 2.9201 (3.0859)	LR 1.250e-04
0: TRAIN [3][230/3880]	Time 0.655 (0.372)	Data 1.08e-04 (1.50e-03)	Tok/s 45482 (36856)	Loss/tok 3.4871 (3.0914)	LR 1.250e-04
0: TRAIN [3][240/3880]	Time 0.310 (0.370)	Data 1.11e-04 (1.44e-03)	Tok/s 32970 (36782)	Loss/tok 2.8932 (3.0872)	LR 1.250e-04
0: TRAIN [3][250/3880]	Time 0.303 (0.370)	Data 1.06e-04 (1.39e-03)	Tok/s 33535 (36796)	Loss/tok 2.9147 (3.0836)	LR 1.250e-04
0: TRAIN [3][260/3880]	Time 0.485 (0.373)	Data 1.10e-04 (1.34e-03)	Tok/s 34346 (36886)	Loss/tok 3.0651 (3.0867)	LR 1.250e-04
0: TRAIN [3][270/3880]	Time 0.339 (0.373)	Data 1.09e-04 (1.30e-03)	Tok/s 30165 (36819)	Loss/tok 2.9754 (3.0869)	LR 1.250e-04
0: TRAIN [3][280/3880]	Time 0.300 (0.374)	Data 1.17e-04 (1.25e-03)	Tok/s 33770 (36814)	Loss/tok 2.9013 (3.0888)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][290/3880]	Time 0.656 (0.376)	Data 1.06e-04 (1.22e-03)	Tok/s 35618 (36939)	Loss/tok 3.2530 (3.0944)	LR 1.250e-04
0: TRAIN [3][300/3880]	Time 0.401 (0.376)	Data 1.04e-04 (1.18e-03)	Tok/s 41707 (36843)	Loss/tok 3.0035 (3.0925)	LR 1.250e-04
0: TRAIN [3][310/3880]	Time 0.773 (0.379)	Data 1.05e-04 (1.14e-03)	Tok/s 38880 (36853)	Loss/tok 3.3926 (3.0989)	LR 1.250e-04
0: TRAIN [3][320/3880]	Time 0.292 (0.380)	Data 1.05e-04 (1.11e-03)	Tok/s 35056 (36856)	Loss/tok 2.8623 (3.1008)	LR 1.250e-04
0: TRAIN [3][330/3880]	Time 0.453 (0.379)	Data 1.08e-04 (1.08e-03)	Tok/s 37167 (36806)	Loss/tok 3.0313 (3.0977)	LR 1.250e-04
0: TRAIN [3][340/3880]	Time 0.398 (0.380)	Data 1.06e-04 (1.05e-03)	Tok/s 41740 (36854)	Loss/tok 3.1319 (3.0978)	LR 1.250e-04
0: TRAIN [3][350/3880]	Time 0.398 (0.378)	Data 1.31e-04 (1.03e-03)	Tok/s 42006 (36822)	Loss/tok 2.9918 (3.0956)	LR 1.250e-04
0: TRAIN [3][360/3880]	Time 0.530 (0.378)	Data 1.06e-04 (1.00e-03)	Tok/s 44703 (36853)	Loss/tok 3.1726 (3.0952)	LR 1.250e-04
0: TRAIN [3][370/3880]	Time 0.299 (0.377)	Data 1.20e-04 (9.77e-04)	Tok/s 34602 (36825)	Loss/tok 3.0198 (3.0937)	LR 1.250e-04
0: TRAIN [3][380/3880]	Time 0.313 (0.375)	Data 2.01e-04 (9.54e-04)	Tok/s 33281 (36689)	Loss/tok 2.9490 (3.0903)	LR 1.250e-04
0: TRAIN [3][390/3880]	Time 0.299 (0.374)	Data 1.07e-04 (9.33e-04)	Tok/s 34242 (36645)	Loss/tok 2.8414 (3.0886)	LR 1.250e-04
0: TRAIN [3][400/3880]	Time 0.401 (0.374)	Data 1.08e-04 (9.12e-04)	Tok/s 42201 (36697)	Loss/tok 3.0609 (3.0873)	LR 1.250e-04
0: TRAIN [3][410/3880]	Time 0.296 (0.373)	Data 1.07e-04 (8.93e-04)	Tok/s 34941 (36698)	Loss/tok 2.8451 (3.0855)	LR 1.250e-04
0: TRAIN [3][420/3880]	Time 0.396 (0.373)	Data 1.14e-04 (8.74e-04)	Tok/s 43156 (36732)	Loss/tok 3.0428 (3.0853)	LR 1.250e-04
0: TRAIN [3][430/3880]	Time 0.628 (0.374)	Data 1.14e-04 (8.56e-04)	Tok/s 46964 (36825)	Loss/tok 3.4476 (3.0872)	LR 1.250e-04
0: TRAIN [3][440/3880]	Time 0.405 (0.373)	Data 1.19e-04 (8.40e-04)	Tok/s 41835 (36833)	Loss/tok 3.1371 (3.0866)	LR 1.250e-04
0: TRAIN [3][450/3880]	Time 0.308 (0.373)	Data 1.10e-04 (8.23e-04)	Tok/s 33079 (36771)	Loss/tok 2.8921 (3.0850)	LR 1.250e-04
0: TRAIN [3][460/3880]	Time 0.300 (0.373)	Data 1.17e-04 (8.08e-04)	Tok/s 34232 (36815)	Loss/tok 2.9517 (3.0859)	LR 1.250e-04
0: TRAIN [3][470/3880]	Time 0.346 (0.373)	Data 1.12e-04 (7.93e-04)	Tok/s 30059 (36757)	Loss/tok 2.8819 (3.0853)	LR 1.250e-04
0: TRAIN [3][480/3880]	Time 0.418 (0.374)	Data 1.07e-04 (7.79e-04)	Tok/s 40188 (36765)	Loss/tok 3.1675 (3.0867)	LR 1.250e-04
0: TRAIN [3][490/3880]	Time 0.494 (0.375)	Data 1.10e-04 (7.65e-04)	Tok/s 47774 (36811)	Loss/tok 3.2660 (3.0896)	LR 1.250e-04
0: TRAIN [3][500/3880]	Time 0.427 (0.375)	Data 1.10e-04 (7.52e-04)	Tok/s 39651 (36780)	Loss/tok 3.1141 (3.0883)	LR 1.250e-04
0: TRAIN [3][510/3880]	Time 0.300 (0.373)	Data 1.10e-04 (7.40e-04)	Tok/s 34431 (36738)	Loss/tok 2.9634 (3.0854)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][520/3880]	Time 0.299 (0.373)	Data 1.10e-04 (7.27e-04)	Tok/s 34381 (36764)	Loss/tok 2.8272 (3.0861)	LR 1.250e-04
0: TRAIN [3][530/3880]	Time 0.696 (0.374)	Data 1.12e-04 (7.16e-04)	Tok/s 42724 (36743)	Loss/tok 3.4877 (3.0877)	LR 1.250e-04
0: TRAIN [3][540/3880]	Time 0.691 (0.375)	Data 1.08e-04 (7.05e-04)	Tok/s 43059 (36798)	Loss/tok 3.5154 (3.0905)	LR 1.250e-04
0: TRAIN [3][550/3880]	Time 0.341 (0.377)	Data 1.08e-04 (6.94e-04)	Tok/s 30587 (36811)	Loss/tok 2.8760 (3.0916)	LR 1.250e-04
0: TRAIN [3][560/3880]	Time 0.302 (0.376)	Data 1.05e-04 (6.83e-04)	Tok/s 34604 (36745)	Loss/tok 2.9820 (3.0898)	LR 1.250e-04
0: TRAIN [3][570/3880]	Time 0.206 (0.377)	Data 1.12e-04 (6.73e-04)	Tok/s 25486 (36801)	Loss/tok 2.5680 (3.0922)	LR 1.250e-04
0: TRAIN [3][580/3880]	Time 0.413 (0.376)	Data 1.08e-04 (6.63e-04)	Tok/s 40343 (36789)	Loss/tok 3.1668 (3.0916)	LR 1.250e-04
0: TRAIN [3][590/3880]	Time 0.398 (0.375)	Data 1.09e-04 (6.54e-04)	Tok/s 42128 (36777)	Loss/tok 3.1382 (3.0910)	LR 1.250e-04
0: TRAIN [3][600/3880]	Time 0.323 (0.376)	Data 1.11e-04 (6.45e-04)	Tok/s 32357 (36734)	Loss/tok 2.8727 (3.0907)	LR 1.250e-04
0: TRAIN [3][610/3880]	Time 0.309 (0.377)	Data 1.09e-04 (6.36e-04)	Tok/s 32772 (36784)	Loss/tok 3.0234 (3.0928)	LR 1.250e-04
0: TRAIN [3][620/3880]	Time 0.297 (0.376)	Data 1.16e-04 (6.28e-04)	Tok/s 34582 (36743)	Loss/tok 2.8580 (3.0915)	LR 1.250e-04
0: TRAIN [3][630/3880]	Time 0.205 (0.375)	Data 1.21e-04 (6.20e-04)	Tok/s 25703 (36707)	Loss/tok 2.5320 (3.0908)	LR 1.250e-04
0: TRAIN [3][640/3880]	Time 0.330 (0.376)	Data 1.14e-04 (6.12e-04)	Tok/s 31406 (36748)	Loss/tok 3.0456 (3.0910)	LR 1.250e-04
0: TRAIN [3][650/3880]	Time 0.504 (0.377)	Data 1.12e-04 (6.04e-04)	Tok/s 46524 (36825)	Loss/tok 3.2731 (3.0937)	LR 1.250e-04
0: TRAIN [3][660/3880]	Time 0.421 (0.378)	Data 1.04e-04 (5.97e-04)	Tok/s 40242 (36818)	Loss/tok 3.1528 (3.0941)	LR 1.250e-04
0: TRAIN [3][670/3880]	Time 0.304 (0.378)	Data 1.10e-04 (5.89e-04)	Tok/s 33637 (36857)	Loss/tok 2.8870 (3.0942)	LR 1.250e-04
0: TRAIN [3][680/3880]	Time 0.392 (0.378)	Data 1.08e-04 (5.82e-04)	Tok/s 42937 (36872)	Loss/tok 3.0195 (3.0935)	LR 1.250e-04
0: TRAIN [3][690/3880]	Time 0.309 (0.378)	Data 1.09e-04 (5.75e-04)	Tok/s 32913 (36837)	Loss/tok 2.9518 (3.0929)	LR 1.250e-04
0: TRAIN [3][700/3880]	Time 0.370 (0.378)	Data 1.09e-04 (5.69e-04)	Tok/s 28073 (36837)	Loss/tok 2.8826 (3.0924)	LR 1.250e-04
0: TRAIN [3][710/3880]	Time 0.320 (0.378)	Data 1.10e-04 (5.62e-04)	Tok/s 32640 (36814)	Loss/tok 2.9449 (3.0921)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][720/3880]	Time 0.301 (0.379)	Data 1.08e-04 (5.56e-04)	Tok/s 34229 (36855)	Loss/tok 2.8575 (3.0948)	LR 1.250e-04
0: TRAIN [3][730/3880]	Time 0.445 (0.378)	Data 1.08e-04 (5.50e-04)	Tok/s 37512 (36810)	Loss/tok 3.0693 (3.0948)	LR 1.250e-04
0: TRAIN [3][740/3880]	Time 0.300 (0.378)	Data 1.05e-04 (5.44e-04)	Tok/s 33992 (36799)	Loss/tok 2.8655 (3.0943)	LR 1.250e-04
0: TRAIN [3][750/3880]	Time 0.202 (0.378)	Data 1.08e-04 (5.38e-04)	Tok/s 26076 (36804)	Loss/tok 2.5584 (3.0956)	LR 1.250e-04
0: TRAIN [3][760/3880]	Time 0.204 (0.377)	Data 1.04e-04 (5.32e-04)	Tok/s 25763 (36782)	Loss/tok 2.4899 (3.0944)	LR 1.250e-04
0: TRAIN [3][770/3880]	Time 0.413 (0.378)	Data 1.09e-04 (5.27e-04)	Tok/s 40809 (36806)	Loss/tok 2.9908 (3.0949)	LR 1.250e-04
0: TRAIN [3][780/3880]	Time 0.413 (0.378)	Data 1.08e-04 (5.22e-04)	Tok/s 40614 (36816)	Loss/tok 3.1416 (3.0949)	LR 1.250e-04
0: TRAIN [3][790/3880]	Time 0.398 (0.377)	Data 1.12e-04 (5.16e-04)	Tok/s 42291 (36814)	Loss/tok 3.0784 (3.0941)	LR 1.250e-04
0: TRAIN [3][800/3880]	Time 0.297 (0.377)	Data 1.06e-04 (5.11e-04)	Tok/s 34629 (36809)	Loss/tok 2.8333 (3.0935)	LR 1.250e-04
0: TRAIN [3][810/3880]	Time 0.291 (0.376)	Data 1.10e-04 (5.06e-04)	Tok/s 35912 (36802)	Loss/tok 2.8166 (3.0928)	LR 1.250e-04
0: TRAIN [3][820/3880]	Time 0.405 (0.377)	Data 1.01e-04 (5.01e-04)	Tok/s 41480 (36844)	Loss/tok 3.2664 (3.0950)	LR 1.250e-04
0: TRAIN [3][830/3880]	Time 0.416 (0.377)	Data 1.11e-04 (4.97e-04)	Tok/s 40041 (36842)	Loss/tok 3.1996 (3.0944)	LR 1.250e-04
0: TRAIN [3][840/3880]	Time 0.401 (0.377)	Data 1.04e-04 (4.92e-04)	Tok/s 41273 (36876)	Loss/tok 3.2075 (3.0958)	LR 1.250e-04
0: TRAIN [3][850/3880]	Time 0.482 (0.378)	Data 1.05e-04 (4.88e-04)	Tok/s 35088 (36843)	Loss/tok 3.1670 (3.0958)	LR 1.250e-04
0: TRAIN [3][860/3880]	Time 0.413 (0.379)	Data 1.10e-04 (4.83e-04)	Tok/s 40191 (36858)	Loss/tok 3.0610 (3.0968)	LR 1.250e-04
0: TRAIN [3][870/3880]	Time 0.303 (0.378)	Data 1.07e-04 (4.79e-04)	Tok/s 34342 (36849)	Loss/tok 2.7994 (3.0958)	LR 1.250e-04
0: TRAIN [3][880/3880]	Time 0.298 (0.378)	Data 1.11e-04 (4.75e-04)	Tok/s 35087 (36833)	Loss/tok 2.8268 (3.0947)	LR 1.250e-04
0: TRAIN [3][890/3880]	Time 0.400 (0.377)	Data 1.07e-04 (4.71e-04)	Tok/s 42008 (36794)	Loss/tok 3.0869 (3.0933)	LR 1.250e-04
0: TRAIN [3][900/3880]	Time 0.303 (0.377)	Data 1.12e-04 (4.67e-04)	Tok/s 34239 (36807)	Loss/tok 2.8761 (3.0936)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][910/3880]	Time 0.351 (0.377)	Data 1.07e-04 (4.63e-04)	Tok/s 29666 (36822)	Loss/tok 2.8077 (3.0955)	LR 1.250e-04
0: TRAIN [3][920/3880]	Time 0.402 (0.377)	Data 1.03e-04 (4.59e-04)	Tok/s 41434 (36827)	Loss/tok 3.0923 (3.0953)	LR 1.250e-04
0: TRAIN [3][930/3880]	Time 0.418 (0.377)	Data 1.07e-04 (4.55e-04)	Tok/s 40513 (36826)	Loss/tok 3.1290 (3.0950)	LR 1.250e-04
0: TRAIN [3][940/3880]	Time 0.298 (0.377)	Data 1.11e-04 (4.51e-04)	Tok/s 35180 (36831)	Loss/tok 3.0325 (3.0946)	LR 1.250e-04
0: TRAIN [3][950/3880]	Time 0.424 (0.377)	Data 1.06e-04 (4.48e-04)	Tok/s 39275 (36802)	Loss/tok 3.1707 (3.0942)	LR 1.250e-04
0: TRAIN [3][960/3880]	Time 0.305 (0.377)	Data 1.10e-04 (4.44e-04)	Tok/s 34318 (36791)	Loss/tok 2.8719 (3.0935)	LR 1.250e-04
0: TRAIN [3][970/3880]	Time 0.368 (0.377)	Data 1.07e-04 (4.41e-04)	Tok/s 28513 (36772)	Loss/tok 2.9939 (3.0930)	LR 1.250e-04
0: TRAIN [3][980/3880]	Time 0.300 (0.377)	Data 1.13e-04 (4.37e-04)	Tok/s 33924 (36790)	Loss/tok 2.8939 (3.0941)	LR 1.250e-04
0: TRAIN [3][990/3880]	Time 0.297 (0.377)	Data 1.09e-04 (4.34e-04)	Tok/s 34321 (36788)	Loss/tok 2.8494 (3.0932)	LR 1.250e-04
0: TRAIN [3][1000/3880]	Time 0.548 (0.377)	Data 1.08e-04 (4.31e-04)	Tok/s 43085 (36814)	Loss/tok 3.1465 (3.0938)	LR 1.250e-04
0: TRAIN [3][1010/3880]	Time 0.307 (0.377)	Data 1.10e-04 (4.28e-04)	Tok/s 34211 (36790)	Loss/tok 2.9096 (3.0932)	LR 1.250e-04
0: TRAIN [3][1020/3880]	Time 0.316 (0.377)	Data 1.06e-04 (4.25e-04)	Tok/s 32551 (36825)	Loss/tok 2.8502 (3.0937)	LR 1.250e-04
0: TRAIN [3][1030/3880]	Time 0.400 (0.378)	Data 1.08e-04 (4.21e-04)	Tok/s 41977 (36863)	Loss/tok 3.1076 (3.0953)	LR 1.250e-04
0: TRAIN [3][1040/3880]	Time 0.407 (0.378)	Data 1.07e-04 (4.18e-04)	Tok/s 41375 (36878)	Loss/tok 3.0240 (3.0951)	LR 1.250e-04
0: TRAIN [3][1050/3880]	Time 0.305 (0.378)	Data 1.08e-04 (4.16e-04)	Tok/s 34356 (36868)	Loss/tok 3.0699 (3.0949)	LR 1.250e-04
0: TRAIN [3][1060/3880]	Time 0.304 (0.377)	Data 1.10e-04 (4.13e-04)	Tok/s 33809 (36857)	Loss/tok 3.0395 (3.0938)	LR 1.250e-04
0: TRAIN [3][1070/3880]	Time 0.414 (0.377)	Data 1.09e-04 (4.10e-04)	Tok/s 40072 (36853)	Loss/tok 3.1210 (3.0935)	LR 1.250e-04
0: TRAIN [3][1080/3880]	Time 0.306 (0.377)	Data 1.07e-04 (4.07e-04)	Tok/s 34018 (36842)	Loss/tok 2.8945 (3.0928)	LR 1.250e-04
0: TRAIN [3][1090/3880]	Time 0.205 (0.377)	Data 1.10e-04 (4.04e-04)	Tok/s 25652 (36847)	Loss/tok 2.5528 (3.0935)	LR 1.250e-04
0: TRAIN [3][1100/3880]	Time 0.407 (0.376)	Data 1.05e-04 (4.02e-04)	Tok/s 41443 (36826)	Loss/tok 3.1118 (3.0926)	LR 1.250e-04
0: TRAIN [3][1110/3880]	Time 0.409 (0.376)	Data 1.12e-04 (3.99e-04)	Tok/s 41519 (36846)	Loss/tok 3.1307 (3.0925)	LR 1.250e-04
0: TRAIN [3][1120/3880]	Time 0.501 (0.376)	Data 1.10e-04 (3.96e-04)	Tok/s 46873 (36831)	Loss/tok 3.2165 (3.0919)	LR 1.250e-04
0: TRAIN [3][1130/3880]	Time 0.611 (0.376)	Data 1.12e-04 (3.94e-04)	Tok/s 37947 (36821)	Loss/tok 3.2336 (3.0922)	LR 1.250e-04
0: TRAIN [3][1140/3880]	Time 0.428 (0.376)	Data 1.11e-04 (3.91e-04)	Tok/s 38659 (36795)	Loss/tok 3.1619 (3.0923)	LR 1.250e-04
0: TRAIN [3][1150/3880]	Time 0.410 (0.376)	Data 1.08e-04 (3.89e-04)	Tok/s 40865 (36796)	Loss/tok 3.0738 (3.0916)	LR 1.250e-04
0: TRAIN [3][1160/3880]	Time 0.305 (0.377)	Data 1.28e-04 (3.86e-04)	Tok/s 34025 (36830)	Loss/tok 2.9672 (3.0927)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][1170/3880]	Time 0.210 (0.377)	Data 1.09e-04 (3.84e-04)	Tok/s 25470 (36855)	Loss/tok 2.4895 (3.0931)	LR 1.250e-04
0: TRAIN [3][1180/3880]	Time 0.300 (0.377)	Data 1.06e-04 (3.82e-04)	Tok/s 34298 (36876)	Loss/tok 2.9728 (3.0938)	LR 1.250e-04
0: TRAIN [3][1190/3880]	Time 0.779 (0.378)	Data 1.09e-04 (3.79e-04)	Tok/s 37840 (36876)	Loss/tok 3.4780 (3.0946)	LR 1.250e-04
0: TRAIN [3][1200/3880]	Time 0.429 (0.378)	Data 1.12e-04 (3.77e-04)	Tok/s 38564 (36858)	Loss/tok 3.0848 (3.0943)	LR 1.250e-04
0: TRAIN [3][1210/3880]	Time 0.419 (0.378)	Data 1.06e-04 (3.75e-04)	Tok/s 39956 (36858)	Loss/tok 3.0643 (3.0939)	LR 1.250e-04
0: TRAIN [3][1220/3880]	Time 0.544 (0.378)	Data 1.09e-04 (3.73e-04)	Tok/s 31523 (36875)	Loss/tok 3.1849 (3.0943)	LR 1.250e-04
0: TRAIN [3][1230/3880]	Time 0.423 (0.378)	Data 1.06e-04 (3.71e-04)	Tok/s 39501 (36859)	Loss/tok 3.0416 (3.0943)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1240/3880]	Time 0.295 (0.379)	Data 1.13e-04 (3.69e-04)	Tok/s 35364 (36888)	Loss/tok 2.9927 (3.0956)	LR 1.250e-04
0: TRAIN [3][1250/3880]	Time 0.424 (0.379)	Data 1.10e-04 (3.66e-04)	Tok/s 39964 (36871)	Loss/tok 3.1222 (3.0954)	LR 1.250e-04
0: TRAIN [3][1260/3880]	Time 0.297 (0.379)	Data 1.07e-04 (3.64e-04)	Tok/s 34877 (36886)	Loss/tok 2.9133 (3.0963)	LR 1.250e-04
0: TRAIN [3][1270/3880]	Time 0.314 (0.379)	Data 1.14e-04 (3.62e-04)	Tok/s 33933 (36894)	Loss/tok 2.9946 (3.0965)	LR 1.250e-04
0: TRAIN [3][1280/3880]	Time 0.684 (0.380)	Data 1.09e-04 (3.60e-04)	Tok/s 44171 (36921)	Loss/tok 3.4624 (3.0973)	LR 1.250e-04
0: TRAIN [3][1290/3880]	Time 0.540 (0.379)	Data 1.08e-04 (3.58e-04)	Tok/s 43256 (36903)	Loss/tok 3.3397 (3.0968)	LR 1.250e-04
0: TRAIN [3][1300/3880]	Time 0.407 (0.380)	Data 1.09e-04 (3.57e-04)	Tok/s 41150 (36929)	Loss/tok 3.1495 (3.0973)	LR 1.250e-04
0: TRAIN [3][1310/3880]	Time 0.309 (0.379)	Data 1.11e-04 (3.55e-04)	Tok/s 33543 (36900)	Loss/tok 2.9880 (3.0966)	LR 1.250e-04
0: TRAIN [3][1320/3880]	Time 0.409 (0.379)	Data 1.12e-04 (3.53e-04)	Tok/s 41272 (36915)	Loss/tok 3.0946 (3.0965)	LR 1.250e-04
0: TRAIN [3][1330/3880]	Time 0.508 (0.379)	Data 1.18e-04 (3.51e-04)	Tok/s 45988 (36908)	Loss/tok 3.2638 (3.0965)	LR 1.250e-04
0: TRAIN [3][1340/3880]	Time 0.415 (0.379)	Data 1.06e-04 (3.49e-04)	Tok/s 40943 (36904)	Loss/tok 3.1053 (3.0967)	LR 1.250e-04
0: TRAIN [3][1350/3880]	Time 0.343 (0.379)	Data 1.08e-04 (3.47e-04)	Tok/s 30136 (36897)	Loss/tok 2.9705 (3.0970)	LR 1.250e-04
0: TRAIN [3][1360/3880]	Time 0.413 (0.379)	Data 1.07e-04 (3.46e-04)	Tok/s 40321 (36880)	Loss/tok 3.1692 (3.0967)	LR 1.250e-04
0: TRAIN [3][1370/3880]	Time 0.312 (0.379)	Data 1.05e-04 (3.44e-04)	Tok/s 33284 (36873)	Loss/tok 2.9113 (3.0966)	LR 1.250e-04
0: TRAIN [3][1380/3880]	Time 0.519 (0.379)	Data 1.14e-04 (3.42e-04)	Tok/s 44897 (36858)	Loss/tok 3.3397 (3.0961)	LR 1.250e-04
0: TRAIN [3][1390/3880]	Time 0.293 (0.379)	Data 1.12e-04 (3.41e-04)	Tok/s 35684 (36869)	Loss/tok 2.9203 (3.0968)	LR 1.250e-04
0: TRAIN [3][1400/3880]	Time 0.322 (0.378)	Data 1.09e-04 (3.39e-04)	Tok/s 31993 (36841)	Loss/tok 2.9292 (3.0961)	LR 1.250e-04
0: TRAIN [3][1410/3880]	Time 0.416 (0.378)	Data 1.03e-04 (3.37e-04)	Tok/s 40687 (36835)	Loss/tok 3.0618 (3.0955)	LR 1.250e-04
0: TRAIN [3][1420/3880]	Time 0.513 (0.378)	Data 1.13e-04 (3.36e-04)	Tok/s 45259 (36843)	Loss/tok 3.3342 (3.0953)	LR 1.250e-04
0: TRAIN [3][1430/3880]	Time 0.299 (0.378)	Data 1.07e-04 (3.34e-04)	Tok/s 34166 (36860)	Loss/tok 2.8936 (3.0956)	LR 1.250e-04
0: TRAIN [3][1440/3880]	Time 0.302 (0.378)	Data 1.11e-04 (3.32e-04)	Tok/s 33756 (36877)	Loss/tok 2.9393 (3.0963)	LR 1.250e-04
0: TRAIN [3][1450/3880]	Time 0.408 (0.379)	Data 1.11e-04 (3.31e-04)	Tok/s 41363 (36898)	Loss/tok 3.0022 (3.0965)	LR 1.250e-04
0: TRAIN [3][1460/3880]	Time 0.405 (0.378)	Data 1.11e-04 (3.29e-04)	Tok/s 40812 (36897)	Loss/tok 3.2008 (3.0960)	LR 1.250e-04
0: TRAIN [3][1470/3880]	Time 0.362 (0.378)	Data 1.10e-04 (3.28e-04)	Tok/s 28199 (36858)	Loss/tok 2.8402 (3.0954)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1480/3880]	Time 0.314 (0.378)	Data 1.05e-04 (3.26e-04)	Tok/s 32861 (36840)	Loss/tok 2.8802 (3.0949)	LR 1.250e-04
0: TRAIN [3][1490/3880]	Time 0.404 (0.378)	Data 1.11e-04 (3.25e-04)	Tok/s 41499 (36850)	Loss/tok 3.0980 (3.0960)	LR 1.250e-04
0: TRAIN [3][1500/3880]	Time 0.299 (0.378)	Data 1.35e-04 (3.24e-04)	Tok/s 35252 (36863)	Loss/tok 2.9438 (3.0963)	LR 1.250e-04
0: TRAIN [3][1510/3880]	Time 0.626 (0.379)	Data 1.10e-04 (3.22e-04)	Tok/s 47063 (36884)	Loss/tok 3.4247 (3.0968)	LR 1.250e-04
0: TRAIN [3][1520/3880]	Time 0.563 (0.379)	Data 1.13e-04 (3.21e-04)	Tok/s 40836 (36864)	Loss/tok 3.2508 (3.0978)	LR 1.250e-04
0: TRAIN [3][1530/3880]	Time 0.523 (0.379)	Data 1.11e-04 (3.19e-04)	Tok/s 44139 (36873)	Loss/tok 3.3286 (3.0978)	LR 1.250e-04
0: TRAIN [3][1540/3880]	Time 0.297 (0.379)	Data 1.15e-04 (3.18e-04)	Tok/s 35011 (36859)	Loss/tok 2.8561 (3.0973)	LR 1.250e-04
0: TRAIN [3][1550/3880]	Time 0.393 (0.379)	Data 1.10e-04 (3.17e-04)	Tok/s 42635 (36882)	Loss/tok 3.0461 (3.0975)	LR 1.250e-04
0: TRAIN [3][1560/3880]	Time 0.320 (0.379)	Data 1.10e-04 (3.15e-04)	Tok/s 32675 (36842)	Loss/tok 2.9550 (3.0974)	LR 1.250e-04
0: TRAIN [3][1570/3880]	Time 0.641 (0.380)	Data 1.12e-04 (3.14e-04)	Tok/s 46247 (36866)	Loss/tok 3.4788 (3.0980)	LR 1.250e-04
0: TRAIN [3][1580/3880]	Time 0.295 (0.380)	Data 1.09e-04 (3.13e-04)	Tok/s 34958 (36890)	Loss/tok 2.9239 (3.0985)	LR 1.250e-04
0: TRAIN [3][1590/3880]	Time 0.299 (0.380)	Data 1.11e-04 (3.12e-04)	Tok/s 34188 (36900)	Loss/tok 2.8689 (3.0991)	LR 1.250e-04
0: TRAIN [3][1600/3880]	Time 0.214 (0.380)	Data 1.11e-04 (3.10e-04)	Tok/s 24831 (36896)	Loss/tok 2.4855 (3.0994)	LR 1.250e-04
0: TRAIN [3][1610/3880]	Time 0.523 (0.380)	Data 1.06e-04 (3.09e-04)	Tok/s 44455 (36898)	Loss/tok 3.3135 (3.0995)	LR 1.250e-04
0: TRAIN [3][1620/3880]	Time 0.300 (0.380)	Data 1.10e-04 (3.08e-04)	Tok/s 34054 (36900)	Loss/tok 2.9034 (3.0994)	LR 1.250e-04
0: TRAIN [3][1630/3880]	Time 0.400 (0.380)	Data 1.09e-04 (3.07e-04)	Tok/s 41961 (36891)	Loss/tok 3.1135 (3.0988)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1640/3880]	Time 0.307 (0.380)	Data 1.08e-04 (3.05e-04)	Tok/s 33350 (36896)	Loss/tok 2.9502 (3.0994)	LR 1.250e-04
0: TRAIN [3][1650/3880]	Time 0.346 (0.380)	Data 1.10e-04 (3.04e-04)	Tok/s 30347 (36870)	Loss/tok 2.9040 (3.0990)	LR 1.250e-04
0: TRAIN [3][1660/3880]	Time 0.502 (0.380)	Data 1.05e-04 (3.03e-04)	Tok/s 45951 (36872)	Loss/tok 3.3760 (3.0993)	LR 1.250e-04
0: TRAIN [3][1670/3880]	Time 0.298 (0.380)	Data 1.08e-04 (3.02e-04)	Tok/s 34549 (36856)	Loss/tok 2.8827 (3.0986)	LR 1.250e-04
0: TRAIN [3][1680/3880]	Time 0.316 (0.380)	Data 1.07e-04 (3.01e-04)	Tok/s 32929 (36854)	Loss/tok 2.9526 (3.0984)	LR 1.250e-04
0: TRAIN [3][1690/3880]	Time 0.347 (0.380)	Data 1.08e-04 (3.00e-04)	Tok/s 30107 (36843)	Loss/tok 2.9074 (3.0984)	LR 1.250e-04
0: TRAIN [3][1700/3880]	Time 0.409 (0.380)	Data 1.10e-04 (2.98e-04)	Tok/s 41195 (36838)	Loss/tok 2.9148 (3.0981)	LR 1.250e-04
0: TRAIN [3][1710/3880]	Time 0.412 (0.379)	Data 1.09e-04 (2.97e-04)	Tok/s 40490 (36837)	Loss/tok 3.1772 (3.0976)	LR 1.250e-04
0: TRAIN [3][1720/3880]	Time 0.305 (0.380)	Data 1.11e-04 (2.96e-04)	Tok/s 34043 (36853)	Loss/tok 2.9755 (3.0981)	LR 1.250e-04
0: TRAIN [3][1730/3880]	Time 0.409 (0.380)	Data 1.10e-04 (2.95e-04)	Tok/s 41610 (36854)	Loss/tok 3.1672 (3.0985)	LR 1.250e-04
0: TRAIN [3][1740/3880]	Time 0.333 (0.380)	Data 1.08e-04 (2.94e-04)	Tok/s 31034 (36845)	Loss/tok 2.8195 (3.0982)	LR 1.250e-04
0: TRAIN [3][1750/3880]	Time 0.300 (0.379)	Data 1.12e-04 (2.93e-04)	Tok/s 34322 (36828)	Loss/tok 2.9936 (3.0975)	LR 1.250e-04
0: TRAIN [3][1760/3880]	Time 0.392 (0.379)	Data 1.11e-04 (2.92e-04)	Tok/s 42783 (36822)	Loss/tok 3.0353 (3.0969)	LR 1.250e-04
0: TRAIN [3][1770/3880]	Time 0.526 (0.379)	Data 1.06e-04 (2.91e-04)	Tok/s 44011 (36818)	Loss/tok 3.2770 (3.0970)	LR 1.250e-04
0: TRAIN [3][1780/3880]	Time 0.300 (0.379)	Data 1.10e-04 (2.90e-04)	Tok/s 34433 (36820)	Loss/tok 2.8938 (3.0969)	LR 1.250e-04
0: TRAIN [3][1790/3880]	Time 0.318 (0.379)	Data 1.13e-04 (2.89e-04)	Tok/s 32421 (36831)	Loss/tok 2.9384 (3.0969)	LR 1.250e-04
0: TRAIN [3][1800/3880]	Time 0.415 (0.379)	Data 1.06e-04 (2.88e-04)	Tok/s 40571 (36842)	Loss/tok 3.0565 (3.0971)	LR 1.250e-04
0: TRAIN [3][1810/3880]	Time 0.605 (0.379)	Data 1.10e-04 (2.87e-04)	Tok/s 38611 (36846)	Loss/tok 3.2910 (3.0970)	LR 1.250e-04
0: TRAIN [3][1820/3880]	Time 0.435 (0.379)	Data 1.10e-04 (2.86e-04)	Tok/s 39199 (36832)	Loss/tok 3.0964 (3.0966)	LR 1.250e-04
0: TRAIN [3][1830/3880]	Time 0.292 (0.379)	Data 1.09e-04 (2.85e-04)	Tok/s 35304 (36831)	Loss/tok 2.9027 (3.0962)	LR 1.250e-04
0: TRAIN [3][1840/3880]	Time 0.388 (0.379)	Data 1.13e-04 (2.84e-04)	Tok/s 42940 (36850)	Loss/tok 3.0657 (3.0968)	LR 1.250e-04
0: TRAIN [3][1850/3880]	Time 0.455 (0.380)	Data 1.13e-04 (2.83e-04)	Tok/s 36825 (36824)	Loss/tok 3.1440 (3.0971)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1860/3880]	Time 0.522 (0.380)	Data 1.11e-04 (2.82e-04)	Tok/s 44082 (36823)	Loss/tok 3.3865 (3.0976)	LR 1.250e-04
0: TRAIN [3][1870/3880]	Time 0.306 (0.380)	Data 1.09e-04 (2.81e-04)	Tok/s 33785 (36835)	Loss/tok 2.9868 (3.0976)	LR 1.250e-04
0: TRAIN [3][1880/3880]	Time 0.303 (0.380)	Data 1.11e-04 (2.80e-04)	Tok/s 34020 (36847)	Loss/tok 2.9333 (3.0978)	LR 1.250e-04
0: TRAIN [3][1890/3880]	Time 0.393 (0.380)	Data 1.10e-04 (2.79e-04)	Tok/s 42647 (36853)	Loss/tok 3.1923 (3.0977)	LR 1.250e-04
0: TRAIN [3][1900/3880]	Time 0.308 (0.380)	Data 1.09e-04 (2.78e-04)	Tok/s 33112 (36849)	Loss/tok 2.9686 (3.0981)	LR 1.250e-04
0: TRAIN [3][1910/3880]	Time 0.417 (0.380)	Data 1.12e-04 (2.78e-04)	Tok/s 39976 (36851)	Loss/tok 3.0066 (3.0978)	LR 1.250e-04
0: TRAIN [3][1920/3880]	Time 0.400 (0.380)	Data 1.10e-04 (2.77e-04)	Tok/s 41732 (36842)	Loss/tok 3.0775 (3.0974)	LR 1.250e-04
0: TRAIN [3][1930/3880]	Time 0.460 (0.380)	Data 1.09e-04 (2.76e-04)	Tok/s 36421 (36838)	Loss/tok 3.0999 (3.0972)	LR 1.250e-04
0: TRAIN [3][1940/3880]	Time 0.499 (0.380)	Data 1.10e-04 (2.75e-04)	Tok/s 46295 (36843)	Loss/tok 3.2838 (3.0972)	LR 1.250e-04
0: TRAIN [3][1950/3880]	Time 0.491 (0.380)	Data 1.05e-04 (2.74e-04)	Tok/s 33953 (36840)	Loss/tok 3.0420 (3.0968)	LR 1.250e-04
0: TRAIN [3][1960/3880]	Time 0.311 (0.380)	Data 1.09e-04 (2.73e-04)	Tok/s 33668 (36827)	Loss/tok 2.9270 (3.0963)	LR 1.250e-04
0: TRAIN [3][1970/3880]	Time 0.409 (0.379)	Data 1.07e-04 (2.72e-04)	Tok/s 40983 (36827)	Loss/tok 3.0576 (3.0960)	LR 1.250e-04
0: TRAIN [3][1980/3880]	Time 0.365 (0.379)	Data 1.08e-04 (2.72e-04)	Tok/s 28187 (36818)	Loss/tok 2.8760 (3.0958)	LR 1.250e-04
0: TRAIN [3][1990/3880]	Time 0.666 (0.380)	Data 1.15e-04 (2.71e-04)	Tok/s 45257 (36818)	Loss/tok 3.3517 (3.0964)	LR 1.250e-04
0: TRAIN [3][2000/3880]	Time 0.300 (0.380)	Data 1.09e-04 (2.70e-04)	Tok/s 34656 (36837)	Loss/tok 2.9624 (3.0970)	LR 1.250e-04
0: TRAIN [3][2010/3880]	Time 0.483 (0.380)	Data 1.09e-04 (2.69e-04)	Tok/s 34933 (36837)	Loss/tok 3.1311 (3.0967)	LR 1.250e-04
0: TRAIN [3][2020/3880]	Time 0.313 (0.380)	Data 1.10e-04 (2.68e-04)	Tok/s 33109 (36833)	Loss/tok 2.8734 (3.0968)	LR 1.250e-04
0: TRAIN [3][2030/3880]	Time 0.412 (0.380)	Data 1.11e-04 (2.68e-04)	Tok/s 40550 (36838)	Loss/tok 3.0865 (3.0967)	LR 1.250e-04
0: TRAIN [3][2040/3880]	Time 0.508 (0.380)	Data 1.09e-04 (2.67e-04)	Tok/s 45662 (36841)	Loss/tok 3.2787 (3.0967)	LR 1.250e-04
0: TRAIN [3][2050/3880]	Time 0.680 (0.380)	Data 1.09e-04 (2.66e-04)	Tok/s 43437 (36847)	Loss/tok 3.4581 (3.0978)	LR 1.250e-04
0: TRAIN [3][2060/3880]	Time 0.386 (0.380)	Data 1.12e-04 (2.65e-04)	Tok/s 44030 (36857)	Loss/tok 3.1897 (3.0980)	LR 1.250e-04
0: TRAIN [3][2070/3880]	Time 0.355 (0.381)	Data 1.10e-04 (2.64e-04)	Tok/s 30153 (36860)	Loss/tok 2.9078 (3.0983)	LR 1.250e-04
0: TRAIN [3][2080/3880]	Time 0.655 (0.381)	Data 1.02e-04 (2.64e-04)	Tok/s 44808 (36845)	Loss/tok 3.4984 (3.0982)	LR 1.250e-04
0: TRAIN [3][2090/3880]	Time 0.522 (0.381)	Data 1.08e-04 (2.63e-04)	Tok/s 44292 (36855)	Loss/tok 3.2758 (3.0983)	LR 1.250e-04
0: TRAIN [3][2100/3880]	Time 0.289 (0.381)	Data 1.14e-04 (2.62e-04)	Tok/s 34631 (36861)	Loss/tok 3.0406 (3.0987)	LR 1.250e-04
0: TRAIN [3][2110/3880]	Time 0.390 (0.381)	Data 1.06e-04 (2.62e-04)	Tok/s 43292 (36853)	Loss/tok 3.0168 (3.0984)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][2120/3880]	Time 0.677 (0.380)	Data 1.08e-04 (2.61e-04)	Tok/s 44147 (36844)	Loss/tok 3.3770 (3.0986)	LR 1.250e-04
0: TRAIN [3][2130/3880]	Time 0.309 (0.380)	Data 1.05e-04 (2.60e-04)	Tok/s 33761 (36837)	Loss/tok 2.9535 (3.0982)	LR 1.250e-04
0: TRAIN [3][2140/3880]	Time 0.422 (0.380)	Data 1.09e-04 (2.59e-04)	Tok/s 39658 (36845)	Loss/tok 3.1426 (3.0982)	LR 1.250e-04
0: TRAIN [3][2150/3880]	Time 0.409 (0.380)	Data 1.13e-04 (2.59e-04)	Tok/s 41095 (36851)	Loss/tok 3.1351 (3.0983)	LR 1.250e-04
0: TRAIN [3][2160/3880]	Time 0.300 (0.380)	Data 1.11e-04 (2.58e-04)	Tok/s 34045 (36853)	Loss/tok 2.9339 (3.0983)	LR 1.250e-04
0: TRAIN [3][2170/3880]	Time 0.400 (0.380)	Data 1.07e-04 (2.57e-04)	Tok/s 41363 (36867)	Loss/tok 3.1819 (3.0989)	LR 1.250e-04
0: TRAIN [3][2180/3880]	Time 0.688 (0.381)	Data 1.11e-04 (2.57e-04)	Tok/s 43214 (36874)	Loss/tok 3.4433 (3.0993)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2190/3880]	Time 0.621 (0.381)	Data 1.09e-04 (2.56e-04)	Tok/s 47632 (36877)	Loss/tok 3.3638 (3.1000)	LR 1.250e-04
0: TRAIN [3][2200/3880]	Time 0.349 (0.381)	Data 1.11e-04 (2.55e-04)	Tok/s 29488 (36858)	Loss/tok 2.9253 (3.0998)	LR 1.250e-04
0: TRAIN [3][2210/3880]	Time 0.391 (0.381)	Data 1.13e-04 (2.55e-04)	Tok/s 43392 (36864)	Loss/tok 3.0867 (3.1005)	LR 1.250e-04
0: TRAIN [3][2220/3880]	Time 0.440 (0.381)	Data 1.04e-04 (2.54e-04)	Tok/s 23487 (36860)	Loss/tok 2.8729 (3.1004)	LR 1.250e-04
0: TRAIN [3][2230/3880]	Time 0.319 (0.382)	Data 1.10e-04 (2.53e-04)	Tok/s 32346 (36827)	Loss/tok 2.8687 (3.1002)	LR 1.250e-04
0: TRAIN [3][2240/3880]	Time 0.533 (0.382)	Data 1.03e-04 (2.53e-04)	Tok/s 43925 (36825)	Loss/tok 3.1864 (3.1004)	LR 1.250e-04
0: TRAIN [3][2250/3880]	Time 0.304 (0.381)	Data 1.08e-04 (2.52e-04)	Tok/s 34326 (36809)	Loss/tok 2.8858 (3.1000)	LR 1.250e-04
0: TRAIN [3][2260/3880]	Time 0.516 (0.381)	Data 1.08e-04 (2.51e-04)	Tok/s 45978 (36809)	Loss/tok 3.2223 (3.0999)	LR 1.250e-04
0: TRAIN [3][2270/3880]	Time 0.410 (0.381)	Data 1.09e-04 (2.51e-04)	Tok/s 41639 (36810)	Loss/tok 3.0565 (3.0998)	LR 1.250e-04
0: TRAIN [3][2280/3880]	Time 0.304 (0.381)	Data 1.09e-04 (2.50e-04)	Tok/s 34178 (36815)	Loss/tok 2.9187 (3.0997)	LR 1.250e-04
0: TRAIN [3][2290/3880]	Time 0.624 (0.381)	Data 1.08e-04 (2.49e-04)	Tok/s 47526 (36809)	Loss/tok 3.4254 (3.0995)	LR 1.250e-04
0: TRAIN [3][2300/3880]	Time 0.388 (0.381)	Data 1.10e-04 (2.49e-04)	Tok/s 43440 (36816)	Loss/tok 3.0872 (3.0994)	LR 1.250e-04
0: TRAIN [3][2310/3880]	Time 0.386 (0.381)	Data 1.10e-04 (2.48e-04)	Tok/s 43858 (36796)	Loss/tok 2.9508 (3.0997)	LR 1.250e-04
0: TRAIN [3][2320/3880]	Time 0.336 (0.381)	Data 1.11e-04 (2.48e-04)	Tok/s 30971 (36778)	Loss/tok 2.9112 (3.0994)	LR 1.250e-04
0: TRAIN [3][2330/3880]	Time 0.307 (0.381)	Data 1.08e-04 (2.47e-04)	Tok/s 32869 (36780)	Loss/tok 2.8751 (3.0996)	LR 1.250e-04
0: TRAIN [3][2340/3880]	Time 0.414 (0.381)	Data 1.07e-04 (2.46e-04)	Tok/s 40107 (36785)	Loss/tok 3.2047 (3.0995)	LR 1.250e-04
0: TRAIN [3][2350/3880]	Time 0.295 (0.382)	Data 1.12e-04 (2.46e-04)	Tok/s 35100 (36796)	Loss/tok 2.8082 (3.0995)	LR 1.250e-04
0: TRAIN [3][2360/3880]	Time 0.311 (0.381)	Data 1.11e-04 (2.45e-04)	Tok/s 33238 (36786)	Loss/tok 2.8580 (3.0992)	LR 1.250e-04
0: TRAIN [3][2370/3880]	Time 0.385 (0.381)	Data 1.08e-04 (2.45e-04)	Tok/s 43131 (36794)	Loss/tok 3.1860 (3.0992)	LR 1.250e-04
0: TRAIN [3][2380/3880]	Time 0.213 (0.381)	Data 1.08e-04 (2.44e-04)	Tok/s 24610 (36788)	Loss/tok 2.5118 (3.0988)	LR 1.250e-04
0: TRAIN [3][2390/3880]	Time 0.312 (0.381)	Data 1.11e-04 (2.44e-04)	Tok/s 32972 (36786)	Loss/tok 3.0111 (3.0988)	LR 1.250e-04
0: TRAIN [3][2400/3880]	Time 0.302 (0.381)	Data 1.10e-04 (2.43e-04)	Tok/s 34468 (36784)	Loss/tok 2.8518 (3.0989)	LR 1.250e-04
0: TRAIN [3][2410/3880]	Time 0.203 (0.381)	Data 1.06e-04 (2.42e-04)	Tok/s 25300 (36761)	Loss/tok 2.5284 (3.0986)	LR 1.250e-04
0: TRAIN [3][2420/3880]	Time 0.238 (0.381)	Data 1.09e-04 (2.42e-04)	Tok/s 22654 (36759)	Loss/tok 2.5316 (3.0986)	LR 1.250e-04
0: TRAIN [3][2430/3880]	Time 0.400 (0.381)	Data 1.06e-04 (2.41e-04)	Tok/s 42187 (36742)	Loss/tok 3.0809 (3.0983)	LR 1.250e-04
0: TRAIN [3][2440/3880]	Time 0.466 (0.381)	Data 1.09e-04 (2.41e-04)	Tok/s 35854 (36740)	Loss/tok 3.1676 (3.0988)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][2450/3880]	Time 0.313 (0.381)	Data 1.13e-04 (2.40e-04)	Tok/s 32444 (36744)	Loss/tok 2.8812 (3.0993)	LR 1.250e-04
0: TRAIN [3][2460/3880]	Time 0.324 (0.381)	Data 1.10e-04 (2.40e-04)	Tok/s 31748 (36740)	Loss/tok 2.9200 (3.0991)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2470/3880]	Time 0.205 (0.381)	Data 1.12e-04 (2.39e-04)	Tok/s 25129 (36735)	Loss/tok 2.4440 (3.0989)	LR 1.250e-04
0: TRAIN [3][2480/3880]	Time 0.299 (0.381)	Data 1.10e-04 (2.39e-04)	Tok/s 33795 (36731)	Loss/tok 2.8368 (3.0984)	LR 1.250e-04
0: TRAIN [3][2490/3880]	Time 0.207 (0.380)	Data 1.09e-04 (2.38e-04)	Tok/s 25566 (36727)	Loss/tok 2.4829 (3.0979)	LR 1.250e-04
0: TRAIN [3][2500/3880]	Time 0.305 (0.381)	Data 1.06e-04 (2.38e-04)	Tok/s 33832 (36731)	Loss/tok 2.9231 (3.0981)	LR 1.250e-04
0: TRAIN [3][2510/3880]	Time 0.531 (0.381)	Data 1.15e-04 (2.37e-04)	Tok/s 44086 (36734)	Loss/tok 3.1529 (3.0981)	LR 1.250e-04
0: TRAIN [3][2520/3880]	Time 0.298 (0.380)	Data 1.07e-04 (2.37e-04)	Tok/s 34317 (36732)	Loss/tok 2.9275 (3.0978)	LR 1.250e-04
0: TRAIN [3][2530/3880]	Time 0.455 (0.381)	Data 1.05e-04 (2.36e-04)	Tok/s 35826 (36726)	Loss/tok 3.1523 (3.0981)	LR 1.250e-04
0: TRAIN [3][2540/3880]	Time 0.299 (0.381)	Data 1.10e-04 (2.36e-04)	Tok/s 34741 (36721)	Loss/tok 2.8889 (3.0978)	LR 1.250e-04
0: TRAIN [3][2550/3880]	Time 0.506 (0.381)	Data 1.11e-04 (2.35e-04)	Tok/s 46186 (36726)	Loss/tok 3.2599 (3.0978)	LR 1.250e-04
0: TRAIN [3][2560/3880]	Time 0.572 (0.381)	Data 1.08e-04 (2.35e-04)	Tok/s 40375 (36733)	Loss/tok 3.3032 (3.0979)	LR 1.250e-04
0: TRAIN [3][2570/3880]	Time 0.452 (0.381)	Data 1.09e-04 (2.34e-04)	Tok/s 36867 (36742)	Loss/tok 3.1726 (3.0984)	LR 1.250e-04
0: TRAIN [3][2580/3880]	Time 0.293 (0.381)	Data 1.10e-04 (2.34e-04)	Tok/s 35002 (36740)	Loss/tok 2.9592 (3.0984)	LR 1.250e-04
0: TRAIN [3][2590/3880]	Time 0.238 (0.381)	Data 1.09e-04 (2.33e-04)	Tok/s 22239 (36729)	Loss/tok 2.5603 (3.0987)	LR 1.250e-04
0: TRAIN [3][2600/3880]	Time 0.319 (0.381)	Data 1.10e-04 (2.33e-04)	Tok/s 32036 (36713)	Loss/tok 2.9194 (3.0988)	LR 1.250e-04
0: TRAIN [3][2610/3880]	Time 0.306 (0.381)	Data 1.07e-04 (2.32e-04)	Tok/s 33669 (36721)	Loss/tok 2.8268 (3.0989)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2620/3880]	Time 0.435 (0.381)	Data 1.15e-04 (2.32e-04)	Tok/s 38803 (36733)	Loss/tok 3.0451 (3.0991)	LR 1.250e-04
0: TRAIN [3][2630/3880]	Time 0.307 (0.381)	Data 1.11e-04 (2.31e-04)	Tok/s 33948 (36731)	Loss/tok 3.0100 (3.0990)	LR 1.250e-04
0: TRAIN [3][2640/3880]	Time 0.303 (0.381)	Data 1.09e-04 (2.31e-04)	Tok/s 34310 (36728)	Loss/tok 2.8954 (3.0989)	LR 1.250e-04
0: TRAIN [3][2650/3880]	Time 0.299 (0.381)	Data 1.06e-04 (2.30e-04)	Tok/s 34069 (36737)	Loss/tok 2.8548 (3.0991)	LR 1.250e-04
0: TRAIN [3][2660/3880]	Time 0.327 (0.382)	Data 1.10e-04 (2.30e-04)	Tok/s 31315 (36730)	Loss/tok 2.7727 (3.0990)	LR 1.250e-04
0: TRAIN [3][2670/3880]	Time 0.304 (0.382)	Data 1.10e-04 (2.29e-04)	Tok/s 33330 (36743)	Loss/tok 2.9536 (3.0996)	LR 1.250e-04
0: TRAIN [3][2680/3880]	Time 0.304 (0.382)	Data 1.09e-04 (2.29e-04)	Tok/s 33615 (36739)	Loss/tok 2.8800 (3.0994)	LR 1.250e-04
0: TRAIN [3][2690/3880]	Time 0.402 (0.382)	Data 1.08e-04 (2.29e-04)	Tok/s 41401 (36747)	Loss/tok 3.0472 (3.0995)	LR 1.250e-04
0: TRAIN [3][2700/3880]	Time 0.367 (0.382)	Data 1.16e-04 (2.28e-04)	Tok/s 28087 (36745)	Loss/tok 2.8387 (3.0996)	LR 1.250e-04
0: TRAIN [3][2710/3880]	Time 0.417 (0.382)	Data 1.09e-04 (2.28e-04)	Tok/s 40743 (36742)	Loss/tok 3.0231 (3.0996)	LR 1.250e-04
0: TRAIN [3][2720/3880]	Time 0.402 (0.382)	Data 1.07e-04 (2.27e-04)	Tok/s 41822 (36741)	Loss/tok 3.1807 (3.0995)	LR 1.250e-04
0: TRAIN [3][2730/3880]	Time 0.413 (0.382)	Data 1.09e-04 (2.27e-04)	Tok/s 40379 (36755)	Loss/tok 3.1283 (3.0996)	LR 1.250e-04
0: TRAIN [3][2740/3880]	Time 0.299 (0.382)	Data 1.10e-04 (2.26e-04)	Tok/s 34659 (36761)	Loss/tok 2.8939 (3.0996)	LR 1.250e-04
0: TRAIN [3][2750/3880]	Time 0.400 (0.382)	Data 1.08e-04 (2.26e-04)	Tok/s 41640 (36751)	Loss/tok 3.1311 (3.0992)	LR 1.250e-04
0: TRAIN [3][2760/3880]	Time 0.512 (0.381)	Data 1.08e-04 (2.25e-04)	Tok/s 44995 (36756)	Loss/tok 3.3761 (3.0992)	LR 1.250e-04
0: TRAIN [3][2770/3880]	Time 0.526 (0.381)	Data 1.09e-04 (2.25e-04)	Tok/s 44143 (36758)	Loss/tok 3.3157 (3.0992)	LR 1.250e-04
0: TRAIN [3][2780/3880]	Time 0.405 (0.381)	Data 1.09e-04 (2.25e-04)	Tok/s 41957 (36766)	Loss/tok 3.1415 (3.0992)	LR 1.250e-04
0: TRAIN [3][2790/3880]	Time 0.305 (0.381)	Data 1.08e-04 (2.24e-04)	Tok/s 34284 (36765)	Loss/tok 2.8996 (3.0990)	LR 1.250e-04
0: TRAIN [3][2800/3880]	Time 0.390 (0.381)	Data 1.09e-04 (2.24e-04)	Tok/s 43265 (36769)	Loss/tok 3.1863 (3.0992)	LR 1.250e-04
0: TRAIN [3][2810/3880]	Time 0.317 (0.381)	Data 1.13e-04 (2.23e-04)	Tok/s 32787 (36758)	Loss/tok 2.9550 (3.0990)	LR 1.250e-04
0: TRAIN [3][2820/3880]	Time 0.206 (0.381)	Data 1.09e-04 (2.23e-04)	Tok/s 25719 (36758)	Loss/tok 2.5189 (3.0988)	LR 1.250e-04
0: TRAIN [3][2830/3880]	Time 0.521 (0.381)	Data 1.09e-04 (2.23e-04)	Tok/s 44980 (36755)	Loss/tok 3.1869 (3.0986)	LR 1.250e-04
0: TRAIN [3][2840/3880]	Time 0.416 (0.381)	Data 1.14e-04 (2.22e-04)	Tok/s 40297 (36752)	Loss/tok 3.1071 (3.0985)	LR 1.250e-04
0: TRAIN [3][2850/3880]	Time 0.422 (0.381)	Data 1.10e-04 (2.22e-04)	Tok/s 40020 (36756)	Loss/tok 3.1484 (3.0988)	LR 1.250e-04
0: TRAIN [3][2860/3880]	Time 0.525 (0.381)	Data 1.13e-04 (2.21e-04)	Tok/s 44342 (36757)	Loss/tok 3.2836 (3.0988)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2870/3880]	Time 0.316 (0.381)	Data 1.11e-04 (2.21e-04)	Tok/s 32992 (36762)	Loss/tok 2.9493 (3.0989)	LR 1.250e-04
0: TRAIN [3][2880/3880]	Time 0.300 (0.381)	Data 1.07e-04 (2.21e-04)	Tok/s 34586 (36771)	Loss/tok 2.9194 (3.0989)	LR 1.250e-04
0: TRAIN [3][2890/3880]	Time 0.508 (0.381)	Data 1.10e-04 (2.20e-04)	Tok/s 45169 (36776)	Loss/tok 3.3810 (3.0993)	LR 1.250e-04
0: TRAIN [3][2900/3880]	Time 0.210 (0.381)	Data 1.08e-04 (2.20e-04)	Tok/s 25169 (36779)	Loss/tok 2.5569 (3.0996)	LR 1.250e-04
0: TRAIN [3][2910/3880]	Time 0.565 (0.382)	Data 1.10e-04 (2.19e-04)	Tok/s 41116 (36779)	Loss/tok 3.3027 (3.0997)	LR 1.250e-04
0: TRAIN [3][2920/3880]	Time 0.299 (0.381)	Data 1.09e-04 (2.19e-04)	Tok/s 34393 (36778)	Loss/tok 2.9864 (3.0998)	LR 1.250e-04
0: TRAIN [3][2930/3880]	Time 0.644 (0.381)	Data 1.05e-04 (2.19e-04)	Tok/s 46482 (36778)	Loss/tok 3.4703 (3.0999)	LR 1.250e-04
0: TRAIN [3][2940/3880]	Time 0.212 (0.381)	Data 1.08e-04 (2.18e-04)	Tok/s 24793 (36760)	Loss/tok 2.5337 (3.0995)	LR 1.250e-04
0: TRAIN [3][2950/3880]	Time 0.381 (0.381)	Data 1.06e-04 (2.18e-04)	Tok/s 27516 (36762)	Loss/tok 3.0255 (3.1000)	LR 1.250e-04
0: TRAIN [3][2960/3880]	Time 0.493 (0.381)	Data 1.13e-04 (2.18e-04)	Tok/s 47629 (36755)	Loss/tok 3.1683 (3.1001)	LR 1.250e-04
0: TRAIN [3][2970/3880]	Time 0.315 (0.382)	Data 1.11e-04 (2.17e-04)	Tok/s 33376 (36762)	Loss/tok 2.8617 (3.1005)	LR 1.250e-04
0: TRAIN [3][2980/3880]	Time 0.294 (0.382)	Data 1.11e-04 (2.17e-04)	Tok/s 35366 (36758)	Loss/tok 2.8462 (3.1001)	LR 1.250e-04
0: TRAIN [3][2990/3880]	Time 0.209 (0.381)	Data 1.10e-04 (2.16e-04)	Tok/s 24945 (36757)	Loss/tok 2.5791 (3.1002)	LR 1.250e-04
0: TRAIN [3][3000/3880]	Time 0.285 (0.381)	Data 1.04e-04 (2.16e-04)	Tok/s 36420 (36761)	Loss/tok 2.9084 (3.1001)	LR 1.250e-04
0: TRAIN [3][3010/3880]	Time 0.207 (0.381)	Data 1.09e-04 (2.16e-04)	Tok/s 25475 (36747)	Loss/tok 2.5471 (3.0998)	LR 1.250e-04
0: TRAIN [3][3020/3880]	Time 0.394 (0.382)	Data 1.10e-04 (2.15e-04)	Tok/s 42278 (36759)	Loss/tok 3.1156 (3.1002)	LR 1.250e-04
0: TRAIN [3][3030/3880]	Time 0.422 (0.382)	Data 1.09e-04 (2.15e-04)	Tok/s 39801 (36755)	Loss/tok 3.0316 (3.1002)	LR 1.250e-04
0: TRAIN [3][3040/3880]	Time 0.318 (0.382)	Data 1.09e-04 (2.15e-04)	Tok/s 32172 (36752)	Loss/tok 2.9644 (3.0999)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3050/3880]	Time 0.611 (0.382)	Data 1.05e-04 (2.14e-04)	Tok/s 48587 (36752)	Loss/tok 3.4520 (3.1001)	LR 1.250e-04
0: TRAIN [3][3060/3880]	Time 0.529 (0.382)	Data 1.11e-04 (2.14e-04)	Tok/s 44304 (36758)	Loss/tok 3.2137 (3.1003)	LR 1.250e-04
0: TRAIN [3][3070/3880]	Time 0.206 (0.382)	Data 1.12e-04 (2.14e-04)	Tok/s 25139 (36747)	Loss/tok 2.5405 (3.1003)	LR 1.250e-04
0: TRAIN [3][3080/3880]	Time 0.210 (0.382)	Data 1.06e-04 (2.13e-04)	Tok/s 25454 (36750)	Loss/tok 2.5337 (3.1003)	LR 1.250e-04
0: TRAIN [3][3090/3880]	Time 0.568 (0.382)	Data 1.07e-04 (2.13e-04)	Tok/s 41235 (36758)	Loss/tok 3.3089 (3.1007)	LR 1.250e-04
0: TRAIN [3][3100/3880]	Time 0.298 (0.382)	Data 1.17e-04 (2.13e-04)	Tok/s 34350 (36763)	Loss/tok 2.9679 (3.1005)	LR 1.250e-04
0: TRAIN [3][3110/3880]	Time 0.682 (0.382)	Data 1.14e-04 (2.12e-04)	Tok/s 43816 (36776)	Loss/tok 3.3495 (3.1011)	LR 1.250e-04
0: TRAIN [3][3120/3880]	Time 0.672 (0.382)	Data 1.16e-04 (2.12e-04)	Tok/s 43702 (36772)	Loss/tok 3.5450 (3.1012)	LR 1.250e-04
0: TRAIN [3][3130/3880]	Time 0.208 (0.382)	Data 1.10e-04 (2.12e-04)	Tok/s 25473 (36771)	Loss/tok 2.4577 (3.1010)	LR 1.250e-04
0: TRAIN [3][3140/3880]	Time 0.399 (0.382)	Data 1.11e-04 (2.11e-04)	Tok/s 42643 (36776)	Loss/tok 3.0767 (3.1010)	LR 1.250e-04
0: TRAIN [3][3150/3880]	Time 0.304 (0.382)	Data 1.08e-04 (2.11e-04)	Tok/s 33828 (36781)	Loss/tok 2.8830 (3.1010)	LR 1.250e-04
0: TRAIN [3][3160/3880]	Time 0.427 (0.382)	Data 1.09e-04 (2.11e-04)	Tok/s 39017 (36781)	Loss/tok 3.1524 (3.1009)	LR 1.250e-04
0: TRAIN [3][3170/3880]	Time 0.298 (0.382)	Data 1.07e-04 (2.10e-04)	Tok/s 34342 (36779)	Loss/tok 2.9606 (3.1008)	LR 1.250e-04
0: TRAIN [3][3180/3880]	Time 0.202 (0.382)	Data 1.10e-04 (2.10e-04)	Tok/s 26586 (36783)	Loss/tok 2.6306 (3.1008)	LR 1.250e-04
0: TRAIN [3][3190/3880]	Time 0.203 (0.382)	Data 1.11e-04 (2.10e-04)	Tok/s 25740 (36784)	Loss/tok 2.5700 (3.1008)	LR 1.250e-04
0: TRAIN [3][3200/3880]	Time 0.558 (0.382)	Data 1.13e-04 (2.09e-04)	Tok/s 41299 (36786)	Loss/tok 3.2256 (3.1011)	LR 1.250e-04
0: TRAIN [3][3210/3880]	Time 0.347 (0.382)	Data 1.10e-04 (2.09e-04)	Tok/s 29515 (36768)	Loss/tok 2.9712 (3.1009)	LR 1.250e-04
0: TRAIN [3][3220/3880]	Time 0.209 (0.382)	Data 1.11e-04 (2.09e-04)	Tok/s 25198 (36759)	Loss/tok 2.5967 (3.1008)	LR 1.250e-04
0: TRAIN [3][3230/3880]	Time 0.502 (0.382)	Data 1.04e-04 (2.08e-04)	Tok/s 46391 (36768)	Loss/tok 3.2904 (3.1012)	LR 1.250e-04
0: TRAIN [3][3240/3880]	Time 0.389 (0.382)	Data 1.09e-04 (2.08e-04)	Tok/s 43239 (36769)	Loss/tok 3.0835 (3.1011)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3250/3880]	Time 0.364 (0.382)	Data 1.14e-04 (2.08e-04)	Tok/s 28784 (36774)	Loss/tok 2.9199 (3.1012)	LR 1.250e-04
0: TRAIN [3][3260/3880]	Time 0.400 (0.382)	Data 1.05e-04 (2.08e-04)	Tok/s 42087 (36777)	Loss/tok 3.0515 (3.1016)	LR 1.250e-04
0: TRAIN [3][3270/3880]	Time 0.295 (0.382)	Data 1.07e-04 (2.07e-04)	Tok/s 34586 (36780)	Loss/tok 2.8774 (3.1015)	LR 1.250e-04
0: TRAIN [3][3280/3880]	Time 0.219 (0.382)	Data 1.14e-04 (2.07e-04)	Tok/s 24412 (36779)	Loss/tok 2.4903 (3.1015)	LR 1.250e-04
0: TRAIN [3][3290/3880]	Time 0.419 (0.382)	Data 1.13e-04 (2.07e-04)	Tok/s 40085 (36775)	Loss/tok 3.1186 (3.1014)	LR 1.250e-04
0: TRAIN [3][3300/3880]	Time 0.434 (0.382)	Data 1.08e-04 (2.06e-04)	Tok/s 38372 (36766)	Loss/tok 3.0719 (3.1012)	LR 1.250e-04
0: TRAIN [3][3310/3880]	Time 0.300 (0.382)	Data 1.08e-04 (2.06e-04)	Tok/s 34248 (36764)	Loss/tok 2.8937 (3.1013)	LR 1.250e-04
0: TRAIN [3][3320/3880]	Time 0.204 (0.382)	Data 1.11e-04 (2.06e-04)	Tok/s 25754 (36769)	Loss/tok 2.4967 (3.1015)	LR 1.250e-04
0: TRAIN [3][3330/3880]	Time 0.507 (0.382)	Data 1.04e-04 (2.06e-04)	Tok/s 46407 (36773)	Loss/tok 3.2488 (3.1016)	LR 1.250e-04
0: TRAIN [3][3340/3880]	Time 0.311 (0.382)	Data 1.08e-04 (2.05e-04)	Tok/s 32832 (36767)	Loss/tok 2.7763 (3.1014)	LR 1.250e-04
0: TRAIN [3][3350/3880]	Time 0.382 (0.382)	Data 1.08e-04 (2.05e-04)	Tok/s 43570 (36772)	Loss/tok 3.1561 (3.1016)	LR 1.250e-04
0: TRAIN [3][3360/3880]	Time 0.416 (0.382)	Data 1.11e-04 (2.05e-04)	Tok/s 40169 (36774)	Loss/tok 3.0945 (3.1017)	LR 1.250e-04
0: TRAIN [3][3370/3880]	Time 0.426 (0.382)	Data 1.06e-04 (2.04e-04)	Tok/s 39211 (36776)	Loss/tok 3.1042 (3.1018)	LR 1.250e-04
0: TRAIN [3][3380/3880]	Time 0.210 (0.382)	Data 1.07e-04 (2.04e-04)	Tok/s 25155 (36779)	Loss/tok 2.4751 (3.1017)	LR 1.250e-04
0: TRAIN [3][3390/3880]	Time 0.307 (0.382)	Data 1.13e-04 (2.04e-04)	Tok/s 33440 (36778)	Loss/tok 2.9835 (3.1016)	LR 1.250e-04
0: TRAIN [3][3400/3880]	Time 0.491 (0.382)	Data 1.08e-04 (2.04e-04)	Tok/s 34160 (36772)	Loss/tok 3.1487 (3.1013)	LR 1.250e-04
0: TRAIN [3][3410/3880]	Time 0.310 (0.382)	Data 1.09e-04 (2.03e-04)	Tok/s 33959 (36768)	Loss/tok 2.9012 (3.1011)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3420/3880]	Time 0.422 (0.382)	Data 1.09e-04 (2.03e-04)	Tok/s 39501 (36768)	Loss/tok 3.2234 (3.1011)	LR 1.250e-04
0: TRAIN [3][3430/3880]	Time 0.514 (0.382)	Data 1.11e-04 (2.03e-04)	Tok/s 45257 (36779)	Loss/tok 3.1790 (3.1012)	LR 1.250e-04
0: TRAIN [3][3440/3880]	Time 0.290 (0.382)	Data 1.11e-04 (2.02e-04)	Tok/s 35512 (36778)	Loss/tok 2.8306 (3.1015)	LR 1.250e-04
0: TRAIN [3][3450/3880]	Time 0.309 (0.382)	Data 1.11e-04 (2.02e-04)	Tok/s 33083 (36778)	Loss/tok 2.9426 (3.1016)	LR 1.250e-04
0: TRAIN [3][3460/3880]	Time 0.525 (0.382)	Data 1.08e-04 (2.02e-04)	Tok/s 44599 (36781)	Loss/tok 3.2727 (3.1016)	LR 1.250e-04
0: TRAIN [3][3470/3880]	Time 0.423 (0.382)	Data 1.08e-04 (2.02e-04)	Tok/s 39966 (36782)	Loss/tok 3.0308 (3.1015)	LR 1.250e-04
0: TRAIN [3][3480/3880]	Time 0.306 (0.382)	Data 1.10e-04 (2.01e-04)	Tok/s 33744 (36783)	Loss/tok 3.0082 (3.1014)	LR 1.250e-04
0: TRAIN [3][3490/3880]	Time 0.638 (0.382)	Data 1.08e-04 (2.01e-04)	Tok/s 46479 (36792)	Loss/tok 3.5268 (3.1017)	LR 1.250e-04
0: TRAIN [3][3500/3880]	Time 0.545 (0.382)	Data 1.10e-04 (2.01e-04)	Tok/s 42279 (36789)	Loss/tok 3.3174 (3.1017)	LR 1.250e-04
0: TRAIN [3][3510/3880]	Time 0.311 (0.382)	Data 1.12e-04 (2.01e-04)	Tok/s 33046 (36791)	Loss/tok 2.8933 (3.1020)	LR 1.250e-04
0: TRAIN [3][3520/3880]	Time 0.410 (0.382)	Data 1.01e-04 (2.00e-04)	Tok/s 41760 (36789)	Loss/tok 3.0255 (3.1019)	LR 1.250e-04
0: TRAIN [3][3530/3880]	Time 0.313 (0.382)	Data 1.11e-04 (2.00e-04)	Tok/s 32760 (36781)	Loss/tok 2.9910 (3.1017)	LR 1.250e-04
0: TRAIN [3][3540/3880]	Time 0.291 (0.382)	Data 1.11e-04 (2.00e-04)	Tok/s 35458 (36785)	Loss/tok 2.8860 (3.1017)	LR 1.250e-04
0: TRAIN [3][3550/3880]	Time 0.554 (0.382)	Data 1.08e-04 (2.00e-04)	Tok/s 41674 (36795)	Loss/tok 3.2898 (3.1022)	LR 1.250e-04
0: TRAIN [3][3560/3880]	Time 0.395 (0.382)	Data 1.11e-04 (1.99e-04)	Tok/s 42827 (36794)	Loss/tok 3.1289 (3.1022)	LR 1.250e-04
0: TRAIN [3][3570/3880]	Time 0.421 (0.383)	Data 1.10e-04 (1.99e-04)	Tok/s 39158 (36803)	Loss/tok 3.0928 (3.1025)	LR 1.250e-04
0: TRAIN [3][3580/3880]	Time 0.431 (0.383)	Data 1.08e-04 (1.99e-04)	Tok/s 39054 (36800)	Loss/tok 3.1019 (3.1026)	LR 1.250e-04
0: TRAIN [3][3590/3880]	Time 0.493 (0.383)	Data 1.12e-04 (1.99e-04)	Tok/s 47227 (36810)	Loss/tok 3.2822 (3.1029)	LR 1.250e-04
0: TRAIN [3][3600/3880]	Time 0.401 (0.383)	Data 1.09e-04 (1.98e-04)	Tok/s 42428 (36817)	Loss/tok 3.0559 (3.1030)	LR 1.250e-04
0: TRAIN [3][3610/3880]	Time 0.205 (0.383)	Data 1.08e-04 (1.98e-04)	Tok/s 25447 (36810)	Loss/tok 2.5970 (3.1027)	LR 1.250e-04
0: TRAIN [3][3620/3880]	Time 0.210 (0.383)	Data 1.12e-04 (1.98e-04)	Tok/s 24675 (36815)	Loss/tok 2.5273 (3.1029)	LR 1.250e-04
0: TRAIN [3][3630/3880]	Time 0.338 (0.383)	Data 1.07e-04 (1.98e-04)	Tok/s 30582 (36812)	Loss/tok 2.9203 (3.1029)	LR 1.250e-04
0: TRAIN [3][3640/3880]	Time 0.429 (0.383)	Data 1.08e-04 (1.97e-04)	Tok/s 39631 (36821)	Loss/tok 3.0485 (3.1030)	LR 1.250e-04
0: TRAIN [3][3650/3880]	Time 0.391 (0.383)	Data 1.12e-04 (1.97e-04)	Tok/s 42805 (36822)	Loss/tok 3.1039 (3.1030)	LR 1.250e-04
0: TRAIN [3][3660/3880]	Time 0.320 (0.383)	Data 1.10e-04 (1.97e-04)	Tok/s 32326 (36825)	Loss/tok 2.8027 (3.1033)	LR 1.250e-04
0: TRAIN [3][3670/3880]	Time 0.303 (0.383)	Data 1.08e-04 (1.97e-04)	Tok/s 33664 (36823)	Loss/tok 2.8557 (3.1032)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][3680/3880]	Time 0.312 (0.383)	Data 1.16e-04 (1.96e-04)	Tok/s 33085 (36823)	Loss/tok 3.0890 (3.1034)	LR 1.250e-04
0: TRAIN [3][3690/3880]	Time 0.289 (0.383)	Data 1.07e-04 (1.96e-04)	Tok/s 35537 (36812)	Loss/tok 2.8976 (3.1030)	LR 1.250e-04
0: TRAIN [3][3700/3880]	Time 0.207 (0.383)	Data 1.07e-04 (1.96e-04)	Tok/s 26004 (36810)	Loss/tok 2.5467 (3.1030)	LR 1.250e-04
0: TRAIN [3][3710/3880]	Time 0.408 (0.383)	Data 1.11e-04 (1.96e-04)	Tok/s 41331 (36808)	Loss/tok 3.0933 (3.1027)	LR 1.250e-04
0: TRAIN [3][3720/3880]	Time 0.289 (0.383)	Data 1.03e-04 (1.96e-04)	Tok/s 34719 (36799)	Loss/tok 2.9317 (3.1028)	LR 1.250e-04
0: TRAIN [3][3730/3880]	Time 0.405 (0.383)	Data 1.05e-04 (1.95e-04)	Tok/s 41249 (36808)	Loss/tok 3.0818 (3.1030)	LR 1.250e-04
0: TRAIN [3][3740/3880]	Time 0.319 (0.383)	Data 1.09e-04 (1.95e-04)	Tok/s 32178 (36804)	Loss/tok 2.9457 (3.1028)	LR 1.250e-04
0: TRAIN [3][3750/3880]	Time 0.635 (0.383)	Data 1.06e-04 (1.95e-04)	Tok/s 46578 (36804)	Loss/tok 3.5413 (3.1030)	LR 1.250e-04
0: TRAIN [3][3760/3880]	Time 0.538 (0.383)	Data 1.07e-04 (1.95e-04)	Tok/s 42961 (36800)	Loss/tok 3.1868 (3.1028)	LR 1.250e-04
0: TRAIN [3][3770/3880]	Time 0.305 (0.383)	Data 1.09e-04 (1.94e-04)	Tok/s 34146 (36805)	Loss/tok 2.8918 (3.1029)	LR 1.250e-04
0: TRAIN [3][3780/3880]	Time 0.299 (0.383)	Data 1.08e-04 (1.94e-04)	Tok/s 34324 (36818)	Loss/tok 2.8805 (3.1031)	LR 1.250e-04
0: TRAIN [3][3790/3880]	Time 0.428 (0.383)	Data 1.10e-04 (1.94e-04)	Tok/s 39131 (36805)	Loss/tok 3.1876 (3.1030)	LR 1.250e-04
0: TRAIN [3][3800/3880]	Time 0.301 (0.383)	Data 1.11e-04 (1.94e-04)	Tok/s 34289 (36806)	Loss/tok 2.9058 (3.1028)	LR 1.250e-04
0: TRAIN [3][3810/3880]	Time 0.295 (0.383)	Data 1.08e-04 (1.93e-04)	Tok/s 34512 (36809)	Loss/tok 2.8651 (3.1027)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][3820/3880]	Time 0.323 (0.383)	Data 1.12e-04 (1.93e-04)	Tok/s 31336 (36811)	Loss/tok 2.9139 (3.1026)	LR 1.250e-04
0: TRAIN [3][3830/3880]	Time 0.299 (0.383)	Data 1.05e-04 (1.93e-04)	Tok/s 34944 (36814)	Loss/tok 2.9673 (3.1026)	LR 1.250e-04
0: TRAIN [3][3840/3880]	Time 0.399 (0.383)	Data 1.08e-04 (1.93e-04)	Tok/s 42206 (36819)	Loss/tok 3.1412 (3.1025)	LR 1.250e-04
0: TRAIN [3][3850/3880]	Time 0.313 (0.383)	Data 1.09e-04 (1.93e-04)	Tok/s 33342 (36820)	Loss/tok 2.9656 (3.1024)	LR 1.250e-04
0: TRAIN [3][3860/3880]	Time 0.298 (0.382)	Data 1.03e-04 (1.92e-04)	Tok/s 34976 (36822)	Loss/tok 2.9275 (3.1023)	LR 1.250e-04
0: TRAIN [3][3870/3880]	Time 0.308 (0.382)	Data 1.08e-04 (1.92e-04)	Tok/s 34056 (36819)	Loss/tok 2.9114 (3.1021)	LR 1.250e-04
:::MLL 1572991004.017 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1572991004.018 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/6]	Time 0.792 (0.792)	Decoder iters 100.0 (100.0)	Tok/s 20639 (20639)
0: Running moses detokenizer
0: BLEU(score=23.91226041674173, counts=[37038, 18494, 10499, 6229], totals=[65436, 62433, 59430, 56432], precisions=[56.60187052998349, 29.622154950106513, 17.66616187110887, 11.03806351006521], bp=1.0, sys_len=65436, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1572991007.159 eval_accuracy: {"value": 23.91, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1572991007.159 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.1026	Test BLEU: 23.91
0: Performance: Epoch: 3	Training: 147283 Tok/s
0: Finished epoch 3
:::MLL 1572991007.160 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
:::MLL 1572991007.160 block_start: {"value": null, "metadata": {"first_epoch_num": 5, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1572991007.160 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 514}}
0: Starting epoch 4
0: Executing preallocation
0: Sampler for epoch 4 uses seed 3156120348
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][0/3880]	Time 0.949 (0.949)	Data 2.75e-01 (2.75e-01)	Tok/s 31926 (31926)	Loss/tok 3.3663 (3.3663)	LR 1.250e-04
0: TRAIN [4][10/3880]	Time 0.298 (0.451)	Data 1.06e-04 (2.51e-02)	Tok/s 33783 (39972)	Loss/tok 2.9263 (3.1664)	LR 1.250e-04
0: TRAIN [4][20/3880]	Time 0.295 (0.422)	Data 1.10e-04 (1.32e-02)	Tok/s 35773 (39270)	Loss/tok 2.8754 (3.1539)	LR 1.250e-04
0: TRAIN [4][30/3880]	Time 0.405 (0.413)	Data 1.06e-04 (8.99e-03)	Tok/s 41227 (39381)	Loss/tok 3.0970 (3.1524)	LR 1.250e-04
0: TRAIN [4][40/3880]	Time 0.433 (0.403)	Data 1.11e-04 (6.82e-03)	Tok/s 38745 (38159)	Loss/tok 3.1651 (3.1261)	LR 1.250e-04
0: TRAIN [4][50/3880]	Time 0.397 (0.401)	Data 1.08e-04 (5.51e-03)	Tok/s 42598 (38318)	Loss/tok 2.9826 (3.1124)	LR 1.250e-04
0: TRAIN [4][60/3880]	Time 0.404 (0.391)	Data 1.10e-04 (4.62e-03)	Tok/s 42004 (37857)	Loss/tok 2.9452 (3.0983)	LR 1.250e-04
0: TRAIN [4][70/3880]	Time 0.465 (0.388)	Data 1.11e-04 (3.99e-03)	Tok/s 35461 (37355)	Loss/tok 3.0040 (3.0876)	LR 1.250e-04
0: TRAIN [4][80/3880]	Time 0.307 (0.390)	Data 1.12e-04 (3.51e-03)	Tok/s 33544 (37194)	Loss/tok 2.8430 (3.0887)	LR 1.250e-04
0: TRAIN [4][90/3880]	Time 0.403 (0.388)	Data 1.11e-04 (3.13e-03)	Tok/s 41415 (37352)	Loss/tok 3.1304 (3.0883)	LR 1.250e-04
0: TRAIN [4][100/3880]	Time 0.306 (0.385)	Data 1.11e-04 (2.83e-03)	Tok/s 34137 (37283)	Loss/tok 2.9866 (3.0858)	LR 1.250e-04
0: TRAIN [4][110/3880]	Time 0.316 (0.395)	Data 1.11e-04 (2.59e-03)	Tok/s 32196 (37628)	Loss/tok 2.8567 (3.1038)	LR 1.250e-04
0: TRAIN [4][120/3880]	Time 0.586 (0.396)	Data 1.11e-04 (2.38e-03)	Tok/s 39509 (37717)	Loss/tok 3.2394 (3.1051)	LR 1.250e-04
0: TRAIN [4][130/3880]	Time 0.416 (0.394)	Data 1.06e-04 (2.21e-03)	Tok/s 39976 (37493)	Loss/tok 3.1129 (3.1053)	LR 1.250e-04
0: TRAIN [4][140/3880]	Time 0.206 (0.393)	Data 1.05e-04 (2.06e-03)	Tok/s 25394 (37429)	Loss/tok 2.5289 (3.1030)	LR 1.250e-04
0: TRAIN [4][150/3880]	Time 0.519 (0.394)	Data 1.04e-04 (1.93e-03)	Tok/s 45115 (37613)	Loss/tok 3.3653 (3.1089)	LR 1.250e-04
0: TRAIN [4][160/3880]	Time 0.304 (0.390)	Data 1.08e-04 (1.82e-03)	Tok/s 32926 (37483)	Loss/tok 2.9301 (3.1035)	LR 1.250e-04
0: TRAIN [4][170/3880]	Time 0.335 (0.389)	Data 1.03e-04 (1.72e-03)	Tok/s 30893 (37154)	Loss/tok 3.0136 (3.0971)	LR 1.250e-04
0: TRAIN [4][180/3880]	Time 0.406 (0.388)	Data 1.08e-04 (1.63e-03)	Tok/s 40819 (37138)	Loss/tok 3.1563 (3.0979)	LR 1.250e-04
0: TRAIN [4][190/3880]	Time 0.516 (0.388)	Data 1.13e-04 (1.55e-03)	Tok/s 44872 (37220)	Loss/tok 3.3113 (3.0974)	LR 1.250e-04
0: TRAIN [4][200/3880]	Time 0.542 (0.390)	Data 1.11e-04 (1.48e-03)	Tok/s 43066 (37300)	Loss/tok 3.2478 (3.1000)	LR 1.250e-04
0: TRAIN [4][210/3880]	Time 0.419 (0.392)	Data 1.10e-04 (1.41e-03)	Tok/s 40249 (37427)	Loss/tok 3.1233 (3.1004)	LR 1.250e-04
0: TRAIN [4][220/3880]	Time 0.312 (0.394)	Data 1.09e-04 (1.35e-03)	Tok/s 32690 (37367)	Loss/tok 2.9205 (3.1028)	LR 1.250e-04
0: TRAIN [4][230/3880]	Time 0.401 (0.392)	Data 1.09e-04 (1.30e-03)	Tok/s 41861 (37241)	Loss/tok 3.0194 (3.1039)	LR 1.250e-04
0: TRAIN [4][240/3880]	Time 0.209 (0.392)	Data 1.10e-04 (1.25e-03)	Tok/s 25245 (37145)	Loss/tok 2.5711 (3.1034)	LR 1.250e-04
0: TRAIN [4][250/3880]	Time 0.295 (0.390)	Data 1.08e-04 (1.21e-03)	Tok/s 35125 (37109)	Loss/tok 2.9131 (3.1008)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][260/3880]	Time 0.384 (0.388)	Data 1.09e-04 (1.16e-03)	Tok/s 43863 (37106)	Loss/tok 3.0031 (3.0982)	LR 1.250e-04
0: TRAIN [4][270/3880]	Time 0.295 (0.387)	Data 1.13e-04 (1.12e-03)	Tok/s 34691 (37074)	Loss/tok 2.8900 (3.0955)	LR 1.250e-04
0: TRAIN [4][280/3880]	Time 0.515 (0.388)	Data 1.09e-04 (1.09e-03)	Tok/s 45698 (37177)	Loss/tok 3.2468 (3.0955)	LR 1.250e-04
0: TRAIN [4][290/3880]	Time 0.515 (0.387)	Data 1.08e-04 (1.05e-03)	Tok/s 45523 (37198)	Loss/tok 3.1995 (3.0942)	LR 1.250e-04
0: TRAIN [4][300/3880]	Time 0.315 (0.386)	Data 1.09e-04 (1.02e-03)	Tok/s 33286 (37118)	Loss/tok 2.8463 (3.0951)	LR 1.250e-04
0: TRAIN [4][310/3880]	Time 0.500 (0.389)	Data 1.07e-04 (9.94e-04)	Tok/s 46454 (37290)	Loss/tok 3.2875 (3.1001)	LR 1.250e-04
0: TRAIN [4][320/3880]	Time 0.329 (0.390)	Data 1.08e-04 (9.66e-04)	Tok/s 31270 (37048)	Loss/tok 2.8613 (3.0961)	LR 1.250e-04
0: TRAIN [4][330/3880]	Time 0.288 (0.388)	Data 1.04e-04 (9.40e-04)	Tok/s 36080 (36942)	Loss/tok 2.9323 (3.0930)	LR 1.250e-04
0: TRAIN [4][340/3880]	Time 0.538 (0.388)	Data 1.10e-04 (9.16e-04)	Tok/s 43544 (37032)	Loss/tok 3.2492 (3.0960)	LR 1.250e-04
0: TRAIN [4][350/3880]	Time 0.639 (0.388)	Data 1.13e-04 (8.93e-04)	Tok/s 46169 (37035)	Loss/tok 3.4152 (3.0971)	LR 1.250e-04
0: TRAIN [4][360/3880]	Time 0.300 (0.389)	Data 1.10e-04 (8.72e-04)	Tok/s 34146 (37141)	Loss/tok 2.8856 (3.0974)	LR 1.250e-04
0: TRAIN [4][370/3880]	Time 0.692 (0.393)	Data 1.09e-04 (8.51e-04)	Tok/s 42723 (37124)	Loss/tok 3.4014 (3.1010)	LR 1.250e-04
0: TRAIN [4][380/3880]	Time 0.497 (0.393)	Data 1.11e-04 (8.32e-04)	Tok/s 47387 (37147)	Loss/tok 3.1688 (3.1015)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][390/3880]	Time 0.202 (0.393)	Data 1.08e-04 (8.13e-04)	Tok/s 25982 (37151)	Loss/tok 2.5068 (3.1023)	LR 1.250e-04
0: TRAIN [4][400/3880]	Time 0.433 (0.393)	Data 1.08e-04 (7.95e-04)	Tok/s 38748 (37091)	Loss/tok 3.1633 (3.1011)	LR 1.250e-04
0: TRAIN [4][410/3880]	Time 0.306 (0.394)	Data 1.15e-04 (7.79e-04)	Tok/s 33145 (37127)	Loss/tok 2.8122 (3.1028)	LR 1.250e-04
0: TRAIN [4][420/3880]	Time 0.394 (0.394)	Data 1.12e-04 (7.63e-04)	Tok/s 42692 (37138)	Loss/tok 3.1207 (3.1031)	LR 1.250e-04
0: TRAIN [4][430/3880]	Time 0.316 (0.396)	Data 1.11e-04 (7.48e-04)	Tok/s 32642 (37107)	Loss/tok 2.9798 (3.1055)	LR 1.250e-04
0: TRAIN [4][440/3880]	Time 0.510 (0.395)	Data 1.10e-04 (7.33e-04)	Tok/s 44908 (37122)	Loss/tok 3.2910 (3.1049)	LR 1.250e-04
0: TRAIN [4][450/3880]	Time 0.300 (0.394)	Data 1.07e-04 (7.19e-04)	Tok/s 34540 (37105)	Loss/tok 2.7772 (3.1039)	LR 1.250e-04
0: TRAIN [4][460/3880]	Time 0.419 (0.393)	Data 1.08e-04 (7.06e-04)	Tok/s 39914 (37102)	Loss/tok 3.1255 (3.1013)	LR 1.250e-04
0: TRAIN [4][470/3880]	Time 0.420 (0.394)	Data 1.15e-04 (6.94e-04)	Tok/s 24540 (37121)	Loss/tok 2.9450 (3.1005)	LR 1.250e-04
0: TRAIN [4][480/3880]	Time 0.317 (0.394)	Data 1.23e-04 (6.81e-04)	Tok/s 32982 (37015)	Loss/tok 3.0815 (3.0983)	LR 1.250e-04
0: TRAIN [4][490/3880]	Time 0.402 (0.393)	Data 1.11e-04 (6.70e-04)	Tok/s 41464 (37012)	Loss/tok 3.1135 (3.0976)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][500/3880]	Time 0.479 (0.393)	Data 1.08e-04 (6.59e-04)	Tok/s 49262 (37015)	Loss/tok 3.1752 (3.0971)	LR 1.250e-04
0: TRAIN [4][510/3880]	Time 0.534 (0.392)	Data 1.08e-04 (6.48e-04)	Tok/s 43791 (37000)	Loss/tok 3.1814 (3.0953)	LR 1.250e-04
0: TRAIN [4][520/3880]	Time 0.501 (0.392)	Data 1.09e-04 (6.37e-04)	Tok/s 46157 (37011)	Loss/tok 3.3497 (3.0964)	LR 1.250e-04
0: TRAIN [4][530/3880]	Time 0.310 (0.391)	Data 1.13e-04 (6.28e-04)	Tok/s 33608 (36977)	Loss/tok 2.9399 (3.0964)	LR 1.250e-04
0: TRAIN [4][540/3880]	Time 0.308 (0.391)	Data 1.11e-04 (6.18e-04)	Tok/s 33417 (36998)	Loss/tok 2.8707 (3.0973)	LR 1.250e-04
0: TRAIN [4][550/3880]	Time 0.395 (0.392)	Data 1.06e-04 (6.09e-04)	Tok/s 43053 (37033)	Loss/tok 3.1875 (3.0996)	LR 1.250e-04
0: TRAIN [4][560/3880]	Time 0.321 (0.393)	Data 1.07e-04 (6.00e-04)	Tok/s 32424 (36987)	Loss/tok 2.7899 (3.0994)	LR 1.250e-04
0: TRAIN [4][570/3880]	Time 0.310 (0.392)	Data 1.10e-04 (5.91e-04)	Tok/s 33336 (36917)	Loss/tok 2.8991 (3.0974)	LR 1.250e-04
0: TRAIN [4][580/3880]	Time 0.406 (0.391)	Data 1.09e-04 (5.83e-04)	Tok/s 40961 (36943)	Loss/tok 3.0711 (3.0970)	LR 1.250e-04
0: TRAIN [4][590/3880]	Time 0.387 (0.392)	Data 1.03e-04 (5.75e-04)	Tok/s 43591 (36955)	Loss/tok 3.0890 (3.0967)	LR 1.250e-04
0: TRAIN [4][600/3880]	Time 0.426 (0.392)	Data 1.08e-04 (5.67e-04)	Tok/s 38847 (36983)	Loss/tok 3.0353 (3.0979)	LR 1.250e-04
0: TRAIN [4][610/3880]	Time 0.308 (0.391)	Data 1.11e-04 (5.60e-04)	Tok/s 33708 (36961)	Loss/tok 2.9008 (3.0963)	LR 1.250e-04
0: TRAIN [4][620/3880]	Time 0.309 (0.390)	Data 1.08e-04 (5.52e-04)	Tok/s 33325 (36940)	Loss/tok 2.9073 (3.0945)	LR 1.250e-04
0: TRAIN [4][630/3880]	Time 0.364 (0.389)	Data 1.07e-04 (5.45e-04)	Tok/s 28089 (36918)	Loss/tok 2.8650 (3.0931)	LR 1.250e-04
0: TRAIN [4][640/3880]	Time 0.418 (0.390)	Data 1.13e-04 (5.38e-04)	Tok/s 40509 (36906)	Loss/tok 3.1632 (3.0929)	LR 1.250e-04
0: TRAIN [4][650/3880]	Time 0.207 (0.390)	Data 1.13e-04 (5.32e-04)	Tok/s 25465 (36908)	Loss/tok 2.4896 (3.0934)	LR 1.250e-04
0: TRAIN [4][660/3880]	Time 0.342 (0.390)	Data 1.10e-04 (5.26e-04)	Tok/s 30552 (36881)	Loss/tok 2.8919 (3.0934)	LR 1.250e-04
0: TRAIN [4][670/3880]	Time 0.308 (0.389)	Data 1.04e-04 (5.19e-04)	Tok/s 33742 (36844)	Loss/tok 2.8385 (3.0923)	LR 1.250e-04
0: TRAIN [4][680/3880]	Time 0.407 (0.388)	Data 1.10e-04 (5.13e-04)	Tok/s 41144 (36832)	Loss/tok 3.0459 (3.0911)	LR 1.250e-04
0: TRAIN [4][690/3880]	Time 0.305 (0.388)	Data 1.11e-04 (5.07e-04)	Tok/s 34085 (36834)	Loss/tok 2.9835 (3.0907)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][700/3880]	Time 0.325 (0.389)	Data 1.08e-04 (5.02e-04)	Tok/s 31578 (36867)	Loss/tok 2.9383 (3.0936)	LR 1.250e-04
0: TRAIN [4][710/3880]	Time 0.398 (0.389)	Data 1.07e-04 (4.96e-04)	Tok/s 42105 (36845)	Loss/tok 3.0723 (3.0923)	LR 1.250e-04
0: TRAIN [4][720/3880]	Time 0.467 (0.389)	Data 1.14e-04 (4.91e-04)	Tok/s 35954 (36872)	Loss/tok 3.1082 (3.0926)	LR 1.250e-04
0: TRAIN [4][730/3880]	Time 0.502 (0.389)	Data 1.09e-04 (4.86e-04)	Tok/s 46781 (36889)	Loss/tok 3.2254 (3.0924)	LR 1.250e-04
0: TRAIN [4][740/3880]	Time 0.428 (0.388)	Data 1.08e-04 (4.81e-04)	Tok/s 39925 (36889)	Loss/tok 3.1030 (3.0919)	LR 1.250e-04
0: TRAIN [4][750/3880]	Time 0.401 (0.389)	Data 1.08e-04 (4.76e-04)	Tok/s 42142 (36918)	Loss/tok 3.1730 (3.0926)	LR 1.250e-04
0: TRAIN [4][760/3880]	Time 0.329 (0.389)	Data 1.07e-04 (4.71e-04)	Tok/s 31778 (36916)	Loss/tok 2.8729 (3.0937)	LR 1.250e-04
0: TRAIN [4][770/3880]	Time 0.313 (0.389)	Data 1.07e-04 (4.66e-04)	Tok/s 32969 (36897)	Loss/tok 2.9373 (3.0925)	LR 1.250e-04
0: TRAIN [4][780/3880]	Time 0.301 (0.389)	Data 1.13e-04 (4.62e-04)	Tok/s 34073 (36904)	Loss/tok 2.9881 (3.0924)	LR 1.250e-04
0: TRAIN [4][790/3880]	Time 0.501 (0.389)	Data 1.10e-04 (4.57e-04)	Tok/s 45815 (36933)	Loss/tok 3.2702 (3.0922)	LR 1.250e-04
0: TRAIN [4][800/3880]	Time 0.295 (0.388)	Data 1.09e-04 (4.53e-04)	Tok/s 35649 (36940)	Loss/tok 2.8755 (3.0915)	LR 1.250e-04
0: TRAIN [4][810/3880]	Time 0.318 (0.388)	Data 1.13e-04 (4.49e-04)	Tok/s 32889 (36916)	Loss/tok 2.7546 (3.0917)	LR 1.250e-04
0: TRAIN [4][820/3880]	Time 0.300 (0.388)	Data 1.05e-04 (4.44e-04)	Tok/s 33395 (36912)	Loss/tok 2.8729 (3.0913)	LR 1.250e-04
0: TRAIN [4][830/3880]	Time 0.523 (0.389)	Data 1.14e-04 (4.40e-04)	Tok/s 44657 (36961)	Loss/tok 3.2235 (3.0926)	LR 1.250e-04
0: TRAIN [4][840/3880]	Time 0.302 (0.388)	Data 1.06e-04 (4.36e-04)	Tok/s 34233 (36949)	Loss/tok 2.8703 (3.0916)	LR 1.250e-04
0: TRAIN [4][850/3880]	Time 0.582 (0.389)	Data 1.09e-04 (4.33e-04)	Tok/s 39952 (36948)	Loss/tok 3.2232 (3.0928)	LR 1.250e-04
0: TRAIN [4][860/3880]	Time 0.311 (0.389)	Data 1.08e-04 (4.29e-04)	Tok/s 33733 (36911)	Loss/tok 2.9434 (3.0923)	LR 1.250e-04
0: TRAIN [4][870/3880]	Time 0.508 (0.388)	Data 1.11e-04 (4.25e-04)	Tok/s 46017 (36890)	Loss/tok 3.2016 (3.0917)	LR 1.250e-04
0: TRAIN [4][880/3880]	Time 0.302 (0.388)	Data 1.09e-04 (4.22e-04)	Tok/s 33891 (36900)	Loss/tok 2.8524 (3.0925)	LR 1.250e-04
0: TRAIN [4][890/3880]	Time 0.307 (0.388)	Data 1.18e-04 (4.18e-04)	Tok/s 33557 (36900)	Loss/tok 2.8507 (3.0923)	LR 1.250e-04
0: TRAIN [4][900/3880]	Time 0.631 (0.388)	Data 1.08e-04 (4.15e-04)	Tok/s 36992 (36898)	Loss/tok 3.3206 (3.0927)	LR 1.250e-04
0: TRAIN [4][910/3880]	Time 0.351 (0.388)	Data 1.08e-04 (4.11e-04)	Tok/s 28814 (36863)	Loss/tok 2.8533 (3.0916)	LR 1.250e-04
0: TRAIN [4][920/3880]	Time 0.322 (0.388)	Data 1.22e-04 (4.08e-04)	Tok/s 32037 (36819)	Loss/tok 2.9746 (3.0914)	LR 1.250e-04
0: TRAIN [4][930/3880]	Time 0.398 (0.388)	Data 1.09e-04 (4.05e-04)	Tok/s 41986 (36821)	Loss/tok 3.0629 (3.0908)	LR 1.250e-04
0: TRAIN [4][940/3880]	Time 0.398 (0.388)	Data 1.09e-04 (4.02e-04)	Tok/s 42352 (36836)	Loss/tok 2.9863 (3.0905)	LR 1.250e-04
0: TRAIN [4][950/3880]	Time 0.315 (0.388)	Data 1.01e-04 (3.99e-04)	Tok/s 32273 (36823)	Loss/tok 2.9455 (3.0908)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][960/3880]	Time 0.298 (0.388)	Data 1.08e-04 (3.96e-04)	Tok/s 34995 (36833)	Loss/tok 2.7861 (3.0918)	LR 1.250e-04
0: TRAIN [4][970/3880]	Time 0.207 (0.388)	Data 1.07e-04 (3.93e-04)	Tok/s 25442 (36782)	Loss/tok 2.5212 (3.0906)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][980/3880]	Time 0.293 (0.387)	Data 1.18e-04 (3.90e-04)	Tok/s 35107 (36782)	Loss/tok 2.8150 (3.0905)	LR 1.250e-04
0: TRAIN [4][990/3880]	Time 0.304 (0.387)	Data 1.08e-04 (3.87e-04)	Tok/s 33338 (36763)	Loss/tok 2.9242 (3.0904)	LR 1.250e-04
0: TRAIN [4][1000/3880]	Time 0.492 (0.387)	Data 1.04e-04 (3.84e-04)	Tok/s 47125 (36808)	Loss/tok 3.3276 (3.0909)	LR 1.250e-04
0: TRAIN [4][1010/3880]	Time 0.517 (0.388)	Data 1.12e-04 (3.82e-04)	Tok/s 44566 (36851)	Loss/tok 3.3570 (3.0921)	LR 1.250e-04
0: TRAIN [4][1020/3880]	Time 0.539 (0.388)	Data 1.09e-04 (3.79e-04)	Tok/s 43025 (36850)	Loss/tok 3.2020 (3.0922)	LR 1.250e-04
0: TRAIN [4][1030/3880]	Time 0.289 (0.388)	Data 1.08e-04 (3.76e-04)	Tok/s 35612 (36844)	Loss/tok 2.8628 (3.0913)	LR 1.250e-04
0: TRAIN [4][1040/3880]	Time 0.422 (0.388)	Data 1.10e-04 (3.74e-04)	Tok/s 40563 (36852)	Loss/tok 3.0475 (3.0910)	LR 1.250e-04
0: TRAIN [4][1050/3880]	Time 0.426 (0.388)	Data 1.16e-04 (3.71e-04)	Tok/s 40022 (36873)	Loss/tok 3.0555 (3.0910)	LR 1.250e-04
0: TRAIN [4][1060/3880]	Time 0.391 (0.389)	Data 1.10e-04 (3.69e-04)	Tok/s 42443 (36906)	Loss/tok 3.2139 (3.0920)	LR 1.250e-04
0: TRAIN [4][1070/3880]	Time 0.683 (0.389)	Data 1.08e-04 (3.66e-04)	Tok/s 43855 (36901)	Loss/tok 3.3648 (3.0931)	LR 1.250e-04
0: TRAIN [4][1080/3880]	Time 0.412 (0.389)	Data 1.08e-04 (3.64e-04)	Tok/s 41342 (36917)	Loss/tok 3.0095 (3.0927)	LR 1.250e-04
0: TRAIN [4][1090/3880]	Time 0.521 (0.389)	Data 1.13e-04 (3.62e-04)	Tok/s 32115 (36918)	Loss/tok 2.9915 (3.0930)	LR 1.250e-04
0: TRAIN [4][1100/3880]	Time 0.293 (0.390)	Data 1.05e-04 (3.59e-04)	Tok/s 35216 (36905)	Loss/tok 2.7739 (3.0932)	LR 1.250e-04
0: TRAIN [4][1110/3880]	Time 0.392 (0.389)	Data 1.09e-04 (3.57e-04)	Tok/s 43502 (36909)	Loss/tok 2.9811 (3.0924)	LR 1.250e-04
0: TRAIN [4][1120/3880]	Time 0.301 (0.389)	Data 1.12e-04 (3.55e-04)	Tok/s 34136 (36895)	Loss/tok 2.8167 (3.0916)	LR 1.250e-04
0: TRAIN [4][1130/3880]	Time 0.408 (0.388)	Data 1.08e-04 (3.53e-04)	Tok/s 41406 (36904)	Loss/tok 3.0893 (3.0916)	LR 1.250e-04
0: TRAIN [4][1140/3880]	Time 0.411 (0.389)	Data 1.12e-04 (3.51e-04)	Tok/s 40958 (36937)	Loss/tok 3.1193 (3.0922)	LR 1.250e-04
0: TRAIN [4][1150/3880]	Time 0.301 (0.388)	Data 1.08e-04 (3.48e-04)	Tok/s 34250 (36936)	Loss/tok 2.8651 (3.0914)	LR 1.250e-04
0: TRAIN [4][1160/3880]	Time 0.368 (0.388)	Data 1.13e-04 (3.46e-04)	Tok/s 27702 (36932)	Loss/tok 2.8237 (3.0910)	LR 1.250e-04
0: TRAIN [4][1170/3880]	Time 0.310 (0.388)	Data 1.09e-04 (3.44e-04)	Tok/s 33853 (36909)	Loss/tok 2.8863 (3.0909)	LR 1.250e-04
0: TRAIN [4][1180/3880]	Time 0.301 (0.388)	Data 1.09e-04 (3.42e-04)	Tok/s 34791 (36903)	Loss/tok 2.8612 (3.0906)	LR 1.250e-04
0: TRAIN [4][1190/3880]	Time 0.222 (0.389)	Data 1.10e-04 (3.40e-04)	Tok/s 23776 (36903)	Loss/tok 2.5419 (3.0917)	LR 1.250e-04
0: TRAIN [4][1200/3880]	Time 0.406 (0.388)	Data 1.13e-04 (3.38e-04)	Tok/s 41307 (36898)	Loss/tok 3.1359 (3.0915)	LR 1.250e-04
0: TRAIN [4][1210/3880]	Time 0.409 (0.388)	Data 1.06e-04 (3.37e-04)	Tok/s 41241 (36902)	Loss/tok 2.9593 (3.0909)	LR 1.250e-04
0: TRAIN [4][1220/3880]	Time 0.510 (0.388)	Data 1.08e-04 (3.35e-04)	Tok/s 46272 (36919)	Loss/tok 3.2940 (3.0914)	LR 1.250e-04
0: TRAIN [4][1230/3880]	Time 0.309 (0.388)	Data 1.09e-04 (3.33e-04)	Tok/s 32988 (36921)	Loss/tok 2.8736 (3.0911)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][1240/3880]	Time 0.299 (0.388)	Data 1.10e-04 (3.31e-04)	Tok/s 34615 (36902)	Loss/tok 2.8829 (3.0906)	LR 1.250e-04
0: TRAIN [4][1250/3880]	Time 0.550 (0.387)	Data 1.05e-04 (3.29e-04)	Tok/s 42375 (36884)	Loss/tok 3.2243 (3.0900)	LR 1.250e-04
0: TRAIN [4][1260/3880]	Time 0.304 (0.387)	Data 1.07e-04 (3.27e-04)	Tok/s 34412 (36889)	Loss/tok 2.9641 (3.0900)	LR 1.250e-04
0: TRAIN [4][1270/3880]	Time 0.422 (0.387)	Data 1.11e-04 (3.26e-04)	Tok/s 39895 (36897)	Loss/tok 3.1211 (3.0899)	LR 1.250e-04
0: TRAIN [4][1280/3880]	Time 0.504 (0.387)	Data 1.13e-04 (3.24e-04)	Tok/s 33300 (36890)	Loss/tok 3.0421 (3.0899)	LR 1.250e-04
0: TRAIN [4][1290/3880]	Time 0.308 (0.387)	Data 1.08e-04 (3.22e-04)	Tok/s 34237 (36882)	Loss/tok 2.7781 (3.0893)	LR 1.250e-04
0: TRAIN [4][1300/3880]	Time 0.303 (0.387)	Data 1.11e-04 (3.21e-04)	Tok/s 34126 (36891)	Loss/tok 3.0015 (3.0896)	LR 1.250e-04
0: TRAIN [4][1310/3880]	Time 0.481 (0.388)	Data 1.15e-04 (3.19e-04)	Tok/s 34814 (36861)	Loss/tok 3.0316 (3.0901)	LR 1.250e-04
0: TRAIN [4][1320/3880]	Time 0.303 (0.388)	Data 1.09e-04 (3.18e-04)	Tok/s 34608 (36848)	Loss/tok 2.9341 (3.0899)	LR 1.250e-04
0: TRAIN [4][1330/3880]	Time 0.300 (0.388)	Data 1.14e-04 (3.16e-04)	Tok/s 34603 (36861)	Loss/tok 2.9492 (3.0900)	LR 1.250e-04
0: TRAIN [4][1340/3880]	Time 0.299 (0.388)	Data 1.09e-04 (3.15e-04)	Tok/s 34522 (36865)	Loss/tok 2.8375 (3.0895)	LR 1.250e-04
0: TRAIN [4][1350/3880]	Time 0.410 (0.387)	Data 1.09e-04 (3.13e-04)	Tok/s 41490 (36854)	Loss/tok 3.1043 (3.0888)	LR 1.250e-04
0: TRAIN [4][1360/3880]	Time 0.407 (0.387)	Data 1.05e-04 (3.11e-04)	Tok/s 40702 (36849)	Loss/tok 3.1444 (3.0884)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][1370/3880]	Time 0.309 (0.387)	Data 1.08e-04 (3.10e-04)	Tok/s 32928 (36859)	Loss/tok 2.9840 (3.0879)	LR 1.250e-04
0: TRAIN [4][1380/3880]	Time 0.415 (0.386)	Data 1.09e-04 (3.09e-04)	Tok/s 40510 (36826)	Loss/tok 3.0342 (3.0871)	LR 1.250e-04
0: TRAIN [4][1390/3880]	Time 0.492 (0.386)	Data 1.08e-04 (3.07e-04)	Tok/s 47740 (36826)	Loss/tok 3.2704 (3.0872)	LR 1.250e-04
0: TRAIN [4][1400/3880]	Time 0.423 (0.386)	Data 1.07e-04 (3.06e-04)	Tok/s 39702 (36830)	Loss/tok 3.0263 (3.0866)	LR 1.250e-04
0: TRAIN [4][1410/3880]	Time 0.315 (0.386)	Data 1.09e-04 (3.04e-04)	Tok/s 32673 (36834)	Loss/tok 2.9708 (3.0867)	LR 1.250e-04
0: TRAIN [4][1420/3880]	Time 0.302 (0.386)	Data 1.58e-04 (3.03e-04)	Tok/s 34870 (36825)	Loss/tok 2.9561 (3.0862)	LR 1.250e-04
0: TRAIN [4][1430/3880]	Time 0.418 (0.386)	Data 1.08e-04 (3.02e-04)	Tok/s 40985 (36842)	Loss/tok 3.0201 (3.0861)	LR 1.250e-04
0: TRAIN [4][1440/3880]	Time 0.206 (0.385)	Data 1.06e-04 (3.00e-04)	Tok/s 25924 (36809)	Loss/tok 2.5259 (3.0850)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1450/3880]	Time 0.399 (0.385)	Data 1.09e-04 (2.99e-04)	Tok/s 41686 (36819)	Loss/tok 3.0216 (3.0849)	LR 1.250e-04
0: TRAIN [4][1460/3880]	Time 0.332 (0.385)	Data 1.16e-04 (2.98e-04)	Tok/s 30934 (36813)	Loss/tok 2.9007 (3.0850)	LR 1.250e-04
0: TRAIN [4][1470/3880]	Time 0.514 (0.385)	Data 1.08e-04 (2.96e-04)	Tok/s 45324 (36826)	Loss/tok 3.2955 (3.0852)	LR 1.250e-04
0: TRAIN [4][1480/3880]	Time 0.299 (0.386)	Data 1.12e-04 (2.95e-04)	Tok/s 35354 (36852)	Loss/tok 3.0246 (3.0856)	LR 1.250e-04
0: TRAIN [4][1490/3880]	Time 0.560 (0.386)	Data 1.09e-04 (2.94e-04)	Tok/s 41533 (36853)	Loss/tok 3.2924 (3.0862)	LR 1.250e-04
0: TRAIN [4][1500/3880]	Time 0.298 (0.386)	Data 1.13e-04 (2.93e-04)	Tok/s 34425 (36861)	Loss/tok 2.9755 (3.0863)	LR 1.250e-04
0: TRAIN [4][1510/3880]	Time 0.403 (0.386)	Data 1.06e-04 (2.91e-04)	Tok/s 42017 (36858)	Loss/tok 3.1065 (3.0856)	LR 1.250e-04
0: TRAIN [4][1520/3880]	Time 0.393 (0.386)	Data 1.11e-04 (2.90e-04)	Tok/s 43053 (36870)	Loss/tok 3.0112 (3.0855)	LR 1.250e-04
0: TRAIN [4][1530/3880]	Time 0.338 (0.386)	Data 1.12e-04 (2.89e-04)	Tok/s 30265 (36854)	Loss/tok 2.9023 (3.0850)	LR 1.250e-04
0: TRAIN [4][1540/3880]	Time 0.416 (0.386)	Data 1.11e-04 (2.88e-04)	Tok/s 40092 (36851)	Loss/tok 3.1853 (3.0848)	LR 1.250e-04
0: TRAIN [4][1550/3880]	Time 0.396 (0.385)	Data 1.07e-04 (2.87e-04)	Tok/s 42232 (36846)	Loss/tok 2.9738 (3.0841)	LR 1.250e-04
0: TRAIN [4][1560/3880]	Time 0.212 (0.386)	Data 1.08e-04 (2.86e-04)	Tok/s 25165 (36815)	Loss/tok 2.5787 (3.0843)	LR 1.250e-04
0: TRAIN [4][1570/3880]	Time 0.305 (0.385)	Data 1.09e-04 (2.84e-04)	Tok/s 33514 (36809)	Loss/tok 2.9159 (3.0840)	LR 1.250e-04
0: TRAIN [4][1580/3880]	Time 0.512 (0.386)	Data 1.08e-04 (2.83e-04)	Tok/s 45621 (36833)	Loss/tok 3.2079 (3.0846)	LR 1.250e-04
0: TRAIN [4][1590/3880]	Time 0.397 (0.385)	Data 1.10e-04 (2.82e-04)	Tok/s 42314 (36829)	Loss/tok 3.0713 (3.0841)	LR 1.250e-04
0: TRAIN [4][1600/3880]	Time 0.423 (0.385)	Data 1.09e-04 (2.81e-04)	Tok/s 39740 (36832)	Loss/tok 3.1308 (3.0837)	LR 1.250e-04
0: TRAIN [4][1610/3880]	Time 0.416 (0.385)	Data 1.07e-04 (2.80e-04)	Tok/s 40743 (36821)	Loss/tok 2.9859 (3.0831)	LR 1.250e-04
0: TRAIN [4][1620/3880]	Time 0.383 (0.385)	Data 1.08e-04 (2.79e-04)	Tok/s 43918 (36820)	Loss/tok 3.0938 (3.0831)	LR 1.250e-04
0: TRAIN [4][1630/3880]	Time 0.490 (0.385)	Data 1.06e-04 (2.78e-04)	Tok/s 47413 (36843)	Loss/tok 3.3659 (3.0837)	LR 1.250e-04
0: TRAIN [4][1640/3880]	Time 0.446 (0.386)	Data 1.11e-04 (2.77e-04)	Tok/s 37430 (36823)	Loss/tok 3.0979 (3.0843)	LR 1.250e-04
0: TRAIN [4][1650/3880]	Time 0.304 (0.385)	Data 1.09e-04 (2.76e-04)	Tok/s 33535 (36810)	Loss/tok 2.8595 (3.0843)	LR 1.250e-04
0: TRAIN [4][1660/3880]	Time 0.313 (0.385)	Data 1.11e-04 (2.75e-04)	Tok/s 33038 (36828)	Loss/tok 2.9948 (3.0848)	LR 1.250e-04
0: TRAIN [4][1670/3880]	Time 0.464 (0.385)	Data 1.09e-04 (2.74e-04)	Tok/s 36146 (36838)	Loss/tok 3.1648 (3.0848)	LR 1.250e-04
0: TRAIN [4][1680/3880]	Time 0.310 (0.386)	Data 1.13e-04 (2.73e-04)	Tok/s 33475 (36836)	Loss/tok 2.9428 (3.0846)	LR 1.250e-04
0: TRAIN [4][1690/3880]	Time 0.305 (0.385)	Data 1.08e-04 (2.72e-04)	Tok/s 33632 (36816)	Loss/tok 2.8747 (3.0838)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][1700/3880]	Time 0.315 (0.385)	Data 1.08e-04 (2.71e-04)	Tok/s 33699 (36839)	Loss/tok 2.9274 (3.0847)	LR 1.250e-04
0: TRAIN [4][1710/3880]	Time 0.292 (0.385)	Data 1.10e-04 (2.70e-04)	Tok/s 35190 (36840)	Loss/tok 2.9623 (3.0848)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1720/3880]	Time 0.438 (0.386)	Data 1.13e-04 (2.69e-04)	Tok/s 37859 (36872)	Loss/tok 3.1649 (3.0855)	LR 1.250e-04
0: TRAIN [4][1730/3880]	Time 0.298 (0.386)	Data 1.10e-04 (2.68e-04)	Tok/s 34753 (36859)	Loss/tok 2.9077 (3.0849)	LR 1.250e-04
0: TRAIN [4][1740/3880]	Time 0.423 (0.386)	Data 1.12e-04 (2.67e-04)	Tok/s 39607 (36863)	Loss/tok 3.0907 (3.0850)	LR 1.250e-04
0: TRAIN [4][1750/3880]	Time 0.292 (0.386)	Data 1.10e-04 (2.66e-04)	Tok/s 35365 (36871)	Loss/tok 2.9292 (3.0856)	LR 1.250e-04
0: TRAIN [4][1760/3880]	Time 0.422 (0.386)	Data 1.07e-04 (2.65e-04)	Tok/s 39744 (36876)	Loss/tok 3.0007 (3.0855)	LR 1.250e-04
0: TRAIN [4][1770/3880]	Time 0.321 (0.386)	Data 1.05e-04 (2.65e-04)	Tok/s 31102 (36865)	Loss/tok 2.8768 (3.0850)	LR 1.250e-04
0: TRAIN [4][1780/3880]	Time 0.416 (0.385)	Data 1.07e-04 (2.64e-04)	Tok/s 39938 (36854)	Loss/tok 3.1105 (3.0848)	LR 1.250e-04
0: TRAIN [4][1790/3880]	Time 0.663 (0.385)	Data 1.09e-04 (2.63e-04)	Tok/s 44883 (36854)	Loss/tok 3.3706 (3.0847)	LR 1.250e-04
0: TRAIN [4][1800/3880]	Time 0.414 (0.385)	Data 1.08e-04 (2.62e-04)	Tok/s 41147 (36860)	Loss/tok 3.0661 (3.0842)	LR 1.250e-04
0: TRAIN [4][1810/3880]	Time 0.409 (0.385)	Data 1.04e-04 (2.61e-04)	Tok/s 40374 (36858)	Loss/tok 3.1832 (3.0841)	LR 1.250e-04
0: TRAIN [4][1820/3880]	Time 0.771 (0.385)	Data 1.13e-04 (2.60e-04)	Tok/s 37871 (36845)	Loss/tok 3.4799 (3.0840)	LR 1.250e-04
0: TRAIN [4][1830/3880]	Time 0.211 (0.385)	Data 1.10e-04 (2.59e-04)	Tok/s 25120 (36839)	Loss/tok 2.5777 (3.0841)	LR 1.250e-04
0: TRAIN [4][1840/3880]	Time 0.445 (0.385)	Data 1.11e-04 (2.59e-04)	Tok/s 37353 (36838)	Loss/tok 3.0973 (3.0843)	LR 1.250e-04
0: TRAIN [4][1850/3880]	Time 0.498 (0.385)	Data 1.08e-04 (2.58e-04)	Tok/s 46537 (36836)	Loss/tok 3.1781 (3.0842)	LR 1.250e-04
0: TRAIN [4][1860/3880]	Time 0.409 (0.385)	Data 1.07e-04 (2.57e-04)	Tok/s 40749 (36828)	Loss/tok 3.1277 (3.0842)	LR 1.250e-04
0: TRAIN [4][1870/3880]	Time 0.405 (0.385)	Data 1.07e-04 (2.56e-04)	Tok/s 41444 (36824)	Loss/tok 3.0997 (3.0837)	LR 1.250e-04
0: TRAIN [4][1880/3880]	Time 0.299 (0.384)	Data 1.14e-04 (2.56e-04)	Tok/s 34150 (36818)	Loss/tok 3.0286 (3.0837)	LR 1.250e-04
0: TRAIN [4][1890/3880]	Time 0.319 (0.384)	Data 1.05e-04 (2.55e-04)	Tok/s 32085 (36803)	Loss/tok 2.9480 (3.0833)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1900/3880]	Time 0.404 (0.384)	Data 1.09e-04 (2.54e-04)	Tok/s 41756 (36795)	Loss/tok 3.0053 (3.0832)	LR 1.250e-04
0: TRAIN [4][1910/3880]	Time 0.416 (0.384)	Data 1.06e-04 (2.53e-04)	Tok/s 40679 (36809)	Loss/tok 3.0933 (3.0835)	LR 1.250e-04
0: TRAIN [4][1920/3880]	Time 0.633 (0.384)	Data 1.06e-04 (2.52e-04)	Tok/s 46930 (36817)	Loss/tok 3.3614 (3.0836)	LR 1.250e-04
0: TRAIN [4][1930/3880]	Time 0.521 (0.384)	Data 1.07e-04 (2.52e-04)	Tok/s 44609 (36836)	Loss/tok 3.4431 (3.0839)	LR 1.250e-04
0: TRAIN [4][1940/3880]	Time 0.397 (0.384)	Data 1.09e-04 (2.51e-04)	Tok/s 42539 (36836)	Loss/tok 3.1089 (3.0838)	LR 1.250e-04
0: TRAIN [4][1950/3880]	Time 0.390 (0.385)	Data 1.10e-04 (2.50e-04)	Tok/s 43268 (36831)	Loss/tok 3.1422 (3.0840)	LR 1.250e-04
0: TRAIN [4][1960/3880]	Time 0.306 (0.384)	Data 1.10e-04 (2.50e-04)	Tok/s 33391 (36825)	Loss/tok 2.9205 (3.0836)	LR 1.250e-04
0: TRAIN [4][1970/3880]	Time 0.302 (0.384)	Data 1.06e-04 (2.49e-04)	Tok/s 34576 (36819)	Loss/tok 2.8241 (3.0833)	LR 1.250e-04
0: TRAIN [4][1980/3880]	Time 0.462 (0.384)	Data 1.08e-04 (2.48e-04)	Tok/s 36709 (36814)	Loss/tok 3.0424 (3.0831)	LR 1.250e-04
0: TRAIN [4][1990/3880]	Time 0.414 (0.384)	Data 1.15e-04 (2.47e-04)	Tok/s 40560 (36803)	Loss/tok 2.9917 (3.0829)	LR 1.250e-04
0: TRAIN [4][2000/3880]	Time 0.506 (0.384)	Data 1.11e-04 (2.47e-04)	Tok/s 45842 (36819)	Loss/tok 3.2106 (3.0834)	LR 1.250e-04
0: TRAIN [4][2010/3880]	Time 0.212 (0.384)	Data 1.08e-04 (2.46e-04)	Tok/s 25385 (36828)	Loss/tok 2.4843 (3.0839)	LR 1.250e-04
0: TRAIN [4][2020/3880]	Time 0.399 (0.385)	Data 1.11e-04 (2.45e-04)	Tok/s 42345 (36834)	Loss/tok 3.0937 (3.0844)	LR 1.250e-04
0: TRAIN [4][2030/3880]	Time 0.384 (0.385)	Data 1.08e-04 (2.45e-04)	Tok/s 27115 (36837)	Loss/tok 2.8519 (3.0847)	LR 1.250e-04
0: TRAIN [4][2040/3880]	Time 0.679 (0.385)	Data 1.09e-04 (2.44e-04)	Tok/s 43737 (36816)	Loss/tok 3.4819 (3.0850)	LR 1.250e-04
0: TRAIN [4][2050/3880]	Time 0.537 (0.385)	Data 1.10e-04 (2.43e-04)	Tok/s 42915 (36827)	Loss/tok 3.2835 (3.0852)	LR 1.250e-04
0: TRAIN [4][2060/3880]	Time 0.205 (0.385)	Data 1.09e-04 (2.43e-04)	Tok/s 25586 (36822)	Loss/tok 2.4859 (3.0850)	LR 1.250e-04
0: TRAIN [4][2070/3880]	Time 0.465 (0.385)	Data 1.12e-04 (2.42e-04)	Tok/s 36069 (36807)	Loss/tok 3.1222 (3.0848)	LR 1.250e-04
0: TRAIN [4][2080/3880]	Time 0.421 (0.385)	Data 1.08e-04 (2.41e-04)	Tok/s 40184 (36803)	Loss/tok 3.1069 (3.0852)	LR 1.250e-04
0: TRAIN [4][2090/3880]	Time 0.300 (0.385)	Data 1.05e-04 (2.41e-04)	Tok/s 34666 (36809)	Loss/tok 2.9464 (3.0854)	LR 1.250e-04
0: TRAIN [4][2100/3880]	Time 0.199 (0.385)	Data 1.08e-04 (2.40e-04)	Tok/s 26311 (36799)	Loss/tok 2.6769 (3.0848)	LR 1.250e-04
0: TRAIN [4][2110/3880]	Time 0.328 (0.385)	Data 1.12e-04 (2.40e-04)	Tok/s 31679 (36799)	Loss/tok 2.9912 (3.0848)	LR 1.250e-04
0: TRAIN [4][2120/3880]	Time 0.411 (0.385)	Data 1.11e-04 (2.39e-04)	Tok/s 41261 (36795)	Loss/tok 3.0931 (3.0849)	LR 1.250e-04
0: TRAIN [4][2130/3880]	Time 0.403 (0.385)	Data 1.12e-04 (2.38e-04)	Tok/s 41472 (36795)	Loss/tok 3.0185 (3.0845)	LR 1.250e-04
0: TRAIN [4][2140/3880]	Time 0.300 (0.385)	Data 1.14e-04 (2.38e-04)	Tok/s 34413 (36799)	Loss/tok 2.8549 (3.0846)	LR 1.250e-04
0: TRAIN [4][2150/3880]	Time 0.201 (0.384)	Data 1.07e-04 (2.37e-04)	Tok/s 26305 (36783)	Loss/tok 2.5738 (3.0844)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][2160/3880]	Time 0.417 (0.384)	Data 1.08e-04 (2.36e-04)	Tok/s 40831 (36791)	Loss/tok 3.0336 (3.0844)	LR 1.250e-04
0: TRAIN [4][2170/3880]	Time 0.205 (0.384)	Data 1.10e-04 (2.36e-04)	Tok/s 25778 (36791)	Loss/tok 2.4515 (3.0842)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2180/3880]	Time 0.637 (0.384)	Data 1.08e-04 (2.35e-04)	Tok/s 46590 (36801)	Loss/tok 3.3686 (3.0844)	LR 1.250e-04
0: TRAIN [4][2190/3880]	Time 0.435 (0.384)	Data 1.10e-04 (2.35e-04)	Tok/s 38407 (36797)	Loss/tok 3.0008 (3.0841)	LR 1.250e-04
0: TRAIN [4][2200/3880]	Time 0.307 (0.384)	Data 1.10e-04 (2.34e-04)	Tok/s 33829 (36804)	Loss/tok 2.8987 (3.0840)	LR 1.250e-04
0: TRAIN [4][2210/3880]	Time 0.300 (0.384)	Data 1.09e-04 (2.34e-04)	Tok/s 34292 (36806)	Loss/tok 2.8637 (3.0838)	LR 1.250e-04
0: TRAIN [4][2220/3880]	Time 0.291 (0.384)	Data 1.09e-04 (2.33e-04)	Tok/s 36593 (36804)	Loss/tok 2.8975 (3.0834)	LR 1.250e-04
0: TRAIN [4][2230/3880]	Time 0.202 (0.384)	Data 1.09e-04 (2.33e-04)	Tok/s 25762 (36812)	Loss/tok 2.5133 (3.0837)	LR 1.250e-04
0: TRAIN [4][2240/3880]	Time 0.581 (0.384)	Data 1.09e-04 (2.32e-04)	Tok/s 29240 (36810)	Loss/tok 3.1287 (3.0835)	LR 1.250e-04
0: TRAIN [4][2250/3880]	Time 0.438 (0.384)	Data 1.11e-04 (2.31e-04)	Tok/s 38247 (36784)	Loss/tok 3.1754 (3.0833)	LR 1.250e-04
0: TRAIN [4][2260/3880]	Time 0.390 (0.384)	Data 1.11e-04 (2.31e-04)	Tok/s 43498 (36796)	Loss/tok 3.1680 (3.0837)	LR 1.250e-04
0: TRAIN [4][2270/3880]	Time 0.310 (0.384)	Data 1.11e-04 (2.30e-04)	Tok/s 33566 (36801)	Loss/tok 2.8604 (3.0846)	LR 1.250e-04
0: TRAIN [4][2280/3880]	Time 0.398 (0.384)	Data 1.07e-04 (2.30e-04)	Tok/s 42616 (36805)	Loss/tok 3.0343 (3.0847)	LR 1.250e-04
0: TRAIN [4][2290/3880]	Time 0.239 (0.384)	Data 1.19e-04 (2.29e-04)	Tok/s 21857 (36807)	Loss/tok 2.5334 (3.0849)	LR 1.250e-04
0: TRAIN [4][2300/3880]	Time 0.315 (0.384)	Data 1.10e-04 (2.29e-04)	Tok/s 31988 (36776)	Loss/tok 2.9219 (3.0848)	LR 1.250e-04
0: TRAIN [4][2310/3880]	Time 0.305 (0.384)	Data 1.10e-04 (2.28e-04)	Tok/s 33891 (36781)	Loss/tok 2.9098 (3.0845)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2320/3880]	Time 0.640 (0.385)	Data 1.11e-04 (2.28e-04)	Tok/s 46666 (36793)	Loss/tok 3.3503 (3.0849)	LR 1.250e-04
0: TRAIN [4][2330/3880]	Time 0.518 (0.385)	Data 1.14e-04 (2.27e-04)	Tok/s 32596 (36796)	Loss/tok 3.0627 (3.0851)	LR 1.250e-04
0: TRAIN [4][2340/3880]	Time 0.317 (0.385)	Data 1.20e-04 (2.27e-04)	Tok/s 32539 (36780)	Loss/tok 2.9170 (3.0850)	LR 1.250e-04
0: TRAIN [4][2350/3880]	Time 0.302 (0.385)	Data 1.07e-04 (2.26e-04)	Tok/s 34315 (36783)	Loss/tok 2.7990 (3.0851)	LR 1.250e-04
0: TRAIN [4][2360/3880]	Time 0.303 (0.385)	Data 1.13e-04 (2.26e-04)	Tok/s 34431 (36781)	Loss/tok 2.8649 (3.0854)	LR 1.250e-04
0: TRAIN [4][2370/3880]	Time 0.462 (0.384)	Data 1.04e-04 (2.25e-04)	Tok/s 35543 (36770)	Loss/tok 3.1070 (3.0850)	LR 1.250e-04
0: TRAIN [4][2380/3880]	Time 0.522 (0.385)	Data 1.04e-04 (2.25e-04)	Tok/s 44684 (36783)	Loss/tok 3.2098 (3.0859)	LR 1.250e-04
0: TRAIN [4][2390/3880]	Time 0.319 (0.385)	Data 1.12e-04 (2.24e-04)	Tok/s 33081 (36788)	Loss/tok 2.9906 (3.0859)	LR 1.250e-04
0: TRAIN [4][2400/3880]	Time 0.342 (0.385)	Data 1.13e-04 (2.24e-04)	Tok/s 30193 (36780)	Loss/tok 2.9140 (3.0858)	LR 1.250e-04
0: TRAIN [4][2410/3880]	Time 0.529 (0.385)	Data 1.06e-04 (2.23e-04)	Tok/s 43772 (36772)	Loss/tok 3.2723 (3.0858)	LR 1.250e-04
0: TRAIN [4][2420/3880]	Time 0.628 (0.385)	Data 1.08e-04 (2.23e-04)	Tok/s 46902 (36789)	Loss/tok 3.3677 (3.0863)	LR 1.250e-04
0: TRAIN [4][2430/3880]	Time 0.556 (0.385)	Data 1.50e-04 (2.22e-04)	Tok/s 42285 (36774)	Loss/tok 3.2168 (3.0860)	LR 1.250e-04
0: TRAIN [4][2440/3880]	Time 0.497 (0.385)	Data 1.14e-04 (2.22e-04)	Tok/s 46712 (36781)	Loss/tok 3.2027 (3.0863)	LR 1.250e-04
0: TRAIN [4][2450/3880]	Time 0.409 (0.385)	Data 1.04e-04 (2.22e-04)	Tok/s 41080 (36783)	Loss/tok 3.0959 (3.0862)	LR 1.250e-04
0: TRAIN [4][2460/3880]	Time 0.301 (0.385)	Data 1.10e-04 (2.21e-04)	Tok/s 33931 (36773)	Loss/tok 2.9701 (3.0860)	LR 1.250e-04
0: TRAIN [4][2470/3880]	Time 0.209 (0.385)	Data 1.10e-04 (2.21e-04)	Tok/s 24809 (36761)	Loss/tok 2.6013 (3.0859)	LR 1.250e-04
0: TRAIN [4][2480/3880]	Time 0.292 (0.385)	Data 1.13e-04 (2.20e-04)	Tok/s 35048 (36759)	Loss/tok 2.8103 (3.0855)	LR 1.250e-04
0: TRAIN [4][2490/3880]	Time 0.316 (0.385)	Data 1.08e-04 (2.20e-04)	Tok/s 32567 (36739)	Loss/tok 2.9540 (3.0851)	LR 1.250e-04
0: TRAIN [4][2500/3880]	Time 0.407 (0.384)	Data 1.06e-04 (2.19e-04)	Tok/s 41591 (36745)	Loss/tok 3.0780 (3.0850)	LR 1.250e-04
0: TRAIN [4][2510/3880]	Time 0.544 (0.385)	Data 1.11e-04 (2.19e-04)	Tok/s 42415 (36757)	Loss/tok 3.2917 (3.0857)	LR 1.250e-04
0: TRAIN [4][2520/3880]	Time 0.306 (0.384)	Data 1.05e-04 (2.18e-04)	Tok/s 34017 (36743)	Loss/tok 3.0338 (3.0854)	LR 1.250e-04
0: TRAIN [4][2530/3880]	Time 0.485 (0.384)	Data 1.09e-04 (2.18e-04)	Tok/s 34149 (36734)	Loss/tok 3.1490 (3.0851)	LR 1.250e-04
0: TRAIN [4][2540/3880]	Time 0.300 (0.384)	Data 1.07e-04 (2.18e-04)	Tok/s 33710 (36722)	Loss/tok 2.9920 (3.0848)	LR 1.250e-04
0: TRAIN [4][2550/3880]	Time 0.391 (0.384)	Data 1.10e-04 (2.17e-04)	Tok/s 43072 (36728)	Loss/tok 3.0415 (3.0845)	LR 1.250e-04
0: TRAIN [4][2560/3880]	Time 0.331 (0.384)	Data 1.03e-04 (2.17e-04)	Tok/s 31258 (36707)	Loss/tok 2.8432 (3.0845)	LR 1.250e-04
0: TRAIN [4][2570/3880]	Time 0.302 (0.384)	Data 1.07e-04 (2.16e-04)	Tok/s 34239 (36709)	Loss/tok 2.8402 (3.0848)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][2580/3880]	Time 0.295 (0.384)	Data 1.06e-04 (2.16e-04)	Tok/s 34448 (36719)	Loss/tok 2.8683 (3.0848)	LR 1.250e-04
0: TRAIN [4][2590/3880]	Time 0.713 (0.384)	Data 1.11e-04 (2.15e-04)	Tok/s 41621 (36727)	Loss/tok 3.4293 (3.0851)	LR 1.250e-04
0: TRAIN [4][2600/3880]	Time 0.415 (0.384)	Data 1.09e-04 (2.15e-04)	Tok/s 40159 (36722)	Loss/tok 3.0149 (3.0852)	LR 1.250e-04
0: TRAIN [4][2610/3880]	Time 0.410 (0.384)	Data 1.09e-04 (2.15e-04)	Tok/s 41282 (36717)	Loss/tok 3.0445 (3.0849)	LR 1.250e-04
0: TRAIN [4][2620/3880]	Time 0.208 (0.384)	Data 1.09e-04 (2.14e-04)	Tok/s 25378 (36707)	Loss/tok 2.5838 (3.0845)	LR 1.250e-04
0: TRAIN [4][2630/3880]	Time 0.535 (0.384)	Data 1.09e-04 (2.14e-04)	Tok/s 43627 (36723)	Loss/tok 3.2750 (3.0850)	LR 1.250e-04
0: TRAIN [4][2640/3880]	Time 0.336 (0.384)	Data 1.08e-04 (2.13e-04)	Tok/s 30076 (36710)	Loss/tok 2.9507 (3.0848)	LR 1.250e-04
0: TRAIN [4][2650/3880]	Time 0.298 (0.384)	Data 1.09e-04 (2.13e-04)	Tok/s 35018 (36713)	Loss/tok 2.7822 (3.0849)	LR 1.250e-04
0: TRAIN [4][2660/3880]	Time 0.209 (0.384)	Data 1.07e-04 (2.13e-04)	Tok/s 24759 (36702)	Loss/tok 2.5065 (3.0846)	LR 1.250e-04
0: TRAIN [4][2670/3880]	Time 0.395 (0.384)	Data 1.08e-04 (2.12e-04)	Tok/s 42483 (36703)	Loss/tok 3.0009 (3.0845)	LR 1.250e-04
0: TRAIN [4][2680/3880]	Time 0.361 (0.384)	Data 1.11e-04 (2.12e-04)	Tok/s 28583 (36696)	Loss/tok 2.8727 (3.0840)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2690/3880]	Time 0.314 (0.384)	Data 1.07e-04 (2.11e-04)	Tok/s 32627 (36685)	Loss/tok 2.8730 (3.0841)	LR 1.250e-04
0: TRAIN [4][2700/3880]	Time 0.401 (0.384)	Data 1.13e-04 (2.11e-04)	Tok/s 41803 (36690)	Loss/tok 3.0345 (3.0840)	LR 1.250e-04
0: TRAIN [4][2710/3880]	Time 0.292 (0.384)	Data 1.07e-04 (2.11e-04)	Tok/s 34349 (36693)	Loss/tok 2.9504 (3.0839)	LR 1.250e-04
0: TRAIN [4][2720/3880]	Time 0.516 (0.383)	Data 1.12e-04 (2.10e-04)	Tok/s 45079 (36694)	Loss/tok 3.2428 (3.0837)	LR 1.250e-04
0: TRAIN [4][2730/3880]	Time 0.315 (0.383)	Data 1.08e-04 (2.10e-04)	Tok/s 32885 (36684)	Loss/tok 2.8674 (3.0836)	LR 1.250e-04
0: TRAIN [4][2740/3880]	Time 0.354 (0.383)	Data 1.14e-04 (2.10e-04)	Tok/s 28849 (36675)	Loss/tok 2.8975 (3.0836)	LR 1.250e-04
0: TRAIN [4][2750/3880]	Time 0.417 (0.383)	Data 1.12e-04 (2.09e-04)	Tok/s 40100 (36665)	Loss/tok 3.1607 (3.0837)	LR 1.250e-04
0: TRAIN [4][2760/3880]	Time 0.297 (0.383)	Data 1.08e-04 (2.09e-04)	Tok/s 33792 (36671)	Loss/tok 2.9636 (3.0836)	LR 1.250e-04
0: TRAIN [4][2770/3880]	Time 0.202 (0.383)	Data 1.14e-04 (2.08e-04)	Tok/s 26087 (36657)	Loss/tok 2.4816 (3.0834)	LR 1.250e-04
0: TRAIN [4][2780/3880]	Time 0.461 (0.383)	Data 1.09e-04 (2.08e-04)	Tok/s 36049 (36660)	Loss/tok 3.1523 (3.0837)	LR 1.250e-04
0: TRAIN [4][2790/3880]	Time 0.475 (0.383)	Data 1.12e-04 (2.08e-04)	Tok/s 35351 (36651)	Loss/tok 3.1494 (3.0836)	LR 1.250e-04
0: TRAIN [4][2800/3880]	Time 0.417 (0.383)	Data 1.08e-04 (2.07e-04)	Tok/s 40062 (36654)	Loss/tok 3.1553 (3.0837)	LR 1.250e-04
0: TRAIN [4][2810/3880]	Time 0.304 (0.383)	Data 1.11e-04 (2.07e-04)	Tok/s 33356 (36654)	Loss/tok 2.9762 (3.0836)	LR 1.250e-04
0: TRAIN [4][2820/3880]	Time 0.510 (0.383)	Data 1.08e-04 (2.07e-04)	Tok/s 45853 (36663)	Loss/tok 3.2060 (3.0838)	LR 1.250e-04
0: TRAIN [4][2830/3880]	Time 0.498 (0.383)	Data 1.10e-04 (2.06e-04)	Tok/s 46639 (36669)	Loss/tok 3.3200 (3.0839)	LR 1.250e-04
0: TRAIN [4][2840/3880]	Time 0.208 (0.383)	Data 1.12e-04 (2.06e-04)	Tok/s 25005 (36669)	Loss/tok 2.5583 (3.0836)	LR 1.250e-04
0: TRAIN [4][2850/3880]	Time 0.294 (0.383)	Data 1.10e-04 (2.06e-04)	Tok/s 34545 (36665)	Loss/tok 2.9331 (3.0833)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2860/3880]	Time 0.674 (0.383)	Data 1.12e-04 (2.05e-04)	Tok/s 34322 (36666)	Loss/tok 3.3055 (3.0836)	LR 1.250e-04
0: TRAIN [4][2870/3880]	Time 0.301 (0.384)	Data 1.09e-04 (2.05e-04)	Tok/s 34311 (36661)	Loss/tok 2.8491 (3.0836)	LR 1.250e-04
0: TRAIN [4][2880/3880]	Time 0.501 (0.384)	Data 1.07e-04 (2.05e-04)	Tok/s 46591 (36665)	Loss/tok 3.2403 (3.0835)	LR 1.250e-04
0: TRAIN [4][2890/3880]	Time 0.316 (0.384)	Data 1.09e-04 (2.04e-04)	Tok/s 33285 (36661)	Loss/tok 2.9754 (3.0835)	LR 1.250e-04
0: TRAIN [4][2900/3880]	Time 0.408 (0.383)	Data 1.07e-04 (2.04e-04)	Tok/s 41231 (36655)	Loss/tok 3.0797 (3.0832)	LR 1.250e-04
0: TRAIN [4][2910/3880]	Time 0.342 (0.383)	Data 1.11e-04 (2.04e-04)	Tok/s 30739 (36657)	Loss/tok 2.9787 (3.0832)	LR 1.250e-04
0: TRAIN [4][2920/3880]	Time 0.309 (0.383)	Data 1.17e-04 (2.03e-04)	Tok/s 32946 (36649)	Loss/tok 2.8865 (3.0829)	LR 1.250e-04
0: TRAIN [4][2930/3880]	Time 0.297 (0.383)	Data 1.14e-04 (2.03e-04)	Tok/s 34978 (36646)	Loss/tok 2.8601 (3.0827)	LR 1.250e-04
0: TRAIN [4][2940/3880]	Time 0.397 (0.383)	Data 1.07e-04 (2.03e-04)	Tok/s 42487 (36649)	Loss/tok 3.1034 (3.0825)	LR 1.250e-04
0: TRAIN [4][2950/3880]	Time 0.463 (0.383)	Data 1.13e-04 (2.02e-04)	Tok/s 35936 (36658)	Loss/tok 2.9818 (3.0826)	LR 1.250e-04
0: TRAIN [4][2960/3880]	Time 0.421 (0.383)	Data 1.11e-04 (2.02e-04)	Tok/s 40164 (36662)	Loss/tok 3.0896 (3.0827)	LR 1.250e-04
0: TRAIN [4][2970/3880]	Time 0.519 (0.383)	Data 1.10e-04 (2.02e-04)	Tok/s 32064 (36666)	Loss/tok 3.0763 (3.0827)	LR 1.250e-04
0: TRAIN [4][2980/3880]	Time 0.664 (0.384)	Data 1.06e-04 (2.01e-04)	Tok/s 45331 (36667)	Loss/tok 3.2744 (3.0831)	LR 1.250e-04
0: TRAIN [4][2990/3880]	Time 0.415 (0.383)	Data 1.14e-04 (2.01e-04)	Tok/s 40313 (36665)	Loss/tok 3.0636 (3.0828)	LR 1.250e-04
0: TRAIN [4][3000/3880]	Time 0.301 (0.383)	Data 1.09e-04 (2.01e-04)	Tok/s 33658 (36668)	Loss/tok 2.7968 (3.0824)	LR 1.250e-04
0: TRAIN [4][3010/3880]	Time 0.493 (0.383)	Data 1.09e-04 (2.01e-04)	Tok/s 33937 (36662)	Loss/tok 3.0074 (3.0821)	LR 1.250e-04
0: TRAIN [4][3020/3880]	Time 0.412 (0.383)	Data 1.12e-04 (2.00e-04)	Tok/s 40608 (36660)	Loss/tok 3.1346 (3.0822)	LR 1.250e-04
0: TRAIN [4][3030/3880]	Time 0.426 (0.383)	Data 1.12e-04 (2.00e-04)	Tok/s 39911 (36656)	Loss/tok 3.0235 (3.0821)	LR 1.250e-04
0: TRAIN [4][3040/3880]	Time 0.401 (0.383)	Data 1.10e-04 (2.00e-04)	Tok/s 41205 (36656)	Loss/tok 3.1210 (3.0819)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3050/3880]	Time 0.504 (0.383)	Data 1.11e-04 (1.99e-04)	Tok/s 46693 (36657)	Loss/tok 3.2002 (3.0817)	LR 1.250e-04
0: TRAIN [4][3060/3880]	Time 0.464 (0.383)	Data 1.12e-04 (1.99e-04)	Tok/s 36333 (36654)	Loss/tok 3.2620 (3.0817)	LR 1.250e-04
0: TRAIN [4][3070/3880]	Time 0.481 (0.383)	Data 1.09e-04 (1.99e-04)	Tok/s 35081 (36658)	Loss/tok 2.9847 (3.0823)	LR 1.250e-04
0: TRAIN [4][3080/3880]	Time 0.293 (0.383)	Data 1.11e-04 (1.99e-04)	Tok/s 33776 (36664)	Loss/tok 2.8711 (3.0823)	LR 1.250e-04
0: TRAIN [4][3090/3880]	Time 0.530 (0.384)	Data 1.10e-04 (1.98e-04)	Tok/s 44211 (36675)	Loss/tok 3.3720 (3.0827)	LR 1.250e-04
0: TRAIN [4][3100/3880]	Time 0.470 (0.384)	Data 1.13e-04 (1.98e-04)	Tok/s 35575 (36676)	Loss/tok 3.1112 (3.0828)	LR 1.250e-04
0: TRAIN [4][3110/3880]	Time 0.300 (0.384)	Data 1.12e-04 (1.98e-04)	Tok/s 34668 (36676)	Loss/tok 2.8103 (3.0826)	LR 1.250e-04
0: TRAIN [4][3120/3880]	Time 0.392 (0.384)	Data 1.13e-04 (1.97e-04)	Tok/s 42828 (36683)	Loss/tok 3.0075 (3.0825)	LR 1.250e-04
0: TRAIN [4][3130/3880]	Time 0.533 (0.384)	Data 1.11e-04 (1.97e-04)	Tok/s 43600 (36689)	Loss/tok 3.2354 (3.0828)	LR 1.250e-04
0: TRAIN [4][3140/3880]	Time 0.412 (0.384)	Data 1.10e-04 (1.97e-04)	Tok/s 40875 (36692)	Loss/tok 3.0677 (3.0827)	LR 1.250e-04
0: TRAIN [4][3150/3880]	Time 0.306 (0.384)	Data 1.11e-04 (1.97e-04)	Tok/s 33612 (36694)	Loss/tok 2.8187 (3.0824)	LR 1.250e-04
0: TRAIN [4][3160/3880]	Time 0.320 (0.384)	Data 1.13e-04 (1.96e-04)	Tok/s 32454 (36694)	Loss/tok 2.8579 (3.0826)	LR 1.250e-04
0: TRAIN [4][3170/3880]	Time 0.321 (0.384)	Data 1.12e-04 (1.96e-04)	Tok/s 33024 (36672)	Loss/tok 3.0079 (3.0824)	LR 1.250e-04
0: TRAIN [4][3180/3880]	Time 0.528 (0.384)	Data 2.48e-04 (1.96e-04)	Tok/s 44356 (36671)	Loss/tok 3.1296 (3.0824)	LR 1.250e-04
0: TRAIN [4][3190/3880]	Time 0.405 (0.384)	Data 1.02e-04 (1.95e-04)	Tok/s 41636 (36669)	Loss/tok 3.0392 (3.0822)	LR 1.250e-04
0: TRAIN [4][3200/3880]	Time 0.308 (0.384)	Data 1.04e-04 (1.95e-04)	Tok/s 33089 (36670)	Loss/tok 2.8633 (3.0821)	LR 1.250e-04
0: TRAIN [4][3210/3880]	Time 0.211 (0.383)	Data 1.10e-04 (1.95e-04)	Tok/s 24948 (36662)	Loss/tok 2.5801 (3.0818)	LR 1.250e-04
0: TRAIN [4][3220/3880]	Time 0.413 (0.383)	Data 1.10e-04 (1.95e-04)	Tok/s 40771 (36664)	Loss/tok 3.1741 (3.0820)	LR 1.250e-04
0: TRAIN [4][3230/3880]	Time 0.689 (0.384)	Data 1.07e-04 (1.94e-04)	Tok/s 43459 (36668)	Loss/tok 3.4005 (3.0821)	LR 1.250e-04
0: TRAIN [4][3240/3880]	Time 0.425 (0.383)	Data 1.08e-04 (1.94e-04)	Tok/s 39000 (36670)	Loss/tok 3.1382 (3.0821)	LR 1.250e-04
0: TRAIN [4][3250/3880]	Time 0.303 (0.383)	Data 1.08e-04 (1.94e-04)	Tok/s 33816 (36669)	Loss/tok 3.0005 (3.0819)	LR 1.250e-04
0: TRAIN [4][3260/3880]	Time 0.305 (0.383)	Data 1.12e-04 (1.94e-04)	Tok/s 34482 (36664)	Loss/tok 2.9437 (3.0817)	LR 1.250e-04
0: TRAIN [4][3270/3880]	Time 0.304 (0.383)	Data 1.10e-04 (1.93e-04)	Tok/s 34214 (36671)	Loss/tok 2.8813 (3.0818)	LR 1.250e-04
0: TRAIN [4][3280/3880]	Time 0.208 (0.383)	Data 1.09e-04 (1.93e-04)	Tok/s 25943 (36673)	Loss/tok 2.5751 (3.0818)	LR 1.250e-04
0: TRAIN [4][3290/3880]	Time 0.384 (0.383)	Data 1.10e-04 (1.93e-04)	Tok/s 27295 (36673)	Loss/tok 2.9011 (3.0816)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3300/3880]	Time 0.534 (0.384)	Data 1.09e-04 (1.93e-04)	Tok/s 43500 (36669)	Loss/tok 3.2545 (3.0819)	LR 1.250e-04
0: TRAIN [4][3310/3880]	Time 0.304 (0.383)	Data 1.10e-04 (1.92e-04)	Tok/s 32729 (36674)	Loss/tok 2.9273 (3.0818)	LR 1.250e-04
0: TRAIN [4][3320/3880]	Time 0.415 (0.383)	Data 1.10e-04 (1.92e-04)	Tok/s 40656 (36676)	Loss/tok 3.2281 (3.0820)	LR 1.250e-04
0: TRAIN [4][3330/3880]	Time 0.307 (0.383)	Data 1.11e-04 (1.92e-04)	Tok/s 34641 (36676)	Loss/tok 2.9869 (3.0819)	LR 1.250e-04
0: TRAIN [4][3340/3880]	Time 0.303 (0.383)	Data 1.10e-04 (1.92e-04)	Tok/s 34281 (36678)	Loss/tok 2.8068 (3.0820)	LR 1.250e-04
0: TRAIN [4][3350/3880]	Time 0.362 (0.383)	Data 1.49e-04 (1.91e-04)	Tok/s 28590 (36669)	Loss/tok 2.8631 (3.0817)	LR 1.250e-04
0: TRAIN [4][3360/3880]	Time 0.420 (0.383)	Data 1.05e-04 (1.91e-04)	Tok/s 39871 (36664)	Loss/tok 2.9656 (3.0817)	LR 1.250e-04
0: TRAIN [4][3370/3880]	Time 0.447 (0.384)	Data 1.14e-04 (1.91e-04)	Tok/s 38321 (36676)	Loss/tok 3.0367 (3.0819)	LR 1.250e-04
0: TRAIN [4][3380/3880]	Time 0.389 (0.384)	Data 1.08e-04 (1.91e-04)	Tok/s 43731 (36677)	Loss/tok 3.1332 (3.0820)	LR 1.250e-04
0: TRAIN [4][3390/3880]	Time 0.312 (0.383)	Data 1.06e-04 (1.90e-04)	Tok/s 33082 (36666)	Loss/tok 2.8390 (3.0817)	LR 1.250e-04
0: TRAIN [4][3400/3880]	Time 0.205 (0.383)	Data 1.09e-04 (1.90e-04)	Tok/s 25759 (36665)	Loss/tok 2.4430 (3.0816)	LR 1.250e-04
0: TRAIN [4][3410/3880]	Time 0.314 (0.383)	Data 1.08e-04 (1.90e-04)	Tok/s 32436 (36659)	Loss/tok 2.8288 (3.0813)	LR 1.250e-04
0: TRAIN [4][3420/3880]	Time 0.529 (0.383)	Data 1.09e-04 (1.90e-04)	Tok/s 44566 (36661)	Loss/tok 3.1010 (3.0815)	LR 1.250e-04
0: TRAIN [4][3430/3880]	Time 0.412 (0.383)	Data 1.10e-04 (1.89e-04)	Tok/s 40798 (36672)	Loss/tok 3.2006 (3.0818)	LR 1.250e-04
0: TRAIN [4][3440/3880]	Time 0.303 (0.383)	Data 1.11e-04 (1.89e-04)	Tok/s 34393 (36675)	Loss/tok 2.9140 (3.0817)	LR 1.250e-04
0: TRAIN [4][3450/3880]	Time 0.395 (0.383)	Data 1.10e-04 (1.89e-04)	Tok/s 42534 (36670)	Loss/tok 3.1447 (3.0815)	LR 1.250e-04
0: TRAIN [4][3460/3880]	Time 0.566 (0.383)	Data 1.10e-04 (1.89e-04)	Tok/s 41711 (36672)	Loss/tok 3.1313 (3.0817)	LR 1.250e-04
0: TRAIN [4][3470/3880]	Time 0.308 (0.383)	Data 1.09e-04 (1.89e-04)	Tok/s 33491 (36669)	Loss/tok 2.9411 (3.0819)	LR 1.250e-04
0: TRAIN [4][3480/3880]	Time 0.305 (0.383)	Data 1.13e-04 (1.88e-04)	Tok/s 34056 (36671)	Loss/tok 3.0683 (3.0820)	LR 1.250e-04
0: TRAIN [4][3490/3880]	Time 0.518 (0.383)	Data 1.09e-04 (1.88e-04)	Tok/s 44692 (36672)	Loss/tok 3.2319 (3.0819)	LR 1.250e-04
0: TRAIN [4][3500/3880]	Time 0.303 (0.383)	Data 1.10e-04 (1.88e-04)	Tok/s 33475 (36679)	Loss/tok 2.8826 (3.0821)	LR 1.250e-04
0: TRAIN [4][3510/3880]	Time 0.397 (0.383)	Data 1.10e-04 (1.88e-04)	Tok/s 42295 (36686)	Loss/tok 3.0419 (3.0822)	LR 1.250e-04
0: TRAIN [4][3520/3880]	Time 0.411 (0.383)	Data 1.09e-04 (1.87e-04)	Tok/s 40813 (36686)	Loss/tok 3.0801 (3.0820)	LR 1.250e-04
0: TRAIN [4][3530/3880]	Time 0.416 (0.383)	Data 1.09e-04 (1.87e-04)	Tok/s 40551 (36684)	Loss/tok 3.1715 (3.0820)	LR 1.250e-04
0: TRAIN [4][3540/3880]	Time 0.353 (0.383)	Data 1.13e-04 (1.87e-04)	Tok/s 29316 (36681)	Loss/tok 2.8619 (3.0818)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3550/3880]	Time 0.536 (0.383)	Data 1.11e-04 (1.87e-04)	Tok/s 42841 (36677)	Loss/tok 3.4247 (3.0820)	LR 1.250e-04
0: TRAIN [4][3560/3880]	Time 0.234 (0.383)	Data 1.11e-04 (1.87e-04)	Tok/s 22701 (36686)	Loss/tok 2.4579 (3.0824)	LR 1.250e-04
0: TRAIN [4][3570/3880]	Time 0.310 (0.383)	Data 1.12e-04 (1.86e-04)	Tok/s 34149 (36682)	Loss/tok 2.9187 (3.0823)	LR 1.250e-04
0: TRAIN [4][3580/3880]	Time 0.298 (0.383)	Data 1.07e-04 (1.86e-04)	Tok/s 34287 (36686)	Loss/tok 2.9640 (3.0823)	LR 1.250e-04
0: TRAIN [4][3590/3880]	Time 0.526 (0.383)	Data 1.08e-04 (1.86e-04)	Tok/s 44858 (36681)	Loss/tok 3.2183 (3.0822)	LR 1.250e-04
0: TRAIN [4][3600/3880]	Time 0.511 (0.383)	Data 1.10e-04 (1.86e-04)	Tok/s 32559 (36678)	Loss/tok 3.2223 (3.0822)	LR 1.250e-04
0: TRAIN [4][3610/3880]	Time 0.662 (0.384)	Data 1.09e-04 (1.85e-04)	Tok/s 45440 (36673)	Loss/tok 3.3886 (3.0823)	LR 1.250e-04
0: TRAIN [4][3620/3880]	Time 0.401 (0.383)	Data 1.07e-04 (1.85e-04)	Tok/s 42127 (36673)	Loss/tok 3.0577 (3.0823)	LR 1.250e-04
0: TRAIN [4][3630/3880]	Time 0.636 (0.384)	Data 1.13e-04 (1.85e-04)	Tok/s 47039 (36682)	Loss/tok 3.3606 (3.0826)	LR 1.250e-04
0: TRAIN [4][3640/3880]	Time 0.772 (0.384)	Data 1.10e-04 (1.85e-04)	Tok/s 38585 (36678)	Loss/tok 3.3459 (3.0827)	LR 1.250e-04
0: TRAIN [4][3650/3880]	Time 0.308 (0.384)	Data 1.13e-04 (1.85e-04)	Tok/s 33943 (36671)	Loss/tok 2.9103 (3.0825)	LR 1.250e-04
0: TRAIN [4][3660/3880]	Time 0.363 (0.384)	Data 1.12e-04 (1.84e-04)	Tok/s 28983 (36673)	Loss/tok 2.8157 (3.0827)	LR 1.250e-04
0: TRAIN [4][3670/3880]	Time 0.205 (0.384)	Data 1.07e-04 (1.84e-04)	Tok/s 26401 (36671)	Loss/tok 2.5841 (3.0829)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3680/3880]	Time 0.306 (0.384)	Data 1.09e-04 (1.84e-04)	Tok/s 33546 (36672)	Loss/tok 2.9960 (3.0830)	LR 1.250e-04
0: TRAIN [4][3690/3880]	Time 0.550 (0.384)	Data 1.06e-04 (1.84e-04)	Tok/s 42632 (36678)	Loss/tok 3.0832 (3.0832)	LR 1.250e-04
0: TRAIN [4][3700/3880]	Time 0.683 (0.384)	Data 1.11e-04 (1.84e-04)	Tok/s 43863 (36678)	Loss/tok 3.4054 (3.0831)	LR 1.250e-04
0: TRAIN [4][3710/3880]	Time 0.316 (0.384)	Data 1.08e-04 (1.83e-04)	Tok/s 32768 (36673)	Loss/tok 2.8856 (3.0828)	LR 1.250e-04
0: TRAIN [4][3720/3880]	Time 0.530 (0.384)	Data 1.08e-04 (1.83e-04)	Tok/s 44461 (36672)	Loss/tok 3.2273 (3.0828)	LR 1.250e-04
0: TRAIN [4][3730/3880]	Time 0.301 (0.384)	Data 1.11e-04 (1.83e-04)	Tok/s 34152 (36676)	Loss/tok 2.8442 (3.0827)	LR 1.250e-04
0: TRAIN [4][3740/3880]	Time 0.344 (0.384)	Data 1.12e-04 (1.83e-04)	Tok/s 29774 (36671)	Loss/tok 3.0043 (3.0826)	LR 1.250e-04
0: TRAIN [4][3750/3880]	Time 0.303 (0.384)	Data 1.06e-04 (1.83e-04)	Tok/s 34324 (36661)	Loss/tok 2.9289 (3.0823)	LR 1.250e-04
0: TRAIN [4][3760/3880]	Time 0.411 (0.384)	Data 1.16e-04 (1.82e-04)	Tok/s 40917 (36664)	Loss/tok 3.0564 (3.0823)	LR 1.250e-04
0: TRAIN [4][3770/3880]	Time 0.490 (0.384)	Data 1.13e-04 (1.82e-04)	Tok/s 34400 (36674)	Loss/tok 3.1692 (3.0828)	LR 1.250e-04
0: TRAIN [4][3780/3880]	Time 0.681 (0.384)	Data 1.09e-04 (1.82e-04)	Tok/s 43635 (36665)	Loss/tok 3.3385 (3.0827)	LR 1.250e-04
0: TRAIN [4][3790/3880]	Time 0.207 (0.384)	Data 1.07e-04 (1.82e-04)	Tok/s 26177 (36669)	Loss/tok 2.5307 (3.0827)	LR 1.250e-04
0: TRAIN [4][3800/3880]	Time 0.302 (0.384)	Data 1.07e-04 (1.82e-04)	Tok/s 34159 (36662)	Loss/tok 2.8990 (3.0825)	LR 1.250e-04
0: TRAIN [4][3810/3880]	Time 0.391 (0.384)	Data 1.16e-04 (1.82e-04)	Tok/s 43313 (36663)	Loss/tok 3.1361 (3.0827)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3820/3880]	Time 0.314 (0.384)	Data 1.10e-04 (1.81e-04)	Tok/s 32506 (36669)	Loss/tok 2.8783 (3.0832)	LR 1.250e-04
0: TRAIN [4][3830/3880]	Time 0.214 (0.384)	Data 1.08e-04 (1.81e-04)	Tok/s 24916 (36671)	Loss/tok 2.5453 (3.0832)	LR 1.250e-04
0: TRAIN [4][3840/3880]	Time 0.408 (0.384)	Data 1.10e-04 (1.81e-04)	Tok/s 41441 (36664)	Loss/tok 3.0849 (3.0830)	LR 1.250e-04
0: TRAIN [4][3850/3880]	Time 0.306 (0.384)	Data 1.10e-04 (1.81e-04)	Tok/s 34367 (36671)	Loss/tok 2.9254 (3.0830)	LR 1.250e-04
0: TRAIN [4][3860/3880]	Time 0.413 (0.384)	Data 1.12e-04 (1.81e-04)	Tok/s 40522 (36674)	Loss/tok 3.0703 (3.0829)	LR 1.250e-04
0: TRAIN [4][3870/3880]	Time 0.209 (0.384)	Data 1.08e-04 (1.80e-04)	Tok/s 25236 (36677)	Loss/tok 2.4815 (3.0831)	LR 1.250e-04
:::MLL 1572992497.939 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 524}}
:::MLL 1572992497.940 eval_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [4][0/6]	Time 0.800 (0.800)	Decoder iters 100.0 (100.0)	Tok/s 20554 (20554)
0: Running moses detokenizer
0: BLEU(score=24.02799271910073, counts=[37084, 18568, 10592, 6292], totals=[65509, 62506, 59503, 56505], precisions=[56.60901555511457, 29.70594822897002, 17.800783153790565, 11.135297761260066], bp=1.0, sys_len=65509, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLL 1572992501.108 eval_accuracy: {"value": 24.03, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 535}}
:::MLL 1572992501.108 eval_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 4	Training Loss: 3.0843	Test BLEU: 24.03
0: Performance: Epoch: 4	Training: 146689 Tok/s
0: Finished epoch 4
:::MLL 1572992501.109 block_stop: {"value": null, "metadata": {"first_epoch_num": 5, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1572992501.109 run_stop: {"value": null, "metadata": {"status": "success", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2019-11-05 10:21:45 PM
RESULT,RNN_TRANSLATOR,,7450,nvidia,2019-11-05 08:17:35 PM
